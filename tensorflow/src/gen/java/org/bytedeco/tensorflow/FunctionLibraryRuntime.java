// Targeted by JavaCPP version 1.5-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.tensorflow;

import org.bytedeco.tensorflow.Allocator;
import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.tensorflow.global.tensorflow.*;

// Forward declare. Defined in common_runtime/device_mgr.h

@Namespace("tensorflow") @Properties(inherit = org.bytedeco.tensorflow.presets.tensorflow.class)
public class FunctionLibraryRuntime extends Pointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public FunctionLibraryRuntime(Pointer p) { super(p); }


  // Instantiate a function with the given "attrs".
  //
  // Returns OK and fills in "handle" if the instantiation succeeds.
  // Otherwise returns an error and "handle" is undefined.
  public static class InstantiateOptions extends Pointer {
      static { Loader.load(); }
      /** Default native constructor. */
      public InstantiateOptions() { super((Pointer)null); allocate(); }
      /** Native array allocator. Access with {@link Pointer#position(long)}. */
      public InstantiateOptions(long size) { super((Pointer)null); allocateArray(size); }
      /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
      public InstantiateOptions(Pointer p) { super(p); }
      private native void allocate();
      private native void allocateArray(long size);
      @Override public InstantiateOptions position(long position) {
          return (InstantiateOptions)super.position(position);
      }
  
    // The canonical device name of the device on which the function
    // should be instantiated. If empty, the function will be
    // instantiated on the local device.
    public native @StdString BytePointer target(); public native InstantiateOptions target(BytePointer target);

    // This interface is EXPERIMENTAL and subject to change.
    //
    // If non-null, the runtime will use `overlay_lib` to resolve
    // function(s) named in `function_name` and `attrs`. Otherwise,
    // the runtime will use its internal library.
    // NOTE(mrry): If provided, all functions defined in `overlay_lib`
    // must be self-contained, and cannot refer to functions defined
    // in other libraries.
    // TODO(mrry): Provide a mechanism for sharing core functions
    // between a set of libraries (e.g. by allowing a
    // `FunctionLibraryDefinition` to store an `outer_scope` pointer
    // and implementing name resolution across libraries).
    @MemberGetter public native @Const FunctionLibraryDefinition overlay_lib();

    // This interface is EXPERIMENTAL and subject to change.
    //
    // If non-empty, the runtime will use `state_handle` to identify
    // cached state related the instantiated function. Two functions
    // of the same name and attrs, instantiated with the same
    // `state_handle` will have the same handle and share the same
    // state (in stateful kernels); and two functions with different
    // values for `state_handle` will have independent state.
    public native @StdString BytePointer state_handle(); public native InstantiateOptions state_handle(BytePointer state_handle);

    // This interface is EXPERIMENTAL and subject to change.
    //
    // Instantiates the function using an executor of the given type. If empty,
    // the default TensorFlow executor will be used.
    public native @StdString BytePointer executor_type(); public native InstantiateOptions executor_type(BytePointer executor_type);

    // If true, the runtime will attempt to create kernels for the function at
    // instantiation time, rather than on the first run. This can be used to
    // surface errors earlier.
    public native @Cast("bool") boolean create_kernels_eagerly(); public native InstantiateOptions create_kernels_eagerly(boolean create_kernels_eagerly);
  }
  public native @ByVal Status Instantiate(@StdString BytePointer function_name, @ByVal AttrSlice attrs,
                               @Const @ByRef InstantiateOptions options,
                               @Cast("tensorflow::FunctionLibraryRuntime::Handle*") LongPointer handle);
  public native @ByVal Status Instantiate(@StdString String function_name, @ByVal AttrSlice attrs,
                               @Const @ByRef InstantiateOptions options,
                               @Cast("tensorflow::FunctionLibraryRuntime::Handle*") LongBuffer handle);
  public native @ByVal Status Instantiate(@StdString BytePointer function_name, @ByVal AttrSlice attrs,
                               @Const @ByRef InstantiateOptions options,
                               @Cast("tensorflow::FunctionLibraryRuntime::Handle*") long... handle);
  public native @ByVal Status Instantiate(@StdString String function_name, @ByVal AttrSlice attrs,
                               @Const @ByRef InstantiateOptions options,
                               @Cast("tensorflow::FunctionLibraryRuntime::Handle*") LongPointer handle);
  public native @ByVal Status Instantiate(@StdString BytePointer function_name, @ByVal AttrSlice attrs,
                               @Const @ByRef InstantiateOptions options,
                               @Cast("tensorflow::FunctionLibraryRuntime::Handle*") LongBuffer handle);
  public native @ByVal Status Instantiate(@StdString String function_name, @ByVal AttrSlice attrs,
                               @Const @ByRef InstantiateOptions options,
                               @Cast("tensorflow::FunctionLibraryRuntime::Handle*") long... handle);
  public native @ByVal Status Instantiate(@StdString BytePointer function_name, @ByVal AttrSlice attrs,
                       @Cast("tensorflow::FunctionLibraryRuntime::Handle*") LongPointer handle);
  public native @ByVal Status Instantiate(@StdString String function_name, @ByVal AttrSlice attrs,
                       @Cast("tensorflow::FunctionLibraryRuntime::Handle*") LongBuffer handle);
  public native @ByVal Status Instantiate(@StdString BytePointer function_name, @ByVal AttrSlice attrs,
                       @Cast("tensorflow::FunctionLibraryRuntime::Handle*") long... handle);
  public native @ByVal Status Instantiate(@StdString String function_name, @ByVal AttrSlice attrs,
                       @Cast("tensorflow::FunctionLibraryRuntime::Handle*") LongPointer handle);
  public native @ByVal Status Instantiate(@StdString BytePointer function_name, @ByVal AttrSlice attrs,
                       @Cast("tensorflow::FunctionLibraryRuntime::Handle*") LongBuffer handle);
  public native @ByVal Status Instantiate(@StdString String function_name, @ByVal AttrSlice attrs,
                       @Cast("tensorflow::FunctionLibraryRuntime::Handle*") long... handle);

  // Releases state associated with the handle.
  public native @ByVal Status ReleaseHandle(@Cast("tensorflow::FunctionLibraryRuntime::Handle") long handle);

  // Returns the function body for the instantiated function given its
  // handle 'h'. Returns nullptr if "h" is not found.
  //
  // *this keeps the ownership of the returned object, which remains alive
  // as long as *this.
  public native @Const FunctionBody GetFunctionBody(@Cast("tensorflow::FunctionLibraryRuntime::Handle") long h);

  // Asynchronously invokes the instantiated function identified by
  // "handle".
  //
  // If function execution succeeds, "done" is called with OK and
  // "*rets" is filled with the function's return values. Otheriwse,
  // "done" is called with an error status.
  //
  // Does not take ownership of "rets".
  // In the cross-process scenario, runner isn't used for making the Async
  // RPC calls.
  public static class Options extends Pointer {
      static { Loader.load(); }
      /** Default native constructor. */
      public Options() { super((Pointer)null); allocate(); }
      /** Native array allocator. Access with {@link Pointer#position(long)}. */
      public Options(long size) { super((Pointer)null); allocateArray(size); }
      /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
      public Options(Pointer p) { super(p); }
      private native void allocate();
      private native void allocateArray(long size);
      @Override public Options position(long position) {
          return (Options)super.position(position);
      }
  
    // The id of the step that is calling this function.
    public native @Cast("tensorflow::int64") long step_id(); public native Options step_id(long step_id);
    public native Rendezvous rendezvous(); public native Options rendezvous(Rendezvous rendezvous);
    public native CancellationManager cancellation_manager(); public native Options cancellation_manager(CancellationManager cancellation_manager);
    public native CollectiveExecutor collective_executor(); public native Options collective_executor(CollectiveExecutor collective_executor);
    public native ScopedStepContainer step_container(); public native Options step_container(ScopedStepContainer step_container);
    public native StepStatsCollectorInterface stats_collector(); public native Options stats_collector(StepStatsCollectorInterface stats_collector);

    public native @Cast("std::function<void(std::function<void()>)>*") Pointer runner(); public native Options runner(Pointer runner);

    // Parameters for remote function execution.
    public native @Cast("bool") boolean remote_execution(); public native Options remote_execution(boolean remote_execution);
    public native @StdString BytePointer source_device(); public native Options source_device(BytePointer source_device);  // Fully specified device name.

    // Allocator attributes specifying where the args are / rets should be put.
    // These should either be {} or match the length of args / retvals. If {},
    // the default allocator attributes will be assumed for all args / retvals.
    public native @StdVector AllocatorAttributes args_alloc_attrs(); public native Options args_alloc_attrs(AllocatorAttributes args_alloc_attrs);
    public native @StdVector AllocatorAttributes rets_alloc_attrs(); public native Options rets_alloc_attrs(AllocatorAttributes rets_alloc_attrs);

    // If true, we create a new IntraProcessRendezvous, else use the existing
    // one.
    public native @Cast("bool") boolean create_rendezvous(); public native Options create_rendezvous(boolean create_rendezvous);

    // If True, allow returning dead tensors.
    public native @Cast("bool") boolean allow_dead_tensors(); public native Options allow_dead_tensors(boolean allow_dead_tensors);
  }
  public native void Run(@Const @ByRef Options opts, @Cast("tensorflow::FunctionLibraryRuntime::Handle") long handle,
                     @ByVal TensorVector args, TensorVector rets,
                     @ByVal @Cast("tensorflow::FunctionLibraryRuntime::DoneCallback*") Pointer done);
  public native void Run(@Const @ByRef Options opts, @Cast("tensorflow::FunctionLibraryRuntime::Handle") long handle,
                     CallFrameInterface call_frame, @ByVal @Cast("tensorflow::FunctionLibraryRuntime::DoneCallback*") Pointer done);

  // Creates a "kernel" for the given node def "ndef".
  //
  // If succeeds, returns OK and the caller takes the ownership of the
  // returned "*kernel". Otherwise, returns an error.
  public native @ByVal Status CreateKernel(@Const @ByRef NodeDef ndef, @Cast("tensorflow::OpKernel**") PointerPointer kernel);
  public native @ByVal Status CreateKernel(@Const @ByRef NodeDef ndef, @ByPtrPtr OpKernel kernel);

  // Returns true iff the function named `function_name` is stateful.
  // NOTE(mrry): This method assumes that the runtime is associated with a
  // default function library, and looks up `function_name` in that library.
  // It does not support overlay libraries.
  public native @Cast("bool") boolean IsStateful(@StdString BytePointer function_name);
  public native @Cast("bool") boolean IsStateful(@StdString String function_name);

  // Returns the device on which the function executes.
  public native Device device();

  // Get the DeviceMgr from which the device was obtained.
  public native @Const DeviceMgr device_mgr();

  // Returns the function library definition that backs this runtime.
  // NOTE(mrry): The returned library definition is the default function library
  // for this runtime. The runtime may instantiate functions from separate
  // overlay libraries, which are not returned by this function.
  public native @Const FunctionLibraryDefinition GetFunctionLibraryDefinition();

  // Returns the environment on which the function executes.
  public native Env env();

  // Returns a debug string showing the definition of the function of
  // 'handle'.
  public native @StdString BytePointer DebugString(@Cast("tensorflow::FunctionLibraryRuntime::Handle") long handle);

  // Returns the graph version number.
  public native int graph_def_version();

  public native @ByVal Status Clone(@UniquePtr FunctionLibraryDefinition out_lib_def,
                         @UniquePtr ProcessFunctionLibraryRuntime out_pflr,
                         @Cast("tensorflow::FunctionLibraryRuntime**") PointerPointer out_flr);
  public native @ByVal Status Clone(@UniquePtr FunctionLibraryDefinition out_lib_def,
                         @UniquePtr ProcessFunctionLibraryRuntime out_pflr,
                         @ByPtrPtr FunctionLibraryRuntime out_flr);

  // Returns the name of the executor class (in the sense of
  // `ExecutorFactory::GetFactory()`) that will be used based on the given
  // dynamic `options` and static `attrs`. If none is specified, this method
  // will return an empty string, which leaves the decision up to the runtime.
  public static native @StdString BytePointer ExecutorType(@Const @ByRef InstantiateOptions options,
                               @ByVal AttrSlice attrs);
}

JavaCPP Presets for Tritonserver
============================

[![Gitter](https://badges.gitter.im/bytedeco/javacpp.svg)](https://gitter.im/bytedeco/javacpp) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.bytedeco/tritonserver/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.bytedeco/tritonserver) [![Sonatype Nexus (Snapshots)](https://img.shields.io/nexus/s/https/oss.sonatype.org/org.bytedeco/tritonserver.svg)](http://bytedeco.org/builds/)  
<sup>Build status for all platforms:</sup> [![tritonserver](https://github.com/bytedeco/javacpp-presets/workflows/tritonserver/badge.svg)](https://github.com/bytedeco/javacpp-presets/actions?query=workflow%3Atritonserver)  <sup>Commercial support:</sup> [![xscode](https://img.shields.io/badge/Available%20on-xs%3Acode-blue?style=?style=plastic&logo=appveyor&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAMAAACdt4HsAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRF////////VXz1bAAAAAJ0Uk5T/wDltzBKAAAAlUlEQVR42uzXSwqAMAwE0Mn9L+3Ggtgkk35QwcnSJo9S+yGwM9DCooCbgn4YrJ4CIPUcQF7/XSBbx2TEz4sAZ2q1RAECBAiYBlCtvwN+KiYAlG7UDGj59MViT9hOwEqAhYCtAsUZvL6I6W8c2wcbd+LIWSCHSTeSAAECngN4xxIDSK9f4B9t377Wd7H5Nt7/Xz8eAgwAvesLRjYYPuUAAAAASUVORK5CYII=)](https://xscode.com/bytedeco/javacpp-presets)


License Agreements
------------------
By downloading these archives, you agree to the terms of the license agreements for NVIDIA software included in the archives.

### Tritonserver
To view the license for Tritonserver included in these archives, click [here](https://github.com/triton-inference-server/server)

 * Tritonserver is widely used software package for inference service
 * Triton supports almost all kinds of model generated by different DL frameworks or tools, such as TensorFLow, PyTorch, ONNX, TensorRT, OpenVINO...
 * Triton supports both CPU and GPU
 * Triton can be used both as an application and as a shared library. In case you already have your own inference service framework but want to add more features, just try Triton as a shared library.
 * Triton supports Java as a shared library through JavaAPP Presets

Introduction
------------
This directory contains the JavaCPP Presets module for:

 * Tritonserver 2.14  https://github.com/triton-inference-server/server

Please refer to the parent README.md file for more detailed information about the JavaCPP Presets.


Documentation
-------------
Java API documentation is available here:

 * http://bytedeco.org/javacpp-presets/tritonserver/apidocs/


Sample Usage
------------
Here is a example of Tritonserver ported to Java from the `simple.cc` sample file available at:

 * https://github.com/triton-inference-server/server/tree/main/src/servers

This sample intends to show how to call JAVA based Triton API to execute inference requests.
We can use [Maven 3](http://maven.apache.org/) to download and install automatically all the class files as well as the native binaries. To run this sample code, simply execute on the command line:
```bash
 $ mvn exec:java -Djavacpp.platform=linux-x86_64 -Dexec.args="-r /workspace/tritonserver_21.09_source/server-2.14.0/docs/examples/model_repository/models"
```

### Steps to run this sample

**1, Get the source code of Tritonserver to set the model repository up:**
```bash
 $ wget https://github.com/triton-inference-server/server/archive/refs/tags/v2.14.0.tar.gz
 $ tar zxvf v2.14.0.tar.gz
 $ cd server-2.14.0/docs/examples/model_repository
 $ mkdir models
 $ cd models; cp -a ../simple .
```
Now, this models directory will be our model repository.

**2, Start the Docker container to run the sample (Note: 1, now we are under the "model_repository" dir as above):**
```bash
 $ docker run -it --gpus=all -v $(pwd):/workspace nvcr.io/nvidia/tritonserver:21.09-py3 bash
 $ git clone https://github.com/bytedeco/javacpp-presets.git
 $ cd javacpp-presets/tritonserver/samples
 $ mvn exec:java -Djavacpp.platform=linux-x86_64 -Dexec.args="-r /workspace/tritonserver_21.09_source/server-2.14.0/docs/examples/model_repository/models"
```

This sample is the JAVA implementation of [C API](https://github.com/triton-inference-server/server/blob/main/docs/inference_protocols.md#c-api) 



// Targeted by JavaCPP version 1.5.11-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.tensorrt.nvinfer;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.javacpp.presets.javacpp.*;
import org.bytedeco.cuda.cudart.*;
import static org.bytedeco.cuda.global.cudart.*;
import org.bytedeco.cuda.cublas.*;
import static org.bytedeco.cuda.global.cublas.*;
import org.bytedeco.cuda.cudnn.*;
import static org.bytedeco.cuda.global.cudnn.*;
import org.bytedeco.cuda.nvrtc.*;
import static org.bytedeco.cuda.global.nvrtc.*;

import static org.bytedeco.tensorrt.global.nvinfer.*;


/**
 *  \class IDequantizeLayer
 * 
 *  \brief A Dequantize layer in a network definition.
 * 
 *  This layer accepts a quantized type input tensor, and uses the configured scale and zeroPt inputs to
 *  dequantize the input according to:
 *  \p output = (\p input - \p zeroPt) * \p scale
 * 
 *  The first input (index 0) is the tensor to be quantized.
 *  The second (index 1) and third (index 2) are the scale and zero point respectively.
 *  \p scale and \p zeroPt should have identical dimensions, and rank lower or equal to 2.
 * 
 *  The \p zeroPt tensor is optional, and if not set, will be assumed to be zero. Its data type must be identical to
 *  the input's data type. \p zeroPt must only contain zero-valued coefficients, because only symmetric quantization is
 *  supported.
 *  The \p scale value must be either a scalar for per-tensor quantization, a 1-D tensor for per-channel quantization,
 *  or a 2-D tensor for block quantization (supported for DataType::kINT4 only). All \p scale coefficients must have
 *  positive values. The size of the 1-D \p scale tensor must match the size of the quantization axis. For block
 *  quantization, the shape of \p scale tensor must match the shape of the input, except for one dimension in which
 *  blocking occurs. The size of \p zeroPt must match the size of \p scale.
 * 
 *  The subgraph which terminates with the \p scale tensor must be a build-time constant.  The same restrictions apply
 *  to the \p zeroPt.
 *  The output type, if constrained, must be constrained to DataType::kFLOAT, DataType::kHALF, or DataType::kBF16. The
 *  input type, if constrained, must be constrained to DataType::kINT8, DataType::kFP8 or DataType::kINT4. The output
 *  size is the same as the input size. The quantization axis is in reference to the input tensor's dimensions.
 * 
 *  IDequantizeLayer supports DataType::kINT8, DataType::kFP8 or DataType::kINT4 precision and will default to
 *  DataType::kINT8 precision during instantiation. For strongly typed networks, \p input data type must be same as
 *  \p zeroPt data type.
 * 
 *  IDequantizeLayer supports DataType::kFLOAT, DataType::kHALF, or DataType::kBF16 output. For strongly typed
 *  networks, \p output data type is inferred from \p scale data type.
 * 
 *  As an example of the operation of this layer, imagine a 4D NCHW activation input which can be quantized using a
 *  single scale coefficient (referred to as per-tensor quantization):
 *      For each n in N:
 *          For each c in C:
 *              For each h in H:
 *                  For each w in W:
 *                      output[n,c,h,w] = (\p input[n,c,h,w] - \p zeroPt) * \p scale
 * 
 *  Per-channel dequantization is supported only for input that is rooted at an IConstantLayer (i.e. weights).
 *  Activations cannot be quantized per-channel. As an example of per-channel operation, imagine a 4D KCRS weights input
 *  and K (dimension 0) as the quantization axis. The scale is an array of coefficients, which is the same size as the
 *  quantization axis.
 *      For each k in K:
 *          For each c in C:
 *              For each r in R:
 *                  For each s in S:
 *                      output[k,c,r,s] = (\p input[k,c,r,s] - \p zeroPt[k]) * \p scale[k]
 * 
 *  Block dequantization is supported only for 2-D input tensors with DataType::kINT4 that are rooted at an
 *  IConstantLayer (i.e. weights). As an example of blocked operation, imagine a 2-D RS weights input with R
 *  (dimension 0) as the blocking axis and B as the block size. The scale is a 2-D array of coefficients, with
 *  dimensions (R//B, S).
 *  For each r in R:
 *      For each s in S:
 *          output[r,s] = (\p input[r,s] - \p zeroPt[r//B, s]) * \p scale[r//B, s]
 * 
 *  \note Only symmetric quantization is supported.
 *  \note Currently the only allowed build-time constant \p scale and \p zeroPt subgraphs are:
 *  1. Constant -> Quantize
 *  2. Constant -> Cast -> Quantize
 * 
 *  \note The input tensor for this layer must not be a scalar.
 * 
 *  \warning Do not inherit from this class, as doing so will break forward-compatibility of the API and ABI.
 *  */
@Namespace("nvinfer1") @NoOffset @Properties(inherit = org.bytedeco.tensorrt.presets.nvinfer.class)
public class IDequantizeLayer extends ILayer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public IDequantizeLayer(Pointer p) { super(p); }

    /**
     *  \brief Get the quantization axis.
     * 
     *  @return axis parameter set by setAxis().
     *  The return value is the index of the quantization axis in the input tensor's dimensions.
     *  A value of -1 indicates per-tensor quantization.
     *  The default value is -1.
     *  */
    
    //!
    //!
    //!
    public native @NoException(true) int getAxis();
    /**
     *  \brief Set the quantization axis.
     * 
     *  Set the index of the quantization axis (with reference to the input tensor's dimensions).
     *  The axis must be a valid axis if the scale tensor has more than one coefficient.
     *  The axis value will be ignored if the scale tensor has exactly one coefficient (per-tensor quantization).
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    public native @NoException(true) void setAxis(int axis);

    /**
     *  \brief Set the Dequantize layer output type.
     * 
     *  @param toType The DataType of the output tensor.
     * 
     *  Set the output type of the dequantize layer. Valid values are DataType::kFLOAT and DataType::kHALF.
     *  If the network is strongly typed, setToType must be used to set the output type, and use of setOutputType
     *  is an error. Otherwise, types passed to setOutputType and setToType must be the same.
     * 
     *  @see NetworkDefinitionCreationFlag::kSTRONGLY_TYPED
     *  */
    
    
    //!
    //!
    //!
    public native @NoException(true) void setToType(DataType toType);
    public native @NoException(true) void setToType(@Cast("nvinfer1::DataType") int toType);

    /**
     *  \brief Return the Dequantize layer output type.
     * 
     *  @return toType parameter set during layer creation or by setToType().
     *  The return value is the output type of the quantize layer.
     *  The default value is DataType::kFLOAT.
     *  */
    public native @NoException(true) DataType getToType();
}

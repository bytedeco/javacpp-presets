// Targeted by JavaCPP version 1.5.13-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.tensorrt.nvinfer;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.javacpp.presets.javacpp.*;
import org.bytedeco.cuda.cudart.*;
import static org.bytedeco.cuda.global.cudart.*;
import org.bytedeco.cuda.cublas.*;
import static org.bytedeco.cuda.global.cublas.*;
import org.bytedeco.cuda.cudnn.*;
import static org.bytedeco.cuda.global.cudnn.*;
import org.bytedeco.cuda.nvrtc.*;
import static org.bytedeco.cuda.global.nvrtc.*;

import static org.bytedeco.tensorrt.global.nvinfer.*;


/**
 *  \class IExecutionContext
 * 
 *  \brief Context for executing inference using an engine, with functionally unsafe features.
 * 
 *  Multiple execution contexts may exist for one ICudaEngine instance, allowing the same
 *  engine to be used for the execution of multiple batches simultaneously. If the engine supports
 *  dynamic shapes, each execution context in concurrent use must use a separate optimization profile.
 * 
 *  \warning Do not inherit from this class, as doing so will break forward-compatibility of the API and ABI. */
@Namespace("nvinfer1") @NoOffset @Properties(inherit = org.bytedeco.tensorrt.presets.nvinfer.class)
public class IExecutionContext extends INoCopy {
    static { Loader.load(); }
    /** Default native constructor. */
    public IExecutionContext() { super((Pointer)null); allocate(); }
    /** Native array allocator. Access with {@link Pointer#position(long)}. */
    public IExecutionContext(long size) { super((Pointer)null); allocateArray(size); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public IExecutionContext(Pointer p) { super(p); }
    private native void allocate();
    private native void allocateArray(long size);
    @Override public IExecutionContext position(long position) {
        return (IExecutionContext)super.position(position);
    }
    @Override public IExecutionContext getPointer(long i) {
        return new IExecutionContext((Pointer)this).offsetAddress(i);
    }


    /**
     *  \brief Set the debug sync flag.
     * 
     *  If this flag is set to true, the engine will log the successful execution for each kernel during executeV2(). It
     *  has no effect when using enqueueV3().
     * 
     *  @see getDebugSync()
     *  */
    
    
    //!
    //!
    //!
    public native @NoException(true) void setDebugSync(@Cast("bool") boolean sync);

    /**
     *  \brief Get the debug sync flag.
     * 
     *  @see setDebugSync()
     *  */
    
    
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean getDebugSync();

    /**
     *  \brief Set the profiler.
     * 
     *  @see IProfiler getProfiler()
     *  */
    
    
    //!
    //!
    //!
    public native @NoException(true) void setProfiler(IProfiler profiler);

    /**
     *  \brief Get the profiler.
     * 
     *  @see IProfiler setProfiler()
     *  */
    
    
    //!
    //!
    //!
    public native @NoException(true) IProfiler getProfiler();

    /**
     *  \brief Get the associated engine.
     * 
     *  @see ICudaEngine
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    public native @Const @ByRef @NoException(true) ICudaEngine getEngine();

    /**
     *  \brief Set the name of the execution context.
     * 
     *  This method copies the name string.
     * 
     *  \warning The string name must be null-terminated, and be at most 4096 bytes including the terminator.
     * 
     *  @see getName()
     *  */
    
    
    //!
    //!
    //!
    public native @NoException(true) void setName(String name);
    public native @NoException(true) void setName(@Cast("const char*") BytePointer name);

    /**
     *  \brief Return the name of the execution context.
     * 
     *  @see setName()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @NoException(true) String getName();

    /**
     *  \brief Set the device memory for use by this execution context.
     * 
     *  The memory must be aligned with CUDA memory alignment property (using cudaGetDeviceProperties()), and its size
     *  must be large enough for performing inference with the given network inputs. getDeviceMemorySize() and
     *  getDeviceMemorySizeForProfile() report upper bounds of the size. Setting memory to nullptr is acceptable if the
     *  reported size is 0. If using enqueueV3() to run the network, the memory is in use from the invocation of
     *  enqueueV3() until network execution is complete. If using executeV2(), it is in use until executeV2() returns.
     *  Releasing or otherwise using the memory for other purposes, including using it in another execution context
     *  running in parallel, during this time will result in undefined behavior.
     * 
     *  @deprecated Deprecated in TensorRT 10.1. Superseded by setDeviceMemoryV2().
     * 
     *  \warning Weight streaming related scratch memory will be allocated by TensorRT if the memory is set by this API.
     *           Please use setDeviceMemoryV2() instead.
     * 
     *  @see ICudaEngine::getDeviceMemorySize()
     *  @see ICudaEngine::getDeviceMemorySizeForProfile()
     *  @see ExecutionContextAllocationStrategy
     *  @see ICudaEngine::createExecutionContext()
     *  @see ICudaEngine::createExecutionContextWithoutDeviceMemory()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @NoException(true) void setDeviceMemory(Pointer memory);

    /**
     *  \brief Set the device memory and its corresponding size for use by this execution context.
     * 
     *  The memory must be aligned with CUDA memory alignment property (using cudaGetDeviceProperties()), and its size
     *  must be large enough for performing inference with the given network inputs. getDeviceMemorySize() and
     *  getDeviceMemorySizeForProfile() report upper bounds of the size. Setting memory to nullptr is acceptable if the
     *  reported size is 0. If using enqueueV3() to run the network, the memory is in use from the invocation of
     *  enqueueV3() until network execution is complete. If using executeV2(), it is in use until executeV2() returns.
     *  Releasing or otherwise using the memory for other purposes, including using it in another execution context
     *  running in parallel, during this time will result in undefined behavior.
     * 
     *  @see ICudaEngine::getDeviceMemorySizeV2()
     *  @see ICudaEngine::getDeviceMemorySizeForProfileV2()
     *  @see ExecutionContextAllocationStrategy
     *  @see ICudaEngine::createExecutionContext()
     *  @see ICudaEngine::createExecutionContextWithoutDeviceMemory()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native @NoException(true) void setDeviceMemoryV2(Pointer memory, @Cast("int64_t") long size);

    /**
     *  \brief Return the strides of the buffer for the given tensor name.
     * 
     *  The strides are in units of elements, not components or bytes.
     *  For example, for TensorFormat::kHWC8, a stride of one spans 8 scalars.
     * 
     *  Note that strides can be different for different execution contexts
     *  with dynamic shapes.
     * 
     *  If the provided name does not map to an input or output tensor, or there are dynamic dimensions that have not
     *  been set yet, return Dims{-1, {}}
     * 
     *  @param tensorName The name of an input or output tensor.
     * 
     *  \warning The string tensorName must be null-terminated, and be at most 4096 bytes including the terminator.
     *  */
    public native @ByVal @Cast("nvinfer1::Dims*") @NoException(true) Dims64 getTensorStrides(String tensorName);
    public native @ByVal @Cast("nvinfer1::Dims*") @NoException(true) Dims64 getTensorStrides(@Cast("const char*") BytePointer tensorName);
    /**
     *  \brief Get the index of the currently selected optimization profile.
     * 
     *  If the profile index has not been set yet (implicitly to 0 if no other execution context has been set to
     *  profile 0, or explicitly for all subsequent contexts), an invalid value of -1 will be returned
     *  and all calls to enqueueV3()/executeV2() will fail until a valid profile index has been set.
     *  This behavior is deprecated in TensorRT 8.6, all profiles will default to optimization
     *  profile 0 and -1 will no longer be returned.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @NoException(true) int getOptimizationProfile();

    /**
     *  \brief Set shape of given input.
     * 
     *  @param tensorName The name of an input tensor.
     *  @param dims The shape of an input tensor.
     * 
     *  @return True on success, false if the provided name does not map to an input tensor, or if some other error
     *  occurred.
     * 
     *  Each dimension must agree with the network dimension unless the latter was -1.
     * 
     *  \warning The string tensorName must be null-terminated, and be at most 4096 bytes including the terminator.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean setInputShape(String tensorName, @Cast("const nvinfer1::Dims*") @ByRef Dims64 dims);
    public native @Cast("bool") @NoException(true) boolean setInputShape(@Cast("const char*") BytePointer tensorName, @Cast("const nvinfer1::Dims*") @ByRef Dims64 dims);

    /**
     *  \brief Return the shape of the given input or output.
     * 
     *  @param tensorName The name of an input or output tensor.
     * 
     *  Return Dims{-1, {}} if the provided name does not map to an input or output tensor.
     *  Otherwise return the shape of the input or output tensor.
     * 
     *  A dimension in an input tensor will have a -1 wildcard value if all the following are true:
     *   * setInputShape() has not yet been called for this tensor
     *   * The dimension is a runtime dimension that is not implicitly constrained to be a single value.
     * 
     *  A dimension in an output tensor will have a -1 wildcard value if the dimension depends
     *  on values of execution tensors OR if all the following are true:
     *   * It is a runtime dimension.
     *   * setInputShape() has NOT been called for some input tensor(s) with a runtime shape.
     *   * setTensorAddress() has NOT been called for some input tensor(s) with isShapeInferenceIO() = true.
     * 
     *  An output tensor may also have -1 wildcard dimensions if its shape depends on values of tensors supplied to
     *  enqueueV3().
     * 
     *  If the request is for the shape of an output tensor with runtime dimensions,
     *  all input tensors with isShapeInferenceIO() = true should have their value already set,
     *  since these values might be needed to compute the output shape.
     * 
     *  Examples of an input dimension that is implicitly constrained to a single value:
     *  * The optimization profile specifies equal min and max values.
     *  * The dimension is named and only one value meets the optimization profile requirements
     *    for dimensions with that name.
     * 
     *  \warning The string tensorName must be null-terminated, and be at most 4096 bytes including the terminator.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    public native @ByVal @Cast("nvinfer1::Dims*") @NoException(true) Dims64 getTensorShape(String tensorName);
    public native @ByVal @Cast("nvinfer1::Dims*") @NoException(true) Dims64 getTensorShape(@Cast("const char*") BytePointer tensorName);

    /**
     *  \brief Whether all dynamic dimensions of input tensors have been specified
     * 
     *  @return True if all dynamic dimensions of input tensors have been specified
     *          by calling setInputShape().
     * 
     *  Trivially true if network has no dynamically shaped input tensors.
     * 
     *  Does not work with name-base interfaces eg. IExecutionContext::setInputShape(). Use
     *  IExecutionContext::inferShapes() instead.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean allInputDimensionsSpecified();

    /**
     *  \brief Whether all input shape bindings have been specified
     * 
     *  @return True if all input shape bindings have been specified by setInputShapeBinding().
     * 
     *  Trivially true if network has no input shape bindings.
     * 
     *  Does not work with name-base interfaces eg. IExecutionContext::setInputShape(). Use
     *  IExecutionContext::inferShapes() instead.
     * 
     *  @deprecated Deprecated in TensorRT 10.0. setInputShapeBinding() is removed since TensorRT 10.0.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Cast("bool") @Deprecated @NoException(true) boolean allInputShapesSpecified();

    /**
     *  \brief Set the ErrorRecorder for this interface
     * 
     *  Assigns the ErrorRecorder to this interface. The ErrorRecorder will track all errors during execution.
     *  This function will call incRefCount of the registered ErrorRecorder at least once. Setting
     *  recorder to nullptr unregisters the recorder with the interface, resulting in a call to decRefCount if
     *  a recorder has been registered.
     * 
     *  If an error recorder is not set, messages will be sent to the global log stream.
     * 
     *  @param recorder The error recorder to register with this interface.
     * 
     *  @see getErrorRecorder()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    public native @NoException(true) void setErrorRecorder(IErrorRecorder recorder);

    /**
     *  \brief Get the ErrorRecorder assigned to this interface.
     * 
     *  Retrieves the assigned error recorder object for the given class. A nullptr will be returned if
     *  an error handler has not been set.
     * 
     *  @return A pointer to the IErrorRecorder object that has been registered.
     * 
     *  @see setErrorRecorder()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @NoException(true) IErrorRecorder getErrorRecorder();

    /**
     *  \brief Synchronously execute a network.
     * 
     *  This method requires an array of input and output buffers. The mapping
     *  from indices to tensor names can be queried using ICudaEngine::getIOTensorName().
     * 
     *  @param bindings An array of pointers to input and output buffers for the network.
     * 
     *  @return True if execution succeeded.
     * 
     *  @see ICudaEngine::getIOTensorName()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean executeV2(@Cast("void*const*") PointerPointer bindings);
    public native @Cast("bool") @NoException(true) boolean executeV2(@Cast("void*const*") @ByPtrPtr Pointer bindings);

    /**
     *  \brief Select an optimization profile for the current context with async
     *  semantics.
     * 
     *  @param profileIndex Index of the profile. The value must lie between 0 and
     *         getEngine().getNbOptimizationProfiles() - 1
     * 
     *  @param stream A CUDA stream on which the cudaMemcpyAsyncs may be
     *  enqueued
     * 
     *  When an optimization profile is switched via this API, TensorRT may
     *  require that data is copied via cudaMemcpyAsync. It is the
     *  applicationâ€™s responsibility to guarantee that synchronization between
     *  the profile sync stream and the enqueue stream occurs.
     * 
     *  The selected profile will be used in subsequent calls to executeV2()/enqueueV3().
     *  If the associated CUDA engine has inputs with dynamic shapes, the optimization profile must
     *  be set with its corresponding profileIndex before calling execute or enqueue. The newly created execution
     *  context will be assigned optimization profile 0.
     * 
     *  If the associated CUDA engine does not have inputs with dynamic shapes,
     *  this method need not be called, in which case the default profile index
     *  of 0 will be used.
     * 
     *  setOptimizationProfileAsync() must be called before calling
     *  setInputShape() for all dynamic input
     *  tensors or input shape tensors, which in turn must be called before
     *  executeV2()/enqueueV3().
     * 
     *  \warning This function will trigger layer resource updates on the next call of
     *           executeV2()/enqueueV3(), possibly resulting in performance bottlenecks.
     * 
     *  \warning Not synchronizing the stream used at enqueue with the stream
     *  used to set optimization profile asynchronously using this API will
     *  result in undefined behavior.
     * 
     *  @return true if the call succeeded, else false (e.g. input out of range)
     * 
     *  @see ICudaEngine::getNbOptimizationProfiles() */
    
    
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean setOptimizationProfileAsync(int profileIndex, CUstream_st stream);

    /**
     *  \brief Set whether enqueue emits layer timing to the profiler
     * 
     *  If set to true (default), enqueue is synchronous and does layer timing profiling implicitly if
     *  there is a profiler attached.
     *  If set to false, enqueue will be asynchronous if there is a profiler attached. An extra method
     *  reportToProfiler() needs to be called to obtain the profiling data and report to the profiler attached.
     * 
     *  @see IExecutionContext::getEnqueueEmitsProfile()
     *  @see IExecutionContext::reportToProfiler()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @NoException(true) void setEnqueueEmitsProfile(@Cast("bool") boolean enqueueEmitsProfile);

    /**
     *  \brief Get the enqueueEmitsProfile state.
     * 
     *  @return The enqueueEmitsProfile state.
     * 
     *  @see IExecutionContext::setEnqueueEmitsProfile()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean getEnqueueEmitsProfile();

    /**
     *  \brief Calculate layer timing info for the current optimization profile in IExecutionContext
     *  and update the profiler after one iteration of inference launch.
     * 
     *  If IExecutionContext::getEnqueueEmitsProfile() returns true, the enqueue function will calculate layer timing
     *  implicitly if a profiler is provided. This function returns true and does nothing.
     * 
     *  If IExecutionContext::getEnqueueEmitsProfile() returns false, the enqueue function will record the CUDA event
     *  timers if a profiler is provided. But it will not perform the layer timing calculation.
     *  IExecutionContext::reportToProfiler() needs to be called explicitly to calculate layer timing for the previous
     *  inference launch.
     * 
     *  In the CUDA graph launch scenario, it will record the same set of CUDA events
     *  as in regular enqueue functions if the graph is captured from an IExecutionContext with profiler enabled.
     *  This function needs to be called after graph launch to report the layer timing info to the profiler.
     * 
     *  \warning profiling CUDA graphs is only available from CUDA 11.1 onwards.
     *  \warning reportToProfiler uses the stream of the previous enqueue call, so the stream must be live otherwise
     *  behavior is undefined.
     * 
     *  @return true if the call succeeded, else false (e.g. profiler not provided, in CUDA graph capture mode, etc.)
     * 
     *  @see IExecutionContext::setEnqueueEmitsProfile()
     *  @see IExecutionContext::getEnqueueEmitsProfile()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean reportToProfiler();

    /**
     *  \brief Set memory address for given input or output tensor.
     * 
     *  @param tensorName The name of an input or output tensor.
     *  @param data The pointer (void*) to the data owned by the user.
     * 
     *  @return True on success, false if error occurred.
     * 
     *  An address defaults to nullptr.
     *  Pass data=nullptr to reset to the default state.
     * 
     *  Return false if the provided name does not map to an input or output tensor.
     * 
     *  If an input pointer has type (void const*), use setInputTensorAddress() instead.
     * 
     *  Before calling enqueueV3(), each input must have a non-null address and
     *  each output must have a non-null address or an IOutputAllocator to set it later.
     * 
     *  If the TensorLocation of the tensor is kHOST:
     *  - The pointer must point to a host buffer of sufficient size.
     *  - Data representing shape values is not copied until enqueueV3 is invoked.
     * 
     *  If the TensorLocation of the tensor is kDEVICE:
     *  - The pointer must point to a device buffer of sufficient size and alignment, or
     *  - Be nullptr if the tensor is an output tensor that will be allocated by IOutputAllocator.
     * 
     *  If getTensorShape(name) reports a -1 for any dimension of an output after all
     *  input shapes have been set, use setOutputAllocator() to associate an IOutputAllocator
     *  to which the dimensions will be reported when known.
     * 
     *  Calling both setTensorAddress and setOutputAllocator() for the same output is allowed,
     *  and can be useful for preallocating memory, and then reallocating if it's not big enough.
     * 
     *  The pointer must have at least 256-byte alignment.
     * 
     *  \warning The string tensorName must be null-terminated, and be at most 4096 bytes including the terminator.
     * 
     *  @see setInputTensorAddress() setOutputTensorAddress() getTensorShape() setOutputAllocator() IOutputAllocator
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean setTensorAddress(String tensorName, Pointer data);
    public native @Cast("bool") @NoException(true) boolean setTensorAddress(@Cast("const char*") BytePointer tensorName, Pointer data);

    /**
     *  \brief Get memory address bound to given input or output tensor, or nullptr if the provided name does not map to
     *  an input or output tensor.
     * 
     *  @param tensorName The name of an input or output tensor.
     * 
     *  Use method getOutputTensorAddress() if a non-const pointer for an output tensor is required.
     * 
     *  \warning The string tensorName must be null-terminated, and be at most 4096 bytes including the terminator.
     * 
     *  @see getOutputTensorAddress()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Const @NoException(true) Pointer getTensorAddress(String tensorName);
    public native @Const @NoException(true) Pointer getTensorAddress(@Cast("const char*") BytePointer tensorName);

    /**
     *  \brief Set the memory address for a given output tensor.
     * 
     *  @param tensorName The name of an output tensor.
     *  @param data The pointer to the buffer to which to write the output.
     * 
     *  @return True on success, false if the provided name does not map to an output tensor, does not meet alignment
     *  requirements, or some other error occurred.
     * 
     *  Output addresses can also be set using method setTensorAddress. This method is provided for applications which
     *  prefer to use different methods for setting input and output tensors.
     * 
     *  See setTensorAddress() for alignment and data type constraints.
     * 
     *  \warning The string tensorName must be null-terminated, and be at most 4096 bytes including the terminator.
     * 
     *  @see setTensorAddress()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean setOutputTensorAddress(String tensorName, Pointer data);
    public native @Cast("bool") @NoException(true) boolean setOutputTensorAddress(@Cast("const char*") BytePointer tensorName, Pointer data);

    /**
     *  \brief Set memory address for given input.
     * 
     *  @param tensorName The name of an input tensor.
     *  @param data The pointer (void const*) to the const data owned by the user.
     * 
     *  @return True on success, false if the provided name does not map to an input tensor, does not meet alignment
     *  requirements, or some other error occurred.
     * 
     *  Input addresses can also be set using method setTensorAddress, which requires a (void*).
     * 
     *  See description of method setTensorAddress() for alignment and data type constraints.
     * 
     *  \warning The string tensorName must be null-terminated, and be at most 4096 bytes including the terminator.
     * 
     *  @see setTensorAddress()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean setInputTensorAddress(String tensorName, @Const Pointer data);
    public native @Cast("bool") @NoException(true) boolean setInputTensorAddress(@Cast("const char*") BytePointer tensorName, @Const Pointer data);

    /**
     *  \brief Get memory address for given output.
     * 
     *  @param tensorName The name of an output tensor.
     * 
     *  @return Raw output data pointer (void*) for given output tensor, or nullptr if the provided name does not map to
     *  an output tensor.
     * 
     *  If only a (void const*) pointer is needed, an alternative is to call method getTensorAddress().
     * 
     *  \warning The string tensorName must be null-terminated, and be at most 4096 bytes including the terminator.
     * 
     *  @see getTensorAddress()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native @NoException(true) Pointer getOutputTensorAddress(String tensorName);
    public native @NoException(true) Pointer getOutputTensorAddress(@Cast("const char*") BytePointer tensorName);

    /**
     *  \brief Run shape calculations.
     * 
     *  @param nbMaxNames Maximum number of names to write to tensorNames.
     *         When the return value is a positive value n and tensorNames != nullptr,
     *         the names of min(n,nbMaxNames) insufficiently specified input tensors are
     *         written to tensorNames.
     * 
     *  @param tensorNames Buffer in which to place names of insufficiently specified input tensors.
     * 
     *  @return 0 on success.
     *          Positive value n if n input tensors were not sufficiently specified.
     *          -1 for other errors.
     * 
     *  An input tensor is insufficiently specified if either of the following is true:
     * 
     *  * It has dynamic dimensions and its runtime dimensions have not yet
     *    been specified via IExecutionContext::setInputShape.
     * 
     *  * isShapeInferenceIO(t)=true and the tensor's address has not yet been set.
     * 
     *  If an output tensor has isShapeInferenceIO(t)=true and its address has been specified,
     *  then its value is written.
     * 
     *  Returns -1 if tensorNames == nullptr and nbMaxNames != 0.
     *  Returns -1 if nbMaxNames < 0.
     *  Returns -1 if a tensor's dimensions are invalid, e.g. a tensor ends up with a negative dimension.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    public native @NoException(true) int inferShapes(int nbMaxNames, @Cast("const char**") PointerPointer tensorNames);
    public native @NoException(true) int inferShapes(int nbMaxNames, @Cast("const char**") @ByPtrPtr BytePointer tensorNames);
    public native @NoException(true) int inferShapes(int nbMaxNames, @Cast("const char**") @ByPtrPtr ByteBuffer tensorNames);
    public native @NoException(true) int inferShapes(int nbMaxNames, @Cast("const char**") @ByPtrPtr byte[] tensorNames);

    /**
     *  \brief Recompute the internal activation buffer sizes based on the current input shapes, and return the total
     *  amount of memory required.
     * 
     *  Users can allocate the device memory based on the size returned and provided the memory to TRT with
     *  IExecutionContext::setDeviceMemory(). Must specify all input shapes and the optimization profile to use before
     *  calling this function, otherwise the partition will be invalidated.
     * 
     *  @return Total amount of memory required on success, 0 if error occurred.
     * 
     *  @see IExecutionContext::setDeviceMemory()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Cast("size_t") @NoException(true) long updateDeviceMemorySizeForShapes();

    /**
     *  \brief Mark input as consumed.
     * 
     *  @param event The CUDA event that is triggered after all input tensors have been consumed.
     * 
     *  \warning The set event must be valid during the inference.
     * 
     *  @return True on success, false if error occurred.
     * 
     *  Passing event==nullptr removes whatever event was set, if any.
     *  */
    
    
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean setInputConsumedEvent(CUevent_st event);

    /**
     *  \brief The event associated with consuming the input.
     * 
     *  @return The CUDA event. Nullptr will be returned if the event is not set yet.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @NoException(true) CUevent_st getInputConsumedEvent();

    /**
     *  \brief Set output allocator to use for output tensor of given name.
     *  Pass nullptr to outputAllocator to unset.
     *  The allocator is called by enqueueV3().
     * 
     *  @param tensorName The name of an output tensor.
     *  @param outputAllocator IOutputAllocator for the tensors.
     * 
     *  @return True if success, false if the provided name does not map to an output or, if some other error occurred.
     * 
     *  \warning The string tensorName must be null-terminated, and be at most 4096 bytes including the terminator.
     * 
     *  @see enqueueV3() IOutputAllocator
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean setOutputAllocator(String tensorName, IOutputAllocator outputAllocator);
    public native @Cast("bool") @NoException(true) boolean setOutputAllocator(@Cast("const char*") BytePointer tensorName, IOutputAllocator outputAllocator);

    /**
     *  \brief Get output allocator associated with output tensor of given name, or nullptr if the provided name does
     *  not map to an output tensor.
     * 
     *  \warning The string tensorName must be null-terminated, and be at most 4096 bytes including the terminator.
     * 
     *  @see IOutputAllocator
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @NoException(true) IOutputAllocator getOutputAllocator(String tensorName);
    public native @NoException(true) IOutputAllocator getOutputAllocator(@Cast("const char*") BytePointer tensorName);

    /**
     *  \brief Get upper bound on an output tensor's size, in bytes, based on
     *  the current optimization profile and input dimensions.
     * 
     *  If the profile or input dimensions are not yet set, or the provided name
     *  does not map to an output, returns -1.
     * 
     *  @param tensorName The name of an output tensor.
     * 
     *  @return Upper bound in bytes.
     * 
     *  \warning The string tensorName must be null-terminated, and be at most 4096 bytes including the terminator.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Cast("int64_t") @NoException(true) long getMaxOutputSize(String tensorName);
    public native @Cast("int64_t") @NoException(true) long getMaxOutputSize(@Cast("const char*") BytePointer tensorName);

    /**
     *  \brief Specify allocator to use for internal temporary storage.
     * 
     *  This allocator is used only by enqueueV3() for temporary storage whose size cannot be
     *  predicted ahead of enqueueV3(). It is not used for output tensors, because memory
     *  allocation for those is allocated by the allocator set by setOutputAllocator().
     *  All memory allocated is freed by the time enqueueV3() returns.
     * 
     *  @param allocator pointer to allocator to use. Pass nullptr to revert to using TensorRT's
     *         default allocator.
     * 
     *  @return True on success, false if error occurred.
     * 
     *  @see enqueueV3() setOutputAllocator()
     *  */
    
    
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean setTemporaryStorageAllocator(IGpuAllocator allocator);

    /**
     *  \brief Get allocator set by setTemporaryStorageAllocator.
     * 
     *  Returns a nullptr if a nullptr was passed with setTemporaryStorageAllocator().
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native @NoException(true) IGpuAllocator getTemporaryStorageAllocator();

    /**
     *  \brief Enqueue inference on a stream.
     * 
     *  @param stream A CUDA stream on which the inference kernels will be enqueued.
     * 
     *  @return True if the kernels were enqueued successfully, false otherwise.
     * 
     *  Modifying or releasing memory that has been registered for the tensors before stream
     *  synchronization or the event passed to setInputConsumedEvent has been being triggered results in undefined
     *  behavior.
     *  Input tensor can be released after the setInputConsumedEvent whereas output tensors require stream
     *  synchronization.
     * 
     *  \warning Using default stream may lead to performance issues due to additional cudaDeviceSynchronize() calls by
     *           TensorRT to ensure correct synchronizations. Please use non-default stream instead.
     * 
     *  \warning If the Engine is streaming weights, enqueueV3 will become synchronous, and
     *           the graph will not be capturable.
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean enqueueV3(CUstream_st stream);

    /**
     *  \brief Set the maximum size for persistent cache usage.
     * 
     *  This function sets the maximum persistent L2 cache that this execution context may use for activation caching.
     *  Activation caching is not supported on all architectures - see "How TensorRT uses Memory" in the developer guide
     *  for details
     * 
     *  @param size the size of persistent cache limitation in bytes.
     *  The default is 0 Bytes.
     * 
     *  @see getPersistentCacheLimit */
    
    
    //!
    //!
    //!
    public native @NoException(true) void setPersistentCacheLimit(@Cast("size_t") long size);

    /**
     *  \brief Get the maximum size for persistent cache usage.
     * 
     *  @return The size of the persistent cache limit
     * 
     *  @see setPersistentCacheLimit */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Cast("size_t") @NoException(true) long getPersistentCacheLimit();

    /**
     *  \brief Set the verbosity of the NVTX markers in the execution context.
     * 
     *  Building with kDETAILED verbosity will generally increase latency in enqueueV3(). Call this method
     *  to select NVTX verbosity in this execution context at runtime.
     * 
     *  The default is the verbosity with which the engine was built, and the verbosity may not be raised above that
     *  level.
     * 
     *  This function does not affect how IEngineInspector interacts with the engine.
     * 
     *  @param verbosity The verbosity of the NVTX markers.
     * 
     *  @return True if the NVTX verbosity is set successfully. False if the provided verbosity level is higher than the
     *  profiling verbosity of the corresponding engine.
     * 
     *  @see getNvtxVerbosity()
     *  @see ICudaEngine::getProfilingVerbosity()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean setNvtxVerbosity(ProfilingVerbosity verbosity);
    public native @Cast("bool") @NoException(true) boolean setNvtxVerbosity(@Cast("nvinfer1::ProfilingVerbosity") int verbosity);

    /**
     *  \brief Get the NVTX verbosity of the execution context.
     * 
     *  @return The current NVTX verbosity of the execution context.
     * 
     *  @see setNvtxVerbosity()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native @NoException(true) ProfilingVerbosity getNvtxVerbosity();

    /**
     *  \brief Set the auxiliary streams that TensorRT should launch kernels on in the next enqueueV3() call.
     * 
     *  If set, TensorRT will launch the kernels that are supposed to run on the auxiliary streams using the streams
     *  provided by the user with this API. If this API is not called before the enqueueV3() call, then TensorRT will
     *  use the auxiliary streams created by TensorRT internally.
     * 
     *  TensorRT will always insert event synchronizations between the main stream provided via enqueueV3() call and the
     *  auxiliary streams:
     *   - At the beginning of the enqueueV3() call, TensorRT will make sure that all the auxiliary streams wait on
     *     the activities on the main stream.
     *   - At the end of the enqueueV3() call, TensorRT will make sure that the main stream wait on the activities on
     *     all the auxiliary streams.
     * 
     *  @param auxStreams The pointer to an array of cudaStream_t with the array length equal to nbStreams.
     *  @param nbStreams The number of auxiliary streams provided. If nbStreams is greater than
     *         {@code engine->getNbAuxStreams()}, then only the first {@code engine->getNbAuxStreams()} streams will be used. If
     *         {@code nbStreams} is less than {@code engine->getNbAuxStreams()}, such as setting {@code nbStreams} to 0, then TensorRT
     *         will use the provided streams for the first {@code nbStreams} auxiliary streams, and will create additional
     *         streams internally for the rest of the auxiliary streams.
     * 
     *  \note The provided auxiliary streams must not be the default stream and must all be different to avoid
     *        deadlocks.
     * 
     *  @see enqueueV3(), IBuilderConfig::setMaxAuxStreams(), ICudaEngine::getNbAuxStreams()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @NoException(true) void setAuxStreams(@ByPtrPtr CUstream_st auxStreams, int nbStreams);

    /**
     *  \brief Set DebugListener for this execution context.
     * 
     *  @param listener DebugListener for this execution context.
     * 
     *  @return true if succeed, false if failure.
     *  */
    
    
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean setDebugListener(IDebugListener listener);

    /**
     *  \brief Get the DebugListener of this execution context.
     * 
     *  @return DebugListener of this execution context.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @NoException(true) IDebugListener getDebugListener();

    /**
     *  \brief Set debug state of tensor given the tensor name.
     * 
     *  Turn the debug state of a tensor on or off.
     *  A tensor with the parameter tensor name must exist in the network, and the tensor must have
     *  been marked as a debug tensor during build time. Otherwise, an error is thrown.
     * 
     *  @param name Name of target tensor.
     * 
     *  @param flag True if turning on debug state, false if turning off debug state of tensor
     *  The default is off.
     * 
     *  @return True if successful, false otherwise.
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean setTensorDebugState(String name, @Cast("bool") boolean flag);
    public native @Cast("bool") @NoException(true) boolean setTensorDebugState(@Cast("const char*") BytePointer name, @Cast("bool") boolean flag);

    /**
     *  \brief Get the debug state.
     * 
     *  @param name Name of target tensor.
     * 
     *  @return true if there is a debug tensor with the given name and it has debug state turned on.
     *  */
    
    
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean getDebugState(String name);
    public native @Cast("bool") @NoException(true) boolean getDebugState(@Cast("const char*") BytePointer name);

    /**
     *  \brief Get the runtime config object used during execution context creation.
     * 
     *  @return The runtime config object.
     *  */
    
    //!
    //!
    //!
    //!
    public native @NoException(true) IRuntimeConfig getRuntimeConfig();

    /** \brief Turn the debug state of all debug tensors on or off.
     * 
     *  @param flag true if turning on debug state, false if turning off debug state.
     * 
     *  @return true if successful, false otherwise.
     * 
     *  The default is off.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean setAllTensorsDebugState(@Cast("bool") boolean flag);

    /**
     *  \brief Turn the debug state of unfused tensors on or off.
     * 
     *  The default is off.
     * 
     *  @param flag true if turning on debug state, false if turning off debug state.
     * 
     *  @return true if successful, false otherwise.
     * 
     *  @see INetworkDefinition::markUnfusedTensorsAsDebugTensors()
     *  */
    
    
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean setUnfusedTensorsDebugState(@Cast("bool") boolean flag);

    /**
     *  \brief Get the debug state of unfused tensors.
     * 
     *  @return true if unfused tensors debug state is on. False if unfused tensors debug state is off.
     *  */
    public native @Cast("bool") @NoException(true) boolean getUnfusedTensorsDebugState();
}

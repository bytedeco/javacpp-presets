// Targeted by JavaCPP version 1.5.5: DO NOT EDIT THIS FILE

package org.bytedeco.tensorrt.nvinfer;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.javacpp.presets.javacpp.*;
import org.bytedeco.cuda.cudart.*;
import static org.bytedeco.cuda.global.cudart.*;
import org.bytedeco.cuda.cublas.*;
import static org.bytedeco.cuda.global.cublas.*;
import org.bytedeco.cuda.cudnn.*;
import static org.bytedeco.cuda.global.cudnn.*;
import org.bytedeco.cuda.nvrtc.*;
import static org.bytedeco.cuda.global.nvrtc.*;

import static org.bytedeco.tensorrt.global.nvinfer.*;


/**
 *  \class INetworkDefinition
 * 
 *  \brief A network definition for input to the builder.
 * 
 *  A network definition defines the structure of the network, and combined with a IBuilderConfig, is built
 *  into an engine using an IBuilder. An INetworkDefinition can either have an implicit batch dimensions, specified
 *  at runtime, or all dimensions explicit, full dims mode, in the network definition. When a network has been
 *  created using createNetwork(), only implicit batch size mode is supported. The function hasImplicitBatchSize()
 *  is used to query the mode of the network.
 * 
 *  A network with implicit batch dimensions returns the dimensions of a layer without the implicit dimension,
 *  and instead the batch is specified at execute/enqueue time. If the network has all dimensions specified, then
 *  the first dimension follows elementwise broadcast rules: if it is 1 for some inputs and is some value N for all
 *  other inputs, then the first dimension of each outut is N, and the inputs with 1 for the first dimension are
 *  broadcast. Having divergent batch sizes across inputs to a layer is not supported.
 * 
 *  \warning Do not inherit from this class, as doing so will break forward-compatibility of the API and ABI.
 *  */
@Namespace("nvinfer1") @Properties(inherit = org.bytedeco.tensorrt.presets.nvinfer.class)
public class INetworkDefinition extends Pointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public INetworkDefinition(Pointer p) { super(p); }

    /**
     *  \brief Add an input tensor to the network.
     * 
     *  The name of the input tensor is used to find the index into the buffer array for an engine built from
     *  the network. The volume of the dimensions must be less than 2^30 elements.
     <p>
     *  For networks with an implicit batch dimension, this volume includes the batch dimension with its length set
     *  to the maximum batch size. For networks with all explicit dimensions and with wildcard dimensions, the volume
     *  is based on the maxima specified by an IOptimizationProfile.Dimensions are normally non-negative integers. The
     *  exception is that in networks with all explicit dimensions, -1 can be used as a wildcard for a dimension to
     *  be specified at runtime. Input tensors with such a wildcard must have a corresponding entry in the
     *  IOptimizationProfiles indicating the permitted extrema, and the input dimensions must be set by
     *  IExecutionContext::setBindingDimensions. Different IExecutionContext instances can have different dimensions.
     *  Wildcard dimensions are only supported for EngineCapability::kDEFAULT. They are not
     *  supported in safety contexts. DLA does not support Wildcard dimensions.
     * 
     *  Tensor dimensions are specified independent of format.  For example, if a
     *  tensor is formatted in "NHWC" or a vectorized format, the dimensions are
     *  still specified in the order{N, C, H, W}. For 2D images with a channel
     *  dimension, the last three dimensions are always {C,H,W}. For 3D images
     *  with a channel dimension, the last four dimensions are always {C,D,H,W}.
     * 
     *  @param name The name of the tensor.
     *  @param type The type of the data held in the tensor.
     *  @param dimensions The dimensions of the tensor.
     * 
     *  \warning It is an error to specify a wildcard value on a dimension that is determined by trained parameters.
     * 
     *  \warning If run on DLA with explicit dimensions, only leading dimension can be a wildcard. And provided profile
     *  must have same minimum, optimum, and maximum dimensions.
     * 
     *  @see ITensor
     * 
     *  @return The new tensor or nullptr if there is an error.
     *  */
    
    
    //!
    //!
    //!
    //!
    public native ITensor addInput(String name, DataType type, @ByVal Dims dimensions);
    public native ITensor addInput(@Cast("const char*") BytePointer name, @Cast("nvinfer1::DataType") int type, @ByVal Dims dimensions);

    /**
     *  \brief Mark a tensor as a network output.
     * 
     *  @param tensor The tensor to mark as an output tensor.
     * 
     *  \warning It is an error to mark a network input as an output.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native void markOutput(@ByRef ITensor tensor);

    /**
     *  \brief Add a convolution layer to the network.
     * 
     *  @param input The input tensor to the convolution.
     *  @param nbOutputMaps The number of output feature maps for the convolution.
     *  @param kernelSize The HW-dimensions of the convolution kernel.
     *  @param kernelWeights The kernel weights for the convolution.
     *  @param biasWeights The optional bias weights for the convolution.
     * 
     *  @see IConvolutionLayer
     * 
     *  \warning It is an error to specify a wildcard value for the 'C' dimension of the input tensor.
     *  \warning Int32 tensors are not valid input tensors.
     * 
     *  @return The new convolution layer, or nullptr if it could not be created.
     * 
     *  @deprecated Superseded by addConvolutionNd and will be removed in TensorRT 9.0.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Deprecated IConvolutionLayer addConvolution(@ByRef ITensor input, int nbOutputMaps, @ByVal DimsHW kernelSize,
            @ByVal Weights kernelWeights, @ByVal Weights biasWeights);

    /**
     *  \brief Add a fully connected layer to the network.
     * 
     *  @param input The input tensor to the layer.
     *  @param nbOutputs The number of outputs of the layer.
     *  @param kernelWeights The kernel weights for the fully connected layer.
     *  @param biasWeights The optional bias weights for the fully connected layer.
     * 
     *  @see IFullyConnectedLayer
     * 
     *  \warning It is an error to specify a wildcard value for the 'C' dimension of the input tensor.
     *  \warning Int32 tensors are not valid input tensors.
     * 
     *  @return The new fully connected layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native IFullyConnectedLayer addFullyConnected(
            @ByRef ITensor input, int nbOutputs, @ByVal Weights kernelWeights, @ByVal Weights biasWeights);

    /**
     *  \brief Add an activation layer to the network.
     * 
     *  @param input The input tensor to the layer.
     *  @param type The type of activation function to apply.
     * 
     *  Note that the setAlpha() and setBeta() methods must be used on the
     *  output for activations that require these parameters.
     * 
     *  @see IActivationLayer ActivationType
     *  \warning Int32 tensors are not valid input tensors.
     * 
     *  @return The new activation layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native IActivationLayer addActivation(@ByRef ITensor input, ActivationType type);
    public native IActivationLayer addActivation(@ByRef ITensor input, @Cast("nvinfer1::ActivationType") int type);

    /**
     *  \brief Add a pooling layer to the network.
     * 
     *  @param input The input tensor to the layer.
     *  @param type The type of pooling to apply.
     *  @param windowSize The size of the pooling window.
     * 
     *  @see IPoolingLayer PoolingType
     *  \warning Int32 tensors are not valid input tensors.
     * 
     *  @return The new pooling layer, or nullptr if it could not be created.
     * 
     *  @deprecated Superseded by addPoolingNd and will be removed in TensorRT 9.0.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    public native @Deprecated IPoolingLayer addPooling(
            @ByRef ITensor input, PoolingType type, @ByVal DimsHW windowSize);
    public native @Deprecated IPoolingLayer addPooling(
            @ByRef ITensor input, @Cast("nvinfer1::PoolingType") int type, @ByVal DimsHW windowSize);

    /**
     *  \brief Add a LRN layer to the network.
     * 
     *  @param input The input tensor to the layer.
     *  @param window The size of the window.
     *  @param alpha The alpha value for the LRN computation.
     *  @param beta The beta value for the LRN computation.
     *  @param k The k value for the LRN computation.
     * 
     *  @see ILRNLayer
     *  \warning Int32 tensors are not valid input tensors.
     * 
     *  @return The new LRN layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native ILRNLayer addLRN(@ByRef ITensor input, int window, float alpha, float beta, float k);

    /**
     *  \brief Add a Scale layer to the network.
     * 
     *  @param input The input tensor to the layer. This tensor is required to have a minimum of 3 dimensions.
     *  @param mode The scaling mode.
     *  @param shift The shift value.
     *  @param scale The scale value.
     *  @param power The power value.
     * 
     *  If the weights are available, then the size of weights are dependent on the ScaleMode.
     *  For ::kUNIFORM, the number of weights equals 1.
     *  For ::kCHANNEL, the number of weights equals the channel dimension.
     *  For ::kELEMENTWISE, the number of weights equals the product of the last three dimensions of the input.
     * 
     *  @see addScaleNd
     *  @see IScaleLayer
     *  \warning Int32 tensors are not valid input tensors.
     * 
     *  @return The new Scale layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    public native IScaleLayer addScale(@ByRef ITensor input, ScaleMode mode, @ByVal Weights shift, @ByVal Weights scale, @ByVal Weights power);
    public native IScaleLayer addScale(@ByRef ITensor input, @Cast("nvinfer1::ScaleMode") int mode, @ByVal Weights shift, @ByVal Weights scale, @ByVal Weights power);

    /**
     *  \brief Add a SoftMax layer to the network.
     * 
     *  @see ISoftMaxLayer
     *  \warning Int32 tensors are not valid input tensors.
     * 
     *  @return The new SoftMax layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native ISoftMaxLayer addSoftMax(@ByRef ITensor input);

    /**
     *  \brief Add a concatenation layer to the network.
     * 
     *  @param inputs The input tensors to the layer.
     *  @param nbInputs The number of input tensors.
     * 
     *  @see IConcatenationLayer
     * 
     *  @return The new concatenation layer, or nullptr if it could not be created.
     * 
     *  \warning All tensors must have the same dimensions for all dimensions except for channel.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native IConcatenationLayer addConcatenation(@Cast("nvinfer1::ITensor*const*") PointerPointer inputs, int nbInputs);
    public native IConcatenationLayer addConcatenation(@ByPtrPtr ITensor inputs, int nbInputs);

    /**
     *  \brief Add a deconvolution layer to the network.
     * 
     *  @param input The input tensor to the layer.
     *  @param nbOutputMaps The number of output feature maps.
     *  @param kernelSize The HW-dimensions of the deconvolution kernel.
     *  @param kernelWeights The kernel weights for the deconvolution.
     *  @param biasWeights The optional bias weights for the deconvolution.
     * 
     *  @see IDeconvolutionLayer
     * 
     *  \warning It is an error to specify a wildcard value for the 'C' dimension of the input tensor.
     *  \warning Int32 tensors are not valid input tensors.
     * 
     *  @return The new deconvolution layer, or nullptr if it could not be created.
     * 
     *  @deprecated Superseded by addDeconvolutionNd and will be removed in TensorRT 9.0.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Deprecated IDeconvolutionLayer addDeconvolution(@ByRef ITensor input, int nbOutputMaps,
            @ByVal DimsHW kernelSize, @ByVal Weights kernelWeights, @ByVal Weights biasWeights);

    /**
     *  \brief Add an elementwise layer to the network.
     * 
     *  @param input1 The first input tensor to the layer.
     *  @param input2 The second input tensor to the layer.
     *  @param op The binary operation that the layer applies.
     * 
     *  The input tensors must have the same number of dimensions.
     *  For each dimension, their lengths must match, or one of them must be one.
     *  In the latter case, the tensor is broadcast along that axis.
     * 
     *  The output tensor has the same number of dimensions as the inputs.
     *  For each dimension, its length is the maximum of the lengths of the
     *  corresponding input dimension.
     * 
     *  @see IElementWiseLayer
     *  \warning For shape tensors, ElementWiseOperation::kPOW is not a valid op.
     * 
     *  @return The new elementwise layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native IElementWiseLayer addElementWise(@ByRef ITensor input1, @ByRef ITensor input2, ElementWiseOperation op);
    public native IElementWiseLayer addElementWise(@ByRef ITensor input1, @ByRef ITensor input2, @Cast("nvinfer1::ElementWiseOperation") int op);

    /**
     *  \brief Add an \p layerCount deep RNN layer to the network with a
     *  sequence length of \p maxSeqLen and \p hiddenSize internal state per
     *  layer.
     * 
     *  @param inputs The input tensor to the layer.
     *  @param layerCount The number of layers in the RNN.
     *  @param hiddenSize The size of the internal hidden state for each layer.
     *  @param maxSeqLen The maximum length of the time sequence.
     *  @param op The type of RNN to execute.
     *  @param mode The input mode for the RNN.
     *  @param dir The direction to run the RNN.
     *  @param weights The weights for the weight matrix parameters of the RNN.
     *  @param bias The weights for the bias vectors parameters of the RNN.
     * 
     *  The inputs tensor must be of the type DataType::kFLOAT or DataType::kHALF,
     *  and have non-zero volume.
     * 
     *  See IRNNLayer::setWeights() and IRNNLayer::setBias() for details on the required input
     *  format for \p weights and \p bias.
     * 
     *  The layout for the \p input tensor should be {@code {1, S_max, N, E}}, where:
     *    - {@code S_max} is the maximum allowed sequence length (number of RNN iterations)
     *    - {@code N} is the batch size
     *    - {@code E} specifies the embedding length (unless ::kSKIP is set, in which case it should match
     *      getHiddenSize()).
     * 
     *  The first output tensor is the output of the final RNN layer across all timesteps, with dimensions
     *  {@code {S_max, N, H}}:
     * 
     *    - {@code S_max} is the maximum allowed sequence length (number of RNN iterations)
     *    - {@code N} is the batch size
     *    - {@code H} is an output hidden state (equal to getHiddenSize() or 2x getHiddenSize())
     * 
     *  The second tensor is the final hidden state of the RNN across all layers, and if the RNN
     *  is an LSTM (i.e. getOperation() is ::kLSTM), then the third tensor is the final cell
     *  state of the RNN across all layers.  Both the second and third output tensors have dimensions
     *  {@code {L, N, H}}:
     * 
     *   - {@code L} is equal to getLayerCount() if getDirection is ::kUNIDIRECTION,
     *      and 2*getLayerCount() if getDirection is ::kBIDIRECTION.  In the bi-directional
     *      case, layer {@code l}'s final forward hidden state is stored in {@code L = 2*l}, and
     *      final backward hidden state is stored in {@code L = 2*l + 1}.
     *   - {@code N} is the batch size
     *   - {@code H} is getHiddenSize().
     * 
     *  Note that in bidirectional RNNs, the full "hidden state" for a layer {@code l}
     *  is the concatenation of its forward hidden state and its backward hidden
     *  state, and its size is 2*H.
     * 
     *  @deprecated Superseded by addRNNv2 and will be removed in TensorRT 8.0.
     * 
     *  @see IRNNLayer
     * 
     *  \warning This layer does not support wildcard dimensions or explicit batch size networks.
     *  \warning Int32 tensors are not valid input tensors.
     * 
     *  @return The new RNN layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Deprecated IRNNLayer addRNN(@ByRef ITensor inputs, int layerCount, @Cast("std::size_t") long hiddenSize,
            int maxSeqLen, RNNOperation op, RNNInputMode mode, RNNDirection dir, @ByVal Weights weights,
            @ByVal Weights bias);
    public native @Deprecated IRNNLayer addRNN(@ByRef ITensor inputs, int layerCount, @Cast("std::size_t") long hiddenSize,
            int maxSeqLen, @Cast("nvinfer1::RNNOperation") int op, @Cast("nvinfer1::RNNInputMode") int mode, @Cast("nvinfer1::RNNDirection") int dir, @ByVal Weights weights,
            @ByVal Weights bias);

    /**
     *  \brief Add a plugin layer to the network.
     * 
     *  @param inputs The input tensors to the layer.
     *  @param nbInputs The number of input tensors.
     *  @param plugin The layer plugin.
     * 
     *  @see IPluginLayer
     * 
     *  @deprecated Superseded by addPluginV2 and will be removed in TensorRT 8.0.
     * 
     *  \warning Plugin inputs do not support wildcard dimensions or explicit batch size networks.
     *  \warning Int32 tensors are not valid input tensors.
     * 
     *  @return the new plugin layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Deprecated IPluginLayer addPlugin(
            @Cast("nvinfer1::ITensor*const*") PointerPointer inputs, int nbInputs, @ByRef IPlugin plugin);
    public native @Deprecated IPluginLayer addPlugin(
            @ByPtrPtr ITensor inputs, int nbInputs, @ByRef IPlugin plugin);

    /**
     *  \brief Add a unary layer to the network.
     * 
     *  @param input The input tensor to the layer.
     *  @param operation The operation to apply.
     * 
     *  @see IUnaryLayer
     * 
     *  \warning Int32 tensors are not valid input tensors.
     * 
     *  \warning Shape tensors are not supported as outputs.
     * 
     *  @return The new unary layer, or nullptr if it could not be created
     *  */
    
    //!
    //!
    //!
    //!
    //!
    public native IUnaryLayer addUnary(@ByRef ITensor input, UnaryOperation operation);
    public native IUnaryLayer addUnary(@ByRef ITensor input, @Cast("nvinfer1::UnaryOperation") int operation);

    /** \brief Add a padding layer to the network.
     * 
     *  @param input The input tensor to the layer.
     *  @param prePadding The padding to apply to the start of the tensor.
     *  @param postPadding The padding to apply to the end of the tensor.
     * 
     *  @see IPaddingLayer
     * 
     *  @return The new padding layer, or nullptr if it could not be created.
     * 
     *  @deprecated Superseded by addPaddingNd and will be removed in TensorRT 9.0.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    public native @Deprecated IPaddingLayer addPadding(
            @ByRef ITensor input, @ByVal DimsHW prePadding, @ByVal DimsHW postPadding);

    /**
     *  \brief Add a shuffle layer to the network.
     * 
     *  @param input The input tensor to the layer.
     * 
     *  @see IShuffleLayer
     * 
     *  @return The new shuffle layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native IShuffleLayer addShuffle(@ByRef ITensor input);

    /**
     *  \brief Set the pooling output dimensions formula.
     * 
     *  @deprecated This method does not currently work reliably and will be removed in TensorRT 8.0.
     * 
     *  @param formula The formula from computing the pooling output dimensions. If null is passed, the default
     *  formula is used.
     * 
     *  The default formula in each dimension is (inputDim + padding * 2 - kernelSize) / stride + 1.
     * 
     *  \warning Custom output dimensions formulas are not supported with wildcard dimensions.
     * 
     *  @see IOutputDimensionsFormula getPoolingOutputDimensionsFormula()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Deprecated void setPoolingOutputDimensionsFormula(IOutputDimensionsFormula formula);

    /**
     *  \brief Get the pooling output dimensions formula.
     * 
     *  @deprecated This method does not currently work reliably and will be removed in TensorRT 8.0.
     * 
     *  @return The formula from computing the pooling output dimensions.
     * 
     *  \warning Custom output dimensions formulas are not supported with wildcard dimensions.
     * 
     *  @see IOutputDimensionsFormula setPoolingOutputDimensionsFormula()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Deprecated @ByRef IOutputDimensionsFormula getPoolingOutputDimensionsFormula();

    /**
     *  \brief Set the convolution output dimensions formula.
     * 
     *  @deprecated This method does not currently work reliably and will be removed in TensorRT 8.0.
     * 
     *  @param formula The formula from computing the convolution output dimensions. If null is passed, the default
     *  formula is used.
     * 
     *  The default formula in each dimension is (inputDim + padding * 2 - kernelSize) / stride + 1.
     * 
     *  \warning Custom output dimensions formulas are not supported with wildcard dimensions.
     * 
     *  @see IOutputDimensionsFormula getConvolutionOutputDimensionsFormula()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Deprecated void setConvolutionOutputDimensionsFormula(
            IOutputDimensionsFormula formula);

    /**
     *  \brief Get the convolution output dimensions formula.
     * 
     *  @deprecated This method does not currently work reliably and will be removed in TensorRT 8.0.
     * 
     *  @return The formula from computing the convolution output dimensions.
     * 
     *  \warning Custom output dimensions formulas are not supported with wildcard dimensions.
     * 
     *  @see IOutputDimensionsFormula setConvolutionOutputDimensionsFormula()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Deprecated @ByRef IOutputDimensionsFormula getConvolutionOutputDimensionsFormula();

    /**
     *  \brief Set the deconvolution output dimensions formula.
     * 
     *  @deprecated This method does not currently work reliably and will be removed in TensorRT 8.0.
     * 
     *  @param formula The formula from computing the deconvolution output dimensions. If null is passed, the default!
     *  formula is used.
     * 
     *  The default formula in each dimension is (inputDim - 1) * stride + kernelSize - 2 * padding.
     * 
     *  \warning Custom output dimensions formulas are not supported with wildcard dimensions.
     * 
     *  @see IOutputDimensionsFormula getDevonvolutionOutputDimensionsFormula()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Deprecated void setDeconvolutionOutputDimensionsFormula(
            IOutputDimensionsFormula formula);

    /**
     *  \brief Get the deconvolution output dimensions formula.
     * 
     *  @return The formula from computing the deconvolution output dimensions.
     * 
     *  @deprecated This method does not currently work reliably and will be removed in TensorRT 8.0.
     * 
     *  \warning Custom output dimensions formulas are not supported with wildcard dimensions.
     * 
     *  @see IOutputDimensionsFormula setDeconvolutionOutputDimensionsFormula()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @Deprecated @ByRef IOutputDimensionsFormula getDeconvolutionOutputDimensionsFormula();

    /**
     *  \brief Get the number of layers in the network.
     * 
     *  @return The number of layers in the network.
     * 
     *  @see getLayer()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    public native int getNbLayers();

    /**
     *  \brief Get the layer specified by the given index.
     * 
     *  @param index The index of the layer.
     * 
     *  @return The layer, or nullptr if the index is out of range.
     * 
     *  @see getNbLayers()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native ILayer getLayer(int index);

    /**
     *  \brief Get the number of inputs in the network.
     * 
     *  @return The number of inputs in the network.
     * 
     *  @see getInput()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    public native int getNbInputs();

    /**
     *  \brief Get the input tensor specified by the given index.
     * 
     *  @param index The index of the input tensor.
     * 
     *  @return The input tensor, or nullptr if the index is out of range.
     * 
     *  @see getNbInputs()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    public native ITensor getInput(int index); // adding inputs invalidates indexing here

    /**
     *  \brief Get the number of outputs in the network.
     * 
     *  The outputs include those marked by markOutput or markOutputForShapes.
     * 
     *  @return The number of outputs in the network.
     * 
     *  @see getOutput()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    public native int getNbOutputs();

    /**
     *  \brief Get the output tensor specified by the given index.
     * 
     *  @param index The index of the output tensor.
     * 
     *  @return The output tensor, or nullptr if the index is out of range.
     * 
     *  @see getNbOutputs()
     *  */
    
    
    //!
    //!
    public native ITensor getOutput(int index); // adding outputs invalidates indexing here

    /**
     *  \brief Destroy this INetworkDefinition object.
     *  */
    public native void destroy();
    /**
     *  \brief Add a reduce layer to the network.
     * 
     *  @param input The input tensor to the layer.
     *  @param operation The reduction operation to perform.
     *  @param reduceAxes The reduction dimensions.
     *         The bit in position i of bitmask reduceAxes corresponds to explicit dimension i if result.
     *         E.g., the least significant bit corresponds to the first explicit dimension and the next to least
     *         significant bit corresponds to the second explicit dimension.
     * 
     *  @param keepDimensions The boolean that specifies whether or not to keep the reduced dimensions in the
     *  output of the layer.
     * 
     *  The reduce layer works by performing an operation specified by \p operation to reduce the tensor \p input across
     *  the
     *  axes specified by \p reduceAxes.
     * 
     *  @see IReduceLayer
     * 
     *  \warning If output is a shape tensor, ReduceOperation::kAVG is unsupported.
     * 
     *  @return The new reduce layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native IReduceLayer addReduce(@ByRef ITensor input, ReduceOperation operation, @Cast("uint32_t") int reduceAxes, @Cast("bool") boolean keepDimensions);
    public native IReduceLayer addReduce(@ByRef ITensor input, @Cast("nvinfer1::ReduceOperation") int operation, @Cast("uint32_t") int reduceAxes, @Cast("bool") boolean keepDimensions);

    /**
     *  \brief Add a TopK layer to the network.
     * 
     *  The TopK layer has two outputs of the same dimensions. The first contains data values,
     *  the second contains index positions for the values. Output values are sorted, largest first
     *  for operation kMAX and smallest first for operation kMIN.
     * 
     *  Currently only values of K up to 1024 are supported.
     * 
     *  @param input The input tensor to the layer.
     * 
     *  @param op Operation to perform.
     * 
     *  @param k Number of elements to keep.
     * 
     *  @param reduceAxes The reduction dimensions.
     *         The bit in position i of bitmask reduceAxes corresponds to explicit dimension i of the result.
     *         E.g., the least significant bit corresponds to the first explicit dimension and the next to least
     *         significant bit corresponds to the second explicit dimension.
     * 
     *         Currently reduceAxes must specify exactly one dimension, and it must be one of the last four dimensions.
     * 
     *  @see ITopKLayer
     * 
     *  \warning Int32 tensors are not valid input tensors.
     * 
     *  @return The new TopK layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    public native ITopKLayer addTopK(@ByRef ITensor input, TopKOperation op, int k, @Cast("uint32_t") int reduceAxes);
    public native ITopKLayer addTopK(@ByRef ITensor input, @Cast("nvinfer1::TopKOperation") int op, int k, @Cast("uint32_t") int reduceAxes);

    /**
     *  \brief Add a gather layer to the network.
     * 
     *  @param data The tensor to gather values from.
     *  @param indices The tensor to get indices from to populate the output tensor.
     *  @param axis The axis in the data tensor to gather on.
     * 
     *  @see IGatherLayer
     * 
     *  @return The new gather layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native IGatherLayer addGather(@ByRef ITensor data, @ByRef ITensor indices, int axis);

    /**
     *  \brief Add a RaggedSoftMax layer to the network.
     * 
     *  @param input The ZxS input tensor.
     *  @param bounds The Zx1 bounds tensor.
     * 
     *  @see IRaggedSoftMaxLayer
     * 
     *  \warning The bounds tensor cannot have the last dimension be the wildcard character.
     *  \warning Int32 tensors are not valid input tensors.
     * 
     *  @return The new RaggedSoftMax layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native IRaggedSoftMaxLayer addRaggedSoftMax(@ByRef ITensor input, @ByRef ITensor bounds);

    /**
     *  \brief Add a MatrixMultiply layer to the network.
     * 
     *  @param input0 The first input tensor (commonly A).
     *  @param op0 The operation to apply to input0.
     *  @param input1 The second input tensor (commonly B).
     *  @param op1 The operation to apply to input1.
     * 
     *  @see IMatrixMultiplyLayer
     * 
     *  \warning Int32 tensors are not valid input tensors.
     * 
     *  @return The new matrix multiply layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native IMatrixMultiplyLayer addMatrixMultiply(
            @ByRef ITensor input0, MatrixOperation op0, @ByRef ITensor input1, MatrixOperation op1);
    public native IMatrixMultiplyLayer addMatrixMultiply(
            @ByRef ITensor input0, @Cast("nvinfer1::MatrixOperation") int op0, @ByRef ITensor input1, @Cast("nvinfer1::MatrixOperation") int op1);

    /**
     *  \brief Add a MatrixMultiply layer to the network.
     * 
     *  @param input0 The first input tensor (commonly A).
     *  @param transpose0 If true, op(input0)=transpose(input0), else op(input0)=input0.
     *  @param input1 The second input tensor (commonly B).
     *  @param transpose1 If true, op(input1)=transpose(input1), else op(input1)=input1.
     * 
     *  @see IMatrixMultiplyLayer
     * 
     *  @return The new matrix multiply layer, or nullptr if it could not be created.
     * 
     *  \warning Int32 tensors are not valid input tensors.
     * 
     *  @deprecated This interface is superseded by the overload that replaces bool with MatrixOperation and will be
     *  removed in TensorRT 8.0.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Deprecated IMatrixMultiplyLayer addMatrixMultiply(
            @ByRef ITensor input0, @Cast("bool") boolean transpose0, @ByRef ITensor input1, @Cast("bool") boolean transpose1);

    /**
     *  \brief Add a constant layer to the network.
     * 
     *  @param dimensions The dimensions of the constant.
     *  @param weights The constant value, represented as weights.
     * 
     *  @see IConstantLayer
     * 
     *  @return The new constant layer, or nullptr if it could not be created.
     * 
     *  If weights.type is DataType::kINT32, the output is a tensor of 32-bit indices.
     *  Otherwise the output is a tensor of real values and the output type will be
     *  follow TensorRT's normal precision rules.
     * 
     *  If tensors in the network have an implicit batch dimension, the constant
     *  is broadcast over that dimension.
     * 
     *  If a wildcard dimension is used, the volume of the runtime dimensions must equal
     *  the number of weights specified.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native IConstantLayer addConstant(@ByVal Dims dimensions, @ByVal Weights weights);

    /**
     *  \brief Add an \p layerCount deep RNN layer to the network with \p hiddenSize internal states that can
     *  take a batch with fixed or variable sequence lengths.
     * 
     *  @param input The input tensor to the layer (see below).
     *  @param layerCount The number of layers in the RNN.
     *  @param hiddenSize Size of the internal hidden state for each layer.
     *  @param maxSeqLen Maximum sequence length for the input.
     *  @param op The type of RNN to execute.
     * 
     *  By default, the layer is configured with RNNDirection::kUNIDIRECTION and RNNInputMode::kLINEAR.
     *  To change these settings, use IRNNv2Layer::setDirection() and IRNNv2Layer::setInputMode().
     * 
     *  %Weights and biases for the added layer should be set using
     *  IRNNv2Layer::setWeightsForGate() and IRNNv2Layer::setBiasForGate() prior
     *  to building an engine using this network.
     * 
     *  The input tensors must be of the type DataType::kFLOAT or DataType::kHALF.
     *  The layout of the weights is row major and must be the same datatype as the input tensor.
     *  \p weights contain 8 matrices and \p bias contains 8 vectors.
     * 
     *  See IRNNv2Layer::setWeightsForGate() and IRNNv2Layer::setBiasForGate() for details on the required input
     *  format for \p weights and \p bias.
     * 
     *  The \p input ITensor should contain zero or more index dimensions {@code {N1, ..., Np}}, followed by
     *  two dimensions, defined as follows:
     *    - {@code S_max} is the maximum allowed sequence length (number of RNN iterations)
     *    - {@code E} specifies the embedding length (unless ::kSKIP is set, in which case it should match
     *      getHiddenSize()).
     * 
     *  By default, all sequences in the input are assumed to be size \p maxSeqLen.  To provide explicit sequence
     *  lengths for each input sequence in the batch, use IRNNv2Layer::setSequenceLengths().
     * 
     *  The RNN layer outputs up to three tensors.
     * 
     *  The first output tensor is the output of the final RNN layer across all timesteps, with dimensions
     *  {@code {N1, ..., Np, S_max, H}}:
     * 
     *    - {@code N1..Np} are the index dimensions specified by the input tensor
     *    - {@code S_max} is the maximum allowed sequence length (number of RNN iterations)
     *    - {@code H} is an output hidden state (equal to getHiddenSize() or 2x getHiddenSize())
     * 
     *  The second tensor is the final hidden state of the RNN across all layers, and if the RNN
     *  is an LSTM (i.e. getOperation() is ::kLSTM), then the third tensor is the final cell state
     *  of the RNN across all layers.  Both the second and third output tensors have dimensions
     *  {@code {N1, ..., Np, L, H}}:
     * 
     *   - {@code N1..Np} are the index dimensions specified by the input tensor
     *   - {@code L} is the number of layers in the RNN, equal to getLayerCount() if getDirection is ::kUNIDIRECTION,
     *      and 2x getLayerCount() if getDirection is ::kBIDIRECTION. In the bi-directional
     *      case, layer {@code l}'s final forward hidden state is stored in {@code L = 2*l}, and
     *      final backward hidden state is stored in {@code L= 2*l + 1}.
     *   - {@code H} is the hidden state for each layer, equal to getHiddenSize().
     * 
     *  @see IRNNv2Layer
     * 
     *  @deprecated Superseded by ILoop::addLoop and will be removed in TensorRT 9.0.
     * 
     *  \warning RNN inputs do not support wildcard dimensions or explicit batch size networks.
     *  \warning Int32 tensors are not valid input tensors, only for sequence lengths.
     * 
     *  @return The new RNN layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Deprecated IRNNv2Layer addRNNv2(
            @ByRef ITensor input, int layerCount, int hiddenSize, int maxSeqLen, RNNOperation op);
    public native @Deprecated IRNNv2Layer addRNNv2(
            @ByRef ITensor input, int layerCount, int hiddenSize, int maxSeqLen, @Cast("nvinfer1::RNNOperation") int op);

    /**
     *  \brief Add a plugin layer to the network using an IPluginExt interface.
     * 
     *  @param inputs The input tensors to the layer.
     *  @param nbInputs The number of input tensors.
     *  @param plugin The layer plugin.
     * 
     *  @see IPluginLayer
     * 
     *  @deprecated Superseded by addPluginV2 and will be removed in TensorRT 8.0.
     * 
     *  \warning Plugin inputs do not support wildcard dimensions or explicit batch size networks.
     *  \warning Int32 tensors are not valid input tensors.
     * 
     *  @return The new plugin layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Deprecated IPluginLayer addPluginExt(
            @Cast("nvinfer1::ITensor*const*") PointerPointer inputs, int nbInputs, @ByRef IPluginExt plugin);
    public native @Deprecated IPluginLayer addPluginExt(
            @ByPtrPtr ITensor inputs, int nbInputs, @ByRef IPluginExt plugin);

    /**
     *  \brief Add an identity layer.
     * 
     *  @param input The input tensor to the layer.
     * 
     *  @see IIdentityLayer
     * 
     *  \warning Int32 tensors are not valid input tensors.
     * 
     *  @return The new identity layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    public native IIdentityLayer addIdentity(@ByRef ITensor input);

    /**
     *  \brief remove a tensor from the network definition.
     * 
     *  @param tensor the tensor to remove
     * 
     *  It is illegal to remove a tensor that is the input or output of a layer.
     *  if this method is called with such a tensor, a warning will be emitted on the log
     *  and the call will be ignored. Its intended use is to remove detached tensors after
     *  e.g. concatenating two networks with Layer::setInput().
     *  */
    
    
    //!
    //!
    //!
    //!
    public native void removeTensor(@ByRef ITensor tensor);

    /**
     *  \brief unmark a tensor as a network output.
     * 
     *  @param tensor The tensor to unmark as an output tensor.
     * 
     *  see markOutput()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native void unmarkOutput(@ByRef ITensor tensor);

    /**
     *  \brief Add a plugin layer to the network using the IPluginV2 interface.
     * 
     *  @param inputs The input tensors to the layer.
     *  @param nbInputs The number of input tensors.
     *  @param plugin The layer plugin.
     * 
     *  @see IPluginV2Layer
     * 
     *  \warning Dimension wildcard are only supported with IPluginV2DynamicExt or IPluginV2IOExt plugins.
     *  \warning Int32 tensors are not valid input tensors.
     * 
     *  @return The new plugin layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native IPluginV2Layer addPluginV2(@Cast("nvinfer1::ITensor*const*") PointerPointer inputs, int nbInputs, @ByRef IPluginV2 plugin);
    public native IPluginV2Layer addPluginV2(@ByPtrPtr ITensor inputs, int nbInputs, @ByRef IPluginV2 plugin);

    /**
     *  \brief Add a slice layer to the network.
     * 
     *  @param input The input tensor to the layer.
     *  @param start The start offset
     *  @param size The output dimension
     *  @param stride The slicing stride
     * 
     *  Positive, negative, zero stride values, and combinations of them in different dimensions are allowed.
     * 
     *  @see ISliceLayer
     * 
     *  @return The new slice layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native ISliceLayer addSlice(@ByRef ITensor input, @ByVal Dims start, @ByVal Dims size, @ByVal Dims stride);

    /**
     *  \brief Sets the name of the network.
     * 
     *  @param name The name to assign to this network.
     * 
     *  Set the name of the network so that it can be associated with a built
     *  engine. The \p name must be a zero delimited C-style string of length
     *  no greater than 128 characters. TensorRT makes no use of this string
     *  except storing it as part of the engine so that it may be retrieved at
     *  runtime. A name unique to the builder will be generated by default.
     * 
     *  This method copies the name string.
     * 
     *  @see INetworkDefinition::getName(), ISafeCudaEngine::getName()
     * 
     *  @return none
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    public native void setName(String name);
    public native void setName(@Cast("const char*") BytePointer name);

    /**
     *  \brief Returns the name associated with the network.
     * 
     *  The memory pointed to by getName() is owned by the INetworkDefinition object.
     * 
     *  @see INetworkDefinition::setName()
     * 
     *  @return A zero delimited C-style string representing the name of the network.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native String getName();

    /**
     *  \brief Add a shape layer to the network.
     * 
     *  @param input The input tensor to the layer.
     * 
     *  @see IShapeLayer
     * 
     *  \warning addShape is only supported when hasImplicitBatchDimensions is false.
     * 
     *  \warning input to addShape cannot contain wildcard dimension values.
     * 
     *  @return The new shape layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native IShapeLayer addShape(@ByRef ITensor input);

    /**
     *  \brief Query whether the network was created with an implicit batch dimension.
     * 
     *  @return True if tensors have implicit batch dimension, false otherwise.
     * 
     *  This is a network-wide property.  Either all tensors in the network
     *  have an implicit batch dimension or none of them do.
     * 
     *  hasImplicitBatchDimension() is true if and only if this INetworkDefinition
     *  was created with createNetwork() or createNetworkV2() without
     *  NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag.
     * 
     *  @see createNetworkV2
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Cast("bool") boolean hasImplicitBatchDimension();

    /**
     *  \brief Enable tensor's value to be computed by IExecutionContext::getShapeBinding.
     * 
     *  @return True if successful, false if tensor is already marked as an output.
     * 
     *  The tensor must be of type DataType::kINT32 and have no more than one dimension.
     * 
     *  \warning The tensor must have dimensions that can be determined to be constants at build time.
     * 
     *  \warning It is an error to mark a network input as a shape output.
     * 
     *  @see isShapeBinding(), getShapeBinding()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @Cast("bool") boolean markOutputForShapes(@ByRef ITensor tensor);

    /**
     *  \brief Undo markOutputForShapes.
     * 
     *  \warning inputs to addShape cannot contain wildcard dimension values.
     * 
     *  @return True if successful, false if tensor is not marked as an output.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Cast("bool") boolean unmarkOutputForShapes(@ByRef ITensor tensor);

    /**
     *  \brief Add a parametric ReLU layer to the network.
     * 
     *  @param input The input tensor to the layer.
     *  @param slope The slope tensor to the layer. This tensor should be unidirectionally broadcastable
     *         to the input tensor.
     * 
     *  @see IParametricReLULayer
     * 
     *  \warning Int32 tensors are not valid input tensors.
     * 
     *  @return The new parametric ReLU layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @NoException IParametricReLULayer addParametricReLU(@ByRef ITensor input, @ByRef ITensor slope);

    /**
     *  \brief Add a multi-dimension convolution layer to the network.
     * 
     *  @param input The input tensor to the convolution.
     *  @param nbOutputMaps The number of output feature maps for the convolution.
     *  @param kernelSize The multi-dimensions of the convolution kernel.
     *  @param kernelWeights The kernel weights for the convolution.
     *  @param biasWeights The optional bias weights for the convolution.
     * 
     *  @see IConvolutionLayer
     * 
     *  \warning It is an error to specify a wildcard value for the 'C' dimension of the input tensor.
     *  \warning Int32 tensors are not valid input tensors.
     *  \warning Only 2D or 3D convolution is supported.
     * 
     *  @return The new convolution layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native IConvolutionLayer addConvolutionNd(@ByRef ITensor input, int nbOutputMaps, @ByVal Dims kernelSize,
            @ByVal Weights kernelWeights, @ByVal Weights biasWeights);

    /**
     *  \brief Add a multi-dimension pooling layer to the network.
     * 
     *  @param input The input tensor to the layer.
     *  @param type The type of pooling to apply.
     *  @param windowSize The size of the pooling window.
     * 
     *  @see IPoolingLayer PoolingType
     * 
     *  \warning Int32 tensors are not valid input tensors.
     *  \warning Only 2D or 3D pooling is supported.
     * 
     *  @return The new pooling layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    public native IPoolingLayer addPoolingNd(@ByRef ITensor input, PoolingType type, @ByVal Dims windowSize);
    public native IPoolingLayer addPoolingNd(@ByRef ITensor input, @Cast("nvinfer1::PoolingType") int type, @ByVal Dims windowSize);

    /**
     *  \brief Add a multi-dimension deconvolution layer to the network.
     * 
     *  @param input The input tensor to the layer.
     *  @param nbOutputMaps The number of output feature maps.
     *  @param kernelSize The multi-dimensions of the deconvolution kernel.
     *  @param kernelWeights The kernel weights for the deconvolution.
     *  @param biasWeights The optional bias weights for the deconvolution.
     * 
     *  @see IDeconvolutionLayer
     * 
     *  \warning It is an error to specify a wildcard value for the 'C' dimension of the input tensor.
     *  \warning Int32 tensors are not valid input tensors.
     *  \warning Only 2D or 3D deconvolution is supported. */
    //
    /** @return The new deconvolution layer, or nullptr if it could not be created.
    /** */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native IDeconvolutionLayer addDeconvolutionNd(@ByRef ITensor input, int nbOutputMaps, @ByVal Dims kernelSize,
            @ByVal Weights kernelWeights, @ByVal Weights biasWeights);

    /**
     *  \brief Add a multi-dimension scale layer to the network.
     * 
     *  @param input The input tensor to the layer.
     *  @param mode The scaling mode.
     *  @param shift The shift value.
     *  @param scale The scale value.
     *  @param power The power value.
     *  @param channelAxis The channel axis.
     * 
     *  If the weights are available, then the size of weights are dependent on the ScaleMode.
     *  For ::kUNIFORM, the number of weights equals 1.
     *  For ::kCHANNEL, the number of weights equals the channel dimension.
     *  For ::kELEMENTWISE, the number of weights equals the product of all input dimensions at channelAxis and beyond.
     * 
     *  For example, if the inputs dimensions are [A,B,C,D,E,F], and channelAxis=2:
     *  For ::kUNIFORM, the number of weights is equal to 1.
     *  For ::kCHANNEL, the number of weights is C.
     *  For ::kELEMENTWISE, the number of weights is C*D*E*F.
     * 
     *  @see IScaleLayer
     *  \warning Int32 tensors are not valid input tensors.
     *  \warning Only 2D or 3D scale is supported.
     * 
     *  @return The new Scale layer, or nullptr if it could not be created.
     *  */
    
    //!
    //!
    //!
    //!
    //!
    public native IScaleLayer addScaleNd(@ByRef ITensor input, ScaleMode mode, @ByVal Weights shift, @ByVal Weights scale, @ByVal Weights power,
            int channelAxis);
    public native IScaleLayer addScaleNd(@ByRef ITensor input, @Cast("nvinfer1::ScaleMode") int mode, @ByVal Weights shift, @ByVal Weights scale, @ByVal Weights power,
            int channelAxis);

    /** \brief Add a resize layer to the network.
     * 
     *  @param input The input tensor to the layer.
     * 
     *  @see IResizeLayer
     * 
     *  \warning Int32 tensors are not valid input tensors.
     * 
     *  @return The new resize layer, or nullptr if it could not be created.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    public native IResizeLayer addResize(@ByRef ITensor input);

    /**
     *  \brief True if network is an explicit precision network
     * 
     *  hasExplicitPrecision() is true if and only if this INetworkDefinition
     *  was created with createNetworkV2() with NetworkDefinitionCreationFlag::kEXPLICIT_PRECISION set.
     * 
     *  @see createNetworkV2
     * 
     *  @return True if network has explicit precision, false otherwise.
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @Cast("bool") boolean hasExplicitPrecision();

    /**
     *  \brief Add a loop to the network.
     * 
     *  An ILoop provides a way to specify a recurrent subgraph.
     * 
     *  @return Pointer to ILoop that can be used to add loop boundary layers for the loop,
     *          or nullptr if network has an implicit batch dimension or this version
     *          of TensorRT does not support loops.
     *  */
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native @NoException ILoop addLoop();

    /** \brief Add a select layer to the network.
     * 
     *  @param condition The condition tensor to the layer. Must have type DataType::kBOOL.
     *  @param thenInput The "then" input tensor to the layer.
     *  @param elseInput The "else" input tensor to the layer.
     * 
     *  All three input tensors must have the same number of dimensions, and along each axis
     *  must have the same length or a length of one. If the length is one, the tensor
     *  is broadcast along that axis. The output tensor has the dimensions of the inputs AFTER
     *  the broadcast rule is applied. For example, given:
     * 
     *     dimensions of condition:  [1,1,5,9]
     *     dimensions of thenInput:  [1,1,5,9]
     *     dimensions of elseInput:  [1,3,1,9]
     * 
     *  the output dimensions are [1,3,5,9], and the output contents are defined by:
     * 
     *       output[0,i,j,k] = condition[0,0,j,k] ? thenInput[0,0,j,k] : elseInput[0,i,0,k]
     * 
     *  The output dimensions are not necessarily the max of the input dimensions if any input
     *  is an empty tensor. For example, if in the preceding example, 5 is changed to 0:
     * 
     *     dimensions of condition:  [1,1,0,9]
     *     dimensions of thenInput:  [1,1,0,9]
     *     dimensions of elseInput:  [1,3,1,9]
     * 
     *  then the output dimensions are [1,3,0,9].
     * 
     *  @see ISelectLayer
     * 
     *  @return The new select layer, or nullptr if it could not be created. */
    
    //!
    //!
    //!
    //!
    public native ISelectLayer addSelect(@ByRef ITensor condition, @ByRef ITensor thenInput, @ByRef ITensor elseInput);

    /** \brief Add a fill layer to the network.
     * 
     *  @param dimensions The output tensor dimensions.
     *  @param op The fill operation that the layer applies.
     * 
     *  \warning The dimensions's nbDims must be 1.
     * 
     *  @see IFillLayer
     * 
     *  @return The new fill layer, or nullptr if it could not be created. */
    
    //!
    //!
    //!
    //!
    public native @NoException IFillLayer addFill(@ByVal Dims dimensions, FillOperation op);
    public native @NoException IFillLayer addFill(@ByVal Dims dimensions, @Cast("nvinfer1::FillOperation") int op);

    /** \brief Add a padding layer to the network. Only 2D padding is currently supported.
     * 
     *  @param input The input tensor to the layer.
     *  @param prePadding The padding to apply to the start of the tensor.
     *  @param postPadding The padding to apply to the end of the tensor.
     * 
     *  @see IPaddingLayer
     * 
     *  @return The new padding layer, or nullptr if it could not be created.
     *  */
    public native IPaddingLayer addPaddingNd(
            @ByRef ITensor input, @ByVal Dims prePadding, @ByVal Dims postPadding);
}

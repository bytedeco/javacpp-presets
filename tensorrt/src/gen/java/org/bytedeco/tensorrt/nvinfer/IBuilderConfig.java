// Targeted by JavaCPP version 1.5.4: DO NOT EDIT THIS FILE

package org.bytedeco.tensorrt.nvinfer;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.javacpp.presets.javacpp.*;
import org.bytedeco.cuda.cudart.*;
import static org.bytedeco.cuda.global.cudart.*;
import org.bytedeco.cuda.cublas.*;
import static org.bytedeco.cuda.global.cublas.*;
import org.bytedeco.cuda.cudnn.*;
import static org.bytedeco.cuda.global.cudnn.*;
import org.bytedeco.cuda.nvrtc.*;
import static org.bytedeco.cuda.global.nvrtc.*;

import static org.bytedeco.tensorrt.global.nvinfer.*;


/**
 *  \class IBuilderConfig
 * 
 *  \brief Holds properties for configuring a builder to produce an engine. @see BuilderFlags
 *  */
@Namespace("nvinfer1") @Properties(inherit = org.bytedeco.tensorrt.presets.nvinfer.class)
public class IBuilderConfig extends Pointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public IBuilderConfig(Pointer p) { super(p); }

    /**
     *  \brief Set the number of minimization iterations used when timing layers.
     * 
     *  When timing layers, the builder minimizes over a set of average times for layer execution. This parameter
     *  controls the number of iterations used in minimization. The builder may sometimes run layers for more
     *  iterations to improve timing accuracy if this parameter is set to a small value and the runtime of the
     *  layer is short.
     * 
     *  @see getMinTimingIterations()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native void setMinTimingIterations(int minTiming);

    /**
     *  \brief Query the number of minimization iterations.
     * 
     *  By default the minimum number of iterations is 2.
     * 
     *  @see setMinTimingIterations()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native int getMinTimingIterations();

    /**
     *  \brief Set the number of averaging iterations used when timing layers.
     * 
     *  When timing layers, the builder minimizes over a set of average times for layer execution. This parameter
     *  controls the number of iterations used in averaging.
     * 
     *  @see getAvgTimingIterations()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native void setAvgTimingIterations(int avgTiming);

    /**
     *  \brief Query the number of averaging iterations.
     * 
     *  By default the number of averaging iterations is 1.
     * 
     *  @see setAvgTimingIterations()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native int getAvgTimingIterations();

    /**
     *  \brief Configure the builder to target specified EngineCapability flow.
     * 
     *  The flow means a sequence of API calls that allow an application to set up a runtime, engine,
     *  and execution context in order to run inference.
     * 
     *  The supported flows are specified in the EngineCapability enum.
     *  */
    
    
    //!
    //!
    //!
    //!
    public native void setEngineCapability(EngineCapability capability);
    public native void setEngineCapability(@Cast("nvinfer1::EngineCapability") int capability);

    /**
     *  \brief Query EngineCapability flow configured for the builder.
     * 
     *  By default it returns EngineCapability::kDEFAULT.
     * 
     *  @see setEngineCapability()
     *  */
    
    
    //!
    //!
    //!
    public native EngineCapability getEngineCapability();

    /**
     *  \brief Set Int8 Calibration interface.
     * 
     *  The calibrator is to minimize the information loss during the INT8 quantization process.
     *  */
    
    
    //!
    //!
    public native void setInt8Calibrator(IInt8Calibrator calibrator);

    /**
     *  \brief Get Int8 Calibration interface.
     *  */
    
    
    //!
    //!
    //!
    //!
    public native IInt8Calibrator getInt8Calibrator();

    /**
     *  \brief Set the maximum workspace size.
     * 
     *  @param workspaceSize The maximum GPU temporary memory which the engine can use at execution time.
     * 
     *  @see getMaxWorkspaceSize()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    public native void setMaxWorkspaceSize(@Cast("std::size_t") long workspaceSize);

    /**
     *  \brief Get the maximum workspace size.
     * 
     *  By default the workspace size is 0, which means there is no temporary memory.
     * 
     *  @return The maximum workspace size.
     * 
     *  @see setMaxWorkspaceSize()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Cast("std::size_t") long getMaxWorkspaceSize();

    /**
     *  \brief Set the build mode flags to turn on builder options for this network.
     * 
     *  The flags are listed in the BuilderFlags enum.
     *  The flags set configuration options to build the network.
     * 
     *  @param builderFlags The build option for an engine.
     * 
     *  \note This function will override the previous set flags, rather than bitwise ORing the new flag.
     * 
     *  @see getFlags()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native void setFlags(@Cast("nvinfer1::BuilderFlags") int builderFlags);

    /**
     *  \brief Get the build mode flags for this builder config. Defaults to 0.
     * 
     *  @return The build options as a bitmask.
     * 
     *  @see setFlags()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @Cast("nvinfer1::BuilderFlags") int getFlags();

    /**
     *  \brief clear a single build mode flag.
     * 
     *  clears the builder mode flag from the enabled flags.
     * 
     *  @see setFlags()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native void clearFlag(BuilderFlag builderFlag);
    public native void clearFlag(@Cast("nvinfer1::BuilderFlag") int builderFlag);

    /**
     *  \brief Set a single build mode flag.
     * 
     *  Add the input builder mode flag to the already enabled flags.
     * 
     *  @see setFlags()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native void setFlag(BuilderFlag builderFlag);
    public native void setFlag(@Cast("nvinfer1::BuilderFlag") int builderFlag);

    /**
     *  \brief Returns true if the build mode flag is set
     * 
     *  @see getFlags()
     * 
     *  @return True if flag is set, false if unset.
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @Cast("bool") boolean getFlag(BuilderFlag builderFlag);
    public native @Cast("bool") boolean getFlag(@Cast("nvinfer1::BuilderFlag") int builderFlag);

    /**
     *  \brief Set the device that this layer must execute on.
     *  @param deviceType that this layer must execute on.
     *  If DeviceType is not set or is reset, TensorRT will use the default DeviceType set in the builder.
     * 
     *  \note The device type for a layer must be compatible with the safety flow (if specified).
     *  For example a layer cannot be marked for DLA execution while the builder is configured for kSAFE_GPU.
     * 
     *  @see getDeviceType()
     *  */
    
    
    //!
    //!
    public native void setDeviceType(@Const ILayer layer, DeviceType deviceType);
    public native void setDeviceType(@Const ILayer layer, @Cast("nvinfer1::DeviceType") int deviceType);

    /**
     *  \brief Get the device that this layer executes on.
     *  @return Returns DeviceType of the layer.
     *  */
    
    
    //!
    //!
    public native DeviceType getDeviceType(@Const ILayer layer);

    /**
     *  \brief whether the DeviceType has been explicitly set for this layer
     *  @return true if device type is not default
     *  @see setDeviceType() getDeviceType() resetDeviceType()
     *  */
    
    
    //!
    //!
    //!
    public native @Cast("bool") boolean isDeviceTypeSet(@Const ILayer layer);

    /**
     *  \brief reset the DeviceType for this layer
     * 
     *  @see setDeviceType() getDeviceType() isDeviceTypeSet()
     *  */
    
    
    //!
    //!
    public native void resetDeviceType(@Const ILayer layer);

    /**
     *  \brief Checks if a layer can run on DLA.
     *  @return status true if the layer can on DLA else returns false.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    public native @Cast("bool") boolean canRunOnDLA(@Const ILayer layer);

    /**
     *  \brief Sets the DLA core used by the network.
     *  @param dlaCore The DLA core to execute the engine on (0 to N-1). Default value is 0.
     * 
     *  It can be used to specify which DLA core to use via indexing, if multiple DLA cores are available.
     * 
     *  @see IRuntime::setDLACore() getDLACore()
     * 
     *  \warning Starting with TensorRT 8, the default value will be -1 if the DLA is not specified or unused.
     *  */
    
    
    //!
    //!
    //!
    public native void setDLACore(int dlaCore);

    /**
     *  \brief Get the DLA core that the engine executes on.
     *  @return If setDLACore is called, returns DLA core from 0 to N-1, else returns 0.
     * 
     *  \warning Starting with TensorRT 8, the default value will be -1 if the DLA is not specified or unused.
     *  */
    
    
    //!
    //!
    public native int getDLACore();

    /**
     *  \brief Sets the default DeviceType to be used by the builder. It ensures that all the layers that can run on
     *  this device will run on it, unless setDeviceType is used to override the default DeviceType for a layer.
     *  @see getDefaultDeviceType()
     *  */
    
    
    //!
    //!
    //!
    public native void setDefaultDeviceType(DeviceType deviceType);
    public native void setDefaultDeviceType(@Cast("nvinfer1::DeviceType") int deviceType);

    /**
     *  \brief Get the default DeviceType which was set by setDefaultDeviceType.
     * 
     *  By default it returns DeviceType::kGPU.
     *  */
    
    
    //!
    //!
    //!
    public native DeviceType getDefaultDeviceType();

    /**
     *  \brief Resets the builder configuration to defaults.
     * 
     *  When initializing a builder config object, we can call this function.
     *  */
    
    
    //!
    //!
    //!
    public native void reset();

    /**
     *  \brief De-allocates any internally allocated memory.
     * 
     *  When destroying a builder config object, we can call this function.
     *  */
    
    
    //!
    //!
    //!
    //!
    public native void destroy();

    /**
     *  \brief Set the cudaStream that is used to profile this network.
     * 
     *  @param stream The cuda stream used for profiling by the builder.
     * 
     *  @see getProfileStream()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native void setProfileStream(CUstream_st stream);

    /**
     *  \brief Get the cudaStream that is used to profile this network.
     * 
     *  @return The cuda stream used for profiling by the builder.
     * 
     *  @see setProfileStream()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native CUstream_st getProfileStream();

    /**
     *  \brief Add an optimization profile.
     * 
     *  This function must be called at least once if the network has dynamic or shape input tensors.
     *  This function may be called at most once when building a refittable engine, as more than
     *  a single optimization profile are not supported for refittable engines.
     * 
     *  @param profile The new optimization profile, which must satisfy profile->isValid() == true
     *  @return The index of the optimization profile (starting from 0) if the input is valid, or -1 if the input is
     *          not valid.
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @NoException int addOptimizationProfile(@Const IOptimizationProfile profile);

    /**
     *  \brief Get number of optimization profiles.
     * 
     *  This is one higher than the index of the last optimization profile that has be defined (or
     *  zero, if none has been defined yet).
     * 
     *  @return The number of the optimization profiles.
     *  */
    public native @NoException int getNbOptimizationProfiles();
    /**
     *  \brief Set verbosity level of layer information exposed in NVTX annotations.
     * 
     *  Control how much layer information will be exposed in NVTX annotations.
     * 
     *  @see ProfilingVerbosity, getProfilingVerbosity()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native void setProfilingVerbosity(ProfilingVerbosity verbosity);
    public native void setProfilingVerbosity(@Cast("nvinfer1::ProfilingVerbosity") int verbosity);

    /**
     *  \brief Get verbosity level of layer information exposed in NVTX annotations.
     * 
     *  Get the current setting of verbosity level of layer information exposed in
     *  NVTX annotations. Default value is ProfilingVerbosity::kDEFAULT.
     * 
     *  @see ProfilingVerbosity, setProfilingVerbosity()
     *  */
    
    
    //!
    //!
    public native ProfilingVerbosity getProfilingVerbosity();

    /**
     *  \brief Set Algorithm Selector.
     * 
     *  @param selector The algorithm slector to be set in the build config. */
    
    
    //!
    //!
    public native void setAlgorithmSelector(IAlgorithmSelector selector);

    /**
     *  \brief Get Algorithm Selector.
     *  */
    
    
    //!
    //!
    //!
    //!
    public native IAlgorithmSelector getAlgorithmSelector();

    /**
     *  \brief Add a calibration profile.
     * 
     *  Calibration optimization profile must be set if int8 calibration is used to set scales for a network with runtime dimensions.
     * 
     *  @param profile The new calibration profile, which must satisfy profile->isValid() == true or be nullptr.
     *  MIN and MAX values will be overwritten by kOPT.
     *  @return True if the calibration profile was set correctly.
     *  */
    
    
    //!
    //!
    //!
    public native @Cast("bool") @NoException boolean setCalibrationProfile(@Const IOptimizationProfile profile);

    /**
     *  \brief Get the current calibration profile.
     * 
     *  @return A pointer to the current calibration profile or nullptr if calibration profile is unset.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Const @NoException IOptimizationProfile getCalibrationProfile();

    /**
     *  \brief Set the quantization flags.
     * 
     *  The flags are listed in the QuantizationFlag enum.
     *  The flags set configuration options to quantize the network in int8.
     * 
     *  @param flags The quantization flags.
     * 
     *  \note This function will override the previous set flags, rather than bitwise ORing the new flag.
     * 
     *  @see getQuantizationFlags()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native void setQuantizationFlags(@Cast("nvinfer1::QuantizationFlags") int flags);

    /**
     *  \brief Get the quantization flags.
     * 
     *  @return The quantization flags as a bitmask.
     * 
     *  @see setQuantizationFlag()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @Cast("nvinfer1::QuantizationFlags") int getQuantizationFlags();

    /**
     *  \brief clear a quantization flag.
     * 
     *  Clears the quantization flag from the enabled quantization flags.
     * 
     *  @see setQuantizationFlags()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native void clearQuantizationFlag(QuantizationFlag flag);
    public native void clearQuantizationFlag(@Cast("nvinfer1::QuantizationFlag") int flag);

    /**
     *  \brief Set a single quantization flag.
     * 
     *  Add the input quantization flag to the already enabled quantization flags.
     * 
     *  @see setQuantizationFlags()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native void setQuantizationFlag(QuantizationFlag flag);
    public native void setQuantizationFlag(@Cast("nvinfer1::QuantizationFlag") int flag);

    /**
     *  \brief Returns true if the quantization flag is set.
     * 
     *  @see getQuantizationFlags()
     * 
     *  @return True if quantization flag is set, false if unset.
     *  */
    public native @Cast("bool") boolean getQuantizationFlag(QuantizationFlag flag);
    public native @Cast("bool") boolean getQuantizationFlag(@Cast("nvinfer1::QuantizationFlag") int flag);
}

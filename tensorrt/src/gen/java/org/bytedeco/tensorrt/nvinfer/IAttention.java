// Targeted by JavaCPP version 1.5.13-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.tensorrt.nvinfer;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.javacpp.presets.javacpp.*;
import org.bytedeco.cuda.cudart.*;
import static org.bytedeco.cuda.global.cudart.*;
import org.bytedeco.cuda.cublas.*;
import static org.bytedeco.cuda.global.cublas.*;
import org.bytedeco.cuda.cudnn.*;
import static org.bytedeco.cuda.global.cudnn.*;
import org.bytedeco.cuda.nvrtc.*;
import static org.bytedeco.cuda.global.nvrtc.*;

import static org.bytedeco.tensorrt.global.nvinfer.*;


/**
 *  \class IAttention
 * 
 *  \brief Helper for constructing an attention that consumes query, key and value tensors.
 * 
 *  An attention subgraph implicitly includes three main components, two MatrixMultiply layers
 *  known as BMM1 and BMM2, and one normalization operation which defaults to be a Softmax.
 *  By default, IAttention is not decomposable and TensorRT will try to use a single fused kernel, which may be more
 *  efficient than if the subgraph is expressed without IAttention. Setting the IAttention to decomposable=True can
 *  allow IAttention to be decomposed to use multiple kernels if no fused kernel support found.
 * 
 *   Query       Key       Value      Mask (optional)      NormalizationQuantizeScale (optional)
 *     |          |          |          |                    |
 *     |       Transpose     |          |                    |
 *     |          |          |          |                    |
 *     ----BMM1----          |          |                    |
 *           |               |          |                    |
 *           *---------------------------                    |
 *           |               |                               |
 *     Normalization         |                               |
 *           |               |                               |
 *           *------------------------------------------------
 *           |               |
 *           -------BMM2------
 *                   |
 *                 Output
 * 
 *  The attention has the following inputs, in order of input index:
 * 
 *  * Query contains the input query. It is a tensor of type kFLOAT, kHALF or kBF16 with
 *    shape [batchSize, numHeadsQuery, sequenceLengthQuery, dimHead]
 *  * Key contains the input key. It is a tensor of type kFLOAT, kHALF or kBF16 with
 *    shape [batchSize, numHeadsKeyValue, sequenceLengthKeyValue, dimHead]
 *  * Value contains the input value. It is a tensor of type kFLOAT, kHALF or kBF16 with
 *    shape [batchSize, numHeadsKeyValue, sequenceLengthKeyValue, dimHead]
 *  * Mask (optional) contains the mask value. It is a tensor of type kBOOL or the same data type of
 *    BMM1 output with shape [batchSize, numHeadsQuery, sequenceLengthQuery, sequenceLengthKeyValue]
 *    with batchSize and numHeadsQuery broadcastable. For a kBOOL mask, a True value indicates that the corresponding
 *    position is allowed to attend. For other data types, the mask values will be added to the BMM1 output, known
 *    as an add mask.
 *  * NormalizationQuantizeScale (optional) contains the quantization scale for the attention normalization output.
 *    It is a tensor of type kFLOAT, kHALF or kBF16 with dimension 0 or 1.
 * 
 *  \warning Do not inherit from this class, as doing so will break forward-compatibility of the API and ABI.
 *  */
@Namespace("nvinfer1") @NoOffset @Properties(inherit = org.bytedeco.tensorrt.presets.nvinfer.class)
public class IAttention extends INoCopy {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public IAttention(Pointer p) { super(p); }

    /**
     *  \brief Set the normalization operation for the attention.
     * 
     *  @see getNormalizationOperation(), AttentionNormalizationOp
     * 
     *  @return True if the normalization operation is set successfully, false otherwise.
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean setNormalizationOperation(AttentionNormalizationOp op);
    public native @Cast("bool") @NoException(true) boolean setNormalizationOperation(@Cast("nvinfer1::AttentionNormalizationOp") int op);

    /**
     *  \brief Get the normalization operation for the attention.
     * 
     *  @see setNormalizationOperation(), AttentionNormalizationOp
     * 
     *  @return The normalization operation for the attention. Default is kSOFTMAX.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    public native @NoException(true) AttentionNormalizationOp getNormalizationOperation();

    /**
     *  \brief Set whether a mask will be used for the normalization operation.
     * 
     *  @param mask the mask tensor of type kBOOL or the same data type of
     *  BMM1 output with shape [batchSize, sequenceLengthQuery, sequenceLengthKeyValue]. For a kBOOL mask, a True value
     *  indicates that the corresponding position is allowed to attend. For other data types, the mask values will
     *  be added to the BMM1 output, known as an add mask.
     * 
     *  @see getMask
     * 
     *  @return True if the mask is set successfully, false otherwise.
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean setMask(@ByRef ITensor mask);

    /**
     *  \brief Get the optional mask in attention.
     * 
     *  @see setMask
     * 
     *  @return The optional mask in attention, nullptr if no mask is set.
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @NoException(true) ITensor getMask();

    /**
     *  \brief Set whether the attention will run a causal inference.
     *  Cannot be used together with setMask().
     * 
     *  @see getCausal
     * 
     *  @return True if the causal inference is set successfully, false otherwise.
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean setCausal(@Cast("bool") boolean isCausal);

    /**
     *  \brief Get whether the attention will run a causal inference.
     * 
     *  @see setCausal
     * 
     *  @return True if the attention will run a causal inference, false otherwise. Default is false.
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean getCausal();

    /**
     *  \brief Set whether the attention can be decomposed to use multiple kernels if no fused kernel support found.
     * 
     *  @see getDecomposable
     * 
     *  @return True if the decomposable attention is set successfully, false otherwise.
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean setDecomposable(@Cast("bool") boolean decomposable);

    /**
     *  \brief Get whether the attention can be decomposed to use multiple kernels if no fused kernel support found.
     * 
     *  @return True if the attention can be decomposed to use multiple kernels by the compiler,
     *          false otherwise. Default is false.
     * 
     *  @see setDecomposable
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean getDecomposable();

    /**
     *  \brief Append or replace an input of this layer with a specific tensor.
     * 
     *  @param index the index of the input to modify.
     *  @param input the new input tensor.
     * 
     *  The indices are as follows:
     * 
     *  Input 0 is the input query tensor.
     *  Input 1 is the input key tensor.
     *  Input 2 is the input value tensor.
     * 
     *  @return True if the input tensor is set successfully, false otherwise.
     *  */
    
    
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean setInput(int index, @ByRef ITensor input);

    /**
     *  \brief Get the number of inputs of IAttention. IAttention has three inputs.
     * 
     *  @return The number of inputs of IAttention. */
    
    
    //!
    //!
    //!
    //!
    public native @NoException(true) int getNbInputs();

    /**
     *  \brief Get the IAttention input corresponding to the given index.
     * 
     *  @param index The index of the input tensor.
     * 
     *  @return The input tensor, or nullptr if the index is out of range.
     *  */
    
    
    //!
    //!
    public native @NoException(true) ITensor getInput(int index);

    /**
     *  \brief Get the number of outputs of a layer. IAttention has one output.
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @NoException(true) int getNbOutputs();

    /**
     *  \brief Get the IAttention output corresponding to the given index. IAttention has only one output.
     * 
     *  @param index The index of the output tensor.
     * 
     *  @return The indexed output tensor, or nullptr if the index is out of range.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @NoException(true) ITensor getOutput(int index);

    /**
     *  \brief Set the name of the attention.
     * 
     *  The name is used in error diagnostics.
     *  This method copies the name string.
     * 
     *  \warning The string name must be null-terminated, and be at most 4096 bytes including the terminator.
     * 
     *  @see getName()
     * 
     *  @return True if the name is set successfully, false otherwise.
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean setName(String name);
    public native @Cast("bool") @NoException(true) boolean setName(@Cast("const char*") BytePointer name);

    /**
     *  \brief Return the name of the attention.
     * 
     *  @see setName()
     * 
     *  @return The name of the attention.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    public native @NoException(true) String getName();

    /**
     *  \brief Set the quantization scale for the attention normalization output.
     * 
     *  @param tensor for quantization scale. Data type must be DataType::kFLOAT, DataType::kHALF or DataType::kBF16.
     *  Must be a 0-d or 1-d.
     * 
     *  @return True if the quantization scale is set successfully, false otherwise.
     * 
     *  \warning Must be used together with setNormalizationQuantizeToType to set normalization output datatype to
     *  DataType::kFP8 or DataType::kINT8.
     *  */
    
    
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean setNormalizationQuantizeScale(@ByRef ITensor tensor);

    /**
     *  \brief Get the quantization scale for the attention normalization output.
     * 
     *  @return The quantization scale for the attention normalization output or nullptr if no quantization scale is
     *  set.
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @NoException(true) ITensor getNormalizationQuantizeScale();

    /**
     *  \brief Set the datatype the attention normalization is quantized to.
     * 
     *  @param type the datatype the attention normalization is quantized to. Must be one of DataType::kFP8,
     *  DataType::kINT8.
     * 
     *  @return True if the quantization to type is set successfully, false otherwise.
     *  */
    
    
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean setNormalizationQuantizeToType(DataType type);
    public native @Cast("bool") @NoException(true) boolean setNormalizationQuantizeToType(@Cast("nvinfer1::DataType") int type);

    /**
     *  \brief Get the datatype the attention normalization is quantized to.
     * 
     *  @return The datatype the attention normalization is quantized to.
     *  The default value is DataType::kFLOAT.
     * 
     *  \warning Must be used after normalization quantization to type is set by setNormalizationQuantizeToType. */
    public native @NoException(true) DataType getNormalizationQuantizeToType();
}

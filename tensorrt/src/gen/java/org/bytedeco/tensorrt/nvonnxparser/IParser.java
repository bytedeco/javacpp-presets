// Targeted by JavaCPP version 1.5.13-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.tensorrt.nvonnxparser;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.javacpp.presets.javacpp.*;
import org.bytedeco.cuda.cudart.*;
import static org.bytedeco.cuda.global.cudart.*;
import org.bytedeco.cuda.cublas.*;
import static org.bytedeco.cuda.global.cublas.*;
import org.bytedeco.cuda.cudnn.*;
import static org.bytedeco.cuda.global.cudnn.*;
import org.bytedeco.cuda.nvrtc.*;
import static org.bytedeco.cuda.global.nvrtc.*;
import org.bytedeco.tensorrt.nvinfer.*;
import static org.bytedeco.tensorrt.global.nvinfer.*;
import org.bytedeco.tensorrt.nvinfer_plugin.*;
import static org.bytedeco.tensorrt.global.nvinfer_plugin.*;

import static org.bytedeco.tensorrt.global.nvonnxparser.*;


/**
 *  \class IParser
 * 
 *  \brief an object for parsing ONNX models into a TensorRT network definition
 * 
 *  \warning If the ONNX model has a graph output with the same name as a graph input,
 *           the output will be renamed by prepending "__".
 * 
 *  \warning Do not inherit from this class, as doing so will break forward-compatibility of the API and ABI.
 *  */
@Namespace("nvonnxparser") @Properties(inherit = org.bytedeco.tensorrt.presets.nvonnxparser.class)
public class IParser extends Pointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public IParser(Pointer p) { super(p); }

    /**
     *  \brief Parse a serialized ONNX model into the TensorRT network.
     *          This method has very limited diagnostics. If parsing the serialized model
     *          fails for any reason (e.g. unsupported IR version, unsupported opset, etc.)
     *          it the user responsibility to intercept and report the error.
     *          To obtain a better diagnostic, use the parseFromFile method below.
     * 
     *  @param serialized_onnx_model Pointer to the serialized ONNX model. Can be freed after this function returns.
     *  @param serialized_onnx_model_size Size of the serialized ONNX model
     *         in bytes
     *  @param model_path Absolute path to the model file for loading external weights if required
     *  @return true if the model was parsed successfully
     *  @see getNbErrors() getError()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean parse(
            @Const Pointer serialized_onnx_model, @Cast("size_t") long serialized_onnx_model_size, String model_path/*=nullptr*/);
    public native @Cast("bool") @NoException(true) boolean parse(
            @Const Pointer serialized_onnx_model, @Cast("size_t") long serialized_onnx_model_size);
    public native @Cast("bool") @NoException(true) boolean parse(
            @Const Pointer serialized_onnx_model, @Cast("size_t") long serialized_onnx_model_size, @Cast("const char*") BytePointer model_path/*=nullptr*/);

    /**
     *  \brief Parse an onnx model file, which can be a binary protobuf or a text onnx model
     *          calls parse method inside.
     * 
     *  @param onnxModelFile name
     *  @param verbosity Level
     * 
     *  @return true if the model was parsed successfully
     * 
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean parseFromFile(String onnxModelFile, int verbosity);
    public native @Cast("bool") @NoException(true) boolean parseFromFile(@Cast("const char*") BytePointer onnxModelFile, int verbosity);

    /**
     *  [DEPRECATED] Deprecated in TensorRT 10.1. See supportsModelV2.
     * 
     *  \brief Check whether TensorRT supports a particular ONNX model.
     *         If the function returns True, one can proceed to engine building
     *         without having to call \p parse or \p parseFromFile.
     * 
     *  @param serialized_onnx_model Pointer to the serialized ONNX model. Can be freed after this function returns.
     *  @param serialized_onnx_model_size Size of the serialized ONNX model
     *         in bytes
     *  @param sub_graph_collection Container to hold supported subgraphs
     *  @param model_path Absolute path to the model file for loading external weights if required
     *  @return true if the model is supported
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @Cast("bool") @Deprecated @NoException(true) boolean supportsModel(@Const Pointer serialized_onnx_model, @Cast("size_t") long serialized_onnx_model_size,
            @ByRef SubGraphCollection_t sub_graph_collection, String model_path/*=nullptr*/);
    public native @Cast("bool") @Deprecated @NoException(true) boolean supportsModel(@Const Pointer serialized_onnx_model, @Cast("size_t") long serialized_onnx_model_size,
            @ByRef SubGraphCollection_t sub_graph_collection);
    public native @Cast("bool") @Deprecated @NoException(true) boolean supportsModel(@Const Pointer serialized_onnx_model, @Cast("size_t") long serialized_onnx_model_size,
            @ByRef SubGraphCollection_t sub_graph_collection, @Cast("const char*") BytePointer model_path/*=nullptr*/);

    /**
     *  [DEPRECATED] Deprecated in TensorRT 10.13. See loadInitializer().
     * 
     * \brief Parse a serialized ONNX model into the TensorRT network
     *  with consideration of user provided weights
     * 
     *  @param serialized_onnx_model Pointer to the serialized ONNX model. Can be freed after this function returns.
     *  @param serialized_onnx_model_size Size of the serialized ONNX model
     *         in bytes
     *  @return true if the model was parsed successfully
     *  @see getNbErrors() getError()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @Cast("bool") @Deprecated @NoException(true) boolean parseWithWeightDescriptors(
            @Const Pointer serialized_onnx_model, @Cast("size_t") long serialized_onnx_model_size);

    /**
     * \brief Returns whether the specified operator may be supported by the
     *          parser.
     * 
     *  Note that a result of true does not guarantee that the operator will be
     *  supported in all cases (i.e., this function may return false-positives).
     * 
     *  @param op_name The name of the ONNX operator to check for support
     *  */
    
    
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean supportsOperator(String op_name);
    public native @Cast("bool") @NoException(true) boolean supportsOperator(@Cast("const char*") BytePointer op_name);

    /**
     * \brief Get the number of errors that occurred during prior calls to
     *          \p parse
     * 
     *  @see getError() clearErrors() IParserError
     *  */
    
    
    //!
    //!
    //!
    public native @NoException(true) int getNbErrors();

    /**
     * \brief Get an error that occurred during prior calls to \p parse
     * 
     *  @see getNbErrors() clearErrors() IParserError
     *  */
    
    
    //!
    //!
    //!
    public native @Const @NoException(true) IParserError getError(int index);

    /**
     * \brief Clear errors from prior calls to \p parse
     * 
     *  @see getNbErrors() getError() IParserError
     *  */
    public native @NoException(true) void clearErrors();

    /**
     *  \brief Query the plugin libraries needed to implement operations used by the parser in a version-compatible
     *  engine.
     * 
     *  This provides a list of plugin libraries on the filesystem needed to implement operations
     *  in the parsed network.  If you are building a version-compatible engine using this network,
     *  provide this list to IBuilderConfig::setPluginsToSerialize to serialize these plugins along
     *  with the version-compatible engine, or, if you want to ship these plugin libraries externally
     *  to the engine, ensure that IPluginRegistry::loadLibrary is used to load these libraries in the
     *  appropriate runtime before deserializing the corresponding engine.
     * 
     *  @param nbPluginLibs [out] Returns the number of plugin libraries in the array, or -1 if there was an error.
     *  @return Array of {@code nbPluginLibs} C-strings describing plugin library paths on the filesystem if nbPluginLibs > 0,
     *  or nullptr otherwise.  This array is owned by the IParser, and the pointers in the array are only valid until
     *  the next call to parse(), supportsModel(), parseFromFile(), or parseWithWeightDescriptors().
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Cast("const char**") @NoException(true) PointerPointer getUsedVCPluginLibraries(@Cast("int64_t*") @ByRef long[] nbPluginLibs);

    /**
     *  \brief Set the parser flags.
     * 
     *  The flags are listed in the OnnxParserFlag enum.
     * 
     *  @param OnnxParserFlags The flags used when parsing an ONNX model.
     * 
     *  \note This function will override the previous set flags, rather than bitwise ORing the new flag.
     * 
     *  @see getFlags()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @NoException(true) void setFlags(@Cast("nvonnxparser::OnnxParserFlags") int onnxParserFlags);

    /**
     *  \brief Get the parser flags. Defaults to 0.
     * 
     *  @return The parser flags as a bitmask.
     * 
     *  @see setFlags()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @Cast("nvonnxparser::OnnxParserFlags") @NoException(true) int getFlags();

    /**
     *  \brief clear a parser flag.
     * 
     *  clears the parser flag from the enabled flags.
     * 
     *  @see setFlags()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @NoException(true) void clearFlag(OnnxParserFlag onnxParserFlag);
    public native @NoException(true) void clearFlag(@Cast("nvonnxparser::OnnxParserFlag") int onnxParserFlag);

    /**
     *  \brief Set a single parser flag.
     * 
     *  Add the input parser flag to the already enabled flags.
     * 
     *  @see setFlags()
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @NoException(true) void setFlag(OnnxParserFlag onnxParserFlag);
    public native @NoException(true) void setFlag(@Cast("nvonnxparser::OnnxParserFlag") int onnxParserFlag);

    /**
     *  \brief Returns true if the parser flag is set
     * 
     *  @see getFlags()
     * 
     *  @return True if flag is set, false if unset.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean getFlag(OnnxParserFlag onnxParserFlag);
    public native @Cast("bool") @NoException(true) boolean getFlag(@Cast("nvonnxparser::OnnxParserFlag") int onnxParserFlag);

    /**
     * \brief Return the i-th output ITensor object for the ONNX layer "name".
     * 
     *  Return the i-th output ITensor object for the ONNX layer "name".
     *  If "name" is not found or i is out of range, return nullptr.
     *  In the case of multiple nodes sharing the same name this function will return
     *  the output tensors of the first instance of the node in the ONNX graph.
     * 
     *  @param name The name of the ONNX layer.
     * 
     *  @param i The index of the output. i must be in range [0, layer.num_outputs).
     *  */
    
    
    //!
    //!
    //!
    public native @Const @NoException(true) ITensor getLayerOutputTensor(String name, @Cast("int64_t") long i);
    public native @Const @NoException(true) ITensor getLayerOutputTensor(@Cast("const char*") BytePointer name, @Cast("int64_t") long i);

    /**
     *  \brief Check whether TensorRT supports a particular ONNX model.
     *             If the function returns True, one can proceed to engine building
     *             without having to call \p parse or \p parseFromFile.
     *             Results can be queried through \p getNbSubgraphs, \p isSubgraphSupported,
     *             \p getSubgraphNodes.
     * 
     *  @param serializedOnnxModel Pointer to the serialized ONNX model. Can be freed after this function returns.
     *  @param serializedOnnxModelSize Size of the serialized ONNX model in bytes
     *  @param modelPath Absolute path to the model file for loading external weights if required
     *  @return true if the model is supported
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean supportsModelV2(
            @Const Pointer serializedOnnxModel, @Cast("size_t") long serializedOnnxModelSize, String modelPath/*=nullptr*/);
    public native @Cast("bool") @NoException(true) boolean supportsModelV2(
            @Const Pointer serializedOnnxModel, @Cast("size_t") long serializedOnnxModelSize);
    public native @Cast("bool") @NoException(true) boolean supportsModelV2(
            @Const Pointer serializedOnnxModel, @Cast("size_t") long serializedOnnxModelSize, @Cast("const char*") BytePointer modelPath/*=nullptr*/);

    /**
     *  \brief Get the number of subgraphs. Calling this function before calling \p supportsModelV2 results in undefined
     *  behavior.
     * 
     * 
     *  @return Number of subgraphs.
     *  */
    
    
    //!
    //!
    //!
    //!
    public native @Cast("int64_t") @NoException(true) long getNbSubgraphs();

    /**
     *  \brief Returns whether the subgraph is supported. Calling this function before calling \p supportsModelV2
     *  results in undefined behavior.
     * 
     * 
     *  @param index Index of the subgraph.
     *  @return Whether the subgraph is supported.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean isSubgraphSupported(@Cast("const int64_t") long index);

    /**
     *  \brief Get the nodes of the specified subgraph. Calling this function before calling \p supportsModelV2 results
     *  in undefined behavior.
     * 
     * 
     *  @param index Index of the subgraph.
     *  @param subgraphLength Returns the length of the subgraph as reference.
     * 
     *  @return Pointer to the subgraph nodes array. This pointer is owned by the Parser.
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    public native @Cast("int64_t*") @NoException(true) LongPointer getSubgraphNodes(@Cast("const int64_t") long index, @Cast("int64_t*") @ByRef LongPointer subgraphLength);
    public native @Cast("int64_t*") @NoException(true) LongBuffer getSubgraphNodes(@Cast("const int64_t") long index, @Cast("int64_t*") @ByRef LongBuffer subgraphLength);
    public native @Cast("int64_t*") @NoException(true) long[] getSubgraphNodes(@Cast("const int64_t") long index, @Cast("int64_t*") @ByRef long[] subgraphLength);

    /**
     *  \brief Load a serialized ONNX model into the parser. Unlike the parse(), parseFromFile(), or
     *  parseWithWeightDescriptors() functions, this function does not immediately convert the model into a TensorRT
     *  INetworkDefinition. Using this function allows users to provide their own initializers for the ONNX model
     *  through the loadInitializer() function.
     * 
     *  Only one model can be loaded at a time. Subsequent calls to loadModelProto() will result in an error.
     * 
     *  To begin the conversion of the model into a TensorRT INetworkDefinition, use parseModelProto().
     * 
     *  @param serializedOnnxModel Pointer to the serialized ONNX model. Can be freed after this function returns.
     *  @param serializedOnnxModelSize Size of the serialized ONNX model in bytes.
     *  @param modelPath Absolute path to the model file for loading external weights if required.
     *  @return true if the model was loaded successfully
     *  @see getNbErrors() getError()
     *  */
    
    
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean loadModelProto(
            @Const Pointer serializedOnnxModel, @Cast("size_t") long serializedOnnxModelSize, String modelPath/*=nullptr*/);
    public native @Cast("bool") @NoException(true) boolean loadModelProto(
            @Const Pointer serializedOnnxModel, @Cast("size_t") long serializedOnnxModelSize);
    public native @Cast("bool") @NoException(true) boolean loadModelProto(
            @Const Pointer serializedOnnxModel, @Cast("size_t") long serializedOnnxModelSize, @Cast("const char*") BytePointer modelPath/*=nullptr*/);

    /**
     *  \brief Prompt the ONNX parser to load an initializer with user-provided binary data.
     *  The lifetime of the data must exceed the lifetime of the parser.
     * 
     *  All user-provided initializers must be provided prior to calling refitModelProto().
     * 
     *  This function can be called multiple times to specify the names of multiple initializers.
     * 
     *  Calling this function with an initializer previously specified will overwrite the previous instance.
     * 
     * 
     *  This function will return false if initializer validation fails. Possible validation errors are:
     *  * This function was called prior to loadModelProto().
     *  * The requested initializer was not found in the model.
     *  * The size of the data provided is different from the corresponding initializer in the model.
     * 
     *  @param name Name of the initializer.
     *  @param data Binary data containing the values of the initializer.
     *  @param size Size of the initializer in bytes.
     *  @return true if the initializer was loaded successfully
     *  @see loadModelProto()
     *  */
    
    //!
    //!
    public native @Cast("bool") @NoException(true) boolean loadInitializer(String name, @Const Pointer data, @Cast("size_t") long size);
    public native @Cast("bool") @NoException(true) boolean loadInitializer(@Cast("const char*") BytePointer name, @Const Pointer data, @Cast("size_t") long size);

    /** \brief Begin the parsing and conversion process of the loaded ONNX model into a TensorRT INetworkDefinition.
     * 
     *  @return true if conversion was successful
     *  @see getNbErrors() getError() loadModelProto() loadModelProtoFromFile()
     *  */
    public native @Cast("bool") @NoException(true) boolean parseModelProto();
}

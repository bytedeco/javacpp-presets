diff --git a/cmake/CMakeLists.txt b/cmake/CMakeLists.txt
index 603cc75fe..2ee1e60e2 100644
--- a/cmake/CMakeLists.txt
+++ b/cmake/CMakeLists.txt
@@ -299,8 +298,8 @@ endif()
 # Enable space optimization for gcc/clang
 # Cannot use "-ffunction-sections -fdata-sections" if we enable bitcode (iOS)
 if (NOT MSVC AND NOT onnxruntime_ENABLE_BITCODE)
-  string(APPEND CMAKE_CXX_FLAGS " -ffunction-sections -fdata-sections")
-  string(APPEND CMAKE_C_FLAGS " -ffunction-sections -fdata-sections")
+  string(APPEND CMAKE_CXX_FLAGS " -Wextra -I/usr/local/include")
+  string(APPEND CMAKE_C_FLAGS " -Wextra -I/usr/local/include")
 endif()
 
 if (onnxruntime_ENABLE_EAGER_MODE)
diff --git a/cmake/onnxruntime.cmake b/cmake/onnxruntime.cmake
index 64068a03a..78a495940 100644
--- a/cmake/onnxruntime.cmake
+++ b/cmake/onnxruntime.cmake
@@ -108,9 +108,9 @@ target_compile_definitions(onnxruntime PRIVATE VER_STRING=\"${VERSION_STRING}\")
 
 if(UNIX)
   if (APPLE)
-    set(ONNXRUNTIME_SO_LINK_FLAG " -Xlinker -dead_strip")
+    set(ONNXRUNTIME_SO_LINK_FLAG " -L/usr/local/lib -lomp")
   else()
-    set(ONNXRUNTIME_SO_LINK_FLAG " -Xlinker --version-script=${SYMBOL_FILE} -Xlinker --no-undefined -Xlinker --gc-sections -z noexecstack")
+    set(ONNXRUNTIME_SO_LINK_FLAG " -Xlinker --no-undefined -Xlinker -z -Xlinker noexecstack")
   endif()
 else()
   set(ONNXRUNTIME_SO_LINK_FLAG " -DEF:${SYMBOL_FILE}")
diff --git a/onnxruntime/core/session/provider_bridge_ort.cc b/onnxruntime/core/session/provider_bridge_ort.cc
index c730472f3..50808d07e 100644
--- a/onnxruntime/core/session/provider_bridge_ort.cc
+++ b/onnxruntime/core/session/provider_bridge_ort.cc
@@ -953,12 +953,32 @@ struct ProviderHostImpl : ProviderHost {
 #if defined(_MSC_VER) && !defined(__clang__)
 #pragma warning(pop)
 #endif
+
+#ifdef __APPLE__
+#include <mach-o/dyld.h>
+#endif
+
 struct ProviderSharedLibrary {
+
+  std::string libpath;
+
   bool Ensure() {
     if (handle_)
       return true;
 
     std::string full_path = Env::Default().GetRuntimePath() + std::string(LIBRARY_PREFIX "onnxruntime_providers_shared" LIBRARY_EXTENSION);
+#ifdef __APPLE__
+size_t len1 = strlen(LIBRARY_PREFIX "onnxruntime_providers_shared" LIBRARY_EXTENSION);
+for (uint32_t i = 0; i < _dyld_image_count(); i++) {
+    const char *libname = _dyld_get_image_name(i);
+    size_t len2 = strlen(libname);
+    if (len2 > len1 && strcmp(LIBRARY_PREFIX "onnxruntime_providers_shared" LIBRARY_EXTENSION, libname + len2 - len1) == 0) {
+        libpath.assign(libname, len2 - len1);
+        full_path = libname;
+        break;
+    }
+}
+#endif
     auto error = Env::Default().LoadDynamicLibrary(full_path, true /*shared_globals on unix*/, &handle_);
     if (!error.IsOK()) {
       LOGS_DEFAULT(ERROR) << error.ErrorMessage();
@@ -986,7 +1006,7 @@ struct ProviderSharedLibrary {
     }
   }
 
-  ProviderSharedLibrary() = default;
+  ProviderSharedLibrary() { Ensure(); };
   ~ProviderSharedLibrary() {
     // assert(!handle_); // We should already be unloaded at this point (disabled until Python shuts down deterministically)
   }
@@ -1017,6 +1037,18 @@ struct ProviderLibrary {
       return nullptr;
 
     std::string full_path = Env::Default().GetRuntimePath() + std::string(filename_);
+#ifdef __APPLE__
+full_path = s_library_shared.libpath + std::string(filename_);
+size_t len1 = strlen(filename_);
+for (uint32_t i = 0; i < _dyld_image_count(); i++) {
+    const char *libname = _dyld_get_image_name(i);
+    size_t len2 = strlen(libname);
+    if (len2 > len1 && strcmp(filename_, libname + len2 - len1) == 0) {
+        full_path = libname;
+        break;
+    }
+}
+#endif
     auto error = Env::Default().LoadDynamicLibrary(full_path, false, &handle_);
     if (!error.IsOK()) {
       LOGS_DEFAULT(ERROR) << error.ErrorMessage();
diff --git a/onnxruntime/core/providers/shared_library/provider_bridge_provider.cc b/onnxruntime/core/providers/shared_library/provider_bridge_provider.cc
index ccbcd47cd..5d8928c4f 100644
--- a/onnxruntime/core/providers/shared_library/provider_bridge_provider.cc
+++ b/onnxruntime/core/providers/shared_library/provider_bridge_provider.cc
@@ -95,18 +95,21 @@ struct OnUnload {
 
 } g_on_unload;
 
-void* CPUAllocator::Alloc(size_t size) { return g_host->CPUAllocator__Alloc(this, size); }
-void CPUAllocator::Free(void* p) { g_host->CPUAllocator__Free(this, p); }
+void* CPUAllocator::Alloc(size_t size) { g_host = Provider_GetHost(); return g_host->CPUAllocator__Alloc(this, size); }
+void CPUAllocator::Free(void* p) { g_host = Provider_GetHost(); g_host->CPUAllocator__Free(this, p); }
 
 AllocatorPtr CreateAllocator(const AllocatorCreationInfo& info) {
+  g_host = Provider_GetHost();
   return g_host->CreateAllocator(info);
 }
 
 void AllocatorManager::InsertAllocator(AllocatorPtr allocator) {
+  g_host = Provider_GetHost();
   return g_host->AllocatorManager__InsertAllocator(this, allocator);
 }
 
 AllocatorPtr AllocatorManager::GetAllocator(int id, OrtMemType mem_type) const {
+  g_host = Provider_GetHost();
   return g_host->AllocatorManager__GetAllocator(this, id, mem_type);
 }
 
@@ -206,10 +209,12 @@ MLDataType DataTypeImpl::GetSparseTensorType<MLFloat16>() { return Provider_GetH
 #endif
 
 Status IDataTransfer::CopyTensor(const Tensor& src, Tensor& dst) const {
+  g_host = Provider_GetHost();
   return g_host->IDataTransfer__CopyTensor(this, src, dst);
 }
 
 Status IDataTransfer::CopyTensors(const std::vector<SrcDstPair>& src_dst_pairs) const {
+  g_host = Provider_GetHost();
   return g_host->IDataTransfer__CopyTensors(this, src_dst_pairs);
 }
 #if !defined(DISABLE_SPARSE_TENSORS)
@@ -218,7 +223,7 @@ Status IDataTransfer::CopySparseTensors(const std::vector<SparseSrcDstPair>& src
 }
 #endif
 
-const Node& OpKernel::Node() const { return g_host->OpKernel__Node(this); }
+const Node& OpKernel::Node() const { g_host = Provider_GetHost(); return g_host->OpKernel__Node(this); }
 
 TensorShape::TensorShape(gsl::span<const int64_t> dims) {
   Allocate(dims.size());
@@ -246,6 +251,7 @@ int64_t TensorShape::Size() const {
 }
 
 int64_t TensorShape::SizeHelper(size_t start, size_t end) const {
+  g_host = Provider_GetHost();
   return g_host->TensorShape__SizeHelper(this, start, end);
 }
 
@@ -255,6 +261,7 @@ TensorShape TensorShape::Slice(size_t dimstart, size_t dimend) const {
 }
 
 std::string TensorShape::ToString() const {
+  g_host = Provider_GetHost();
   return g_host->TensorShape__ToString(this);
 }
 
@@ -264,86 +271,105 @@ int64_t TensorShape::SizeFromDimension(size_t dimension) const { return g_host->
 std::ostream& operator<<(std::ostream& out, const TensorShape& shape) { return g_host->operator_left_shift(out, shape); }
 
 AllocatorPtr CreateAllocator(AllocatorCreationInfo info) {
+  g_host = Provider_GetHost();
   return g_host->CreateAllocator(info);
 }
 
 std::unique_ptr<IAllocator> CreateCPUAllocator(const OrtMemoryInfo& info) {
+  g_host = Provider_GetHost();
   return g_host->CreateCPUAllocator(info);
 }
 
 bool IAllocator::CalcMemSizeForArrayWithAlignment(size_t nmemb, size_t size, size_t alignment, size_t* out) noexcept {
+  g_host = Provider_GetHost();
   return g_host->IAllocator__CalcMemSizeForArrayWithAlignment(nmemb, size, alignment, out);
 }
 
 AllocatorPtr IExecutionProvider::GetAllocator(int id, OrtMemType mem_type) const {
+  g_host = Provider_GetHost();
   return g_host->IExecutionProvider__GetAllocator(this, id, mem_type);
 }
 
 void IExecutionProvider::InsertAllocator(AllocatorPtr allocator) {
+  g_host = Provider_GetHost();
   g_host->IExecutionProvider__InsertAllocator(this, allocator);
 }
 
 void IExecutionProvider::TryInsertAllocator(AllocatorPtr allocator) {
+  g_host = Provider_GetHost();
   g_host->IExecutionProvider__TryInsertAllocator(this, allocator);
 }
 
 std::vector<std::unique_ptr<ComputeCapability>> IExecutionProvider::GetCapability(const onnxruntime::GraphViewer& graph_viewer,
                                                                                   const std::vector<const KernelRegistry*>& kernel_registries) const {
+  g_host = Provider_GetHost();
   return g_host->IExecutionProvider__GetCapability(this, graph_viewer, kernel_registries);
 }
 
 common::Status IExecutionProvider::Compile(const std::vector<onnxruntime::Node*>& fused_nodes,
                                            std::vector<NodeComputeInfo>& node_compute_funcs) {
+  g_host = Provider_GetHost();
   return g_host->IExecutionProvider__Compile(this, fused_nodes, node_compute_funcs);
 }
 
 common::Status IExecutionProvider::Compile(const std::vector<onnxruntime::Node*>& fused_nodes,
                                            std::string& dll_path) {
+  g_host = Provider_GetHost();
   return g_host->IExecutionProvider__Compile(this, fused_nodes, dll_path);
 }
 
 common::Status IExecutionProvider::Compile(const std::vector<FusedNodeAndGraph>& fused_nodes_and_graphs,
                                            std::vector<NodeComputeInfo>& node_compute_funcs) {
+  g_host = Provider_GetHost();
   return g_host->IExecutionProvider__Compile(this, fused_nodes_and_graphs, node_compute_funcs);
 }
 
 int IExecutionProvider::GenerateMetaDefId(const onnxruntime::GraphViewer& graph_viewer, HashValue& model_hash) const {
+  g_host = Provider_GetHost();
   return g_host->IExecutionProvider__GenerateMetaDefId(this, graph_viewer, model_hash);
 }
 
 void IExecutionProvider::RegisterAllocator(std::shared_ptr<AllocatorManager> allocator_manager) {
+  g_host = Provider_GetHost();
   return g_host->IExecutionProvider__RegisterAllocator(this, allocator_manager);
 }
 
 #ifdef USE_TENSORRT
 std::unique_ptr<IAllocator> CreateCUDAAllocator(int16_t device_id, const char* name) {
+  g_host = Provider_GetHost();
   return g_host->CreateCUDAAllocator(device_id, name);
 }
 
 std::unique_ptr<IAllocator> CreateCUDAPinnedAllocator(int16_t device_id, const char* name) {
+  g_host = Provider_GetHost();
   return g_host->CreateCUDAPinnedAllocator(device_id, name);
 }
 
 std::unique_ptr<IDataTransfer> CreateGPUDataTransfer(void* stream) {
+  g_host = Provider_GetHost();
   return g_host->CreateGPUDataTransfer(stream);
 }
 #endif
 
 #ifdef USE_MIGRAPHX
 std::unique_ptr<IAllocator> CreateROCMAllocator(int16_t device_id, const char* name) {
+  g_host = Provider_GetHost();
   return g_host->CreateROCMAllocator(device_id, name);
 }
 
 std::unique_ptr<IAllocator> CreateROCMPinnedAllocator(int16_t device_id, const char* name) {
+  g_host = Provider_GetHost();
   return g_host->CreateROCMPinnedAllocator(device_id, name);
 }
 
 std::unique_ptr<IDataTransfer> CreateGPUDataTransfer(void* stream) {
+  g_host = Provider_GetHost();
   return g_host->CreateGPUDataTransfer(stream);
 }
 #endif
 
 std::string GetEnvironmentVar(const std::string& var_name) {
+  g_host = Provider_GetHost();
   return g_host->GetEnvironmentVar(var_name);
 }
 
@@ -351,6 +377,7 @@ std::unordered_set<NodeIndex> GetCpuPreferredNodes(const onnxruntime::GraphViewe
                                                    const std::string& provider_type,
                                                    gsl::span<const KernelRegistry* const> kernel_registries,
                                                    gsl::span<const NodeIndex> tentative_nodes) {
+  g_host = Provider_GetHost();
   return g_host->GetCpuPreferredNodes(graph, provider_type, kernel_registries, tentative_nodes);
 }
 
@@ -437,10 +464,12 @@ std::vector<std::string> GetStackTrace() { return g_host->GetStackTrace(); }
 
 void LogRuntimeError(uint32_t session_id, const common::Status& status,
                      const char* file, const char* function, uint32_t line) {
+  g_host = Provider_GetHost();
   return g_host->LogRuntimeError(session_id, status, file, function, line);
 }
 
 std::unique_ptr<OpKernelInfo> CopyOpKernelInfo(const OpKernelInfo& info) {
+  g_host = Provider_GetHost();
   return g_host->CopyOpKernelInfo(info);
 }
 
diff --git a/tools/ci_build/build.py b/tools/ci_build/build.py
index 55f3690b7..f7ee54ea5 100644
--- a/tools/ci_build/build.py
+++ b/tools/ci_build/build.py
@@ -738,6 +738,9 @@ def generate_build_tree(cmake_path, source_dir, build_dir, cuda_home, cudnn_home
         "-Donnxruntime_RUN_ONNX_TESTS=" + ("ON" if args.enable_onnx_tests else "OFF"),
         "-Donnxruntime_BUILD_WINML_TESTS=" + ("OFF" if args.skip_winml_tests else "ON"),
         "-Donnxruntime_GENERATE_TEST_REPORTS=ON",
+        "-Donnxruntime_DEV_MODE=OFF",
+        "-Donnxruntime_ENABLE_LTO=OFF",
+        "-Donnxruntime_BUILD_UNIT_TESTS=OFF",
         # There are two ways of locating python C API header file. "find_package(PythonLibs 3.5 REQUIRED)"
         # and "find_package(Python 3.5 COMPONENTS Development.Module)". The first one is deprecated and it
         # depends on the "PYTHON_EXECUTABLE" variable. The second needs "Python_EXECUTABLE". Here we set both
@@ -2247,6 +2250,9 @@ def main():
             path_to_protoc_exe = build_protoc_for_host(
                 cmake_path, source_dir, build_dir, args)
 
+        if is_linux() and args.arm64:
+            cmake_extra_args = ["-DCMAKE_SYSTEM_NAME=Linux", "-DCMAKE_SYSTEM_PROCESSOR=aarch64"]
+
         if is_ubuntu_1604():
             if (args.arm or args.arm64):
                 raise BuildError(
@@ -2262,11 +2268,7 @@ def main():
             setup_test_data(build_dir, configs)
 
         if args.use_cuda and args.cuda_version is None:
-            if is_windows():
-                # cuda_version is used while generating version_info.py on Windows.
-                raise BuildError("cuda_version must be specified on Windows.")
-            else:
-                args.cuda_version = ""
+            args.cuda_version = ""
         if args.use_rocm and args.rocm_version is None:
             args.rocm_version = ""
 

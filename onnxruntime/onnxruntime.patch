diff --git a/cmake/adjust_global_compile_flags.cmake b/cmake/adjust_global_compile_flags.cmake
index 8f5ef15c53..348f70ef27 100644
--- a/cmake/adjust_global_compile_flags.cmake
+++ b/cmake/adjust_global_compile_flags.cmake
@@ -9,8 +9,8 @@ endif()
 # Enable space optimization for gcc/clang
 # Cannot use "-ffunction-sections -fdata-sections" if we enable bitcode (iOS)
 if (NOT MSVC AND NOT onnxruntime_ENABLE_BITCODE)
-  string(APPEND CMAKE_CXX_FLAGS " -ffunction-sections -fdata-sections")
-  string(APPEND CMAKE_C_FLAGS " -ffunction-sections -fdata-sections")
+  string(APPEND CMAKE_CXX_FLAGS " -Wextra -I/usr/local/include")
+  string(APPEND CMAKE_C_FLAGS " -Wextra -I/usr/local/include")
 endif()
 
 if (CMAKE_SYSTEM_NAME STREQUAL "Emscripten")
diff --git a/cmake/onnxruntime.cmake b/cmake/onnxruntime.cmake
index 6c1d4485eb..14d72dacef 100644
--- a/cmake/onnxruntime.cmake
+++ b/cmake/onnxruntime.cmake
@@ -144,9 +144,9 @@ target_compile_definitions(onnxruntime PRIVATE FILE_NAME=\"onnxruntime.dll\")
 
 if(UNIX)
   if (APPLE)
-    target_link_options(onnxruntime PRIVATE "LINKER:-dead_strip")
+    target_link_options(onnxruntime PRIVATE "LINKER:-L/usr/local/lib")
   elseif(NOT CMAKE_SYSTEM_NAME MATCHES "AIX")
-    target_link_options(onnxruntime PRIVATE  "LINKER:--version-script=${SYMBOL_FILE}" "LINKER:--no-undefined" "LINKER:--gc-sections")
+    target_link_options(onnxruntime PRIVATE "LINKER:--no-undefined")
   endif()
 else()
   target_link_options(onnxruntime PRIVATE  "-DEF:${SYMBOL_FILE}")
diff --git a/onnxruntime/core/providers/shared_library/provider_bridge_provider.cc b/onnxruntime/core/providers/shared_library/provider_bridge_provider.cc
index eee6a05f12..3e3d295dce 100644
--- a/onnxruntime/core/providers/shared_library/provider_bridge_provider.cc
+++ b/onnxruntime/core/providers/shared_library/provider_bridge_provider.cc
@@ -115,10 +115,11 @@ struct OnUnload {
 
 } g_on_unload;
 
-void* CPUAllocator::Alloc(size_t size) { return g_host->CPUAllocator__Alloc(this, size); }
-void CPUAllocator::Free(void* p) { g_host->CPUAllocator__Free(this, p); }
+void* CPUAllocator::Alloc(size_t size) { g_host = Provider_GetHost(); return g_host->CPUAllocator__Alloc(this, size); }
+void CPUAllocator::Free(void* p) { g_host = Provider_GetHost(); g_host->CPUAllocator__Free(this, p); }
 
 AllocatorPtr CreateAllocator(const AllocatorCreationInfo& info) {
+  g_host = Provider_GetHost();
   return g_host->CreateAllocator(info);
 }
 
@@ -262,19 +263,22 @@ MLDataType DataTypeImpl::GetSparseTensorType<Float8E5M2FNUZ>() { return Provider
 #endif
 
 Status IDataTransfer::CopyTensor(const Tensor& src, Tensor& dst) const {
+  g_host = Provider_GetHost();
   return g_host->IDataTransfer__CopyTensor(this, src, dst);
 }
 
 Status IDataTransfer::CopyTensors(const std::vector<SrcDstPair>& src_dst_pairs) const {
+  g_host = Provider_GetHost();
   return g_host->IDataTransfer__CopyTensors(this, src_dst_pairs);
 }
 #if !defined(DISABLE_SPARSE_TENSORS)
 Status IDataTransfer::CopySparseTensors(const std::vector<SparseSrcDstPair>& src_dst_pairs) const {
+  g_host = Provider_GetHost();
   return g_host->IDataTransfer__CopySparseTensors(this, src_dst_pairs);
 }
 #endif
 
-const Node& OpKernel::Node() const { return g_host->OpKernel__Node(this); }
+const Node& OpKernel::Node() const { g_host = Provider_GetHost(); return g_host->OpKernel__Node(this); }
 
 TensorShape::TensorShape(gsl::span<const int64_t> dims) {
   Allocate(dims.size());
@@ -282,16 +286,19 @@ TensorShape::TensorShape(gsl::span<const int64_t> dims) {
 }
 
 TensorShape& TensorShape::operator=(const TensorShape& other) {
+  g_host = Provider_GetHost();
   g_host->TensorShape__operator_assign(this, other);
   return *this;
 }
 
 TensorShape& TensorShape::operator=(TensorShape&& other) noexcept {
+  g_host = Provider_GetHost();
   g_host->TensorShape__operator_move_assign(this, std::move(other));
   return *this;
 }
 
 void TensorShape::Allocate(size_t size) {
+  g_host = Provider_GetHost();
   g_host->TensorShape__Allocate(this, size);
 }
 
@@ -302,6 +309,7 @@ int64_t TensorShape::Size() const {
 }
 
 int64_t TensorShape::SizeHelper(size_t start, size_t end) const {
+  g_host = Provider_GetHost();
   return g_host->TensorShape__SizeHelper(this, start, end);
 }
 
@@ -311,28 +319,33 @@ TensorShape TensorShape::Slice(size_t dimstart, size_t dimend) const {
 }
 
 std::string TensorShape::ToString() const {
+  g_host = Provider_GetHost();
   return g_host->TensorShape__ToString(this);
 }
 
-int64_t TensorShape::SizeToDimension(size_t dimension) const { return g_host->TensorShape__SizeToDimension(this, dimension); }
-int64_t TensorShape::SizeFromDimension(size_t dimension) const { return g_host->TensorShape__SizeFromDimension(this, dimension); }
+int64_t TensorShape::SizeToDimension(size_t dimension) const { g_host = Provider_GetHost(); return g_host->TensorShape__SizeToDimension(this, dimension); }
+int64_t TensorShape::SizeFromDimension(size_t dimension) const { g_host = Provider_GetHost(); return g_host->TensorShape__SizeFromDimension(this, dimension); }
 
-std::ostream& operator<<(std::ostream& out, const TensorShape& shape) { return g_host->operator_left_shift(out, shape); }
+std::ostream& operator<<(std::ostream& out, const TensorShape& shape) { g_host = Provider_GetHost(); return g_host->operator_left_shift(out, shape); }
 
 AllocatorPtr CreateAllocator(AllocatorCreationInfo info) {
+  g_host = Provider_GetHost();
   return g_host->CreateAllocator(info);
 }
 
 std::unique_ptr<IAllocator> CreateCPUAllocator(const OrtMemoryInfo& info) {
+  g_host = Provider_GetHost();
   return g_host->CreateCPUAllocator(info);
 }
 
 bool IAllocator::CalcMemSizeForArrayWithAlignment(size_t nmemb, size_t size, size_t alignment, size_t* out) noexcept {
+  g_host = Provider_GetHost();
   return g_host->IAllocator__CalcMemSizeForArrayWithAlignment(nmemb, size, alignment, out);
 }
 
 std::unique_ptr<IExecutionProvider> IExecutionProviderFactory::CreateProvider(
     const OrtSessionOptions& session_options, const OrtLogger& session_logger) {
+  g_host = Provider_GetHost();
   return g_host->IExecutionProviderFactory__CreateProvider(this, session_options, session_logger);
 }
 
@@ -340,38 +353,46 @@ std::vector<std::unique_ptr<ComputeCapability>> IExecutionProvider::GetCapabilit
                                                                                   const IKernelLookup& kernel_lookup,
                                                                                   const GraphOptimizerRegistry& graph_optimizer_registry,
                                                                                   IResourceAccountant* resource_accountant) const {
+  g_host = Provider_GetHost();
   return g_host->IExecutionProvider__GetCapability(this, graph_viewer, kernel_lookup, graph_optimizer_registry, resource_accountant);
 }
 common::Status IExecutionProvider::Compile(const std::vector<FusedNodeAndGraph>& fused_nodes_and_graphs,
                                            std::vector<NodeComputeInfo>& node_compute_funcs) {
+  g_host = Provider_GetHost();
   return g_host->IExecutionProvider__Compile(this, fused_nodes_and_graphs, node_compute_funcs);
 }
 
 #if defined(USE_TENSORRT) || defined(USE_NV)
 std::unique_ptr<IAllocator> CreateCUDAAllocator(int16_t device_id, const char* name) {
+  g_host = Provider_GetHost();
   return g_host->CreateCUDAAllocator(device_id, name);
 }
 
 std::unique_ptr<IAllocator> CreateCUDAPinnedAllocator(const char* name) {
+  g_host = Provider_GetHost();
   return g_host->CreateCUDAPinnedAllocator(name);
 }
 
 std::unique_ptr<IDataTransfer> CreateGPUDataTransfer() {
+  g_host = Provider_GetHost();
   return g_host->CreateGPUDataTransfer();
 }
 #endif
 
 #ifdef USE_MIGRAPHX
 std::unique_ptr<IAllocator> CreateMIGraphXAllocator(int16_t device_id, const char* name) {
+  g_host = Provider_GetHost();
   return g_host->CreateMIGraphXAllocator(device_id, name);
 }
 
 std::unique_ptr<IAllocator> CreateMIGraphXPinnedAllocator(int16_t device_id, const char* name) {
+  g_host = Provider_GetHost();
   return g_host->CreateMIGraphXPinnedAllocator(device_id, name);
 }
 #endif
 
 std::string GetEnvironmentVar(const std::string& var_name) {
+  g_host = Provider_GetHost();
   return g_host->GetEnvironmentVar(var_name);
 }
 
@@ -379,20 +400,21 @@ std::unordered_set<NodeIndex> GetCpuPreferredNodes(const onnxruntime::GraphViewe
                                                    const IExecutionProvider::IKernelLookup& kernel_lookup,
                                                    gsl::span<const NodeIndex> tentative_nodes,
                                                    const logging::Logger& logger) {
+  g_host = Provider_GetHost();
   return g_host->GetCpuPreferredNodes(graph, kernel_lookup, tentative_nodes, logger);
 }
 
 namespace profiling {
 
-std::string demangle(const char* name) { return g_host->demangle(name); }
-std::string demangle(const std::string& name) { return g_host->demangle(name); }
+std::string demangle(const char* name) { g_host = Provider_GetHost(); return g_host->demangle(name); }
+std::string demangle(const std::string& name) { g_host = Provider_GetHost(); return g_host->demangle(name); }
 
 }  // namespace profiling
 
 namespace logging {
 
-unsigned int GetThreadId() { return g_host->GetThreadId(); }
-unsigned int GetProcessId() { return g_host->GetProcessId(); }
+unsigned int GetThreadId() { g_host = Provider_GetHost(); return g_host->GetThreadId(); }
+unsigned int GetProcessId() { g_host = Provider_GetHost(); return g_host->GetProcessId(); }
 
 const char* Category::onnxruntime = "onnxruntime";
 
@@ -429,7 +451,7 @@ const std::string& Status::ErrorMessage() const noexcept {
   return IsOK() ? EmptyString() : state_->msg;
 }
 
-std::string Status::ToString() const { return g_host->Status__ToString(this); }
+std::string Status::ToString() const { g_host = Provider_GetHost(); return g_host->Status__ToString(this); }
 
 const std::string& Status::EmptyString() noexcept {
   static std::string s_empty;
@@ -439,8 +461,8 @@ const std::string& Status::EmptyString() noexcept {
 }  // namespace common
 
 namespace math {
-uint16_t floatToHalf(float f) { return g_host->math__floatToHalf(f); }
-float halfToFloat(uint16_t h) { return g_host->math__halfToFloat(h); }
+uint16_t floatToHalf(float f) { g_host = Provider_GetHost(); return g_host->math__floatToHalf(f); }
+float halfToFloat(uint16_t h) { g_host = Provider_GetHost(); return g_host->math__halfToFloat(h); }
 
 }  // namespace math
 
@@ -449,22 +471,26 @@ namespace sparse_utils {
 #if !defined(ORT_MINIMAL_BUILD)
 Status DenseTensorToSparseCsr(const DataTransferManager& data_manager, const Tensor& src, const AllocatorPtr& cpu_allocator,
                               const AllocatorPtr& dst_allocator, SparseTensor& dst) {
+  g_host = Provider_GetHost();
   return g_host->sparse_utils__DenseTensorToSparseCsr(data_manager, src, cpu_allocator, dst_allocator, dst);
 }
 
 Status SparseCsrToDenseTensor(const DataTransferManager& data_manager, const SparseTensor& src, const AllocatorPtr& cpu_allocator,
                               const AllocatorPtr& dst_allocator, Tensor& dst) {
+  g_host = Provider_GetHost();
   return g_host->sparse_utils__SparseCsrToDenseTensor(data_manager, src, cpu_allocator, dst_allocator, dst);
 }
 
 Status SparseCooToDenseTensor(const DataTransferManager& data_manager, const SparseTensor& src, const AllocatorPtr& cpu_allocator,
                               const AllocatorPtr& dst_allocator, Tensor& dst) {
+  g_host = Provider_GetHost();
   return g_host->sparse_utils__SparseCooToDenseTensor(data_manager, src, cpu_allocator, dst_allocator, dst);
 }
 #endif  // !ORT_MINIMAL_BUILD
 
 Status DenseTensorToSparseCoo(const DataTransferManager& data_manager, const Tensor& src, const AllocatorPtr& cpu_allocator,
                               const AllocatorPtr& dst_allocator, bool linear_indexs, SparseTensor& dst) {
+  g_host = Provider_GetHost();
   return g_host->sparse_utils__DenseTensorToSparseCoo(data_manager, src, cpu_allocator, dst_allocator, linear_indexs, dst);
 }
 #endif  // !defined(DISABLE_SPARSE_TENSORS)
@@ -475,10 +501,12 @@ std::vector<std::string> GetStackTrace() { return g_host->GetStackTrace(); }
 
 void LogRuntimeError(uint32_t session_id, const common::Status& status,
                      const char* file, const char* function, uint32_t line) {
+  g_host = Provider_GetHost();
   return g_host->LogRuntimeError(session_id, status, file, function, line);
 }
 
 std::unique_ptr<OpKernelInfo> CopyOpKernelInfo(const OpKernelInfo& info) {
+  g_host = Provider_GetHost();
   return g_host->CopyOpKernelInfo(info);
 }
 
diff --git a/onnxruntime/core/session/provider_bridge_ort.cc b/onnxruntime/core/session/provider_bridge_ort.cc
index 7fcaee4858..310214c0b4 100644
--- a/onnxruntime/core/session/provider_bridge_ort.cc
+++ b/onnxruntime/core/session/provider_bridge_ort.cc
@@ -1686,13 +1686,33 @@ struct ProviderHostImpl : ProviderHost {
 #if defined(_MSC_VER) && !defined(__clang__)
 #pragma warning(pop)
 #endif
+
+#ifdef __APPLE__
+#include <mach-o/dyld.h>
+#endif
+
 struct ProviderSharedLibrary {
+
+  std::string libpath;
+
   void Ensure() {
     if (handle_)
       return;
 
     auto full_path = Env::Default().GetRuntimePath() +
                      PathString(LIBRARY_PREFIX ORT_TSTR("onnxruntime_providers_shared") LIBRARY_EXTENSION);
+#ifdef __APPLE__
+size_t len1 = strlen(LIBRARY_PREFIX "onnxruntime_providers_shared" LIBRARY_EXTENSION);
+for (uint32_t i = 0; i < _dyld_image_count(); i++) {
+    const char *libname = _dyld_get_image_name(i);
+    size_t len2 = strlen(libname);
+    if (len2 > len1 && strcmp(LIBRARY_PREFIX "onnxruntime_providers_shared" LIBRARY_EXTENSION, libname + len2 - len1) == 0) {
+        libpath.assign(libname, len2 - len1);
+        full_path = libname;
+        break;
+    }
+}
+#endif
     ORT_THROW_IF_ERROR(Env::Default().LoadDynamicLibrary(full_path, true /*shared_globals on unix*/, &handle_));
 
     void (*PProvider_SetHost)(void*);
@@ -1711,7 +1731,7 @@ struct ProviderSharedLibrary {
     }
   }
 
-  ProviderSharedLibrary() = default;
+  ProviderSharedLibrary() { Ensure(); };
   ~ProviderSharedLibrary() {
     // assert(!handle_); // We should already be unloaded at this point (disabled until Python shuts down deterministically)
   }
@@ -1756,6 +1776,18 @@ Status ProviderLibrary::Load() {
       ORT_RETURN_IF_ERROR(Env::Default().LoadDynamicLibrary(filename_, false, &handle_));
     } else {
       auto full_path = Env::Default().GetRuntimePath() + filename_;
+#ifdef __APPLE__
+full_path = s_library_shared.libpath + std::string(filename_);
+size_t len1 = strlen(filename_);
+for (uint32_t i = 0; i < _dyld_image_count(); i++) {
+    const char *libname = _dyld_get_image_name(i);
+    size_t len2 = strlen(libname);
+    if (len2 > len1 && strcmp(filename_, libname + len2 - len1) == 0) {
+        full_path = libname;
+        break;
+    }
+}
+#endif
       ORT_RETURN_IF_ERROR(Env::Default().LoadDynamicLibrary(full_path, false, &handle_));
     }
 
diff --git a/tools/ci_build/build.py b/tools/ci_build/build.py
index 1835c0b41f..a7a7b4a9bf 100644
--- a/tools/ci_build/build.py
+++ b/tools/ci_build/build.py
@@ -417,6 +417,9 @@ def generate_build_tree(
     cmake_args += [
         "-Donnxruntime_RUN_ONNX_TESTS=" + ("ON" if args.enable_onnx_tests else "OFF"),
         "-Donnxruntime_GENERATE_TEST_REPORTS=ON",
+        "-Donnxruntime_DEV_MODE=OFF",
+        "-Donnxruntime_ENABLE_LTO=OFF",
+        "-Donnxruntime_BUILD_UNIT_TESTS=OFF",
         "-DPython_EXECUTABLE=" + sys.executable,
         "-Donnxruntime_USE_VCPKG=" + ("ON" if args.use_vcpkg else "OFF"),
         "-Donnxruntime_USE_MIMALLOC=" + ("ON" if args.use_mimalloc else "OFF"),
@@ -2406,6 +2409,9 @@ def main():
             log.info("Activating emsdk...")
             run_subprocess([emsdk_file, "activate", emsdk_version], cwd=emsdk_dir)
 
+        if is_linux() and args.arm64:
+            cmake_extra_args = ["-DCMAKE_SYSTEM_NAME=Linux", "-DCMAKE_SYSTEM_PROCESSOR=aarch64"]
+
         if args.enable_pybind and is_windows():
             run_subprocess(
                 [sys.executable, "-m", "pip", "install", "-r", "requirements/pybind/requirements.txt"],
diff --git a/tools/ci_build/build_args.py b/tools/ci_build/build_args.py
index 215ad77335..0584b0533c 100644
--- a/tools/ci_build/build_args.py
+++ b/tools/ci_build/build_args.py
@@ -241,6 +241,11 @@ def add_documentation_args(parser: argparse.ArgumentParser) -> None:
 
 def add_cross_compile_args(parser: argparse.ArgumentParser) -> None:
     """Adds arguments for cross-compiling to non-Windows target CPU architectures."""
+    parser.add_argument(
+        "--arm64",
+        action="store_true",
+        help="[Windows cross-compiling] Target Windows ARM64.",
+    )
     parser.add_argument(
         "--rv64",
         action="store_true",
@@ -409,11 +414,6 @@ def add_windows_specific_args(parser: argparse.ArgumentParser) -> None:
         action="store_true",
         help="[Windows cross-compiling] Target Windows ARM.",
     )
-    parser.add_argument(
-        "--arm64",
-        action="store_true",
-        help="[Windows cross-compiling] Target Windows ARM64.",
-    )
     parser.add_argument(
         "--arm64ec",
         action="store_true",
diff --git a/onnxruntime/core/optimizer/transpose_optimization/optimizer_api.h b/onnxruntime/core/optimizer/transpose_optimization/optimizer_api.h
--- a/onnxruntime/core/optimizer/transpose_optimization/optimizer_api.h
+++ b/onnxruntime/core/optimizer/transpose_optimization/optimizer_api.h
@@ -11,6 +11,7 @@
 #include <unordered_map>
 #include <unordered_set>
 #include <vector>
+#include <cstdint>
 
 namespace onnx_transpose_optimization {
 namespace api {

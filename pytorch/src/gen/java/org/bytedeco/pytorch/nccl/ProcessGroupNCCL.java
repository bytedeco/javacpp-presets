// Targeted by JavaCPP version 1.5.13-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.pytorch.nccl;

import org.bytedeco.pytorch.Allocator;
import org.bytedeco.pytorch.Backend;
import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.javacpp.presets.javacpp.*;
import org.bytedeco.cuda.cudart.*;
import static org.bytedeco.cuda.global.cudart.*;
import org.bytedeco.cuda.nccl.*;
import static org.bytedeco.cuda.global.nccl.*;
import static org.bytedeco.openblas.global.openblas_nolapack.*;
import static org.bytedeco.openblas.global.openblas.*;
import org.bytedeco.javacpp.chrono.*;
import static org.bytedeco.javacpp.global.chrono.*;
import org.bytedeco.pytorch.*;
import static org.bytedeco.pytorch.global.torch.*;
import org.bytedeco.cuda.cublas.*;
import static org.bytedeco.cuda.global.cublas.*;
import org.bytedeco.cuda.cudnn.*;
import static org.bytedeco.cuda.global.cudnn.*;
import org.bytedeco.cuda.cusparse.*;
import static org.bytedeco.cuda.global.cusparse.*;
import org.bytedeco.cuda.cusolver.*;
import static org.bytedeco.cuda.global.cusolver.*;
import org.bytedeco.cuda.cupti.*;
import static org.bytedeco.cuda.global.cupti.*;
import org.bytedeco.pytorch.cuda.*;
import static org.bytedeco.pytorch.global.torch_cuda.*;

import static org.bytedeco.pytorch.global.torch_nccl.*;


// ProcessGroupNCCL implements NCCL bindings for c10d.
//
// All functions of the class are expected to be called in the same order
// across all processes in the process group.  This is the only way that we
// can guarantee to match up the same calls among all processes.
//
// All NCCL functions provided by this class are asynchronous functions. More
// specifically, each NCCL call is scheduled on a separate CUDA stream that is
// different from the current CUDA stream. This is for the purpose of
// achieving potentially concurrency and better performance. As a result,
// it is the callers' responsibility to make sure that the CUDA stream their
// code works on needs to wait for the NCCL operation from
// this class.
//
// This can be done by calling:
//
// either WorkNCCL::wait() or WorkNCCL::synchronize(), both achieves the same
// functionality and are synonyms.
//
// Also note that WorkNCCL::finishedGPUExecution() is a helper function only
// provided by ProcessGroupNCCL to check if the NCCL operation of WorkNCCL has
// finished execution on the GPU (not just scheduled).
//
// Example on using the NCCL process group
//
//   ProcessGroupNCCL pg(store, rank, size);
//   std::shared_ptr<WorkNCCL> work = pg.allreduce(tensors);
//
//   // At this point, NCCL kernel has already by queued successfully
//   // Now, let current stream wait for the NCCL to finish, this function is
//   // async operation as well
//
//   work->wait()
//
//   // Now continue on other work in the current stream.
@Namespace("c10d") @NoOffset @Properties(inherit = org.bytedeco.pytorch.presets.torch_nccl.class)
public class ProcessGroupNCCL extends Backend {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public ProcessGroupNCCL(Pointer p) { super(p); }

  @NoOffset public static class WorkNCCL extends Work {
      static { Loader.load(); }
      /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
      public WorkNCCL(Pointer p) { super(p); }
  

    // Constructor takes a list of CUDA devices
    public WorkNCCL(
            @StdString BytePointer pgUID,
            @StdString BytePointer pgDesc,
            @ByRef Device device,
            int rank,
            @ByVal OpType opType,
            @Cast("uint64_t") long seq,
            @Cast("bool") boolean isP2P/*=false*/,
            @Cast("const char*") BytePointer profilingTitle/*=nullptr*/,
            @Const @ByRef(nullValue = "std::optional<std::vector<at::Tensor> >(std::nullopt)") TensorVectorOptional inputs,
            @Cast("bool") boolean enableTiming/*=false*/,
            @Cast("bool") boolean cudaEventCacheEnabled/*=false*/,
            @ByVal(nullValue = "DebugLevel::Off") DebugLevel distDebugLevel) { super((Pointer)null); allocate(pgUID, pgDesc, device, rank, opType, seq, isP2P, profilingTitle, inputs, enableTiming, cudaEventCacheEnabled, distDebugLevel); }
    @IntrusivePtr @Name("c10::make_intrusive<c10d::ProcessGroupNCCL::WorkNCCL>") private native void allocate(
            @StdString BytePointer pgUID,
            @StdString BytePointer pgDesc,
            @ByRef Device device,
            int rank,
            @ByVal OpType opType,
            @Cast("uint64_t") long seq,
            @Cast("bool") boolean isP2P/*=false*/,
            @Cast("const char*") BytePointer profilingTitle/*=nullptr*/,
            @Const @ByRef(nullValue = "std::optional<std::vector<at::Tensor> >(std::nullopt)") TensorVectorOptional inputs,
            @Cast("bool") boolean enableTiming/*=false*/,
            @Cast("bool") boolean cudaEventCacheEnabled/*=false*/,
            @ByVal(nullValue = "DebugLevel::Off") DebugLevel distDebugLevel);
    public WorkNCCL(
            @StdString BytePointer pgUID,
            @StdString BytePointer pgDesc,
            @ByRef Device device,
            int rank,
            @ByVal OpType opType,
            @Cast("uint64_t") long seq) { super((Pointer)null); allocate(pgUID, pgDesc, device, rank, opType, seq); }
    @IntrusivePtr @Name("c10::make_intrusive<c10d::ProcessGroupNCCL::WorkNCCL>") private native void allocate(
            @StdString BytePointer pgUID,
            @StdString BytePointer pgDesc,
            @ByRef Device device,
            int rank,
            @ByVal OpType opType,
            @Cast("uint64_t") long seq);
    public WorkNCCL(
            @StdString String pgUID,
            @StdString String pgDesc,
            @ByRef Device device,
            int rank,
            @ByVal OpType opType,
            @Cast("uint64_t") long seq,
            @Cast("bool") boolean isP2P/*=false*/,
            String profilingTitle/*=nullptr*/,
            @Const @ByRef(nullValue = "std::optional<std::vector<at::Tensor> >(std::nullopt)") TensorVectorOptional inputs,
            @Cast("bool") boolean enableTiming/*=false*/,
            @Cast("bool") boolean cudaEventCacheEnabled/*=false*/,
            @ByVal(nullValue = "DebugLevel::Off") DebugLevel distDebugLevel) { super((Pointer)null); allocate(pgUID, pgDesc, device, rank, opType, seq, isP2P, profilingTitle, inputs, enableTiming, cudaEventCacheEnabled, distDebugLevel); }
    @IntrusivePtr @Name("c10::make_intrusive<c10d::ProcessGroupNCCL::WorkNCCL>") private native void allocate(
            @StdString String pgUID,
            @StdString String pgDesc,
            @ByRef Device device,
            int rank,
            @ByVal OpType opType,
            @Cast("uint64_t") long seq,
            @Cast("bool") boolean isP2P/*=false*/,
            String profilingTitle/*=nullptr*/,
            @Const @ByRef(nullValue = "std::optional<std::vector<at::Tensor> >(std::nullopt)") TensorVectorOptional inputs,
            @Cast("bool") boolean enableTiming/*=false*/,
            @Cast("bool") boolean cudaEventCacheEnabled/*=false*/,
            @ByVal(nullValue = "DebugLevel::Off") DebugLevel distDebugLevel);
    public WorkNCCL(
            @StdString String pgUID,
            @StdString String pgDesc,
            @ByRef Device device,
            int rank,
            @ByVal OpType opType,
            @Cast("uint64_t") long seq) { super((Pointer)null); allocate(pgUID, pgDesc, device, rank, opType, seq); }
    @IntrusivePtr @Name("c10::make_intrusive<c10d::ProcessGroupNCCL::WorkNCCL>") private native void allocate(
            @StdString String pgUID,
            @StdString String pgDesc,
            @ByRef Device device,
            int rank,
            @ByVal OpType opType,
            @Cast("uint64_t") long seq);
    // Copy constructor doing partial copy without outputs_. Cleanup thread
    // monitors and removes finished works. However it will deadlock when
    // destructs outputs_ tensors who are view tensors in autograd graph.
    public WorkNCCL(@Const @ByRef WorkNCCL w) { super((Pointer)null); allocate(w); }
    @IntrusivePtr @Name("c10::make_intrusive<c10d::ProcessGroupNCCL::WorkNCCL>") private native void allocate(@Const @ByRef WorkNCCL w);

    // Checks if the NCCL kernel has started to execute.
    public native @Cast("bool") boolean isStarted();

    // Checks if request has completed. In this specific case of NCCL, it checks
    // if the NCCL operation has completed on the GPU in its own NCCL stream.
    // Non-blocking operation.
    public native @Cast("bool") boolean isCompleted();

    public native @Cast("bool") boolean isSuccess();

    // Same as calling synchronize() for NCCL work if timeout is not set.
    // Otherwise, it will block the CPU thread until the NCCL work is completed
    // or timed out. If timeout, exception will be thrown.
    public native @Cast("bool") @Name("wait") boolean _wait(@ByVal(nullValue = "std::chrono::milliseconds(kNoTimeout)") Milliseconds timeout);
    public native @Cast("bool") @Name("wait") boolean _wait();

    public native void blockCurrentStream();

    public native void abort();

    // Let current stream wait on the completion of the NCCL work
    // Throws on exceptions.
    public native void synchronize();

    // Synchronize streams by blocking each on the NCCL stream
    public native void synchronizeStream();

    // Helper function to handle exception (throw if needed).
    public native void handleException(@Cast("c10d::ErrorHandlingMode") int asyncErrorHandling);

    // Helper function that checks if the NCCL kernels have finished
    // execution on the GPUs
    public native @Cast("bool") boolean finishedGPUExecution();

    // Get a Future object that will be marked as completed internally.
    public native @IntrusivePtr("c10::ivalue::Future") @Cast({"", "c10::intrusive_ptr<c10::ivalue::Future>&"}) Future getFuture();

    // Get a Future result of each work (e.g. success, different error types).
    // instead of the tensor output.
    public native @IntrusivePtr("c10::ivalue::Future") @Cast({"", "c10::intrusive_ptr<c10::ivalue::Future>&"}) Future getFutureResult();

    public native float getDuration();

    public native @Cast("uint64_t") long getSequencenumber();

    public native @StdString BytePointer logPrefix();

    // Helper function that sets an exception_ptr on the WorkNCCL object.
    public native void setException(@ByVal @Cast("std::exception_ptr*") Pointer exception_ptr);

    // Helper function that returns True if the WorkNCCL object has timed out
    // and False otherwise.
    // In case of timeout, set exception on the WorkNCCL object.
    public native @Cast("bool") boolean checkTimeout(
            @Optional Milliseconds timeout/*=std::nullopt*/);
    public native @Cast("bool") boolean checkTimeout();

    // Print the traceback of the collective at call time
    public native void printTraceback();

    public native @StdString BytePointer getTraceback();

    public native @ByVal TensorVector result();
  }

  @NoOffset public static class Options extends Backend.Options {
      static { Loader.load(); }
      /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
      public Options(Pointer p) { super(p); }
  
    // NOTE: timeout in ProcessGroupNCCL::Options denote the timeout for
    // operations. This is only used when blockingWait_ is enabled.
    public Options(@Cast("bool") boolean is_high_priority_stream/*=false*/) { super((Pointer)null); allocate(is_high_priority_stream); }
    @IntrusivePtr @Name("c10::make_intrusive<c10d::ProcessGroupNCCL::Options>") private native void allocate(@Cast("bool") boolean is_high_priority_stream/*=false*/);
    public Options() { super((Pointer)null); allocate(); }
    @IntrusivePtr @Name("c10::make_intrusive<c10d::ProcessGroupNCCL::Options>") private native void allocate();

    // return intrusive_ptr of the object
    public static native @IntrusivePtr("c10d::ProcessGroupNCCL::Options") @Cast({"", "c10::intrusive_ptr<c10d::ProcessGroupNCCL::Options>&"}) Options create(
            @Cast("bool") boolean is_high_priority_stream/*=false*/);
    public static native @IntrusivePtr("c10d::ProcessGroupNCCL::Options") @Cast({"", "c10::intrusive_ptr<c10d::ProcessGroupNCCL::Options>&"}) Options create();

    // Schedule NCCL operations on high priority CUDA streams
    public native @Cast("bool") boolean is_high_priority_stream(); public native Options is_high_priority_stream(boolean setter);

// #ifdef NCCL_HAS_CONFIG
// #endif

    // Optional "parent" backend and color to create communicators from
    // via `ncclCommSplit`
    public native @IntrusivePtr ProcessGroupNCCL split_from(); public native Options split_from(ProcessGroupNCCL setter);
    // Color to use for `ncclCommSplit`, values:
    // * Non-negative value: in group;
    // * NCCL_SPLIT_NOCOLOR (-1): not in group;
    // * NCCL_SPLIT_NOCOLOR - 1: uninitialized.
    // [Note 1]: the type must be `int` instead of `int64_t` because NCCL API
    // accepts int. Otherwise, an implicit conversion may happen at the API call
    // and the value may become negative.
    // [Note 2]: this member is pybinded to Python, the value passed from Python
    // must be within the numerical range of C++ int. Otherwise, Python will
    // raise a RuntimeError saying type is incompatible. See also
    // `_process_group_color` in `distributed_c10d.py`.
// #ifdef NCCL_HAS_COMM_SPLIT
// #else
    // [Note 3]: for older NCCL versions, NCCL_SPLIT_NOCOLOR is not defined. But
    // `split_color` is pybinded to Python, so we need to define it. So we use
    // the int value of `NCCL_SPLIT_NOCOLOR` (-1) instead.
    public native int split_color(); public native Options split_color(int setter);
// #endif
  }

  // Helper class related to TORCH_NCCL_DESYNC_DEBUG
  public static class DesyncDebugger extends Pointer {
      static { Loader.load(); }
      /** Default native constructor. */
      public DesyncDebugger() { super((Pointer)null); allocate(); }
      /** Native array allocator. Access with {@link Pointer#position(long)}. */
      public DesyncDebugger(long size) { super((Pointer)null); allocateArray(size); }
      /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
      public DesyncDebugger(Pointer p) { super(p); }
      private native void allocate();
      private native void allocateArray(long size);
      @Override public DesyncDebugger position(long position) {
          return (DesyncDebugger)super.position(position);
      }
      @Override public DesyncDebugger getPointer(long i) {
          return new DesyncDebugger((Pointer)this).offsetAddress(i);
      }
  
    // Initialize and enable DesyncDebugger
    public native void init(
            int rank,
            int size,
            int globalRank,
            int pgId,
            @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store);

    // Run desync debug. This function is called by watchdog at time of timeout.
    public native void run();

    // Log work start to store.
    public native void logWorkStart(@ByRef WorkNCCL work);

    // Log work end to store.
    public native void logWorkEnd(@ByRef WorkNCCL work);
  }

  // Class that runs as a separate thread aside from watchdog
  // thread because we need to check the heartbeat from watchdog thread
  // so that when we get stuck in some NCCL/CUDA calls,
  // we can dump the debugging information and abort the process.
  @NoOffset public static class HeartbeatMonitor extends Pointer {
      static { Loader.load(); }
      /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
      public HeartbeatMonitor(Pointer p) { super(p); }
  
    public HeartbeatMonitor(ProcessGroupNCCL pg) { super((Pointer)null); allocate(pg); }
    private native void allocate(ProcessGroupNCCL pg);

    // Start the heartbeat monitor thread.
    public native void start();

    // Join the heartbeat monitor thread.
    public native void join();

    // Run the actual loop to check watchdog heartbeat.
    public native void runLoop();

    // Set the terminal flag and notify the heartbeat monitor thread to stop.
    public native void stop();

    // Set the last update time of watchdog thread.
    public native void setLastWorkListUpdateTime(
            @ByVal SteadyTime time);

    public native int getDumpTimeout();

    // Util function to get the timeout error message
    public native @StdString BytePointer getNCCLWatchdogTimeoutErrorMsg(@StdString BytePointer extraMsg);
    public native @StdString String getNCCLWatchdogTimeoutErrorMsg(@StdString String extraMsg);

    // Util function to get the timeout exit message
    public native @StdString BytePointer getNCCLWatchdogTimeoutExitMsg(@StdString BytePointer exitReason);
    public native @StdString String getNCCLWatchdogTimeoutExitMsg(@StdString String exitReason);
  }

  // Class that runs as a side thread to check whether the NCCL collective
  // is timed out or errors on the cached NCCL communicators.
  @NoOffset public static class Watchdog extends Pointer {
      static { Loader.load(); }
      /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
      public Watchdog(Pointer p) { super(p); }
  
    public Watchdog(ProcessGroupNCCL pg) { super((Pointer)null); allocate(pg); }
    private native void allocate(ProcessGroupNCCL pg);

    // Start the watchdog thread.
    public native void start();

    // Join the watchdog thread.
    public native void join();

    // Function that runs as part of a separate thread and checks for errors on
    // NCCL communicators. We need a separate thread to check for NCCL errors
    // since we can't rely on the user calling certain methods like wait(),
    // isCompleted() etc. to detect and remediate errors. In addition to this,
    // we need a mechanism to safely abort and remove NCCL communicators from
    // our cache. This can be done cleanly by having a thread for the
    // ProcessGroupNCCL class. Attempting to modify the communicator cache from
    // the WorkNCCL class might run into issues with object lifetime since the
    // ProcessGroupNCCL object might get destroyed before the WorkNCCL object.
    public native void run();

    // Watchdog's inside loop.
    // Takes care of cleaning up completed work, and aborting upon failure or
    // timeout.
    public native void runLoop();

    // Notify the loop inside watchdog.
    public native @Name("notify") void _notify();

    public native void checkAndSetRemoteError();

    // A helper function to get the src rank of a signal from the Store. This is
    // nonblocking function returning -1 if the signal is not available yet.
    public native int getSignalSrcRank(
            @IntrusivePtr("c10d::Store") @ByRef Store store,
            @StdString BytePointer signal);
    public native int getSignalSrcRank(
            @IntrusivePtr("c10d::Store") @ByRef Store store,
            @StdString String signal);

    public native @Cast("uint64_t") long getHeartbt();

    public native void setDesyncDebug(@Cast("bool") boolean desyncDebug);
  }

  // If you wish to create multiple process groups, each with a potentially
  // different rank and size, you can do so by passing a new store instance
  // to each one. If you have only a single store object, you can
  // use the `c10d::PrefixStore` to derive scoped instances.
  // This is also what the Python API in torch.distributed does.
  //
  // The process group instance keeps a reference to the store because
  // it may be used long after the constructor runs. In fact, the constructor
  // doesn't create any NCCL communicators. A single NCCL communicator can
  // only be used on a specific set of devices, and are therefore created
  // on-demand when a collective runs. If another collective is executed later,
  // against a different set of devices, the process group creates another NCCL
  // communicator. These NCCL communicators are cached and reused if possible.
  //
  public ProcessGroupNCCL(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size,
        @IntrusivePtr("c10d::ProcessGroupNCCL::Options") @Cast({"", "c10::intrusive_ptr<c10d::ProcessGroupNCCL::Options>&"}) Options options/*=c10d::ProcessGroupNCCL::Options::create()*/) { super((Pointer)null); allocate(store, rank, size, options); }
  @IntrusivePtr @Name("c10::make_intrusive<c10d::ProcessGroupNCCL>") private native void allocate(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size,
        @IntrusivePtr("c10d::ProcessGroupNCCL::Options") @Cast({"", "c10::intrusive_ptr<c10d::ProcessGroupNCCL::Options>&"}) Options options/*=c10d::ProcessGroupNCCL::Options::create()*/);
  public ProcessGroupNCCL(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size) { super((Pointer)null); allocate(store, rank, size); }
  @IntrusivePtr @Name("c10::make_intrusive<c10d::ProcessGroupNCCL>") private native void allocate(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size);

  // This constructor includes the deprecated `groupName` argument.
  // If you have existing code that uses the `groupName`, you can replace
  // it by specifying a `c10d::PrefixStore(groupName, store)` for store.
  @Deprecated public ProcessGroupNCCL(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size,
        @StdString BytePointer groupName,
        @IntrusivePtr("c10d::ProcessGroupNCCL::Options") @Cast({"", "c10::intrusive_ptr<c10d::ProcessGroupNCCL::Options>&"}) Options options/*=c10d::ProcessGroupNCCL::Options::create()*/) { super((Pointer)null); allocate(store, rank, size, groupName, options); }
  @Deprecated @IntrusivePtr @Name("c10::make_intrusive<c10d::ProcessGroupNCCL>") private native void allocate(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size,
        @StdString BytePointer groupName,
        @IntrusivePtr("c10d::ProcessGroupNCCL::Options") @Cast({"", "c10::intrusive_ptr<c10d::ProcessGroupNCCL::Options>&"}) Options options/*=c10d::ProcessGroupNCCL::Options::create()*/);
  @Deprecated public ProcessGroupNCCL(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size,
        @StdString BytePointer groupName) { super((Pointer)null); allocate(store, rank, size, groupName); }
  @Deprecated @IntrusivePtr @Name("c10::make_intrusive<c10d::ProcessGroupNCCL>") private native void allocate(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size,
        @StdString BytePointer groupName);
  @Deprecated public ProcessGroupNCCL(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size,
        @StdString String groupName,
        @IntrusivePtr("c10d::ProcessGroupNCCL::Options") @Cast({"", "c10::intrusive_ptr<c10d::ProcessGroupNCCL::Options>&"}) Options options/*=c10d::ProcessGroupNCCL::Options::create()*/) { super((Pointer)null); allocate(store, rank, size, groupName, options); }
  @Deprecated @IntrusivePtr @Name("c10::make_intrusive<c10d::ProcessGroupNCCL>") private native void allocate(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size,
        @StdString String groupName,
        @IntrusivePtr("c10d::ProcessGroupNCCL::Options") @Cast({"", "c10::intrusive_ptr<c10d::ProcessGroupNCCL::Options>&"}) Options options/*=c10d::ProcessGroupNCCL::Options::create()*/);
  @Deprecated public ProcessGroupNCCL(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size,
        @StdString String groupName) { super((Pointer)null); allocate(store, rank, size, groupName); }
  @Deprecated @IntrusivePtr @Name("c10::make_intrusive<c10d::ProcessGroupNCCL>") private native void allocate(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        int rank,
        int size,
        @StdString String groupName);

  // This function returns a local uid for ProcessGroupNCCL.
  public native @Cast("uint64_t") long getUid();

  public native @IntrusivePtr("c10d::ProcessGroupNCCL::Options") @Cast({"", "c10::intrusive_ptr<c10d::ProcessGroupNCCL::Options>&"}) Options getOptions();

  public native @IntrusivePtr("c10d::Backend::Options") @Cast({"", "c10::intrusive_ptr<c10d::Backend::Options>&"}) Backend.Options getBackendOptions();

  public native @StdString BytePointer getBackendName();

  public native @Cast("bool") boolean supportsSplitting();

  public native @Cast("bool") boolean supportsCoalescing();

  public native @Cast("bool") boolean supportsTimeEstimation();

  public native void setTimeout(@ByVal Milliseconds timeout);

  public native void startCoalescing();

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work endCoalescing();

  public native void startTimeEstimate();

  public native float endTimeEstimate();

  // For specifying a composite optype, such as ALLGATHER and REDUCE_SCATTER
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work endCoalescing(@ByVal OpType optype);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work broadcast(
        @ByRef TensorVector tensors,
        @Const @ByRef(nullValue = "c10d::BroadcastOptions()") BroadcastOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work broadcast(
        @ByRef TensorVector tensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work _broadcast_oop(
        @ByRef Tensor outputTensors,
        @ByRef Tensor inputTensors,
        @Const @ByRef(nullValue = "c10d::BroadcastOptions()") BroadcastOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work _broadcast_oop(
        @ByRef Tensor outputTensors,
        @ByRef Tensor inputTensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allreduce_sparse(
        @ByRef TensorVector tensors,
        @Const @ByRef(nullValue = "c10d::AllreduceOptions()") AllreduceOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allreduce_sparse(
        @ByRef TensorVector tensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allreduce(
        @ByRef TensorVector tensors,
        @Const @ByRef(nullValue = "c10d::AllreduceOptions()") AllreduceOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allreduce(
        @ByRef TensorVector tensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allreduce_coalesced(
        @ByRef TensorVector tensors,
        @Const @ByRef(nullValue = "c10d::AllreduceCoalescedOptions()") AllreduceCoalescedOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allreduce_coalesced(
        @ByRef TensorVector tensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work reduce(
        @ByRef TensorVector tensors,
        @Const @ByRef(nullValue = "c10d::ReduceOptions()") ReduceOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work reduce(
        @ByRef TensorVector tensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work _reduce_oop(
        @ByRef Tensor outputTensors,
        @ByRef Tensor inputTensors,
        @Const @ByRef(nullValue = "c10d::ReduceOptions()") ReduceOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work _reduce_oop(
        @ByRef Tensor outputTensors,
        @ByRef Tensor inputTensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allgather(
        @StdVector TensorVector outputTensors,
        @ByRef TensorVector inputTensors,
        @Const @ByRef(nullValue = "c10d::AllgatherOptions()") AllgatherOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allgather(
        @StdVector TensorVector outputTensors,
        @ByRef TensorVector inputTensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work _allgather_base(
        @ByRef Tensor outputbuffer,
        @ByRef Tensor inputbuffer,
        @Const @ByRef(nullValue = "c10d::AllgatherOptions()") AllgatherOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work _allgather_base(
        @ByRef Tensor outputbuffer,
        @ByRef Tensor inputbuffer);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allgather_coalesced(
        @StdVector TensorVector outputTensorLists,
        @ByRef TensorVector inputTensors,
        @Const @ByRef(nullValue = "c10d::AllgatherOptions()") AllgatherOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allgather_coalesced(
        @StdVector TensorVector outputTensorLists,
        @ByRef TensorVector inputTensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allgather_into_tensor_coalesced(
        @ByRef TensorVector outputs,
        @ByRef TensorVector inputs,
        @Const @ByRef(nullValue = "c10d::AllgatherOptions()") AllgatherOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work allgather_into_tensor_coalesced(
        @ByRef TensorVector outputs,
        @ByRef TensorVector inputs);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work reduce_scatter(
        @ByRef TensorVector outputTensors,
        @StdVector TensorVector inputTensors,
        @Const @ByRef(nullValue = "c10d::ReduceScatterOptions()") ReduceScatterOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work reduce_scatter(
        @ByRef TensorVector outputTensors,
        @StdVector TensorVector inputTensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work _reduce_scatter_base(
        @ByRef Tensor outputTensor,
        @ByRef Tensor inputTensor,
        @Const @ByRef(nullValue = "c10d::ReduceScatterOptions()") ReduceScatterOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work _reduce_scatter_base(
        @ByRef Tensor outputTensor,
        @ByRef Tensor inputTensor);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work reduce_scatter_tensor_coalesced(
        @ByRef TensorVector outputs,
        @ByRef TensorVector inputs,
        @Const @ByRef(nullValue = "c10d::ReduceScatterOptions()") ReduceScatterOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work reduce_scatter_tensor_coalesced(
        @ByRef TensorVector outputs,
        @ByRef TensorVector inputs);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work barrier(
        @Const @ByRef(nullValue = "c10d::BarrierOptions()") BarrierOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work barrier();

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work alltoall_base(
        @ByRef Tensor outputTensor,
        @ByRef Tensor inputTensor,
        @Cast("std::vector<int64_t>*") @ByRef LongVector outputSplitSizes,
        @Cast("std::vector<int64_t>*") @ByRef LongVector inputSplitSizes,
        @Const @ByRef(nullValue = "c10d::AllToAllOptions()") AllToAllOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work alltoall_base(
        @ByRef Tensor outputTensor,
        @ByRef Tensor inputTensor,
        @Cast("std::vector<int64_t>*") @ByRef LongVector outputSplitSizes,
        @Cast("std::vector<int64_t>*") @ByRef LongVector inputSplitSizes);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work alltoall(
        @ByRef TensorVector outputTensors,
        @ByRef TensorVector inputTensors,
        @Const @ByRef(nullValue = "c10d::AllToAllOptions()") AllToAllOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work alltoall(
        @ByRef TensorVector outputTensors,
        @ByRef TensorVector inputTensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work send(
        @ByRef TensorVector tensors,
        int dstRank,
        int tag);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work recv(
        @ByRef TensorVector tensors,
        int srcRank,
        int tag);

  public native @Cast("int64_t") long getCommPtr();

  public native void groupStart();

  public native void groupEnd();

  public native void groupEndNonblocking(@Const @SharedPtr("c10d::NCCLComm") @ByRef NCCLComm comm);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work gather(
        @StdVector TensorVector outputTensors,
        @ByRef TensorVector inputTensors,
        @Const @ByRef(nullValue = "c10d::GatherOptions()") GatherOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work gather(
        @StdVector TensorVector outputTensors,
        @ByRef TensorVector inputTensors);

  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work scatter(
        @ByRef TensorVector outputTensors,
        @StdVector TensorVector inputTensors,
        @Const @ByRef(nullValue = "c10d::ScatterOptions()") ScatterOptions opts);
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work scatter(
        @ByRef TensorVector outputTensors,
        @StdVector TensorVector inputTensors);

  // Unsupported Ops
  public native @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work recvAnysource(
        @ByRef TensorVector tensors,
        int tag);

  // Agrees on an initial sequence number for the whole group by having rank 0
  // create it and broadcast it to other ranks using the store.
  public native void setSequenceNumberForGroup();

  // Retrieves the current sequence number for the whole group, which should be
  // in sync. If the returned number is not consistent across the group, it
  // may indicate that there is some sort of collective desynchronization.
  public native @Cast("uint64_t") long getSequenceNumberForGroup();

  // Return the total number of splits the communicators held by this process
  // group have performed.  Counts ncclCommCreateFromRanks() for ncclx v2.21.5+
  public native @Cast("uint64_t") long getCommSplitCounter();

  public native void registerOnCompletionHook(
        WorkInfoConsumer hook);
  public native void waitForPendingWorks();

  public native void enableCollectivesTiming();

  public native @IntrusivePtr("c10d::Backend") @Cast({"", "c10::intrusive_ptr<c10d::Backend>&"}) Backend split(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        @StdVector IntPointer ranks,
        @IntrusivePtr("c10d::Backend::Options") @Cast({"", "c10::intrusive_ptr<c10d::Backend::Options>&"}) Backend.Options opts);
  public native @IntrusivePtr("c10d::Backend") @Cast({"", "c10::intrusive_ptr<c10d::Backend>&"}) Backend split(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        @StdVector IntBuffer ranks,
        @IntrusivePtr("c10d::Backend::Options") @Cast({"", "c10::intrusive_ptr<c10d::Backend::Options>&"}) Backend.Options opts);
  public native @IntrusivePtr("c10d::Backend") @Cast({"", "c10::intrusive_ptr<c10d::Backend>&"}) Backend split(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        @StdVector int[] ranks,
        @IntrusivePtr("c10d::Backend::Options") @Cast({"", "c10::intrusive_ptr<c10d::Backend::Options>&"}) Backend.Options opts);

  public native @IntrusivePtr("c10d::Backend") @Cast({"", "c10::intrusive_ptr<c10d::Backend>&"}) Backend merge(
        @IntrusivePtr("c10d::Store") @Cast({"", "c10::intrusive_ptr<c10d::Store>&"}) Store store,
        @IntrusivePtr("c10d::Backend::Options") @Cast({"", "c10::intrusive_ptr<c10d::Backend::Options>&"}) Backend.Options opts,
        int rank,
        int size);

  // Helper function for iteratively aborting communicators in the provided map
  public native void abortCommsFromMap(
        @ByRef StringNCCLCommMap ncclCommsMap,
        @Const @ByRef StringOptional abortReason);

  

  // Destroy (shutdown) this backend -- normal exit.
  public native void shutdown();

  // Provides an API to abort the ProcessGroup (similar to ncclCommAbort)
  // instead of relying on ProcessGroupNCCL destructor.
  public native void abort();

  public native void eagerConnectSingleDevice(@ByVal Device device);

  public native void performNocolorSplit(@ByVal Device device);

  // If all comms on this PG are fully initialized, return true.
  public native @Cast("bool") boolean isInitialized();

  public native @ByVal ErrorType getError();

  public native @SharedPtr Allocator getMemAllocator();

  // Allocate tensor from communication-optimized memory pool
  public native @ByVal Tensor allocateTensor(long size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
  public native @ByVal Tensor allocateTensor(long size);

  // Whether tensor allocation from NCCL memory pool is supported
  public native @Cast("bool") boolean supportsTensorAlloc(byte deviceIdx);

  // Performs NCCL user buffer registration for all buffers in
  // the given MemPool
  public native void registerMemPool(MemPool pool, @Cast("bool") boolean symm/*=false*/);
  public native void registerMemPool(MemPool pool);

  // Performs NCCL user buffer de-registration for all buffers in
  // the given MemPool
  public native void deregisterMemPool(MemPool pool);

  // This method adds a temporary extension for the timeout period,
  // applying to all collectives between the calling of this API and
  // the completion of the first collective on the GPU. While this feature
  // provides flexibility in specific scenarios, it introduces statefulness
  // to timeout setting. Therefore, it is advisable to use this API sparingly
  // and consider alternative approaches, such as directly setting the timeout
  // or utilizing a barrier collective (one can set any timeout to the barrier),
  // whenever feasible.
  public native void addEphemeralTimeout(@Const @ByRef Milliseconds timeout);

  // This function is only intended for testing purposes because we don't
  // want to expose the `WorkNCCL` via pybind. It verifies whether the
  // `opTimeout_` of the provided WorkNCCL instance is the same as the specified
  // timeout.
  public native @Cast("bool") boolean verifyWorkTimeoutForTest(
        @IntrusivePtr("c10d::Work") @Cast({"", "c10::intrusive_ptr<c10d::Work>&"}) Work work,
        @Const @ByRef Milliseconds timeout);

  public native void setEnableNanCheck(@Cast("bool") boolean enableNanCheck);
}

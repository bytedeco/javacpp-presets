// Targeted by JavaCPP version 1.5.11-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.pytorch;

import org.bytedeco.pytorch.Allocator;
import org.bytedeco.pytorch.Function;
import org.bytedeco.pytorch.functions.*;
import org.bytedeco.pytorch.chrono.*;
import org.bytedeco.pytorch.Module;
import org.bytedeco.javacpp.annotation.Cast;
import org.bytedeco.pytorch.helper.*;
import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.javacpp.presets.javacpp.*;
import static org.bytedeco.openblas.global.openblas_nolapack.*;
import static org.bytedeco.openblas.global.openblas.*;

import static org.bytedeco.pytorch.global.torch.*;


// DistAutogradContext which stores information for a single distributed
// autograd pass on a worker.
@Namespace("torch::distributed::autograd") @NoOffset @Properties(inherit = org.bytedeco.pytorch.presets.torch.class)
public class DistAutogradContext extends Pointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public DistAutogradContext(Pointer p) { super(p); }


  public DistAutogradContext(@Cast("int64_t") long contextId) { super((Pointer)null); allocate(contextId); }
  @SharedPtr @Name("std::make_shared<torch::distributed::autograd::DistAutogradContext>") private native void allocate(@Cast("int64_t") long contextId);

  // Retrieves the autograd context id for this context.
  public native @Cast("int64_t") long contextId();

  // Records a 'send' autograd function for this context with the provided
  // message id.
  public native void addSendFunction(
        @Const @SharedPtr("torch::distributed::autograd::SendRpcBackward") @ByRef SendRpcBackward func,
        @Cast("int64_t") long autograd_message_id);

  // Records a 'recv' autograd function for this context with the provided
  // message id.
  public native void addRecvFunction(
        @SharedPtr("torch::distributed::autograd::RecvRpcBackward") @ByRef RecvRpcBackward func,
        @Cast("int64_t") long autograd_message_id);

  // Given an autograd_message_id, retrieve the appropriate send function.
  public native @SharedPtr("torch::distributed::autograd::SendRpcBackward") @ByVal SendRpcBackward retrieveSendFunction(
        @Cast("int64_t") long autograd_message_id);

  // Return all send functions for this context.
  public native @ByVal LongSendRpcBackwardMap sendFunctions();

  // Return all recv functions for this context.
  public native @ByVal LongRecvRpcBackwardMap recvFunctions();

  // Adds a future message recording an outstanding RPC.
  public native void addOutstandingRpc(@IntrusivePtr("c10::ivalue::Future") @Cast({"", "c10::intrusive_ptr<c10::ivalue::Future>&"}) Future jitFuture);

  // Returns all gradients.
  public native @Const @ByVal TensorTensorDict getGradients();

  // This function gives a mutable grad reference to the callback.
  // If the callback returns true, it means the grad in the context
  // needs to be updated.
  public native void runGradCallbackForVariable(
        @Const @ByRef Tensor variable,
        @ByRef(true) GradCallback cb);

  
  
  
  

  // records the workerID of a node that we sent an RPC to.
  // workerIDs are added here when we attach a send function to this autograd
  // context
  public native void addKnownWorkerId(short workerId);

  // Retrieves a set containing the known workerIds for this context
  // These are the different workers that this context has sent RPCs to.
  public native @ByVal ShortSet getKnownWorkerIds();
}

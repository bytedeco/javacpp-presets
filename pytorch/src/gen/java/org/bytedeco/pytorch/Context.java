// Targeted by JavaCPP version 1.5.6-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.pytorch;

import org.bytedeco.pytorch.Allocator;
import org.bytedeco.pytorch.Function;
import org.bytedeco.pytorch.Module;
import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.javacpp.presets.javacpp.*;
import static org.bytedeco.openblas.global.openblas_nolapack.*;
import static org.bytedeco.openblas.global.openblas.*;

import static org.bytedeco.pytorch.global.torch.*;


@Namespace("at") @NoOffset @Properties(inherit = org.bytedeco.pytorch.presets.torch.class)
public class Context extends Pointer {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public Context(Pointer p) { super(p); }
    /** Native array allocator. Access with {@link Pointer#position(long)}. */
    public Context(long size) { super((Pointer)null); allocateArray(size); }
    private native void allocateArray(long size);
    @Override public Context position(long position) {
        return (Context)super.position(position);
    }
    @Override public Context getPointer(long i) {
        return new Context((Pointer)this).position(position + i);
    }

  public Context() { super((Pointer)null); allocate(); }
  private native void allocate();

  public native @Const @ByRef Generator defaultGenerator(@ByVal Device device);
  public native @ByVal Device getDeviceFromPtr(Pointer data, DeviceType device_type);
  public native @ByVal Device getDeviceFromPtr(Pointer data, @Cast("c10::DeviceType") byte device_type);
  public native @Cast("bool") boolean isPinnedPtr(Pointer data);
  public native @Cast("bool") boolean hasOpenMP();
  public native @Cast("bool") boolean hasMKL();
  public native @Cast("bool") boolean hasLAPACK();
  public native @Cast("bool") boolean hasMKLDNN();
  public native @Cast("bool") boolean hasMAGMA();
  public native @Cast("bool") boolean hasCUDA();
  public native @Cast("bool") boolean hasCUDART();
  public native long versionCUDART();
  public native @Cast("bool") boolean hasHIP();
  public native @Cast("bool") boolean hasXLA();
  // defined in header so that getNonVariableType has ability to inline
  // call_once check. getNonVariableType is called fairly frequently
  public native @Cast("THCState*") Pointer lazyInitCUDA();
  public native @Cast("THHState*") Pointer lazyInitHIP();
  public native @Cast("const at::cuda::NVRTC*") @ByRef Pointer getNVRTC();
  public native @Cast("THCState*") Pointer getTHCState();
  public native @Cast("THHState*") Pointer getTHHState();

  public native @Cast("bool") boolean setFlushDenormal(@Cast("bool") boolean on);

  // NB: This method is *purely* whether or not a user requested
  // that CuDNN was enabled, it doesn't actually say anything about
  // whether or not CuDNN is actually usable.  Use cudnn_is_acceptable
  // to test this instead
  public native @Cast("bool") boolean userEnabledCuDNN();
  public native void setUserEnabledCuDNN(@Cast("bool") boolean e);
  public native @Cast("bool") boolean userEnabledMkldnn();
  public native void setUserEnabledMkldnn(@Cast("bool") boolean e);
  public native @Cast("bool") boolean benchmarkCuDNN();
  public native void setBenchmarkCuDNN(@Cast("bool") boolean arg0);
  public native @Cast("bool") boolean deterministicCuDNN();
  public native void setDeterministicCuDNN(@Cast("bool") boolean arg0);

  // Note [Enabling Deterministic Operations]
  // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  // Operations in PyTorch that normally act nondeterministically, but have an alternate
  // deterministic implementation, should satisfy the following requirements:
  //
  // * Include this comment: "See Note [Enabling Deterministic Operations]"
  //
  // * Check the value of `at::globalContext().deterministicAlgorithms()` to toggle
  //   between nondeterministic and deterministic implementations.
  //
  // * Have an entry in the list of PyTorch operations that toggle between nondeterministic
  //   and deterministic implementations, in the docstring of `use_deterministic_algorithms()`
  //   in torch/__init__.py
  //
  // `example_func()` below shows an example of toggling between nondeterministic and
  // deterministic implementations:
  //
  //    void example_func() {
  //      // See Note [Enabling Deterministic Operations]
  //      if (at::globalContext().deterministicAlgorithms()) {
  //        example_func_deterministic();
  //      } else {
  //        example_func_nondeterministic();
  //      }
  //    }

  public native @Cast("bool") boolean deterministicAlgorithms();
  public native void setDeterministicAlgorithms(@Cast("bool") boolean arg0);

  // Note [Writing Nondeterministic Operations]
  // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  // Operations in PyTorch that act nondeterministically and do not have an alternate
  // deterministic implementation should satisfy the following requirements:
  //
  // * Include this comment: "See Note [Writing Nondeterministic Operations]"
  //
  // * Include a comment explaining why the operation is nondeterministic.
  //
  // * Throw an error when `Context::deterministicAlgorithms()` is true. Most
  //   of the time, this should be accomplished by calling
  //   `at::globalContext().alertNotDeterminstic()`.  However, if the
  //   nondeterministic behavior is caused by the CuBLAS workspace
  //   configuration in CUDA >= 10.2,
  //   `at::globalContext().alertCuBLASConfigNotDeterministic()` should be
  //   called instead (in this case, a comment explaining why the operation is
  //   nondeterministic is not necessary). See below for details on these
  //   methods.
  //
  // * Have an entry in the list of nondeterministic PyTorch operations in the
  //   docstring of `use_deterministic_algorithms()` in torch/__init__.py
  //
  // `example_func()` below shows an example of the comments and error-throwing code
  // for a nondeterministic operation:
  //
  //    void example_func() {
  //      // See Note [Writing Nondeterministic Operations]
  //      // Nondeterministic because <reason>
  //      at::globalContext().alertNondeterministic("example_func");
  //      ...
  //    }

  // Throws an error if `Context::deterministicAlgorithms()` is true
  public native void alertNotDeterministic(@Cast("const c10::string_view*") @ByRef Pointer caller);

  // Throws an error if `Context::deterministicAlgorithms()` is true, CUDA >= 10.2, and
  // CUBLAS_WORKSPACE_CONFIG is not set to either ":16:8" or ":4096:8". For more details:
  // https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility
  public native void alertCuBLASConfigNotDeterministic();

  public native @Cast("bool") boolean allowTF32CuDNN();
  public native void setAllowTF32CuDNN(@Cast("bool") boolean arg0);
  public native @Cast("bool") boolean allowTF32CuBLAS();
  public native void setAllowTF32CuBLAS(@Cast("bool") boolean arg0);
  public native @ByVal QEngine qEngine();
  public native void setQEngine(@ByVal QEngine e);
  public native @Const @ByRef QEngineVector supportedQEngines();
  public native @Cast("bool") boolean isXNNPACKAvailable();
  // This method is used to release the original weight after pre-packing.
  // It should be called once before loading/running the model.
  // NB: By default it is set to true for mobile builds.
  public native void setReleaseWeightsWhenPrepacking(@Cast("bool") boolean e);
  public native @Cast("bool") boolean releaseWeightsWhenPrepacking();

  public native void setDisplayVmapFallbackWarnings(@Cast("bool") boolean enabled);
  public native @Cast("bool") boolean areVmapFallbackWarningsEnabled();
}

// Targeted by JavaCPP version 1.5.6-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.pytorch;

import org.bytedeco.pytorch.Allocator;
import org.bytedeco.pytorch.Function;
import org.bytedeco.pytorch.Module;
import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.javacpp.presets.javacpp.*;
import static org.bytedeco.openblas.global.openblas_nolapack.*;
import static org.bytedeco.openblas.global.openblas.*;

import static org.bytedeco.pytorch.global.torch.*;


// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Transformer ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

/** A transformer model. User is able to modify the attributes as needed. The architecture
 *  is based on the paper "Attention Is All You Need". Ashish Vaswani, Noam Shazeer,
 *  Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and
 *  Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information
 *  Processing Systems, pages 6000-6010. Users can build the BERT(https://arxiv.org/abs/1810.04805)
 *  model with corresponding parameters.
 * 
 *  See https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html to
 *  learn about the exact behavior of this transformer model
 * 
 *  See the documentation for {@code torch::nn::Transformer} class to learn what
 *  constructor arguments are supported for this encoder layer model
 * 
 *  Example:
 *  <pre>{@code
 *  Transformer trans(TransformerOptions(512, 8));
 *  }</pre> */
@Namespace("torch::nn") @NoOffset @Properties(inherit = org.bytedeco.pytorch.presets.torch.class)
public class TransformerImpl extends TransformerImplCloneable {
    static { Loader.load(); }
    /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
    public TransformerImpl(Pointer p) { super(p); }

    
    ///
    ///
    ///
    ///
    ///
    ///
    public TransformerImpl(@ByVal TransformerOptions options_) { super((Pointer)null); allocate(options_); }
    @NoDeallocator private native void allocate(@ByVal TransformerOptions options_);


    /** forward function for Transformer Module
     *  Args:
     *    src: the sequence to the encoder (required).
     *    tgt: the sequence to the decoder (required).
     *    src_mask: the additive mask for the src sequence (optional).
     *    tgt_mask: the additive mask for the tgt sequence (optional).
     *    memory_mask: the additive mask for the encoder output (optional).
     *    src_key_padding_mask: the ByteTensor mask for src keys per batch (optional).
     *    tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional).
     *    memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional).
     * 
     *  Shape:
     *    src: {@code (S, N, E)}
     *    tgt: {@code (T, N, E)}
     *    src_mask: {@code (S, S)}
     *    tgt_mask: {@code (T, T)}
     *    memory_mask: {@code (T, S)}
     *    src_key_padding_mask: {@code (N, S)}
     *    tgt_key_padding_mask: {@code (N, T)}
     *    memory_key_padding_mask: {@code (N, S)}
     * 
     *    Note:
     *      [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked
     *      positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend
     *      while the zero positions will be unchanged. If a BoolTensor is provided, positions with {@code True}
     *      are not allowed to attend while {@code False} values will be unchanged. If a FloatTensor
     *      is provided, it will be added to the attention weight.
     * 
     *      [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by
     *      the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero
     *      positions will be unchanged. If a BoolTensor is provided, the positions with the
     *      value of {@code True} will be ignored while the position with the value of {@code False} will be unchanged.
     * 
     *    output: {@code (T, N, E)}
     * 
     *    Note:
     *      Due to the multi-head attention architecture in the transformer model,
     *      the output sequence length of a transformer is same as the input sequence
     *      (i.e. target) length of the decode.
     * 
     *    where
     *    S is the source sequence length,
     *    T is the target sequence length,
     *    N is the batch size,
     *    E is the feature number. */
    public native @ByVal Tensor forward(
          @Const @ByRef Tensor src,
          @Const @ByRef Tensor tgt,
          @Const @ByRef(nullValue = "at::Tensor{}") Tensor src_mask,
          @Const @ByRef(nullValue = "at::Tensor{}") Tensor tgt_mask,
          @Const @ByRef(nullValue = "at::Tensor{}") Tensor memory_mask,
          @Const @ByRef(nullValue = "at::Tensor{}") Tensor src_key_padding_mask,
          @Const @ByRef(nullValue = "at::Tensor{}") Tensor tgt_key_padding_mask,
          @Const @ByRef(nullValue = "at::Tensor{}") Tensor memory_key_padding_mask);
    public native @ByVal Tensor forward(
          @Const @ByRef Tensor src,
          @Const @ByRef Tensor tgt);

    public native void reset();

    public native void reset_parameters();

    /** Generate a square mask for the sequence.
     *  The masked positions are filled with {@code -inf} in float type.
     *  Unmasked positions are filled with {@code 0.0} in float type.
     *  Note:
     *    1. This function will always return a CPU tensor.
     *    2. This function requires the platform support IEEE754, since {@code -inf} is guaranteed to
     *       be valid only when IEEE754 is supported. If the platform doesn't support IEEE754,
     *       this function will fill the mask with the smallest float number instead of {@code -inf},
     *       a one time warning will pop up as well. */
    public static native @ByVal Tensor generate_square_subsequent_mask(@Cast("int64_t") long sz);
    /** options with which this {@code Transformer} was constructed */
    public native @ByRef TransformerOptions options(); public native TransformerImpl options(TransformerOptions setter);

    /** encoder module */
    public native @ByRef AnyModule encoder(); public native TransformerImpl encoder(AnyModule setter);

    /** decoder module */
    public native @ByRef AnyModule decoder(); public native TransformerImpl decoder(AnyModule setter);
}

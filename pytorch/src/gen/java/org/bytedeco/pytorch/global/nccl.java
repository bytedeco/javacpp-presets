// Targeted by JavaCPP version 1.5.13-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.pytorch.global;

import org.bytedeco.pytorch.nccl.*;

import org.bytedeco.pytorch.Allocator;
import org.bytedeco.pytorch.Backend;
import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.javacpp.presets.javacpp.*;
import org.bytedeco.cuda.cudart.*;
import static org.bytedeco.cuda.global.cudart.*;
import org.bytedeco.cuda.nccl.*;
import static org.bytedeco.cuda.global.nccl.*;
import static org.bytedeco.openblas.global.openblas_nolapack.*;
import static org.bytedeco.openblas.global.openblas.*;
import org.bytedeco.javacpp.chrono.*;
import static org.bytedeco.javacpp.global.chrono.*;
import org.bytedeco.pytorch.*;
import static org.bytedeco.pytorch.global.torch.*;
import org.bytedeco.cuda.cublas.*;
import static org.bytedeco.cuda.global.cublas.*;
import org.bytedeco.cuda.cudnn.*;
import static org.bytedeco.cuda.global.cudnn.*;
import org.bytedeco.cuda.cusparse.*;
import static org.bytedeco.cuda.global.cusparse.*;
import org.bytedeco.cuda.cusolver.*;
import static org.bytedeco.cuda.global.cusolver.*;
import org.bytedeco.cuda.cupti.*;
import static org.bytedeco.cuda.global.cupti.*;
import org.bytedeco.pytorch.cuda.*;
import static org.bytedeco.pytorch.global.torch_cuda.*;

public class nccl extends org.bytedeco.pytorch.presets.nccl {
    static { Loader.load(); }

// Targeting ../nccl/ScalaTypeDataTypeMap.java


// Targeting ../nccl/StringNCCLCommMap.java


// Parsed from torch/csrc/distributed/c10d/NCCLUtils.hpp

// #pragma once

// #ifdef USE_C10D_NCCL

// #include <sched.h>
// #include <cstdio>
// #include <cstdlib>

// #include <memory>
// #include <mutex>

// #include <ATen/ATen.h>
// #include <ATen/cuda/CUDAEvent.h>
// #include <c10/util/Exception.h>
// #include <nccl.h>
// #include <torch/csrc/cuda/nccl.h>
// #include <optional>

@MemberGetter public static native @Cast("const int64_t") long kCommInitBusyWaitMillis();

// #if NCCL_VERSION_CODE >= NCCL_VERSION(2, 14, 0)
// #define NCCL_HAS_COMM_NONBLOCKING
// #endif

// #if NCCL_VERSION_CODE >= NCCL_VERSION(2, 18, 0)
// #define NCCL_HAS_COMM_SPLIT
// #endif

// #if NCCL_VERSION_CODE >= NCCL_VERSION(2, 23, 0)
// #define NCCL_HAS_INIT_RANK_SCALABLE
// #endif

// ncclGetLastError() is enabled only for NCCL versions 2.13+
// ncclRemoteError only exists in NCCL versions 2.13+
// #if NCCL_VERSION_CODE >= NCCL_VERSION(2, 13, 0)
// #define ENABLE_NCCL_GET_LAST_ERROR
// #define NCCL_REMOTE_ERROR
// #endif
// The following macros represent features supported prior to NCCL 2.7,
// therefore we can define them unconditionally, given the static_assert above.
// TODO: remove these macros from code.
// #define ENABLE_NCCL_ERROR_CHECKING
// #define ENABLE_NCCL_P2P_SUPPORT
// End of macros for NCCL 2.7 and below.

// #if NCCL_VERSION_CODE >= NCCL_VERSION(2, 11, 0)
// #define ENABLE_NCCL_PREMUL_SUM_SUPPORT
// #endif

// Note: the first version that supports ncclConfig_t is 2.14. Here we
// fast-forward the version requirement to 2.17 where ncclConfig_t has CTA and
// CGA fields because they have already been pybinded out.
// #if NCCL_VERSION_CODE >= NCCL_VERSION(2, 17, 0)
// #define NCCL_HAS_CONFIG
// #endif

// #if NCCL_VERSION_CODE >= NCCL_VERSION(2, 19, 0)
// #define NCCL_HAS_COMM_REGISTER
// #endif

// #if NCCL_VERSION_CODE >= NCCL_VERSION(2, 27, 0)
// #define NCCL_HAS_COMM_WINDOW_REGISTER
// #endif

// #if NCCL_VERSION_CODE >= NCCL_VERSION(2, 19, 0)
// #define NCCL_HAS_MEM_ALLOC
// #endif

// #if NCCL_VERSION_CODE >= NCCL_VERSION(2, 26, 0)
// #define NCCL_HAS_QOS
// #endif

// #if NCCL_VERSION_CODE >= NCCL_VERSION(2, 24, 0)
// #define NCCL_SUPPORTS_FP8
// #endif

// #if NCCL_VERSION_CODE >= NCCL_VERSION(2, 27, 0)
// #define NCCL_HAS_COLLNET
// #endif

// #if NCCL_VERSION_CODE >= NCCL_VERSION(2, 27, 0)
// #define NCCL_HAS_CTA_POLICY
// #endif

// #if NCCL_VERSION_CODE >= NCCL_VERSION(2, 27, 0)
// #define NCCL_HAS_NVLS_CTAS
// #endif

// Macro to throw on a non-successful NCCL return value.
// #define C10D_NCCL_CHECK(cmd, failureReason)
//   do {
//     ncclResult_t result = cmd;
//     if (result != ncclSuccess) {
//       std::string err = "NCCL error in: " + std::string(__FILE__) + ":" +
//           std::to_string(__LINE__) + ", " + ncclGetErrorWithVersion(result) +
//           "\n" + getNcclErrorDetailStr(result, failureReason);
//       TORCH_CHECK_WITH(DistBackendError, false, err);
//     }
//   } while (0)

// Macro to throw on a non-successful NCCL return value for NONBLOCKING calls.
// #define C10D_NCCL_CHECK_NONBLOCKING(cmd, failureReason)
//   do {
//     ncclResult_t result = cmd;
//     if (result != ncclSuccess && result != ncclInProgress) {
//       std::string err = "NCCL error in: " + std::string(__FILE__) + ":" +
//           std::to_string(__LINE__) + ", " + ncclGetErrorWithVersion(result) +
//           "\n" + getNcclErrorDetailStr(result, failureReason);
//       TORCH_CHECK_WITH(DistBackendError, false, err);
//     }
//   } while (0)

// Error out if (current time - startTime) is greater than timeout (sec).
// #define C10D_CHECK_TIMEOUT(startTime, timeout)
//   do {
//     auto currentTime = std::chrono::steady_clock::now();
//     auto timeElapsed = std::chrono::duration_cast<std::chrono::seconds>(
//                            currentTime - startTime)
//                            .count();
//     if (timeElapsed > timeout) {
//       std::string err = "NCCL timeout in: " + std::string(__FILE__) + ":" +
//           std::to_string(__LINE__);
//       TORCH_CHECK_WITH(DistBackendError, false, err);
//     }
//   } while (0)

// Macro to throw on a non-successful NCCL return value, non-blocking.
// #define C10D_NCCL_CHECK_TIMEOUT_BASE(cmd, comm, failureReason, yield_fn)
//   do {
//     ncclResult_t result = cmd;
//     auto startTimepoint = std::chrono::steady_clock::now();
//     auto timeout = nccl_nonblocking_timeout();
//     while (result == ncclInProgress) {
//       C10D_CHECK_TIMEOUT(startTimepoint, timeout);
//       yield_fn;
//       ncclCommGetAsyncError(comm, &result);
//     }
//     if (result != ncclSuccess) {
//       std::string err = "NCCL error in: " + std::string(__FILE__) + ":" +
//           std::to_string(__LINE__) + ", " + ncclGetErrorWithVersion(result) +
//           "\n" + getNcclErrorDetailStr(result, failureReason);
//       TORCH_CHECK_WITH(DistBackendError, false, err);
//     }
//   } while (0)

// Sleep for kCommInitBusyWaitMillis milliseconds.
// #define C10D_SCHED_SLEEP()
//   std::this_thread::sleep_for(
//       std::chrono::milliseconds(kCommInitBusyWaitMillis))

// Macro to throw exception on a non-successful NCCL return value or timeout.
// This macro uses sched_yield() to yield the CPU.
// Thus suitable for NCCL calls that would quickly turn ncclSuccess, e.g.
// collectives.
// #define C10D_NCCL_CHECK_TIMEOUT(cmd, comm, failureReason)
//   C10D_NCCL_CHECK_TIMEOUT_BASE(cmd, comm, failureReason, sched_yield())

// Macro to throw exception on a non-successful NCCL return value or timeout.
// This macro uses sleep to yield the CPU.
// Thus suitable for NCCL calls that would take longer to turn ncclSuccess, e.g.
// ncclCommInitRankConfig, ncclCommFinalize, etc.
// #define C10D_NCCL_CHECK_TIMEOUT_SLEEP(cmd, comm, failureReason)
//   C10D_NCCL_CHECK_TIMEOUT_BASE(cmd, comm, failureReason, C10D_SCHED_SLEEP())

// #define C10D_NCCL_CHECK_TIMEOUT_GROUPEND(cmd, comm, failureReason)
//   do {
//     ncclResult_t state = cmd;
//     auto startTimepoint = std::chrono::steady_clock::now();
//     auto timeout = nccl_nonblocking_timeout();
//     if (state == ncclInProgress) {
//       do {
//         C10D_CHECK_TIMEOUT(startTimepoint, timeout);
//         sched_yield();
//         ncclCommGetAsyncError(comm->getNcclComm(), &state);
//       } while (state == ncclInProgress);
//     }
//     if (state != ncclSuccess) {
//       std::string err = "NCCL error in: " + std::string(__FILE__) + ":" +
//           std::to_string(__LINE__) + ", " + ncclGetErrorWithVersion(state) +
//           "\n" + getNcclErrorDetailStr(state, failureReason);
//       TORCH_CHECK_WITH(DistBackendError, false, err);
//     }
//   } while (0)

// Macro to print and abort on a non-successful NCCL return value.
// #define C10D_NCCL_ASSERT(cmd)
//   do {
//     ncclResult_t result = cmd;
//     if (result != ncclSuccess) {
//       std::string err = ncclGetErrorWithVersion(result);
//       fprintf(
//           stderr,
//           "NCCL error in: %s:%d, %s\n",
//           __FILE__,
//           __LINE__,
//           err.c_str());
//       abort();
//     }
//   } while (0)

// NCCL type typing
@Namespace("c10d") public static native @ByRef ScalaTypeDataTypeMap ncclDataType(); public static native void ncclDataType(ScalaTypeDataTypeMap setter);

@Namespace("c10d") public static native @Cast("size_t") long hashTensors(@Const @ByRef TensorVector tensors);
@Namespace("c10d") public static native int genNcclSplitColor(@StdVector IntPointer ranks);
@Namespace("c10d") public static native int genNcclSplitColor(@StdVector IntBuffer ranks);
@Namespace("c10d") public static native int genNcclSplitColor(@StdVector int[] ranks);

@Namespace("c10d") public static native @ByVal T_IntIntInt_T getNcclVersionTuple();
@Namespace("c10d") public static native int getNcclVersionNumber();



// Provides additional detail into NCCL error codes based on when these are
// thrown in the NCCL codebase.


// Helper function that gets the data type and issues error if not supported
@Namespace("c10d") public static native @Cast("ncclDataType_t") int getNcclDataType(ScalarType type);
// Targeting ../nccl/NCCLComm.java


// Targeting ../nccl/ncclRedOpRAII.java



@Namespace("c10d") public static native void printNcclCommProxyTrace(
    @StdString BytePointer dumpReason,
    @Const @ByRef ExtraFilesMap dumpMap);
@Namespace("c10d") public static native void printNcclCommProxyTrace(
    @StdString String dumpReason,
    @Const @ByRef ExtraFilesMap dumpMap);
 // namespace c10d

// #endif // USE_C10D_NCCL


// Parsed from torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp

// #pragma once

// #ifdef USE_C10D_NCCL

// #if defined(__linux__)
// #include <fcntl.h>
// #include <sys/stat.h>
// #include <sys/types.h>
// #include <unistd.h>
// #endif

// #include <atomic>
// #include <chrono>
// #include <deque>
// #include <future>
// #include <iostream>
// #include <list>
// #include <mutex>
// #include <thread>
// #include <unordered_map>

// #include <torch/csrc/distributed/c10d/Backend.hpp>
// #include <torch/csrc/distributed/c10d/NCCLUtils.hpp>
// #include <torch/csrc/distributed/c10d/PrefixStore.hpp>
// #include <torch/csrc/distributed/c10d/Store.hpp>
// #include <torch/csrc/distributed/c10d/cuda/CUDAEventCache.hpp>
// #include <torch/csrc/distributed/c10d/logger.hpp>
// #include <torch/csrc/distributed/c10d/symm_mem/intra_node_comm.hpp>

// #include <ATen/DynamicLibrary.h>
// #include <ATen/cuda/CUDAContext.h>
// #include <ATen/cuda/CUDAEvent.h>
// #include <c10/core/Stream.h>
// #include <c10/core/StreamGuard.h>
// #include <c10/cuda/CUDACachingAllocator.h>
// #include <c10/cuda/CUDAGuard.h>
// #include <c10/cuda/CUDAStream.h>

// #include <torch/custom_class.h>

// Control broadcasting of NCCL uniqueId
@Namespace("c10d") public static native @ByRef StringVector TORCH_NCCL_BCAST_UNIQUEID(); public static native void TORCH_NCCL_BCAST_UNIQUEID(StringVector setter);

// Control EagerInit P2P serialization warning
@Namespace("c10d") public static native @ByRef StringVector TORCH_NCCL_SHOW_EAGER_INIT_P2P_SERIALIZATION_WARNING(); public static native void TORCH_NCCL_SHOW_EAGER_INIT_P2P_SERIALIZATION_WARNING(StringVector setter);

// Control whether to always use high priority streams
@Namespace("c10d") public static native @ByRef StringVector TORCH_NCCL_HIGH_PRIORITY(); public static native void TORCH_NCCL_HIGH_PRIORITY(StringVector setter);

// Control whether or not wait() is blocking or non-blocking.
@Namespace("c10d") public static native @ByRef StringVector TORCH_NCCL_BLOCKING_WAIT(); public static native void TORCH_NCCL_BLOCKING_WAIT(StringVector setter);

// TODO: We want to eventually remove this variable and make users to use
// the default value (3 - SkipCleanUp).
// Control whether or not we perform Async Error Handling with NCCL.
@Namespace("c10d") public static native @ByRef StringVector TORCH_NCCL_ASYNC_ERROR_HANDLING(); public static native void TORCH_NCCL_ASYNC_ERROR_HANDLING(StringVector setter);

// Control whether dumping debug info on watchdog
// timeout is enabled. This variable must be set together with
// TORCH_NCCL_ENABLE_MONITORING=1 and TORCH_NCCL_TRACE_BUFFER_SIZE > 0.
@Namespace("c10d") public static native @ByRef StringVector TORCH_NCCL_DUMP_ON_TIMEOUT(); public static native void TORCH_NCCL_DUMP_ON_TIMEOUT(StringVector setter);

// Control whether to propagate NCCL errors to all ranks through TCPStore.
@Namespace("c10d") public static native @ByRef StringVector TORCH_NCCL_PROPAGATE_ERROR(); public static native void TORCH_NCCL_PROPAGATE_ERROR(StringVector setter);

// Control whether Desync Debug is enabled. This variable must be set
// together with TORCH_NCCL_ASYNC_ERROR_HANDLING.
@Namespace("c10d") public static native @ByRef StringVector TORCH_NCCL_DESYNC_DEBUG(); public static native void TORCH_NCCL_DESYNC_DEBUG(StringVector setter);

// Enable recording start-events for all ProcessGroupNCCL collectives, and
// compute accurate collective timing per-collective. (Note: end-events are
// recorded by default. Turn on this flag can increase chances of a watchdog
// hang due to performing a CUDA event query which eventually calls
// cudaEventElapsedTime() API.
@Namespace("c10d") public static native @ByRef StringVector TORCH_NCCL_ENABLE_TIMING(); public static native void TORCH_NCCL_ENABLE_TIMING(StringVector setter);

// Enable monitoring thread which aborts the process when the ProcessGroupNCCL
// Watchdog thread gets stuck and no heartbeat is detected after
// TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC. This can happen due to calling CUDA/NCCL
// APIs that may hang. It is Useful to prevent jobs being stuck for a prolonged
// time than necessary tying up cluster resources.
@Namespace("c10d") public static native @ByRef StringVector TORCH_NCCL_ENABLE_MONITORING(); public static native void TORCH_NCCL_ENABLE_MONITORING(StringVector setter);

// Control the watchdog heartbeat timeout period after which the monitoring
// thread will abort the process.
@Namespace("c10d") public static native @ByRef StringVector TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC(); public static native void TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC(StringVector setter);

// Whether to rethrow CUDA Errors in the watchdog (default true)
@Namespace("c10d") public static native @ByRef StringVector TORCH_NCCL_RETHROW_CUDA_ERRORS(); public static native void TORCH_NCCL_RETHROW_CUDA_ERRORS(StringVector setter);

// The maximum number of events we store in the flight recorder's ring buffer.
// (One event could be the start or end of a collective, for example).
@Namespace("c10d") public static native @ByRef StringVector TORCH_NCCL_TRACE_BUFFER_SIZE(); public static native void TORCH_NCCL_TRACE_BUFFER_SIZE(StringVector setter);

// Control how much extra time we will wait for dumping the debugging info
// before we exit and throws timeout exception.
@Namespace("c10d") public static native @ByRef StringVector TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC(); public static native void TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC(StringVector setter);

// Control the interval inside the monitoring thread to check the coordinated
// signal from other ranks, e.g. to dump the debugging information.
@Namespace("c10d") public static native @ByRef StringVector TORCH_NCCL_COORD_CHECK_MILSEC(); public static native void TORCH_NCCL_COORD_CHECK_MILSEC(StringVector setter);

// Whether to log C++ stack traces on unclean shutdown (default true)
@Namespace("c10d") public static native @ByRef StringVector TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN(); public static native void TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN(StringVector setter);

// Control whether to use CudaEventCache for the collective in watchdog thread.
// We noticed in the past when cuda global lock is held, destroying CudaEvent
// can cause a hang.
@Namespace("c10d") public static native @ByRef StringVector TORCH_NCCL_CUDA_EVENT_CACHE(); public static native void TORCH_NCCL_CUDA_EVENT_CACHE(StringVector setter);

// Control the number of ranks each root can cover during NCCL comm init.
@Namespace("c10d") public static native @ByRef StringVector TORCH_NCCL_RANKS_PER_ROOT(); public static native void TORCH_NCCL_RANKS_PER_ROOT(StringVector setter);

@Namespace("c10d") public static native @ByRef StringVector TORCH_NCCL_NAN_CHECK(); public static native void TORCH_NCCL_NAN_CHECK(StringVector setter);

@Namespace("c10d") @MemberGetter public static native @Cast("const char*") BytePointer NCCL_BACKEND_NAME();

@Namespace("c10d") @MemberGetter public static native @Cast("const char*") BytePointer kStoreDumpKey();

@Namespace("c10d") @MemberGetter public static native @Cast("const char*") BytePointer kStoreErrorSignalKey();

@Namespace("c10d") @MemberGetter public static native int kWorkStatusUpdatePeriodMs(); // 30 seconds

// NoHandling: do not handle asynchronous NCCL errors
// TearDown: tear down process upon error, see `WorkNCCL::handleException`
// CleanUpOnly: just clean up collectives and abort communicators without
// tearing down process SkipCleanUp: (this is a temporary option and can be
// removed in future) tear down process without cleaning up NCCL communicators.
// This should be used as a last resort in case `ncclCommAbort` itself is
// hanging
/** enum c10d::ErrorHandlingMode */
public static final int
  NoHandling = 0,
  TearDown = 1,
  CleanUpOnly = 2,
  SkipCleanUp = 3;

// #define SHOULD_CLEAN_UP(a) (a != NoHandling && a != SkipCleanUp)

// #define SHOULD_TEAR_DOWN(a) (a != NoHandling && a != CleanUpOnly)

// #define PRINT_COLLECTIVE_HASH_SIGNATURE(phase, opType, numel, hashValue)
//   LOG(WARNING) << logPrefix() << "Hash of " << phase << " to NCCL " << opType
//                << " with size " << numel << " is " << hashValue;

// If set, ProcessGroupNCCL doesn't use recordStream calls to ensure
// caching allocator safety for tensors used on both user-facing and
// internal comm streams.
// Instead, it stashes live references to those tensors until after
// user-facing streams are synced with comm streams.
// See stashed_for_allocator_safety_ below.
@Namespace("c10d") public static native @ByRef StringVector TORCH_NCCL_AVOID_RECORD_STREAMS(); public static native void TORCH_NCCL_AVOID_RECORD_STREAMS(StringVector setter);

// If set, ProcessGroupNCCL registers postAlloc and preFree hooks to cuda cache
// allocator so that whenever a tensor is allocated or freed, ProcessGroupNCCL
// can register/deregister the tensor on all available NCCL communicators.
@Namespace("c10d") public static native @ByRef StringVector TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK(); public static native void TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK(StringVector setter);

// #if defined(__linux__)
// #else
// Targeting ../nccl/TensorShelf.java


// Targeting ../nccl/ProcessGroupNCCL.java



// Dumps the NCCL comm traces and additional information about the Process
// Group.
@Namespace("c10d") public static native @StdString BytePointer dump_nccl_trace(
    @Cast("bool") boolean includeCollectives,
    @Cast("bool") boolean includeStackTraces,
    @Cast("bool") boolean onlyActive);

// Dumps the NCCL comm traces and additional information about the Process
// Group in JSON formatted string.
// We don't include stack traces in JSON format as it is far too much data.
@Namespace("c10d") public static native @StdString BytePointer dump_nccl_trace_json(
    @Cast("bool") boolean includeCollectives,
    @Cast("bool") boolean onlyActive);

// Gets a mutable reference to a global optional function.Heartbeat Monitor
// will use this function to dump traces, if available. Inside fbcode, we
// store a function here that uses an internal tool for process tracing
// Targeting ../nccl/gil_checker_t.java



@Namespace("c10d") public static native @ByPtrRef gil_checker_t get_gil_checker();
 // namespace c10d

// #endif // USE_C10D_NCCL


}

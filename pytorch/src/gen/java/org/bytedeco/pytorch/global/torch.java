// Targeted by JavaCPP version 1.5.7: DO NOT EDIT THIS FILE

package org.bytedeco.pytorch.global;

import org.bytedeco.pytorch.*;

import org.bytedeco.pytorch.Allocator;
import org.bytedeco.pytorch.Function;
import org.bytedeco.pytorch.Module;
import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.javacpp.presets.javacpp.*;
import static org.bytedeco.openblas.global.openblas_nolapack.*;
import static org.bytedeco.openblas.global.openblas.*;

public class torch extends org.bytedeco.pytorch.presets.torch {
    static { Loader.load(); }

// Targeting ../BoolOptional.java


// Targeting ../ByteOptional.java


// Targeting ../IntOptional.java


// Targeting ../LongOptional.java


// Targeting ../DoubleOptional.java


// Targeting ../SizeTOptional.java


// Targeting ../StringOptional.java


// Targeting ../LongVectorOptional.java


// Targeting ../DoubleVectorOptional.java


// Targeting ../SizeTVectorOptional.java


// Targeting ../StringVectorOptional.java


// Targeting ../StrideVectorOptional.java


// Targeting ../ShapeSymbolVectorOptional.java


// Targeting ../TensorVectorOptional.java


// Targeting ../DeviceOptional.java


// Targeting ../LongArrayRefOptional.java


// Targeting ../DoubleArrayRefOptional.java


// Targeting ../LayoutOptional.java


// Targeting ../MemoryFormatOptional.java


// Targeting ../ScalarOptional.java


// Targeting ../ScalarTypeOptional.java


// Targeting ../AliasInfoOptional.java


// Targeting ../IValueOptional.java


// Targeting ../CppSignatureOptional.java


// Targeting ../DispatchKeyOptional.java


// Targeting ../OperatorHandleOptional.java


// Targeting ../OperatorNameOptional.java


// Targeting ../QualifiedNameOptional.java


// Targeting ../StreamOptional.java


// Targeting ../StrideOptional.java


// Targeting ../TypePtrOptional.java


// Targeting ../ClassTypePropertyOptional.java


// Targeting ../DimVectorOptional.java


// Targeting ../DimnameOptional.java


// Targeting ../DimnameListOptional.java


// Targeting ../GeneratorOptional.java


// Targeting ../TensorOptional.java


// Targeting ../TensorListOptional.java


// Targeting ../ThreadLocalStateOptional.java


// Targeting ../TypeMetaOptional.java


// Targeting ../InlinedCallStackOptional.java


// Targeting ../ScopeOptional.java


// Targeting ../ModuleInstanceInfoOptional.java


// Targeting ../SourceRangeOptional.java


// Targeting ../MethodOptional.java


// Targeting ../OperatorOptional.java


// Targeting ../NamedValueOptional.java


// Targeting ../ValueOptional.java


// Targeting ../LongExpandingArrayOptional.java


// Targeting ../DoubleExpandingArrayOptional.java


// Targeting ../StringSizeTSizeTTupleOptional.java


// Targeting ../IValueIValueDict.java


// Targeting ../TensorTensorOptional.java


// Targeting ../StringTensorDict.java


// Targeting ../StringModuleDict.java


// Targeting ../StringAnyModuleDict.java


// Targeting ../StringSharedModuleDict.java


// Targeting ../NonlinearityType.java


// Targeting ../FanModeType.java


// Targeting ../conv_padding_mode_t.java


// Targeting ../conv_padding_t1.java


// Targeting ../conv_padding_t2.java


// Targeting ../conv_padding_t3.java


// Targeting ../EmbeddingBagMode.java


// Targeting ../pad_mode_t.java


// Targeting ../loss_reduction_t.java


// Targeting ../kldiv_loss_reduction_t.java


// Targeting ../grid_sample_mode_t.java


// Targeting ../grid_sample_padding_mode_t.java


// Targeting ../rnn_options_base_mode_t.java


// Targeting ../rnn_nonlinearity_t.java


// Targeting ../upsample_mode_t.java


// Targeting ../interpolate_mode_t.java


// Targeting ../transformer_activation_t.java


// Targeting ../TensorDeque.java


// Targeting ../StringStringMap.java


// Targeting ../StringIntMap.java


// Targeting ../StringLongMap.java


// Targeting ../StringTensorMap.java


// Targeting ../RecordFunctionCallbackHandleVector.java


// Targeting ../DimnameVector.java


// Targeting ../FunctionPreHookVector.java


// Targeting ../FunctionPostHookVector.java


// Targeting ../TokenTrieVector.java


// Targeting ../SavedVariableVector.java


// Targeting ../DefVector.java


// Targeting ../PropertyVector.java


// Targeting ../InstructionVector.java


// Targeting ../CompilationUnitVector.java


// Targeting ../OptimizerParamGroupVector.java


// Targeting ../Bool2Vector.java


// Targeting ../BoolVector.java


// Targeting ../BytePointerVector.java


// Targeting ../LongVector.java


// Targeting ../DoubleVector.java


// Targeting ../SizeTVector.java


// Targeting ../StringVector.java


// Targeting ../StringLongVector.java


// Targeting ../IValueVector.java


// Targeting ../QEngineVector.java


// Targeting ../ScalarTypeVector.java


// Targeting ../SymbolVector.java


// Targeting ../ClassTypeVector.java


// Targeting ../TypeVector.java


// Targeting ../StrideVector.java


// Targeting ../ShapeSymbolVector.java


// Targeting ../TensorImplVector.java


// Targeting ../EdgeVector.java


// Targeting ../TensorVector.java


// Targeting ../TensorIndexVector.java


// Targeting ../TensorOptionalVector.java


// Targeting ../OperatorOptionalVector.java


// Targeting ../FunctionPreVector.java


// Targeting ../FunctionVector.java


// Targeting ../GraphVector.java


// Targeting ../OperatorVector.java


// Targeting ../ResolverVector.java


// Targeting ../SugaredValueVector.java


// Targeting ../StackEntryVector.java


// Targeting ../BlockVector.java


// Targeting ../ValueVector.java


// Targeting ../JitNodeVector.java


// Targeting ../ModuleVector.java


// Targeting ../AnyModuleVector.java


// Targeting ../SharedModuleVector.java


// Targeting ../SharedAnyModuleVector.java


// Targeting ../StringSharedModulePairVector.java


// Targeting ../ExampleVector.java


// Targeting ../EnumNameValue.java


// Targeting ../StringTensorPair.java


// Targeting ../TensorTuple.java


// Targeting ../TensorTensorTuple.java


// Targeting ../TensorTensorTensorTuple.java


// Targeting ../TensorTensorTensorTensorTuple.java


// Targeting ../TensorTensorTensorTensorTensorTuple.java


// Targeting ../TensorTensorTensorTensorVectorTuple.java


// Targeting ../TensorTensorTensorTensorLongTuple.java


// Targeting ../TensorTensorDoubleLongTuple.java


// Targeting ../TensorTensorTensorTupleTuple.java


// Targeting ../TensorMaybeOwnedTensorMaybeOwnedTuple.java


// Targeting ../TensorMaybeOwnedTensorMaybeOwnedTensorMaybeOwnedTuple.java


// Targeting ../PackedSequenceTensorTuple.java


// Targeting ../PackedSequenceTensorTensorTupleTuple.java


// Targeting ../StringSizeTSizeTTuple.java


// Targeting ../IValueIValueMap.java


// Targeting ../LongStringMap.java


// Targeting ../StringSizeTMap.java


// Targeting ../StringIValueMap.java


// Targeting ../StringFunctionMap.java


// Targeting ../StringValueMap.java


// Targeting ../StringLongStringMapMap.java


// Targeting ../ArgumentSpecExecutionPlanMap.java


// Targeting ../ValueValueMap.java


// Targeting ../StringSet.java


// Targeting ../IValueSet.java


// Targeting ../SymbolSet.java


// Targeting ../TensorImplSet.java


// Targeting ../RecordScopeSet.java


// Parsed from c10/macros/cmake_macros.h

// #ifndef C10_MACROS_CMAKE_MACROS_H_
// #define C10_MACROS_CMAKE_MACROS_H_

// Automatically generated header file for the C10 library.
// Do not include this file directly. Instead, include c10/macros/Macros.h.

// #define C10_BUILD_SHARED_LIBS
/* #undef C10_USE_GLOG */
/* #undef C10_USE_GFLAGS */
// #define C10_USE_NUMA
/* #undef C10_USE_MSVC_STATIC_RUNTIME */

// #endif // C10_MACROS_CMAKE_MACROS_H_


// Parsed from c10/macros/Export.h

// #ifndef C10_MACROS_EXPORT_H_
// #define C10_MACROS_EXPORT_H_

/* Header file to define the common scaffolding for exported symbols.
 *
 * Export is by itself a quite tricky situation to deal with, and if you are
 * hitting this file, make sure you start with the background here:
 * - Linux: https://gcc.gnu.org/wiki/Visibility
 * - Windows:
 * https://docs.microsoft.com/en-us/cpp/cpp/dllexport-dllimport?view=vs-2017
 *
 * Do NOT include this file directly. Instead, use c10/macros/Macros.h
 */

// You do not need to edit this part of file unless you are changing the core
// pytorch export abstractions.
//
// This part defines the C10 core export and import macros. This is controlled
// by whether we are building shared libraries or not, which is determined
// during build time and codified in c10/core/cmake_macros.h.
// When the library is built as a shared lib, EXPORT and IMPORT will contain
// visibility attributes. If it is being built as a static lib, then EXPORT
// and IMPORT basically have no effect.

// As a rule of thumb, you should almost NEVER mix static and shared builds for
// libraries that depend on c10. AKA, if c10 is built as a static library, we
// recommend everything dependent on c10 to be built statically. If c10 is built
// as a shared library, everything dependent on it should be built as shared. In
// the PyTorch project, all native libraries shall use the macro
// C10_BUILD_SHARED_LIB to check whether pytorch is building shared or static
// libraries.

// For build systems that do not directly depend on CMake and directly build
// from the source directory (such as Buck), one may not have a cmake_macros.h
// file at all. In this case, the build system is responsible for providing
// correct macro definitions corresponding to the cmake_macros.h.in file.
//
// In such scenarios, one should define the macro
//     C10_USING_CUSTOM_GENERATED_MACROS
// to inform this header that it does not need to include the cmake_macros.h
// file.

// #ifndef C10_USING_CUSTOM_GENERATED_MACROS
// #include <c10/macros/cmake_macros.h>
// #endif // C10_USING_CUSTOM_GENERATED_MACROS

// #ifdef _WIN32
// #else // _WIN32
// #if defined(__GNUC__)
// #define C10_EXPORT __attribute__((__visibility__("default")))
// #define C10_HIDDEN __attribute__((__visibility__("hidden")))
// #else // defined(__GNUC__)
// #define C10_EXPORT
// #define C10_HIDDEN
// #endif // defined(__GNUC__)
// #define C10_IMPORT C10_EXPORT
// #endif // _WIN32

// #ifdef NO_EXPORT
// #undef C10_EXPORT
// #define C10_EXPORT
// #endif

// Definition of an adaptive XX_API macro, that depends on whether you are
// building the library itself or not, routes to XX_EXPORT and XX_IMPORT.
// Basically, you will need to do this for each shared library that you are
// building, and the instruction is as follows: assuming that you are building
// a library called libawesome.so. You should:
// (1) for your cmake target (usually done by "add_library(awesome, ...)"),
//     define a macro called AWESOME_BUILD_MAIN_LIB using
//     target_compile_options.
// (2) define the AWESOME_API macro similar to the one below.
// And in the source file of your awesome library, use AWESOME_API to
// annotate public symbols.

// Here, for the C10 library, we will define the macro C10_API for both import
// and export.

// This one is being used by libc10.so
// #ifdef C10_BUILD_MAIN_LIB
// #define C10_API C10_EXPORT
// #else
// #define C10_API C10_IMPORT
// #endif

// This one is being used by libtorch.so
// #ifdef CAFFE2_BUILD_MAIN_LIB
// #define TORCH_API C10_EXPORT
// #else
// #define TORCH_API C10_IMPORT
// #endif

// You may be wondering: Whose brilliant idea was it to split torch_cuda into
// two pieces with confusing names?
// Once upon a time, there _was_ only TORCH_CUDA_API. All was happy until we
// tried to compile PyTorch for CUDA 11.1, which ran into relocation marker
// issues when linking big binaries.
// (https://github.com/pytorch/pytorch/issues/39968) We had two choices:
//    (1) Stop supporting so many GPU architectures
//    (2) Do something else
// We chose #2 and decided to split the behemoth that was torch_cuda into two
// smaller libraries, one with most of the core kernel functions (torch_cuda_cu)
// and the other that had..well..everything else (torch_cuda_cpp). The idea was
// this: instead of linking our static libraries (like the hefty
// libcudnn_static.a) with another huge library, torch_cuda, and run into pesky
// relocation marker issues, we could link our static libraries to a smaller
// part of torch_cuda (torch_cuda_cpp) and avoid the issues.

// libtorch_cuda_cu.so
// #ifdef TORCH_CUDA_CU_BUILD_MAIN_LIB
// #define TORCH_CUDA_CU_API C10_EXPORT
// #elif defined(BUILD_SPLIT_CUDA)
// #define TORCH_CUDA_CU_API C10_IMPORT
// #endif

// libtorch_cuda_cpp.so
// #ifdef TORCH_CUDA_CPP_BUILD_MAIN_LIB
// #define TORCH_CUDA_CPP_API C10_EXPORT
// #elif defined(BUILD_SPLIT_CUDA)
// #define TORCH_CUDA_CPP_API C10_IMPORT
// #endif

// libtorch_cuda.so (where torch_cuda_cu and torch_cuda_cpp are a part of the
// same api)
// #ifdef TORCH_CUDA_BUILD_MAIN_LIB
// #define TORCH_CUDA_CPP_API C10_EXPORT
// #define TORCH_CUDA_CU_API C10_EXPORT
// #elif !defined(BUILD_SPLIT_CUDA)
// #define TORCH_CUDA_CPP_API C10_IMPORT
// #define TORCH_CUDA_CU_API C10_IMPORT
// #endif

// #if defined(TORCH_HIP_BUILD_MAIN_LIB)
// #define TORCH_HIP_API C10_EXPORT
// #else
// #define TORCH_HIP_API C10_IMPORT
// #endif

// Enums only need to be exported on windows for non-CUDA files
// #if defined(_WIN32) && defined(__CUDACC__)
// #define C10_API_ENUM C10_API
// #else
// #define C10_API_ENUM
// #endif

// #endif // C10_MACROS_MACROS_H_


// Parsed from c10/macros/Macros.h

// #ifndef C10_MACROS_MACROS_H_
// #define C10_MACROS_MACROS_H_

/* Main entry for c10/macros.
 *
 * In your code, include c10/macros/Macros.h directly, instead of individual
 * files in this folder.
 */

// For build systems that do not directly depend on CMake and directly build
// from the source directory (such as Buck), one may not have a cmake_macros.h
// file at all. In this case, the build system is responsible for providing
// correct macro definitions corresponding to the cmake_macros.h.in file.
//
// In such scenarios, one should define the macro
//     C10_USING_CUSTOM_GENERATED_MACROS
// to inform this header that it does not need to include the cmake_macros.h
// file.

// #ifndef C10_USING_CUSTOM_GENERATED_MACROS
// #include <c10/macros/cmake_macros.h>
// #endif // C10_USING_CUSTOM_GENERATED_MACROS

// #include <c10/macros/Export.h>

// #if defined(__clang__)
// #define __ubsan_ignore_float_divide_by_zero__
//   __attribute__((no_sanitize("float-divide-by-zero")))
// #define __ubsan_ignore_undefined__ __attribute__((no_sanitize("undefined")))
// #define __ubsan_ignore_signed_int_overflow__
//   __attribute__((no_sanitize("signed-integer-overflow")))
// #define __ubsan_ignore_function__ __attribute__((no_sanitize("function")))
// #else
// #define __ubsan_ignore_float_divide_by_zero__
// #define __ubsan_ignore_undefined__
// #define __ubsan_ignore_signed_int_overflow__
// #define __ubsan_ignore_function__
// #endif

// Detect address sanitizer as some stuff doesn't work with it
// #undef C10_ASAN_ENABLED

// for clang
// #if defined(__has_feature)
// #if ((__has_feature(address_sanitizer)))
public static final int C10_ASAN_ENABLED = 1;
// #endif
// #endif

// for gcc
// #if defined(__SANITIZE_ADDRESS__)
// #if __SANITIZE_ADDRESS__
// #if !defined(C10_ASAN_ENABLED)
// #endif
// #endif
// #endif

// #if !defined(C10_ASAN_ENABLED)
// #endif

// Disable the copy and assignment operator for a class. Note that this will
// disable the usage of the class in std containers.
// #define C10_DISABLE_COPY_AND_ASSIGN(classname)
//   classname(const classname&) = delete;
//   classname& operator=(const classname&) = delete

// #define C10_CONCATENATE_IMPL(s1, s2) s1##s2
// #define C10_CONCATENATE(s1, s2) C10_CONCATENATE_IMPL(s1, s2)

// #define C10_MACRO_EXPAND(args) args

// #define C10_STRINGIZE_IMPL(x) #x
// #define C10_STRINGIZE(x) C10_STRINGIZE_IMPL(x)

/**
 * C10_ANONYMOUS_VARIABLE(str) introduces an identifier starting with
 * str and ending with a number that varies with the line.
 */
// #ifdef __COUNTER__
// #else
// #define C10_UID __LINE__
// #define C10_ANONYMOUS_VARIABLE(str) C10_CONCATENATE(str, __LINE__)
// #endif

// #ifdef __has_cpp_attribute
// #define C10_HAS_CPP_ATTRIBUTE(x) __has_cpp_attribute(x)
// #else
// #define C10_HAS_CPP_ATTRIBUTE(x) (0)
// #endif

/** C10_NODISCARD - Warn if a type or return value is discarded. */

// Technically, we should check if __cplusplus > 201402L here, because
// [[nodiscard]] is only defined in C++17.  However, some compilers
// we care about don't advertise being C++17 (e.g., clang), but
// support the attribute anyway.  In fact, this is not just a good idea,
// it's the law: clang::warn_unused_result doesn't work on nvcc + clang
// and the best workaround for this case is to use [[nodiscard]]
// instead; see https://github.com/pytorch/pytorch/issues/13118
//
// Note to future editors: if you have noticed that a compiler is
// misbehaving (e.g., it advertises support, but the support doesn't
// actually work, or it is emitting warnings).  Some compilers which
// are strict about the matter include MSVC, which will complain:
//
//  error C2429: attribute 'nodiscard' requires compiler flag '/std:c++latest'
//
// Exhibits:
//  - MSVC 19.14: https://godbolt.org/z/Dzd7gn (requires /std:c++latest)
//  - Clang 8.0.0: https://godbolt.org/z/3PYL4Z (always advertises support)
//  - gcc 8.3: https://godbolt.org/z/4tLMQS (always advertises support)
// #define C10_NODISCARD
// #if defined(__has_cpp_attribute)
// #if __has_cpp_attribute(nodiscard)
// #undef C10_NODISCARD
// #define C10_NODISCARD [[nodiscard]]
// #endif
// Workaround for llvm.org/PR23435, since clang 3.6 and below emit a spurious
// error when __has_cpp_attribute is given a scoped attribute in C mode.
// #elif __cplusplus && defined(__has_cpp_attribute)
// #if __has_cpp_attribute(clang::warn_unused_result)
// TODO: It's possible this is still triggering
// https://github.com/pytorch/pytorch/issues/13118 on Windows; if it is, better
// fix it.
// #undef C10_NODISCARD
// #define C10_NODISCARD [[clang::warn_unused_result]]
// #endif
// #endif

// suppress an unused variable.
// #if defined(_MSC_VER) && !defined(__clang__)
// #define C10_UNUSED __pragma(warning(suppress : 4100 4101))
// #else
// #define C10_UNUSED __attribute__((__unused__))
// #endif //_MSC_VER

// #define C10_RESTRICT __restrict

// Simply define the namespace, in case a dependent library want to refer to
// the c10 namespace but not any nontrivial files.
 // namespace c10

 // namespace c10

 // namespace c10

// Since C10 is the core library for caffe2 (and aten), we will simply reroute
// all abstractions defined in c10 to be available in caffe2 as well.
// This is only for backwards compatibility. Please use the symbols from the
// c10 namespace where possible.



 // namespace at

// WARNING!!! THIS IS A GIANT HACK!!!
// This line means you cannot simultaneously include c10/hip
// and c10/cuda and then use them from the at::cuda namespace.
// This is true in practice, because HIPIFY works inplace on
// files in ATen/cuda, so it assumes that c10::hip is available
// from at::cuda.  This namespace makes that happen.  When
// HIPIFY is no longer out-of-place, we can switch the cuda
// here to hip and everyone is happy.

 // namespace at

// C10_LIKELY/C10_UNLIKELY
//
// These macros provide parentheses, so you can use these macros as:
//
//    if C10_LIKELY(some_expr) {
//      ...
//    }
//
// NB: static_cast to boolean is mandatory in C++, because __builtin_expect
// takes a long argument, which means you may trigger the wrong conversion
// without it.
//
// #if defined(__GNUC__) || defined(__ICL) || defined(__clang__)
// #define C10_LIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 1))
// #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))
// #else
// #define C10_LIKELY(expr) (expr)
// #define C10_UNLIKELY(expr) (expr)
// #endif

/** C10_NOINLINE - Functions whose declaration is annotated with this will not
 *  be inlined. */
// #ifdef __GNUC__
// #define C10_NOINLINE __attribute__((noinline))
// #elif _MSC_VER
// #define C10_NOINLINE __declspec(noinline)
// #else
// #define C10_NOINLINE
// #endif

// #if defined(_MSC_VER)
// #elif __has_attribute(always_inline) || defined(__GNUC__)
// #define C10_ALWAYS_INLINE __attribute__((__always_inline__)) inline
// #else
// #define C10_ALWAYS_INLINE inline
// #endif

// C10_FALLTHROUGH - Annotate fallthrough to the next case in a switch.
// #if C10_HAS_CPP_ATTRIBUTE(fallthrough)
// #define C10_FALLTHROUGH [[fallthrough]]
// #else
// #define C10_FALLTHROUGH
// #endif

// #include <sstream>
// #include <string>

// #ifdef __HIPCC__
// Unlike CUDA, HIP requires a HIP header to be included for __host__ to work.
// We do this #include here so that C10_HOST_DEVICE and friends will Just Work.
// See https://github.com/ROCm-Developer-Tools/HIP/issues/441
// #include <hip/hip_runtime.h>
// #endif

// #if defined(__CUDACC__) || defined(__HIPCC__)
// #else
// #define C10_HOST_DEVICE
// #define C10_HOST
// #define C10_DEVICE
// #endif

// #ifdef __HIP_PLATFORM_HCC__
// #define C10_HIP_HOST_DEVICE __host__ __device__
// #else
// #define C10_HIP_HOST_DEVICE
// #endif

// #ifdef __HIP_PLATFORM_HCC__
// #define C10_WARP_SIZE warpSize // = 64 or 32 (Defined in hip_runtime.h)
// #else
// #define C10_WARP_SIZE 32
// #endif

// #if defined(_MSC_VER) && _MSC_VER <= 1900
// #endif

// CUDA_KERNEL_ASSERT checks the assertion
// even when NDEBUG is defined. This is useful for important assertions in CUDA
// code that would otherwise be suppressed when building Release.
// #if defined(__ANDROID__) || defined(__APPLE__) ||
//     (defined(__HIP_PLATFORM_HCC__) && ROCM_VERSION < 40100)
// Those platforms do not support assert()
// #define CUDA_KERNEL_ASSERT(cond)
// #elif defined(_MSC_VER)
// #else // __APPLE__, _MSC_VER
// #if defined(NDEBUG)
// #endif // NDEBUG
// #define CUDA_KERNEL_ASSERT(cond)
//   if (C10_UNLIKELY(!(cond))) {
//     __assert_fail(
//         #cond, __FILE__, static_cast<unsigned int>(__LINE__), __func__);
//   }
// #endif // __APPLE__

// #ifdef __APPLE__
// #include <TargetConditionals.h>
// #endif

// #if defined(__ANDROID__)
// #elif (
//     defined(__APPLE__) &&
//     (TARGET_IPHONE_SIMULATOR || TARGET_OS_SIMULATOR || TARGET_OS_IPHONE))
public static final int C10_IOS = 1;
public static final int C10_MOBILE = 1;
// #endif // ANDROID / IOS

// Portable determination of whether type T is trivially copyable.
// Warning: __has_trivial_copy for GCC may not always detect the non-POD
// correctly. For example, T = std::unique_ptr may evaluate to true and be
// treated as POD. This can cause unexpected behavior.
// #if defined(__GNUG__) && __GNUC__ < 5
// #define C10_IS_TRIVIALLY_COPYABLE(T) __has_trivial_copy(T)
// #else
// #define C10_IS_TRIVIALLY_COPYABLE(T) std::is_trivially_copyable<T>::value
// #endif

// #if !defined(__clang__) && !defined(_MSC_VER) && defined(__GNUC__) &&
//     __GNUC__ < 6
// #define CONSTEXPR_EXCEPT_GCC5
public static final int IS_NOT_GCC5_CONSTEXPR = 0;
// #else
// #define CONSTEXPR_EXCEPT_GCC5 constexpr
// #endif

// #if defined(__CUDA_ARCH__)
// #if defined(_MSC_VER) && defined(__CUDACC__)
// #define CONSTEXPR_EXCEPT_WIN_CUDA const
// #define C10_HOST_CONSTEXPR_EXCEPT_WIN_CUDA __host__

// Note [static constexpr char* members for windows NVCC]
// The Windows NVCC compiler doesn't handle static constexpr class members,
// although it's fixed in a later version.
// (see
// https://developercommunity.visualstudio.com/t/intellisense-error-c11-static-constexpr-member-ini/245425)
//
// If we want to ensure that our field is static under all builds, then we need
// to work around it specifically for windows NVCC by making it (a) const, (b)
// defined outside of the class definition We need to define it outside of the
// class definition because of the C++ standard; char* is not an integral type
// (see
// https://stackoverflow.com/questions/24278473/intellisense-a-member-of-type-const-char-const-cannot-have-an-in-class-in)
//
// So instead of this:
// struct Foo {
//     static constexpr const char* name = "foo";
// }
// In Windows NVCC, we end up with this:
// struct Foo {
//     static const char* name;
// }
// const char* Foo::name = "foo";
//
// This gives us a small perf hit for any code that wants to access these field
// members, but right now it isn't used in any perf-critical code paths.
// #define STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(field, val)
//   static const char* field;
// #define STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cls, field, val)
//   const char* cls::field = val;
// #else
// #define CONSTEXPR_EXCEPT_WIN_CUDA constexpr
// #define C10_HOST_CONSTEXPR_EXCEPT_WIN_CUDA __host__

// #define STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(field, val)
//   static constexpr const char* field = val;
// #define STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cls, field, val)
// #endif
// #else
// #if defined(_MSC_VER) && defined(__CUDACC__)
// #define CONSTEXPR_EXCEPT_WIN_CUDA const
// #define C10_HOST_CONSTEXPR_EXCEPT_WIN_CUDA

// #define STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(field, val)
//   static const char* field;
// #define STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cls, field, val)
//   const char* cls::field = val;
// #else
// #define CONSTEXPR_EXCEPT_WIN_CUDA constexpr
// #define C10_HOST_CONSTEXPR_EXCEPT_WIN_CUDA constexpr

// #define STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(field, val)
//   static constexpr const char* field = val;
// #define STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cls, field, val)
// #endif
// #endif

// #endif // C10_MACROS_MACROS_H_


// Parsed from c10/util/IdWrapper.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <functional>
// #include <utility>
// Targeting ../TypeIdentifierIdWrapper.java



 // namespace c10

// #define C10_DEFINE_HASH_FOR_IDWRAPPER(ClassName)
//   namespace std {
//   template <>
//   struct hash<ClassName> {
//     size_t operator()(ClassName x) const {
//       return hash_value(x);
//     }
//   };
//   }


// Parsed from c10/util/MaybeOwned.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/in_place.h>

// #include <type_traits>

/** Traits class describing how to borrow from T.  As a synopsis, here
 *  is how we might implement borrowing from an arbitrary type T using
 *  a raw pointer to const:
 * 
 *  template <typename T>
 *  struct MaybeOwnedTraits {
 *    using owned_type = T;
 *    using borrow_type = const T*;
 * 
 *    static borrow_type createBorrow(const owned_type& from) {
 *      return &from;
 *    }
 * 
 *    static void assignBorrow(borrow_type& lhs, borrow_type rhs) {
 *      lhs = rhs;
 *    }
 * 
 *    static void destroyBorrow(borrow_type& toDestroy) {}
 * 
 *    static const owned_type& referenceFromBorrow(const borrow_type& borrow) {
 *      return *borrow;
 *    }
 * 
 *    static const owned_type* pointerFromBorrow(const borrow_type& borrow) {
 *      return borrow;
 *    }
 * 
 *    static bool debugBorrowIsValid(const borrow_type& borrow) {
 *      return borrow != nullptr;
 *    }
 *  };
 * 
 *  (This implementation is not in use because MaybeOwned is an unsafe
 *  abstraction and we don't want to encourage it widely, just for
 *  skipping reference counting in critical paths.)
 * 
 *  For examples that are in use, see intrusive_ptr.h and TensorBody.h. */
// Targeting ../TensorMaybeOwned.java



 // namespace c10


// Parsed from c10/util/typeid.h

// #pragma once

// #include <atomic>
// #include <cassert>
// #include <complex>
// #include <cstdlib>
// #include <memory>
// #include <mutex>
// #include <type_traits>
// #include <unordered_map>
// #include <unordered_set>
// #include <vector>
// #ifdef __GXX_RTTI
// #include <typeinfo>
// #endif

// #include <exception>

// #include <c10/macros/Macros.h>
// #include <c10/util/Backtrace.h>
// #include <c10/util/C++17.h>
// #include <c10/util/Exception.h>
// #include <c10/util/IdWrapper.h>
// #include <c10/util/Type.h>
// #include <c10/util/TypeIndex.h>
// #include <c10/util/TypeTraits.h>
// #include <c10/util/flat_hash_map.h>

// #include <c10/core/ScalarType.h>

/*
 * TypeIdentifier is a small type containing an id.
 * Types must be registered using CAFFE_KNOWN_TYPE() for them to have a type id.
 * If a type is registered, you can also create an object containing meta data
 * like constructor, destructor, stringified name, ... about the type by calling
 * TypeMeta::Make<T>. This returns a TypeMeta() object, which is basically just
 * a pointer to the type information, so it's cheap to pass around.
 */

// TODO: This file is still in the caffe2 namespace, despite living
// in the ATen directory.  This is because the macro
// CAFFE_KNOWN_TYPE defines a template specialization, which relies
// on the namespace of TypeMeta matching the namespace where the macro is
// called.  This requires us to fix all of the call-sites, which I want to do
// later.  So the namespace is not fixed at the moment.

// Make at::Half a fundamental type.
 // namespace guts
 // namespace c10
// Targeting ../TypeIdentifier.java



// Allow usage in std::map / std::set
// TODO Disallow this and rather use std::unordered_map/set everywhere
@Namespace("caffe2") public static native @Cast("const bool") @Name("operator <") boolean lessThan(@ByVal TypeIdentifier lhs, @ByVal TypeIdentifier rhs);

@Namespace("caffe2") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(
    @Cast("std::ostream*") @ByRef Pointer stream,
    @ByVal TypeIdentifier typeId);

 // namespace caffe2

// Targeting ../DeviceTypeHash.java


  
// Targeting ../TypeMetaData.java



// Mechanism for throwing errors which can't be prevented at compile time
// due to type erasure. E.g. somebody calling TypeMeta::copy() for
// non-copyable type. Right now just throws exception but is implemented
// in .cpp to manage dependencies
@Namespace("caffe2::detail") public static native void _ThrowRuntimeTypeLogicError(@StdString BytePointer msg);
@Namespace("caffe2::detail") public static native void _ThrowRuntimeTypeLogicError(@StdString String msg);

/**
 * Placement new function for the type.
 */

/**
 * Typed copy function for classes.
 */

/**
 * A placeholder function for types that do not allow assignment.
 */
// Targeting ../_Uninitialized.java



 // namespace detail

//
// note: this is outside TypeMeta bc gcc seems to have trouble
// with scalarTypeItemSizes as a constexpr static member used by
// a public inline instance method
//

// item sizes for TypeMeta::itemsize() fast path
@Namespace("caffe2") @MemberGetter public static native @Cast("const uint8_t") byte scalarTypeItemSizes(int i);
@Namespace("caffe2") @MemberGetter public static native @Cast("const uint8_t*") BytePointer scalarTypeItemSizes();
// Targeting ../TypeMeta.java



// specializations of TypeMeta::_typeMetaData for ScalarType types

// #define DEFINE_SCALAR_METADATA_INSTANCE(T, name)
//   template <>
//   constexpr uint16_t TypeMeta::_typeMetaData<T>() noexcept {
//     return static_cast<uint16_t>(ScalarType::name);
//   }




@Namespace("caffe2") public static native @Cast("bool") @Name("operator ==") @NoException(true) boolean equals(@Const @ByVal TypeMeta lhs, @Const @ByVal TypeMeta rhs);
@Namespace("caffe2") public static native @Cast("bool") @Name("operator !=") @NoException(true) boolean notEquals(@Const @ByVal TypeMeta lhs, @Const @ByVal TypeMeta rhs);

@Namespace("caffe2") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(
    @Cast("std::ostream*") @ByRef Pointer stream,
    @ByVal TypeMeta typeMeta);

/**
 * Register unique id for a type so it can be used in TypeMeta context, e.g. be
 * used as a type for Blob or for Tensor elements.
 *
 * CAFFE_KNOWN_TYPE does explicit instantiation of TypeIdentifier::Get<T>
 * template function and thus needs to be put in a single translation unit (.cpp
 * file) for a given type T. Other translation units that use type T as a type
 * of the caffe2::Blob or element type of caffe2::Tensor need to depend on the
 * translation unit that contains CAFFE_KNOWN_TYPE declaration via regular
 * linkage dependencies.
 *
 * NOTE: the macro needs to be invoked in ::caffe2 namespace
 */
// Implementation note: in MSVC, we will need to prepend the C10_API
// keyword in order to get things compiled properly. in Linux, gcc seems to
// create attribute ignored error for explicit template instantiations, see
//   http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0537r0.html
//   https://gcc.gnu.org/bugzilla/show_bug.cgi?id=51930
// and as a result, we define these two macros slightly differently.
// #if defined(_MSC_VER) || defined(__clang__)
// #define EXPORT_IF_NOT_GCC C10_EXPORT
// #else
// #define EXPORT_IF_NOT_GCC
// #endif

// #define CAFFE_KNOWN_TYPE(T)
//   template <>
//   EXPORT_IF_NOT_GCC uint16_t TypeMeta::_typeMetaData<T>() noexcept {
//     static const uint16_t index = addTypeMetaData<T>();
//     return index;
//   }

// #define CAFFE_KNOWN_TYPE_NOEXPORT(T)
//   template <>
//   uint16_t TypeMeta::_typeMetaData<T>() noexcept {
//     static const uint16_t index = addTypeMetaData<T>();
//     return index;
//   }

 // namespace caffe2


// Parsed from c10/util/AlignOf.h

//===--- AlignOf.h - Portable calculation of type alignment -----*- C++ -*-===//
//
//                     The LLVM Compiler Infrastructure
//
// This file is distributed under the University of Illinois Open Source
// License. See LICENSE.TXT for details.
//
//===----------------------------------------------------------------------===//
//
// This file defines the AlignedCharArray and AlignedCharArrayUnion classes.
//
//===----------------------------------------------------------------------===//

// ATen: modified from llvm::AlignOf
// replaced LLVM_ALIGNAS with alignas

// #pragma once

// #include <cstddef>

/** \struct AlignedCharArray
 *  \brief Helper for building an aligned character array type.
 * 
 *  This template is used to explicitly build up a collection of aligned
 *  character array types. We have to build these up using a macro and explicit
 *  specialization to cope with MSVC (at least till 2015) where only an
 *  integer literal can be used to specify an alignment constraint. Once built
 *  up here, we can then begin to indirect between these using normal C++
 *  template parameters. */

// MSVC requires special handling here.
// #ifndef _MSC_VER

// #else // _MSC_VER

/** \brief Create a type with an aligned char buffer. */

// We provide special variations of this template for the most common
// alignments because __declspec(align(...)) doesn't actually work when it is
// a member of a by-value function argument in MSVC, even if the alignment
// request is something reasonably like 8-byte or 16-byte. Note that we can't
// even include the declspec with the union that forces the alignment because
// MSVC warns on the existence of the declspec despite the union member forcing
// proper alignment.

// The rest of these are provided with a __declspec(align(...)) and we simply
// can't pass them by-value as function arguments on MSVC.

// #define AT_ALIGNEDCHARARRAY_TEMPLATE_ALIGNMENT(x)
//   template <size_t Size>
//   struct AlignedCharArray<x, Size> {
//     __declspec(align(x)) char buffer[Size];
//   };

// #undef AT_ALIGNEDCHARARRAY_TEMPLATE_ALIGNMENT

// #endif // _MSC_VER
 // end namespace detail

/** \brief This union template exposes a suitably aligned and sized character
 *  array member which can hold elements of any of up to ten types.
 * 
 *  These types may be arrays, structs, or any other types. The goal is to
 *  expose a char array buffer member which can be used as suitable storage for
 *  a placement new of any of these types. Support for more than ten types can
 *  be added at the cost of more boilerplate. */
 // end namespace c10


// Parsed from c10/util/Deprecated.h

// #pragma once

/**
 * This file provides portable macros for marking declarations
 * as deprecated.  You should generally use C10_DEPRECATED,
 * except when marking 'using' declarations as deprecated,
 * in which case you should use C10_DEFINE_DEPRECATED_USING
 * (due to portability concerns).
 */

// Sample usage:
//
//    C10_DEPRECATED void bad_func();
//    struct C10_DEPRECATED BadStruct {
//      ...
//    };

// NB: __cplusplus doesn't work for MSVC, so for now MSVC always uses
// the "__declspec(deprecated)" implementation and not the C++14
// "[[deprecated]]" attribute. We tried enabling "[[deprecated]]" for C++14 on
// MSVC, but ran into issues with some older MSVC versions.
// #if (defined(__cplusplus) && __cplusplus >= 201402L)
// #define C10_DEPRECATED [[deprecated]]
// #define C10_DEPRECATED_MESSAGE(message) [[deprecated(message)]]
// #elif defined(__GNUC__)
// #define C10_DEPRECATED __attribute__((deprecated))
// TODO Is there some way to implement this?
// #define C10_DEPRECATED_MESSAGE(message) __attribute__((deprecated))

// #elif defined(_MSC_VER)
// #else
// #warning "You need to implement C10_DEPRECATED for this compiler"
// #define C10_DEPRECATED
// #endif

// Sample usage:
//
//    C10_DEFINE_DEPRECATED_USING(BadType, int)
//
//   which is the portable version of
//
//    using BadType [[deprecated]] = int;

// technically [[deprecated]] syntax is from c++14 standard, but it works in
// many compilers.
// #if defined(__has_cpp_attribute)
// #if __has_cpp_attribute(deprecated) && !defined(__CUDACC__)
// #define C10_DEFINE_DEPRECATED_USING(TypeName, TypeThingy)
//   using TypeName [[deprecated]] = TypeThingy;
// #endif
// #endif

// #if defined(_MSC_VER)
// #endif

// #if !defined(C10_DEFINE_DEPRECATED_USING) && defined(__GNUC__)
// nvcc has a bug where it doesn't understand __attribute__((deprecated))
// declarations even when the host compiler supports it. We'll only use this gcc
// attribute when not cuda, and when using a GCC compiler that doesn't support
// the c++14 syntax we checked for above (available in __GNUC__ >= 5)
// #if !defined(__CUDACC__)
// #define C10_DEFINE_DEPRECATED_USING(TypeName, TypeThingy)
//   using TypeName __attribute__((deprecated)) = TypeThingy;
// #else
// using cuda + gcc < 5, neither deprecated syntax is available so turning off.
// #define C10_DEFINE_DEPRECATED_USING(TypeName, TypeThingy)
//   using TypeName = TypeThingy;
// #endif
// #endif

// #if !defined(C10_DEFINE_DEPRECATED_USING)
// #warning "You need to implement C10_DEFINE_DEPRECATED_USING for this compiler"
// #define C10_DEFINE_DEPRECATED_USING
// #endif


// Parsed from c10/util/StringUtil.h

// #ifndef C10_UTIL_STRINGUTIL_H_
// #define C10_UTIL_STRINGUTIL_H_

// #include <c10/macros/Macros.h>
// #include <c10/util/string_utils.h>
// #include <c10/util/string_view.h>

// #include <cstddef>
// #include <ostream>
// #include <sstream>
// #include <string>
// #include <vector>

// Obtains the base name from a full path.
@Namespace("c10::detail") public static native @StdString BytePointer StripBasename(@StdString BytePointer full_path);
@Namespace("c10::detail") public static native @StdString String StripBasename(@StdString String full_path);

@Namespace("c10::detail") public static native @StdString BytePointer ExcludeFileExtension(@StdString BytePointer full_path);
@Namespace("c10::detail") public static native @StdString String ExcludeFileExtension(@StdString String full_path);
// Targeting ../CompileTimeEmptyString.java



@Namespace("c10::detail") public static native @Cast("std::ostream*") @ByRef Pointer _str(@Cast("std::ostream*") @ByRef Pointer ss);

@Namespace("c10::detail") public static native @Cast("std::ostream*") @ByRef @Name("_str") Pointer _strCompileTimeEmptyString(@Cast("std::ostream*") @ByRef Pointer ss, @Const @ByRef CompileTimeEmptyString t);
// Targeting ../_str_wrapper.java



// For c10::str() with an empty argument list (which is common in our assert
// macros), we don't want to pay the binary size for constructing and
// destructing a stringstream or even constructing a string.

 // namespace detail

// Convert a list of string-like arguments into a single string.

// Replace all occurrences of "from" substring to "to" string.
// Returns number of replacements
@Namespace("c10") public static native @Cast("size_t") long ReplaceAll(@StdString @ByRef BytePointer s, @Cast("const char*") BytePointer from, @Cast("const char*") BytePointer to);
@Namespace("c10") public static native @Cast("size_t") long ReplaceAll(@StdString @ByRef BytePointer s, String from, String to);
// Targeting ../SourceLocation.java





// unix isprint but insensitive to locale
@Namespace("c10") public static native @Cast("bool") boolean isPrint(@Cast("char") byte s);

@Namespace("c10") public static native void printQuotedString(@Cast("std::ostream*") @ByRef Pointer stmt, @ByVal @Cast("const c10::string_view*") Pointer str);

 // namespace c10

// #endif // C10_UTIL_STRINGUTIL_H_


// Parsed from c10/util/SmallVector.h

//===- llvm/ADT/SmallVector.h - 'Normally small' vectors --------*- C++ -*-===//
//
//                     The LLVM Compiler Infrastructure
//
// This file is distributed under the University of Illinois Open Source
// License. See LICENSE.TXT for details.
//
//===----------------------------------------------------------------------===//
//
// This file defines the SmallVector class.
//
//===----------------------------------------------------------------------===//

// ATen: modified from llvm::SmallVector.
// replaced report_bad_alloc_error with std::bad_alloc
// replaced isPodLike<T> with C10_IS_TRIVIALLY_COPYABLE (moved to Macros.h)
// replaced iterator_range constructor with inline Container&& constructor
// removed LLVM_NODISCARD and LLVM_ATTRIBUTE_ALWAYS_INLINE qualifiers
// removed LLVM_UNLIKELY

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/util/AlignOf.h>

// #include <algorithm>
// #include <cassert>
// #include <cstddef>
// #include <cstdlib>
// #include <cstring>
// #include <initializer_list>
// #include <iterator>
// #include <memory>
// #include <new>
// #include <type_traits>
// #include <utility>

// From llvm/Support/MathExtras.h
@Namespace("c10::detail") public static native @Cast("uint64_t") long NextPowerOf2(@Cast("uint64_t") long A);


// Targeting ../SmallVectorBase.java



/** This is the part of SmallVectorTemplateBase which does not depend on whether
 *  the type T is a POD. The extra dummy template argument is used by ArrayRef
 *  to avoid unnecessarily requiring T to be complete. */

/** SmallVectorTemplateBase<isPodLike = false> - This is where we put method
 *  implementations that are designed to work with non-POD-like T's. */

// Define this out-of-line to dissuade the C++ compiler from inlining it.


/** SmallVectorTemplateBase<isPodLike = true> - This is where we put method
 *  implementations that are designed to work with POD-like T's. */
// Targeting ../DimVectorImpl.java









/** Storage for the SmallVector elements which aren't contained in
 *  SmallVectorTemplateCommon. There are 'N-1' elements here. The remaining '1'
 *  element is in the base class. This is specialized for the N=1 and N=0 cases
 *  to avoid allocating unnecessary storage. */
// Targeting ../DimVector.java





 // end namespace c10

/** Implement std::swap in terms of SmallVector swap. */

/** Implement std::swap in terms of SmallVector swap. */

 // end namespace std


// Parsed from c10/util/Exception.h

// #ifndef C10_UTIL_EXCEPTION_H_
// #define C10_UTIL_EXCEPTION_H_

// #include <c10/macros/Macros.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/StringUtil.h>

// #include <cstddef>
// #include <exception>
// #include <ostream>
// #include <sstream>
// #include <string>
// #include <vector>

// #if defined(_MSC_VER) && _MSC_VER <= 1900
// #endif
// Targeting ../Error.java


// Targeting ../WarningHandler.java



// Note: [Verbatim Warnings]
// Warnings originating in C++ code can appear out-of-place to Python users:
// a user runs a line in Python, but the warning references a line in C++.
// Some parts of PyTorch, like the JIT, are cognizant of this mismatch
// and take care to map warnings back to the user's program, but most
// of PyTorch simply throws a context-free warning. To allow warning
// handlers to add context where appropriate, warn takes the
// "verbatim" flag. When this is false a warning handler might append
// the C++ warning to a Python warning message that relates the warning
// back to the user's program. Callers who have already accounted for
// context in their warnings should set verbatim to true so their warnings
// appear without modification.

/** Issue a warning with a given message. Dispatched to the current
 *  warning handler. */
@Namespace("c10::Warning") public static native void warn(
    @Const @ByRef SourceLocation source_location,
    @StdString BytePointer msg,
    @Cast("bool") boolean verbatim);
@Namespace("c10::Warning") public static native void warn(
    @Const @ByRef SourceLocation source_location,
    @StdString String msg,
    @Cast("bool") boolean verbatim);
@Namespace("c10::Warning") public static native void warn(
    @ByVal SourceLocation source_location,
    @ByVal CompileTimeEmptyString msg,
    @Cast("bool") boolean verbatim);
/** Sets the global warning handler. This is not thread-safe, so it should
 *  generally be called once during initialization or while holding the GIL
 *  for programs that use python.
 *  User is responsible for keeping the WarningHandler alive until
 *  it is not needed. */
@Namespace("c10::Warning") public static native @NoException(true) void set_warning_handler(WarningHandler handler);
/** Gets the global warning handler. */
@Namespace("c10::Warning") public static native @NoException(true) WarningHandler get_warning_handler();

/** The TORCH_WARN_ONCE macro is difficult to test for. Use
 *  setWarnAlways(true) to turn it into TORCH_WARN, which can be
 *  tested for more easily. */
@Namespace("c10::Warning") public static native @NoException(true) void set_warnAlways(@Cast("bool") boolean arg0);
@Namespace("c10::Warning") public static native @Cast("bool") @NoException(true) boolean get_warnAlways();
// Targeting ../WarnAlways.java




// Targeting ../IndexError.java


// Targeting ../ValueError.java


// Targeting ../TypeError.java


// Targeting ../NotImplementedError.java


// Targeting ../EnforceFiniteError.java


// Targeting ../OnnxfiBackendSystemError.java



// A utility function to return an exception std::string by prepending its
// exception type before its what() content
@Namespace("c10") public static native @StdString BytePointer GetExceptionString(@Cast("const std::exception*") @ByRef Pointer e);

 // namespace c10

// Private helper macro for implementing TORCH_INTERNAL_ASSERT and TORCH_CHECK
//
// Note: In the debug build With MSVC, __LINE__ might be of long type (a.k.a
// int32_t), which is different from the definition of `SourceLocation` that
// requires unsigned int (a.k.a uint32_t) and may cause a compile error with the
// message: error C2397: conversion from 'long' to 'uint32_t' requires a
// narrowing conversion Here the static cast is used to pass the build. if this
// is used inside a lambda the __func__ macro expands to operator(), which isn't
// very useful, but hard to fix in a macro so suppressing the warning.
// #define C10_THROW_ERROR(err_type, msg)
//   throw ::c10::err_type(
//       {__func__, __FILE__, static_cast<uint32_t>(__LINE__)}, msg)

// Private helper macro for workaround MSVC misexpansion of nested macro
// invocations involving __VA_ARGS__.  See
// https://stackoverflow.com/questions/5134523/msvc-doesnt-expand-va-args-correctly
// #define C10_EXPAND_MSVC_WORKAROUND(x) x

// On nvcc, C10_UNLIKELY thwarts missing return statement analysis.  In cases
// where the unlikely expression may be a constant, use this macro to ensure
// return statement analysis keeps working (at the cost of not getting the
// likely/unlikely annotation on nvcc).
// https://github.com/pytorch/pytorch/issues/21418
//
// Currently, this is only used in the error reporting macros below.  If you
// want to use it more generally, move me to Macros.h
//
// TODO: Brian Vaughan observed that we might be able to get this to work on
// nvcc by writing some sort of C++ overload that distinguishes constexpr inputs
// from non-constexpr.  Since there isn't any evidence that losing C10_UNLIKELY
// in nvcc is causing us perf problems, this is not yet implemented, but this
// might be an interesting piece of C++ code for an intrepid bootcamper to
// write.
// #if defined(__CUDACC__)
// #define C10_UNLIKELY_OR_CONST(e) e
// #else
// #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)
// #endif

// ----------------------------------------------------------------------------
// Error reporting macros
// ----------------------------------------------------------------------------

// #ifdef STRIP_ERROR_MESSAGES
// #define TORCH_RETHROW(e, ...) throw
// #else
// #define TORCH_RETHROW(e, ...)
//   do {
//     e.add_context(::c10::str(__VA_ARGS__));
//     throw;
//   } while (false)
// #endif

// A utility macro to provide assert()-like functionality; that is, enforcement
// of internal invariants in code.  It supports an arbitrary number of extra
// arguments (evaluated only on failure), which will be printed in the assert
// failure message using operator<< (this is useful to print some variables
// which may be useful for debugging.)
//
// Usage:
//    TORCH_INTERNAL_ASSERT(should_be_true);
//    TORCH_INTERNAL_ASSERT(x == 0, "x = ", x);
//
// Assuming no bugs in PyTorch, the conditions tested by this macro should
// always be true; e.g., it should be possible to disable all of these
// conditions without changing observable user behavior.  If you would like to
// do error reporting for user input, please use TORCH_CHECK instead.
//
// NOTE: It is SAFE to use this macro in production code; on failure, this
// simply raises an exception, it does NOT unceremoniously quit the process
// (unlike assert()).
//
// #ifdef STRIP_ERROR_MESSAGES
// #define TORCH_INTERNAL_ASSERT(cond, ...)
//   if (C10_UNLIKELY_OR_CONST(!(cond))) {
//     ::c10::detail::torchCheckFail(
//         __func__,
//         __FILE__,
//         static_cast<uint32_t>(__LINE__),
//         #cond "INTERNAL ASSERT FAILED at" C10_STRINGIZE(__FILE__));
//   }
// #else
// It would be nice if we could build a combined string literal out of
// the TORCH_INTERNAL_ASSERT prefix and a user-provided string literal
// as the first argument, but there doesn't seem to be any good way to
// do that while still supporting having a first argument that isn't a
// string literal.
// #define TORCH_INTERNAL_ASSERT(cond, ...)
//   if (C10_UNLIKELY_OR_CONST(!(cond))) {
//     ::c10::detail::torchInternalAssertFail(
//         __func__,
//         __FILE__,
//         static_cast<uint32_t>(__LINE__),
//         #cond
//         "INTERNAL ASSERT FAILED at " C10_STRINGIZE(__FILE__) ":" C10_STRINGIZE(
//             __LINE__) ", please report a bug to PyTorch. ",
//         c10::str(__VA_ARGS__));
//   }
// #endif

// A utility macro to make it easier to test for error conditions from user
// input.  Like TORCH_INTERNAL_ASSERT, it supports an arbitrary number of extra
// arguments (evaluated only on failure), which will be printed in the error
// message using operator<< (e.g., you can pass any object which has
// operator<< defined.  Most objects in PyTorch have these definitions!)
//
// Usage:
//    TORCH_CHECK(should_be_true); // A default error message will be provided
//                                 // in this case; but we recommend writing an
//                                 // explicit error message, as it is more
//                                 // user friendly.
//    TORCH_CHECK(x == 0, "Expected x to be 0, but got ", x);
//
// On failure, this macro will raise an exception.  If this exception propagates
// to Python, it will convert into a Python RuntimeError.
//
// NOTE: It is SAFE to use this macro in production code; on failure, this
// simply raises an exception, it does NOT unceremoniously quit the process
// (unlike CHECK() from glog.)
//
// #define TORCH_CHECK_WITH(error_t, cond, ...)
//   TORCH_CHECK_WITH_MSG(error_t, cond, "", __VA_ARGS__)

// #ifdef STRIP_ERROR_MESSAGES
// #define TORCH_CHECK_MSG(cond, type, ...)
//   (#cond #type " CHECK FAILED at " C10_STRINGIZE(__FILE__))
// #define TORCH_CHECK_WITH_MSG(error_t, cond, type, ...)
//   if (C10_UNLIKELY_OR_CONST(!(cond))) {
//     C10_THROW_ERROR(Error, TORCH_CHECK_MSG(cond, type, __VA_ARGS__));
//   }
// #else
@Namespace("c10::detail") public static native @Cast("const char*") BytePointer torchCheckMsgImpl(@Cast("const char*") BytePointer msg);
@Namespace("c10::detail") public static native String torchCheckMsgImpl(String msg);
// If there is just 1 user-provided C-string argument, use it.
@Namespace("c10::detail") public static native @Cast("const char*") BytePointer torchCheckMsgImpl(
    @Cast("const char*") BytePointer msg,
    @Cast("const char*") BytePointer args);
@Namespace("c10::detail") public static native String torchCheckMsgImpl(
    String msg,
    String args);
 // namespace detail
 // namespace c10

// #define TORCH_CHECK_MSG(cond, type, ...)
//   (::c10::detail::torchCheckMsgImpl(
//       "Expected " #cond
//       " to be true, but got false.  "
//       "(Could this error message be improved?  If so, "
//       "please report an enhancement request to PyTorch.)",
//       ##__VA_ARGS__))
// #define TORCH_CHECK_WITH_MSG(error_t, cond, type, ...)
//   if (C10_UNLIKELY_OR_CONST(!(cond))) {
//     C10_THROW_ERROR(error_t, TORCH_CHECK_MSG(cond, type, __VA_ARGS__));
//   }
// #endif

@Namespace("c10::detail") public static native void torchCheckFail(
    @Cast("const char*") BytePointer func,
    @Cast("const char*") BytePointer file,
    @Cast("uint32_t") int line,
    @StdString BytePointer msg);
@Namespace("c10::detail") public static native void torchCheckFail(
    String func,
    String file,
    @Cast("uint32_t") int line,
    @StdString String msg);

// The c10::str() call that creates userMsg can have 1 of 3 return
// types depending on the number and types of arguments passed to
// TORCH_INTERNAL_ASSERT.  0 arguments will get a
// CompileTimeEmptyString, 1 const char * will be passed straight
// through, and anything else will get converted to std::string.
@Namespace("c10::detail") public static native void torchInternalAssertFail(
    @Cast("const char*") BytePointer func,
    @Cast("const char*") BytePointer file,
    @Cast("uint32_t") int line,
    @Cast("const char*") BytePointer condMsg,
    @Cast("const char*") BytePointer userMsg);
@Namespace("c10::detail") public static native void torchInternalAssertFail(
    String func,
    String file,
    @Cast("uint32_t") int line,
    String condMsg,
    String userMsg);
@Namespace("c10::detail") public static native void torchInternalAssertFail(
    @Cast("const char*") BytePointer func,
    @Cast("const char*") BytePointer file,
    @Cast("uint32_t") int line,
    @Cast("const char*") BytePointer condMsg,
    @ByVal CompileTimeEmptyString userMsg);
@Namespace("c10::detail") public static native void torchInternalAssertFail(
    String func,
    String file,
    @Cast("uint32_t") int line,
    String condMsg,
    @ByVal CompileTimeEmptyString userMsg);

 // namespace detail
 // namespace c10

// #ifdef STRIP_ERROR_MESSAGES
// #define TORCH_CHECK(cond, ...)
//   if (C10_UNLIKELY_OR_CONST(!(cond))) {
//     ::c10::detail::torchCheckFail(
//         __func__,
//         __FILE__,
//         static_cast<uint32_t>(__LINE__),
//         TORCH_CHECK_MSG(cond, "", __VA_ARGS__));
//   }
// #else
// #define TORCH_CHECK(cond, ...)
//   if (C10_UNLIKELY_OR_CONST(!(cond))) {
//     ::c10::detail::torchCheckFail(
//         __func__,
//         __FILE__,
//         static_cast<uint32_t>(__LINE__),
//         TORCH_CHECK_MSG(cond, "", ##__VA_ARGS__));
//   }
// #endif

// An utility macro that does what `TORCH_CHECK` does if compiled in the host
// code, otherwise does nothing. Supposed to be used in the code shared between
// host and device code as an alternative for `TORCH_CHECK`.
// #if defined(__CUDACC__) || defined(__HIPCC__)
// #else
// #define TORCH_CHECK_IF_NOT_ON_CUDA(cond, ...) TORCH_CHECK(cond, ##__VA_ARGS__)
// #endif

// Debug only version of TORCH_INTERNAL_ASSERT. This macro only checks in debug
// build, and does nothing in release build.  It is appropriate to use
// in situations where you want to add an assert to a hotpath, but it is
// too expensive to run this assert on production builds.
// #ifdef NDEBUG
// Optimized version - generates no code.
// #define TORCH_INTERNAL_ASSERT_DEBUG_ONLY(...)
//   while (false)
//   C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(__VA_ARGS__))
// #else
// #define TORCH_INTERNAL_ASSERT_DEBUG_ONLY(...)
//   C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(__VA_ARGS__))
// #endif

// TODO: We're going to get a lot of similar looking string literals
// this way; check if this actually affects binary size.

// Like TORCH_CHECK, but raises IndexErrors instead of Errors.
// #define TORCH_CHECK_INDEX(cond, ...)
//   TORCH_CHECK_WITH_MSG(IndexError, cond, "INDEX", __VA_ARGS__)

// Like TORCH_CHECK, but raises ValueErrors instead of Errors.
// #define TORCH_CHECK_VALUE(cond, ...)
//   TORCH_CHECK_WITH_MSG(ValueError, cond, "VALUE", __VA_ARGS__)

// Like TORCH_CHECK, but raises TypeErrors instead of Errors.
// #define TORCH_CHECK_TYPE(cond, ...)
//   TORCH_CHECK_WITH_MSG(TypeError, cond, "TYPE", __VA_ARGS__)

// Like TORCH_CHECK, but raises NotImplementedErrors instead of Errors.
// #define TORCH_CHECK_NOT_IMPLEMENTED(cond, ...)
//   TORCH_CHECK_WITH_MSG(NotImplementedError, cond, "TYPE", __VA_ARGS__)

// Report a warning to the user.  Accepts an arbitrary number of extra
// arguments which are concatenated into the warning message using operator<<
//
// #ifdef STRIP_ERROR_MESSAGES
// #define TORCH_WARN(...)
//   ::c10::Warning::warn(
//       {__func__, __FILE__, static_cast<uint32_t>(__LINE__)},
//       ::c10::detail::CompileTimeEmptyString{},
//       false)
// #else
// #define TORCH_WARN(...)
//   ::c10::Warning::warn(
//       {__func__, __FILE__, static_cast<uint32_t>(__LINE__)},
//       ::c10::str(__VA_ARGS__),
//       false)
// #endif

// Report a warning to the user only once.  Accepts an arbitrary number of extra
// arguments which are concatenated into the warning message using operator<<
//
// #ifdef STRIP_ERROR_MESSAGES
// #define _TORCH_WARN_ONCE(...)
//   C10_UNUSED static const auto C10_ANONYMOUS_VARIABLE(torch_warn_once_) =
//       [&] {
//         ::c10::Warning::warn(
//             {__func__, __FILE__, static_cast<uint32_t>(__LINE__)},
//             ::c10::detail::CompileTimeEmptyString{},
//             false);
//         return true;
//       }()
// #else
// #define _TORCH_WARN_ONCE(...)
//   C10_UNUSED static const auto C10_ANONYMOUS_VARIABLE(torch_warn_once_) =
//       [&] {
//         ::c10::Warning::warn(
//             {__func__, __FILE__, static_cast<uint32_t>(__LINE__)},
//             ::c10::str(__VA_ARGS__),
//             false);
//         return true;
//       }()
// #endif

// #define TORCH_WARN_ONCE(...)
//   if (::c10::Warning::get_warnAlways()) {
//     TORCH_WARN(__VA_ARGS__);
//   } else {
//     _TORCH_WARN_ONCE(__VA_ARGS__);
//   }

// ----------------------------------------------------------------------------
// Deprecated macros
// ----------------------------------------------------------------------------

/*
// Deprecation disabled until we fix sites in our codebase
C10_DEPRECATED_MESSAGE("AT_ERROR(msg) is deprecated, use TORCH_CHECK(false, msg)
instead.")
*/
@Namespace("c10::detail") public static native void deprecated_AT_ERROR();

/*
// Deprecation disabled until we fix sites in our codebase
C10_DEPRECATED_MESSAGE("AT_ASSERT is deprecated, if you mean to indicate an
internal invariant failure, use " \
                       "TORCH_INTERNAL_ASSERT instead; if you mean to do user
error checking, use " \ "TORCH_CHECK.  See
https://github.com/pytorch/pytorch/issues/20287 for more details.")
*/
@Namespace("c10::detail") public static native void deprecated_AT_ASSERT();

/*
// Deprecation disabled until we fix sites in our codebase
C10_DEPRECATED_MESSAGE("AT_ASSERTM is deprecated, if you mean to indicate an
internal invariant failure, use " \
                       "TORCH_INTERNAL_ASSERT instead; if you mean to do user
error checking, use " \ "TORCH_CHECK.  See
https://github.com/pytorch/pytorch/issues/20287 for more details.")
*/
@Namespace("c10::detail") public static native void deprecated_AT_ASSERTM();

 // namespace detail
 // namespace c10

// Deprecated alias; this alias was deprecated because people kept mistakenly
// using it for user error checking.  Use TORCH_INTERNAL_ASSERT or TORCH_CHECK
// instead. See https://github.com/pytorch/pytorch/issues/20287 for more
// details.
// #define AT_ASSERT(...)
//   do {
//     ::c10::detail::deprecated_AT_ASSERT();
//     C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(__VA_ARGS__));
//   } while (false)

// Deprecated alias, like AT_ASSERT.  The new TORCH_INTERNAL_ASSERT macro
// supports both 0-ary and variadic calls, so having a separate
// message-accepting macro is not necessary.
//
// NB: we MUST include cond explicitly here, as MSVC will miscompile the macro
// expansion, shunting all of __VA_ARGS__ to cond.  An alternate workaround
// can be seen at
// https://stackoverflow.com/questions/5134523/msvc-doesnt-expand-va-args-correctly
// #define AT_ASSERTM(cond, ...)
//   do {
//     ::c10::detail::deprecated_AT_ASSERTM();
//     C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__));
//   } while (false)

// Deprecated alias; this alias was deprecated because it represents extra API
// surface that makes it hard for people to understand what macro to use.
// Use TORCH_CHECK(false, ...) or TORCH_INTERNAL_ASSERT(false, ...) to
// unconditionally fail at a line of code.
// #define AT_ERROR(...)
//   do {
//     ::c10::detail::deprecated_AT_ERROR();
//     C10_EXPAND_MSVC_WORKAROUND(TORCH_CHECK(false, ::c10::str(__VA_ARGS__)));
//   } while (false)

// #endif // C10_UTIL_EXCEPTION_H_


// Parsed from c10/util/ArrayRef.h

//===--- ArrayRef.h - Array Reference Wrapper -------------------*- C++ -*-===//
//
//                     The LLVM Compiler Infrastructure
//
// This file is distributed under the University of Illinois Open Source
// License. See LICENSE.TXT for details.
//
//===----------------------------------------------------------------------===//

// ATen: modified from llvm::ArrayRef.
// removed llvm-specific functionality
// removed some implicit const -> non-const conversions that rely on
// complicated std::enable_if meta-programming
// removed a bunch of slice variants for simplicity...

// #pragma once

// #include <c10/util/C++17.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Exception.h>
// #include <c10/util/SmallVector.h>

// #include <array>
// #include <iterator>
// #include <vector>
// Targeting ../ByteArrayRef.java


// Targeting ../ShortArrayRef.java


// Targeting ../IntArrayRef.java


// Targeting ../LongArrayRef.java


// Targeting ../FloatArrayRef.java


// Targeting ../DoubleArrayRef.java


// Targeting ../SizeTArrayRef.java


// Targeting ../StringArrayRef.java


// Targeting ../BoolArrayRef.java


// Targeting ../HalfArrayRef.java


// Targeting ../BFloat16ArrayRef.java


// Targeting ../FloatComplexrrayRef.java


// Targeting ../DoubleComplexrrayRef.java


// Targeting ../ScalarTypeArrayRef.java


// Targeting ../IValueArrayRef.java


// Targeting ../EnumNameValueArrayRef.java


// Targeting ../TypeArrayRef.java


// Targeting ../SymbolArrayRef.java


// Targeting ../StrideArrayRef.java


// Targeting ../DimnameArrayRef.java


// Targeting ../ScalarArrayRef.java


// Targeting ../TensorArrayRef.java


// Targeting ../TensorArgArrayRef.java


// Targeting ../TensorIndexArrayRef.java


// Targeting ../TensorOptionalArrayRef.java


// Targeting ../SavedVariableArrayRef.java


// Targeting ../SugaredValueArrayRef.java


// Targeting ../NamedValueArrayRef.java


// Targeting ../BlockArrayRef.java


// Targeting ../ValueArrayRef.java





// WARNING: Template instantiation will NOT be willing to do an implicit
// conversions to get you to an c10::ArrayRef, which is why we need so
// many overloads.













// This alias is deprecated because it doesn't make ownership
// semantics obvious.  Use IntArrayRef instead!
 // namespace c10


// Parsed from c10/util/complex.h

// #pragma once

// #include <complex>

// #include <c10/macros/Macros.h>

// #if defined(__CUDACC__) || defined(__HIPCC__)
// #endif

// c10::complex is an implementation of complex numbers that aims
// to work on all devices supported by PyTorch
//
// Most of the APIs duplicates std::complex
// Reference: https://en.cppreference.com/w/cpp/numeric/complex
//
// [NOTE: Complex Operator Unification]
// Operators currently use a mix of std::complex, thrust::complex, and
// c10::complex internally. The end state is that all operators will use
// c10::complex internally.  Until then, there may be some hacks to support all
// variants.
//
//
// [Note on Constructors]
//
// The APIs of constructors are mostly copied from C++ standard:
//   https://en.cppreference.com/w/cpp/numeric/complex/complex
//
// Since C++14, all constructors are constexpr in std::complex
//
// There are three types of constructors:
// - initializing from real and imag:
//     `constexpr complex( const T& re = T(), const T& im = T() );`
// - implicitly-declared copy constructor
// - converting constructors
//
// Converting constructors:
// - std::complex defines converting constructor between float/double/long
// double,
//   while we define converting constructor between float/double.
// - For these converting constructors, upcasting is implicit, downcasting is
//   explicit.
// - We also define explicit casting from std::complex/thrust::complex
//   - Note that the conversion from thrust is not constexpr, because
//     thrust does not define them as constexpr ????
//
//
// [Operator =]
//
// The APIs of operator = are mostly copied from C++ standard:
//   https://en.cppreference.com/w/cpp/numeric/complex/operator%3D
//
// Since C++20, all operator= are constexpr. Although we are not building with
// C++20, we also obey this behavior.
//
// There are three types of assign operator:
// - Assign a real value from the same scalar type
//   - In std, this is templated as complex& operator=(const T& x)
//     with specialization `complex& operator=(T x)` for float/double/long
//     double Since we only support float and double, on will use `complex&
//     operator=(T x)`
// - Copy assignment operator and converting assignment operator
//   - There is no specialization of converting assignment operators, which type
//   is
//     convertible is solely dependent on whether the scalar type is convertible
//
// In addition to the standard assignment, we also provide assignment operators
// with std and thrust
//
//
// [Casting operators]
//
// std::complex does not have casting operators. We define casting operators
// casting to std::complex and thrust::complex
//
//
// [Operator ""]
//
// std::complex has custom literals `i`, `if` and `il` defined in namespace
// `std::literals::complex_literals`. We define our own custom literals in the
// namespace `c10::complex_literals`. Our custom literals does not follow the
// same behavior as in std::complex, instead, we define _if, _id to construct
// float/double complex literals.
//
//
// [real() and imag()]
//
// In C++20, there are two overload of these functions, one it to return the
// real/imag, another is to set real/imag, they are both constexpr. We follow
// this design.
//
//
// [Operator +=,-=,*=,/=]
//
// Since C++20, these operators become constexpr. In our implementation, they
// are also constexpr.
//
// There are two types of such operators: operating with a real number, or
// operating with another complex number. For the operating with a real number,
// the generic template form has argument type `const T &`, while the overload
// for float/double/long double has `T`. We will follow the same type as
// float/double/long double in std.
//
// [Unary operator +-]
//
// Since C++20, they are constexpr. We also make them expr
//
// [Binary operators +-*/]
//
// Each operator has three versions (taking + as example):
// - complex + complex
// - complex + real
// - real + complex
//
// [Operator ==, !=]
//
// Each operator has three versions (taking == as example):
// - complex == complex
// - complex == real
// - real == complex
//
// Some of them are removed on C++20, but we decide to keep them
//
// [Operator <<, >>]
//
// These are implemented by casting to std::complex
//
//
//
// TODO(@zasdfgbnm): c10::complex<c10::Half> is not currently supported,
// because:
//  - lots of members and functions of c10::Half are not constexpr
//  - thrust::complex only support float and double









 // namespace complex_literals

// Define operators between integral scalars and c10::complex. std::complex does
// not support this when T is a floating-point number. This is useful because it
// saves a lot of "static_cast" when operate a complex and an integer. This
// makes the code both less verbose and potentially more efficient.
// #define COMPLEX_INTEGER_OP_TEMPLATE_CONDITION
//   typename std::enable_if_t<
//       std::is_floating_point<fT>::value && std::is_integral<iT>::value,
//       int> = 0

// #undef COMPLEX_INTEGER_OP_TEMPLATE_CONDITION















 // namespace c10

// std functions
//
// The implementation of these functions also follow the design of C++20

// #if defined(__CUDACC__) || defined(__HIPCC__)
// #endif

// #ifdef __HIP_PLATFORM_HCC__
// #define ROCm_Bug(x)
// #else
// #define ROCm_Bug(x) x
// #endif

// #undef ROCm_Bug

// For std::conj, there are other versions of it:
//   constexpr std::complex<float> conj( float z );
//   template< class DoubleOrInteger >
//   constexpr std::complex<double> conj( DoubleOrInteger z );
//   constexpr std::complex<long double> conj( long double z );
// These are not implemented
// TODO(@zasdfgbnm): implement them as c10::conj

// Thrust does not have complex --> complex version of thrust::proj,
// so this function is not implemented at c10 right now.
// TODO(@zasdfgbnm): implement it by ourselves

// There is no c10 version of std::polar, because std::polar always
// returns std::complex. Use c10::polar instead;

 // namespace std

 // namespace c10

// #define C10_INTERNAL_INCLUDE_COMPLEX_REMAINING_H
// math functions are included in a separate file
// #include <c10/util/complex_math.h>
// utilities for complex types
// #include <c10/util/complex_utils.h>
// #undef C10_INTERNAL_INCLUDE_COMPLEX_REMAINING_H


// Parsed from c10/util/Half.h

// #pragma once

/** Defines the Half type (half-precision floating-point) including conversions
 *  to standard C types and basic arithmetic operations. Note that arithmetic
 *  operations are implemented by converting to floating point and
 *  performing the operation in float32, instead of using CUDA half intrinsics.
 *  Most uses of this type within ATen are memory bound, including the
 *  element-wise kernels, and the half intrinsics aren't efficient on all GPUs.
 *  If you are writing a compute bound kernel, you can use the CUDA half
 *  intrinsics directly on the Half type from device code. */

// #include <c10/macros/Macros.h>
// #include <c10/util/C++17.h>
// #include <c10/util/complex.h>

// #if defined(__cplusplus) && (__cplusplus >= 201103L)
// #include <cmath>
// #include <cstdint>
// #elif !defined(__OPENCL_VERSION__)
// #include <math.h>
// #include <stdint.h>
// #endif

// #ifdef _MSC_VER
// #include <intrin.h>
// #endif

// #include <complex>
// #include <cstdint>
// #include <cstring>
// #include <iosfwd>
// #include <limits>
// #include <sstream>
// #include <stdexcept>
// #include <string>
// #include <utility>

// #ifdef __CUDACC__
// #include <cuda_fp16.h>
// #endif

// #ifdef __HIPCC__
// #include <hip/hip_fp16.h>
// #endif

// Standard check for compiling CUDA with clang
// #if defined(__clang__) && defined(__CUDA__) && defined(__CUDA_ARCH__)
// #define C10_DEVICE_HOST_FUNCTION __device__ __host__
// #else
// #define C10_DEVICE_HOST_FUNCTION
// #endif

@Namespace("c10::detail") public static native float fp32_from_bits(@Cast("uint32_t") int w);

@Namespace("c10::detail") public static native @Cast("uint32_t") int fp32_to_bits(float f);

/*
 * Convert a 16-bit floating-point number in IEEE half-precision format, in bit
 * representation, to a 32-bit floating-point number in IEEE single-precision
 * format, in bit representation.
 *
 * @note The implementation doesn't use any floating-point operations.
 */
@Namespace("c10::detail") public static native @Cast("uint32_t") int fp16_ieee_to_fp32_bits(@Cast("uint16_t") short h);

/*
 * Convert a 16-bit floating-point number in IEEE half-precision format, in bit
 * representation, to a 32-bit floating-point number in IEEE single-precision
 * format.
 *
 * @note The implementation relies on IEEE-like (no assumption about rounding
 * mode and no operations on denormals) floating-point operations and bitcasts
 * between integer and floating-point variables.
 */
@Namespace("c10::detail") public static native float fp16_ieee_to_fp32_value(@Cast("uint16_t") short h);

/*
 * Convert a 32-bit floating-point number in IEEE single-precision format to a
 * 16-bit floating-point number in IEEE half-precision format, in bit
 * representation.
 *
 * @note The implementation relies on IEEE-like (no assumption about rounding
 * mode and no operations on denormals) floating-point operations and bitcasts
 * between integer and floating-point variables.
 */
@Namespace("c10::detail") public static native @Cast("uint16_t") short fp16_ieee_from_fp32_value(float f);


// Targeting ../Half.java



// This is just a placeholder for whatever complex representation we
// end up deciding to use for half-precision complex numbers.

// In some versions of MSVC, there will be a compiler error when building.
// C4146: unary minus operator applied to unsigned type, result still unsigned
// C4804: unsafe use of type 'bool' in operation
// It can be addressed by disabling the following warning.
// #ifdef _MSC_VER
// #pragma warning(push)
// #pragma warning(disable : 4146)
// #pragma warning(disable : 4804)
// #pragma warning(disable : 4018)
// #endif

// The overflow checks may involve float to int conversion which may
// trigger precision loss warning. Re-enable the warning once the code
// is fixed. See T58053069.
// #ifdef __clang__
// #pragma GCC diagnostic push
// #pragma GCC diagnostic ignored "-Wunknown-warning-option"
// #pragma GCC diagnostic ignored "-Wimplicit-int-float-conversion"
// #endif

// bool can be converted to any type.
// Without specializing on bool, in pytorch_linux_trusty_py2_7_9_build:
// `error: comparison of constant '255' with boolean expression is always false`
// for `f > limit::max()` below

// skip isnan and isinf check for integral types

// #ifdef __clang__
// #pragma GCC diagnostic pop
// #endif

// #ifdef _MSC_VER
// #pragma warning(pop)
// #endif



 // namespace c10

// #include <c10/util/Half-inl.h>


// Parsed from c10/util/qint32.h

// #pragma once
// #include <cstdint>

// #include <c10/macros/Macros.h>
// Targeting ../qint32.java



 // namespace c10


// Parsed from c10/util/qint8.h

// #pragma once
// #include <cstdint>

// #include <c10/macros/Macros.h>
// Targeting ../qint8.java



 // namespace c10


// Parsed from c10/util/quint8.h

// #pragma once
// #include <cstdint>

// #include <c10/macros/Macros.h>
// Targeting ../quint8.java



 // namespace c10


// Parsed from c10/util/BFloat16.h

// #pragma once

// Defines the bloat16 type (brain floating-point). This representation uses
// 1 bit for the sign, 8 bits for the exponent and 7 bits for the mantissa.

// #include <c10/macros/Macros.h>
// #include <cmath>
// #include <cstring>

// #if defined(CUDA_VERSION) && CUDA_VERSION >= 11000
// #endif
@Namespace("c10::detail") public static native float f32_from_bits(@Cast("uint16_t") short src);

@Namespace("c10::detail") public static native @Cast("uint16_t") short bits_from_f32(float src);

@Namespace("c10::detail") public static native @Cast("uint16_t") short round_to_nearest_even(float src);

// Targeting ../BFloat16.java



 // namespace c10

// #include <c10/util/BFloat16-inl.h>


// Parsed from c10/util/quint4x2.h

// #pragma once
// #include <cstdint>

// #include <c10/macros/Macros.h>
// Targeting ../quint4x2.java



 // namespace c10


// Parsed from c10/util/ThreadLocalDebugInfo.h

// #pragma once

// #include <c10/macros/Export.h>
// #include <c10/util/Exception.h>

// #include <memory>
// #include <string>
// #include <unordered_map>

@Namespace("c10") public enum DebugInfoKind {
  PRODUCER_INFO((byte)(0)),
  MOBILE_RUNTIME_INFO((byte)(1)),
  PROFILER_STATE((byte)(2)),
  INFERENCE_CONTEXT((byte)(3)), // for inference usage
  PARAM_COMMS_INFO((byte)(4)),

  TEST_INFO((byte)(5)), // used only in tests
  TEST_INFO_2((byte)(6));// used only in tests

    public final byte value;
    private DebugInfoKind(byte v) { this.value = v; }
    private DebugInfoKind(DebugInfoKind e) { this.value = e.value; }
    public DebugInfoKind intern() { for (DebugInfoKind e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../DebugInfoBase.java


// Targeting ../ThreadLocalDebugInfo.java


// Targeting ../DebugInfoGuard.java



 // namespace c10


// Parsed from c10/util/Type.h

// #ifndef C10_UTIL_TYPE_H_
// #define C10_UTIL_TYPE_H_

// #include <cstddef>
// #include <string>
// #include <typeinfo>

// #include <c10/macros/Macros.h>

/** Utility to demangle a C++ symbol name. */
@Namespace("c10") public static native @StdString BytePointer demangle(@Cast("const char*") BytePointer name);
@Namespace("c10") public static native @StdString String demangle(String name);

/** Returns the printable name of the type. */

 // namespace c10

// #endif // C10_UTIL_TYPE_H_


// Parsed from c10/util/TypeCast.h

// #pragma once
// #include <c10/core/ScalarType.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/BFloat16.h>
// #include <c10/util/Half.h>

// #include <type_traits>

// Note: deliberately ignores undefined behavior, consistent with NumPy.
// PyTorch's type conversions can cause a variety of undefined behavior,
// including float to integral overflow and signed to unsigned integer overflow.
// Some of this undefined behavior is addressed below.

// Partial template instantiation for casting to uint8.
// Note: Converting from negative float values to unsigned integer types is
// undefined behavior in C++, and current CPU and GPU compilers exhibit
// divergent behavior. Casting from negative float values to signed
// integer types and then to unsigned integer types is not undefined,
// however, so this cast improves the consistency of type conversions
// to uint8 across compilers.
// Further note: Type conversions across compilers still have other undefined
// and divergent behavior.

// Dynamic type casting utils:
// - fetch_and_cast
// - cast_and_store
//
// fetch_and_cast fetch a value with dynamic type specified by a ScalarType
// from a void pointer and cast it to a static type.
//
// cast_and_store casts a static typed value into dynamic type specified
// by a ScalarType, and store it into a void pointer.
//
// NOTE:
//
// Dynamic casting allows us to support type promotion without blowing up
// the combination space: For example, without dynamic cast, in order to
// implement `add_` with type promotion, we would need something like
//
// AT_DISPATCH_ALL_TYPES(output.dtype(),
//    AT_DISPATCH_ALL_TYPES(input1.dtype(),
//       AT_DISPATCH_ALL_TYPES(input2.dtype(),
//           [](arg0_t a, arg1_t b) -> out_t { return a + b; }
//       )
//    )
// )
//
// If we support N dtypes, the above code would generate the a+b kernel for
// all the N * N * N different supported types, the compilation time and
// binary size would become horrible.
//
// Dynamic casting might sounds like a bad idea in terms of performance.
// Especially if you ever do it in a loop, you are going to do a billion tests.
// But in practice it is not as bad as it might look:
//
// - on CPU, this is a branch that always has the same outcome, therefore
//   hopefully the branch predictor could do the job pretty well
// - on GPU, these branches will not diverge, so we could still have the same
//   warp executing the same line of code
// - Most kernels, like `add`, are bandwidth bound, adding a few clock cycles to
//   check an integer does not hurt the performance much because the ALUs would
//   wait for load instructions anyway.
//
// For the discussion and benchmark, refer to:
// - https://github.com/pytorch/pytorch/pull/28343
// - https://github.com/pytorch/pytorch/pull/28344
// - https://github.com/pytorch/pytorch/pull/28345
//

// #ifdef C10_HOST_DEVICE
// #else
// #define ERROR_UNSUPPORTED_CAST TORCH_CHECK(false, "Unexpected scalar type");
// #endif

// Fetch a value with dynamic type src_type from ptr, and cast it to static type
// dest_t.
// #define FETCH_AND_CAST_CASE(type, scalartype)
//   case ScalarType::scalartype:
//     return static_cast_with_inter_type<dest_t, type>::apply(*(const type*)ptr);
@Namespace("c10") public static native @ByVal @Name("fetch_and_cast<c10::qint8>") qint8 fetch_and_cast_qint8(
    ScalarType src_type,
    @Const Pointer ptr);
@Namespace("c10") public static native @ByVal @Name("fetch_and_cast<c10::quint8>") quint8 fetch_and_cast_quint8(
    ScalarType src_type,
    @Const Pointer ptr);
@Namespace("c10") public static native @ByVal @Name("fetch_and_cast<c10::qint32>") qint32 fetch_and_cast_qint32(
    ScalarType src_type,
    @Const Pointer ptr);
@Namespace("c10") public static native @ByVal @Name("fetch_and_cast<c10::quint4x2>") quint4x2 fetch_and_cast_quint4x2(
    ScalarType src_type,
    @Const Pointer ptr);

// Cast a value with static type src_t into dynamic dest_type, and store it to
// ptr.
// #define CAST_AND_STORE_CASE(type, scalartype)
//   case ScalarType::scalartype:
//     *(type*)ptr = static_cast_with_inter_type<type, src_t>::apply(value);
//     return;
@Namespace("c10") public static native @Name("cast_and_store<c10::qint8>") void cast_and_store_qint8(
    ScalarType dest_type,
    Pointer ptr,
    @ByVal qint8 value);
@Namespace("c10") public static native @Name("cast_and_store<c10::quint8>") void cast_and_store_quint8(
    ScalarType dest_type,
    Pointer ptr,
    @ByVal quint8 value);
@Namespace("c10") public static native @Name("cast_and_store<c10::qint32>") void cast_and_store_qint32(
    ScalarType dest_type,
    Pointer ptr,
    @ByVal qint32 value);
@Namespace("c10") public static native @Name("cast_and_store<c10::quint4x2>") void cast_and_store_quint4x2(
    ScalarType dest_type,
    Pointer ptr,
    @ByVal quint4x2 value);

// #define DEFINE_UNCASTABLE(T, scalartype_)
//   template <>
//   C10_HOST_DEVICE inline T fetch_and_cast<T>(
//       const ScalarType src_type, const void* ptr) {
//     CUDA_KERNEL_ASSERT(ScalarType::scalartype_ == src_type);
//     return *(const T*)ptr;
//   }
//   template <>
//   C10_HOST_DEVICE inline void cast_and_store<T>(
//       const ScalarType dest_type, void* ptr, T value) {
//     CUDA_KERNEL_ASSERT(ScalarType::scalartype_ == dest_type);
//     *(T*)ptr = value;
//   }



 // namespace c10

// Trigger tests for D25440771. TODO: Remove this line any time you want.


// Parsed from c10/util/Registry.h

// #ifndef C10_UTIL_REGISTRY_H_
// #define C10_UTIL_REGISTRY_H_

/**
 * Simple registry implementation that uses static variables to
 * register object creators during program initialization time.
 */

// NB: This Registry works poorly when you have other namespaces.
// Make all macro invocations from inside the at namespace.

// #include <algorithm>
// #include <cstdio>
// #include <cstdlib>
// #include <functional>
// #include <memory>
// #include <mutex>
// #include <string>
// #include <unordered_map>
// #include <vector>

// #include <c10/macros/Macros.h>
// #include <c10/util/Type.h>

@Namespace("c10") public static native @StdString BytePointer KeyStrRepr(@StdString BytePointer key);
@Namespace("c10") public static native @StdString String KeyStrRepr(@StdString String key);

@Namespace("c10") public enum RegistryPriority {
  REGISTRY_FALLBACK(1),
  REGISTRY_DEFAULT(2),
  REGISTRY_PREFERRED(3);

    public final int value;
    private RegistryPriority(int v) { this.value = v; }
    private RegistryPriority(RegistryPriority e) { this.value = e.value; }
    public RegistryPriority intern() { for (RegistryPriority e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

/**
 * \brief A template class that allows one to register classes by keys.
 *
 * The keys are usually a std::string specifying the name, but can be anything
 * that can be used in a std::map.
 *
 * You should most likely not use the Registry class explicitly, but use the
 * helper macros below to declare specific registries as well as registering
 * objects.
 */

/**
 * C10_DECLARE_TYPED_REGISTRY is a macro that expands to a function
 * declaration, as well as creating a convenient typename for its corresponding
 * registerer.
 */
// Note on C10_IMPORT and C10_EXPORT below: we need to explicitly mark DECLARE
// as import and DEFINE as export, because these registry macros will be used
// in downstream shared libraries as well, and one cannot use *_API - the API
// macro will be defined on a per-shared-library basis. Semantically, when one
// declares a typed registry it is always going to be IMPORT, and when one
// defines a registry (which should happen ONLY ONCE and ONLY IN SOURCE FILE),
// the instantiation unit is always going to be exported.
//
// The only unique condition is when in the same file one does DECLARE and
// DEFINE - in Windows compilers, this generates a warning that dllimport and
// dllexport are mixed, but the warning is fine and linker will be properly
// exporting the symbol. Same thing happens in the gflags flag declaration and
// definition caes.
// #define C10_DECLARE_TYPED_REGISTRY(
//     RegistryName, SrcType, ObjectType, PtrType, ...)
//   C10_IMPORT ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>*
//   RegistryName();
//   typedef ::c10::Registerer<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>
//       Registerer##RegistryName

// #define C10_DEFINE_TYPED_REGISTRY(
//     RegistryName, SrcType, ObjectType, PtrType, ...)
//   C10_EXPORT ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>*
//   RegistryName() {
//     static ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>*
//         registry = new ::c10::
//             Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>();
//     return registry;
//   }

// #define C10_DEFINE_TYPED_REGISTRY_WITHOUT_WARNING(
//     RegistryName, SrcType, ObjectType, PtrType, ...)
//   C10_EXPORT ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>*
//   RegistryName() {
//     static ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>*
//         registry =
//             new ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>(
//                 false);
//     return registry;
//   }

// Note(Yangqing): The __VA_ARGS__ below allows one to specify a templated
// creator with comma in its templated arguments.
// #define C10_REGISTER_TYPED_CREATOR(RegistryName, key, ...)
//   static Registerer##RegistryName C10_ANONYMOUS_VARIABLE(g_##RegistryName)(
//       key, RegistryName(), ##__VA_ARGS__);

// #define C10_REGISTER_TYPED_CREATOR_WITH_PRIORITY(
//     RegistryName, key, priority, ...)
//   static Registerer##RegistryName C10_ANONYMOUS_VARIABLE(g_##RegistryName)(
//       key, priority, RegistryName(), ##__VA_ARGS__);

// #define C10_REGISTER_TYPED_CLASS(RegistryName, key, ...)
//   static Registerer##RegistryName C10_ANONYMOUS_VARIABLE(g_##RegistryName)(
//       key,
//       RegistryName(),
//       Registerer##RegistryName::DefaultCreator<__VA_ARGS__>,
//       ::c10::demangle_type<__VA_ARGS__>());

// #define C10_REGISTER_TYPED_CLASS_WITH_PRIORITY(
//     RegistryName, key, priority, ...)
//   static Registerer##RegistryName C10_ANONYMOUS_VARIABLE(g_##RegistryName)(
//       key,
//       priority,
//       RegistryName(),
//       Registerer##RegistryName::DefaultCreator<__VA_ARGS__>,
//       ::c10::demangle_type<__VA_ARGS__>());

// C10_DECLARE_REGISTRY and C10_DEFINE_REGISTRY are hard-wired to use
// std::string as the key type, because that is the most commonly used cases.
// #define C10_DECLARE_REGISTRY(RegistryName, ObjectType, ...)
//   C10_DECLARE_TYPED_REGISTRY(
//       RegistryName, std::string, ObjectType, std::unique_ptr, ##__VA_ARGS__)

// #define C10_DEFINE_REGISTRY(RegistryName, ObjectType, ...)
//   C10_DEFINE_TYPED_REGISTRY(
//       RegistryName, std::string, ObjectType, std::unique_ptr, ##__VA_ARGS__)

// #define C10_DEFINE_REGISTRY_WITHOUT_WARNING(RegistryName, ObjectType, ...)
//   C10_DEFINE_TYPED_REGISTRY_WITHOUT_WARNING(
//       RegistryName, std::string, ObjectType, std::unique_ptr, ##__VA_ARGS__)

// #define C10_DECLARE_SHARED_REGISTRY(RegistryName, ObjectType, ...)
//   C10_DECLARE_TYPED_REGISTRY(
//       RegistryName, std::string, ObjectType, std::shared_ptr, ##__VA_ARGS__)

// #define C10_DEFINE_SHARED_REGISTRY(RegistryName, ObjectType, ...)
//   C10_DEFINE_TYPED_REGISTRY(
//       RegistryName, std::string, ObjectType, std::shared_ptr, ##__VA_ARGS__)

// #define C10_DEFINE_SHARED_REGISTRY_WITHOUT_WARNING(
//     RegistryName, ObjectType, ...)
//   C10_DEFINE_TYPED_REGISTRY_WITHOUT_WARNING(
//       RegistryName, std::string, ObjectType, std::shared_ptr, ##__VA_ARGS__)

// C10_REGISTER_CREATOR and C10_REGISTER_CLASS are hard-wired to use std::string
// as the key
// type, because that is the most commonly used cases.
// #define C10_REGISTER_CREATOR(RegistryName, key, ...)
//   C10_REGISTER_TYPED_CREATOR(RegistryName, #key, __VA_ARGS__)

// #define C10_REGISTER_CREATOR_WITH_PRIORITY(RegistryName, key, priority, ...)
//   C10_REGISTER_TYPED_CREATOR_WITH_PRIORITY(
//       RegistryName, #key, priority, __VA_ARGS__)

// #define C10_REGISTER_CLASS(RegistryName, key, ...)
//   C10_REGISTER_TYPED_CLASS(RegistryName, #key, __VA_ARGS__)

// #define C10_REGISTER_CLASS_WITH_PRIORITY(RegistryName, key, priority, ...)
//   C10_REGISTER_TYPED_CLASS_WITH_PRIORITY(
//       RegistryName, #key, priority, __VA_ARGS__)

 // namespace c10

// #endif // C10_UTIL_REGISTRY_H_


// Parsed from c10/util/Flags.h

// #ifndef C10_UTIL_FLAGS_H_
// #define C10_UTIL_FLAGS_H_

/* Commandline flags support for C10.
 *
 * This is a portable commandline flags tool for c10, so we can optionally
 * choose to use gflags or a lightweight custom implementation if gflags is
 * not possible on a certain platform. If you have gflags installed, set the
 * macro C10_USE_GFLAGS will seamlessly route everything to gflags.
 *
 * To define a flag foo of type bool default to true, do the following in the
 * *global* namespace:
 *     C10_DEFINE_bool(foo, true, "An example.");
 *
 * To use it in another .cc file, you can use C10_DECLARE_* as follows:
 *     C10_DECLARE_bool(foo);
 *
 * In both cases, you can then access the flag via FLAGS_foo.
 *
 * It is recommended that you build with gflags. To learn more about the flags
 * usage, refer to the gflags page here:
 *
 * https://gflags.github.io/gflags/
 *
 * Note about Python users / devs: gflags is initiated from a C++ function
 * ParseCommandLineFlags, and is usually done in native binaries in the main
 * function. As Python does not have a modifiable main function, it is usually
 * difficult to change the flags after Python starts. Hence, it is recommended
 * that one sets the default value of the flags to one that's acceptable in
 * general - that will allow Python to run without wrong flags.
 */

// #include <string>

// #include <c10/macros/Macros.h>
// #include <c10/util/Registry.h>
/**
 * Sets the usage message when a commandline tool is called with "--help".
 */
@Namespace("c10") public static native void SetUsageMessage(@StdString BytePointer str);
@Namespace("c10") public static native void SetUsageMessage(@StdString String str);

/**
 * Returns the usage message for the commandline tool set by SetUsageMessage.
 */
@Namespace("c10") public static native @Cast("const char*") BytePointer UsageMessage();

/**
 * Parses the commandline flags.
 *
 * This command parses all the commandline arguments passed in via pargc
 * and argv. Once it is finished, partc and argv will contain the remaining
 * commandline args that c10 does not deal with. Note that following
 * convention, argv[0] contains the binary name and is not parsed.
 */
@Namespace("c10") public static native @Cast("bool") boolean ParseCommandLineFlags(IntPointer pargc, @Cast("char***") @ByPtrPtr PointerPointer pargv);
@Namespace("c10") public static native @Cast("bool") boolean ParseCommandLineFlags(IntBuffer pargc, @Cast("char***") @ByPtrPtr PointerPointer pargv);
@Namespace("c10") public static native @Cast("bool") boolean ParseCommandLineFlags(int[] pargc, @Cast("char***") @ByPtrPtr PointerPointer pargv);

/**
 * Checks if the commandline flags has already been passed.
 */
@Namespace("c10") public static native @Cast("bool") boolean CommandLineFlagsHasBeenParsed();

 // namespace c10

////////////////////////////////////////////////////////////////////////////////
// Below are gflags and non-gflags specific implementations.
// In general, they define the following macros for one to declare (use
// C10_DECLARE) or define (use C10_DEFINE) flags:
// C10_{DECLARE,DEFINE}_{int,int64,double,bool,string}
////////////////////////////////////////////////////////////////////////////////

// #ifdef C10_USE_GFLAGS

////////////////////////////////////////////////////////////////////////////////
// Begin gflags section: most functions are basically rerouted to gflags.
////////////////////////////////////////////////////////////////////////////////
// #include <gflags/gflags.h>

// C10 uses hidden visibility by default. However, in gflags, it only uses
// export on Windows platform (with dllexport) but not on linux/mac (with
// default visibility). As a result, to ensure that we are always exporting
// global variables, we will redefine the GFLAGS_DLL_DEFINE_FLAG macro if we
// are building C10 as a shared libray.
// This has to be done after the inclusion of gflags, because some early
// versions of gflags.h (e.g. 2.0 on ubuntu 14.04) directly defines the
// macros, so we need to do definition after gflags is done.
// #ifdef GFLAGS_DLL_DEFINE_FLAG
// #endif // GFLAGS_DLL_DEFINE_FLAG
// #ifdef GFLAGS_DLL_DECLARE_FLAG
// #endif // GFLAGS_DLL_DECLARE_FLAG
// #define GFLAGS_DLL_DEFINE_FLAG C10_EXPORT
// #define GFLAGS_DLL_DECLARE_FLAG C10_IMPORT

// gflags before 2.0 uses namespace google and after 2.1 uses namespace gflags.
// Using GFLAGS_GFLAGS_H_ to capture this change.
// #ifndef GFLAGS_GFLAGS_H_
// #endif // GFLAGS_GFLAGS_H_

// Motivation about the gflags wrapper:
// (1) We would need to make sure that the gflags version and the non-gflags
// version of C10 are going to expose the same flags abstraction. One should
// explicitly use FLAGS_flag_name to access the flags.
// (2) For flag names, it is recommended to start with c10_ to distinguish it
// from regular gflags flags. For example, do
//    C10_DEFINE_BOOL(c10_my_flag, true, "An example");
// to allow one to use FLAGS_c10_my_flag.
// (3) Gflags has a design issue that does not properly expose the global flags,
// if one builds the library with -fvisibility=hidden. The current gflags (as of
// Aug 2018) only deals with the Windows case using dllexport, and not the Linux
// counterparts. As a result, we will explciitly use C10_EXPORT to export the
// flags defined in C10. This is done via a global reference, so the flag
// itself is not duplicated - under the hood it is the same global gflags flag.
// #define C10_GFLAGS_DEF_WRAPPER(type, real_type, name, default_value, help_str)
//   DEFINE_##type(name, default_value, help_str);

// #define C10_DEFINE_int(name, default_value, help_str)
//   C10_GFLAGS_DEF_WRAPPER(int32, gflags::int32, name, default_value, help_str)
// #define C10_DEFINE_int32(name, default_value, help_str)
//   C10_DEFINE_int(name, default_value, help_str)
// #define C10_DEFINE_int64(name, default_value, help_str)
//   C10_GFLAGS_DEF_WRAPPER(int64, gflags::int64, name, default_value, help_str)
// #define C10_DEFINE_double(name, default_value, help_str)
//   C10_GFLAGS_DEF_WRAPPER(double, double, name, default_value, help_str)
// #define C10_DEFINE_bool(name, default_value, help_str)
//   C10_GFLAGS_DEF_WRAPPER(bool, bool, name, default_value, help_str)
// #define C10_DEFINE_string(name, default_value, help_str)
//   C10_GFLAGS_DEF_WRAPPER(string, ::fLS::clstring, name, default_value, help_str)

// DECLARE_typed_var should be used in header files and in the global namespace.
// #define C10_GFLAGS_DECLARE_WRAPPER(type, real_type, name) DECLARE_##type(name);

// #define C10_DECLARE_int(name)
//   C10_GFLAGS_DECLARE_WRAPPER(int32, gflags::int32, name)
// #define C10_DECLARE_int32(name) C10_DECLARE_int(name)
// #define C10_DECLARE_int64(name)
//   C10_GFLAGS_DECLARE_WRAPPER(int64, gflags::int64, name)
// #define C10_DECLARE_double(name)
//   C10_GFLAGS_DECLARE_WRAPPER(double, double, name)
// #define C10_DECLARE_bool(name) C10_GFLAGS_DECLARE_WRAPPER(bool, bool, name)
// #define C10_DECLARE_string(name)
//   C10_GFLAGS_DECLARE_WRAPPER(string, ::fLS::clstring, name)
// Targeting ../C10FlagParser.java





 // namespace c10

// The macros are defined outside the c10 namespace. In your code, you should
// write the C10_DEFINE_* and C10_DECLARE_* macros outside any namespace
// as well.

// #define C10_DEFINE_typed_var(type, name, default_value, help_str)
//   C10_EXPORT type FLAGS_##name = default_value;
//   namespace c10 {
//   namespace {
//   class C10FlagParser_##name : public C10FlagParser {
//    public:
//     explicit C10FlagParser_##name(const std::string& content) {
//       success_ = C10FlagParser::Parse<type>(content, &FLAGS_##name);
//     }
//   };
//   }
//   RegistererC10FlagsRegistry g_C10FlagsRegistry_##name(
//       #name,
//       C10FlagsRegistry(),
//       RegistererC10FlagsRegistry::DefaultCreator<C10FlagParser_##name>,
//       "(" #type ", default " #default_value ") " help_str);
//   }

// #define C10_DEFINE_int(name, default_value, help_str)
//   C10_DEFINE_typed_var(int, name, default_value, help_str)
// #define C10_DEFINE_int32(name, default_value, help_str)
//   C10_DEFINE_int(name, default_value, help_str)
// #define C10_DEFINE_int64(name, default_value, help_str)
//   C10_DEFINE_typed_var(int64_t, name, default_value, help_str)
// #define C10_DEFINE_double(name, default_value, help_str)
//   C10_DEFINE_typed_var(double, name, default_value, help_str)
// #define C10_DEFINE_bool(name, default_value, help_str)
//   C10_DEFINE_typed_var(bool, name, default_value, help_str)
// #define C10_DEFINE_string(name, default_value, help_str)
//   C10_DEFINE_typed_var(std::string, name, default_value, help_str)

// DECLARE_typed_var should be used in header files and in the global namespace.
// #define C10_DECLARE_typed_var(type, name) C10_IMPORT extern type FLAGS_##name

// #define C10_DECLARE_int(name) C10_DECLARE_typed_var(int, name)
// #define C10_DECLARE_int32(name) C10_DECLARE_int(name)
// #define C10_DECLARE_int64(name) C10_DECLARE_typed_var(int64_t, name)
// #define C10_DECLARE_double(name) C10_DECLARE_typed_var(double, name)
// #define C10_DECLARE_bool(name) C10_DECLARE_typed_var(bool, name)
// #define C10_DECLARE_string(name) C10_DECLARE_typed_var(std::string, name)

////////////////////////////////////////////////////////////////////////////////
// End non-gflags section.
////////////////////////////////////////////////////////////////////////////////

// #endif // C10_USE_GFLAGS

// #endif // C10_UTIL_FLAGS_H_


// Parsed from c10/util/Logging.h

// #ifndef C10_UTIL_LOGGING_H_
// #define C10_UTIL_LOGGING_H_

// #include <climits>
// #include <exception>
// #include <functional>
// #include <limits>
// #include <sstream>

// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Flags.h>
// #include <c10/util/StringUtil.h>

// CAFFE2_LOG_THRESHOLD is a compile time flag that would allow us to turn off
// logging at compile time so no logging message below that level is produced
// at all. The value should be between INT_MIN and CAFFE_FATAL.
// #ifndef CAFFE2_LOG_THRESHOLD
// If we have not defined the compile time log threshold, we keep all the
// log cases.
public static native @MemberGetter int CAFFE2_LOG_THRESHOLD();
public static final int CAFFE2_LOG_THRESHOLD = CAFFE2_LOG_THRESHOLD();
// #endif // CAFFE2_LOG_THRESHOLD

// Below are different implementations for glog and non-glog cases.
// #ifdef C10_USE_GLOG
// #include <c10/util/logging_is_google_glog.h>
// #else // !C10_USE_GLOG
// #include <c10/util/logging_is_not_google_glog.h>
// #endif // C10_USE_GLOG




// Some versions of GLOG support less-spammy version of LOG_EVERY_MS. If it's
// not available - just short-circuit to the always working one one.
// We define the C10_ name to avoid confusing other files
// #ifdef LOG_EVERY_MS
// #define C10_LOG_EVERY_MS(severity, ms) LOG_EVERY_MS(severity, ms)
// #else
// #define C10_LOG_EVERY_MS(severity, ms) LOG(severity)
// #endif

// Same for LOG_FIRST_N
// #ifdef LOG_FIRST_N
// #define C10_LOG_FIRST_N(severity, n) LOG_FIRST_N(severity, n)
// #else
// #define C10_LOG_FIRST_N(severity, n) LOG(severity)
// #endif

// Same for LOG_EVERY_N
// #ifdef LOG_EVERY_N
// #define C10_LOG_EVERY_N(severity, n) LOG_EVERY_N(severity, n)
// #else
// #define C10_LOG_EVERY_N(severity, n) LOG(severity)
// #endif

// Functions that we use for initialization.
@Namespace("c10") public static native @Cast("bool") boolean InitCaffeLogging(IntPointer argc, @Cast("char**") PointerPointer argv);
@Namespace("c10") public static native @Cast("bool") boolean InitCaffeLogging(IntPointer argc, @Cast("char**") @ByPtrPtr BytePointer argv);
@Namespace("c10") public static native @Cast("bool") boolean InitCaffeLogging(IntBuffer argc, @Cast("char**") @ByPtrPtr ByteBuffer argv);
@Namespace("c10") public static native @Cast("bool") boolean InitCaffeLogging(int[] argc, @Cast("char**") @ByPtrPtr byte[] argv);
@Namespace("c10") public static native void UpdateLoggingLevelsFromFlags();

@Namespace("c10") public static native void ThrowEnforceNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @StdString BytePointer msg,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @StdString BytePointer msg);
@Namespace("c10") public static native void ThrowEnforceNotMet(
    String file,
    int line,
    String condition,
    @StdString String msg,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceNotMet(
    String file,
    int line,
    String condition,
    @StdString String msg);

@Namespace("c10") public static native void ThrowEnforceNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @ByVal CompileTimeEmptyString msg,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @ByVal CompileTimeEmptyString msg);
@Namespace("c10") public static native void ThrowEnforceNotMet(
    String file,
    int line,
    String condition,
    @ByVal CompileTimeEmptyString msg,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceNotMet(
    String file,
    int line,
    String condition,
    @ByVal CompileTimeEmptyString msg);

@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @StdString BytePointer msg,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @StdString BytePointer msg);
@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    String file,
    int line,
    String condition,
    @StdString String msg,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    String file,
    int line,
    String condition,
    @StdString String msg);

@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @ByVal CompileTimeEmptyString msg,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @ByVal CompileTimeEmptyString msg);
@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    String file,
    int line,
    String condition,
    @ByVal CompileTimeEmptyString msg,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    String file,
    int line,
    String condition,
    @ByVal CompileTimeEmptyString msg);

@Namespace("c10") public static native @Cast("const bool") boolean IsUsingGoogleLogging();

/**
 * A utility to allow one to show log info to stderr after the program starts.
 *
 * This is similar to calling GLOG's --logtostderr, or setting caffe2_log_level
 * to smaller than INFO. You are recommended to only use this in a few sparse
 * cases, such as when you want to write a tutorial or something. Normally, use
 * the commandline flags to set the log level.
 */
@Namespace("c10") public static native void ShowLogInfoToStderr();

@Namespace("c10") public static native void SetStackTraceFetcher(@ByVal Fetcher fetcher);

// #define CAFFE_ENFORCE(condition, ...)
//   do {
//     if (C10_UNLIKELY(!(condition))) {
//       ::c10::ThrowEnforceNotMet(
//           __FILE__, __LINE__, #condition, ::c10::str(__VA_ARGS__));
//     }
//   } while (false)

// #define CAFFE_ENFORCE_FINITE(condition, ...)
//   do {
//     if (C10_UNLIKELY(!(condition))) {
//       ::c10::ThrowEnforceFiniteNotMet(
//           __FILE__, __LINE__, #condition, ::c10::str(__VA_ARGS__));
//     }
//   } while (false)

// #define CAFFE_ENFORCE_WITH_CALLER(condition, ...)
//   do {
//     if (C10_UNLIKELY(!(condition))) {
//       ::c10::ThrowEnforceNotMet(
//           __FILE__, __LINE__, #condition, ::c10::str(__VA_ARGS__), this);
//     }
//   } while (false)

// #define CAFFE_THROW(...)
//   ::c10::ThrowEnforceNotMet(__FILE__, __LINE__, "", ::c10::str(__VA_ARGS__))

/**
 * Rich logging messages
 *
 * CAFFE_ENFORCE_THAT can be used with one of the "checker functions" that
 * capture input argument values and add it to the exception message. E.g.
 * {@code CAFFE_ENFORCE_THAT(Equals(foo(x), bar(y)), "Optional additional message")}
 * would evaluate both foo and bar only once and if the results are not equal -
 * include them in the exception message.
 *
 * Some of the basic checker functions like Equals or Greater are already
 * defined below. Other header might define customized checkers by adding
 * functions to caffe2::enforce_detail namespace. For example:
 *
 *   namespace caffe2 { namespace enforce_detail {
 *   inline EnforceFailMessage IsVector(const vector<int64_t>& shape) {
 *     if (shape.size() == 1) { return EnforceOK(); }
 *     return c10::str("Shape ", shape, " is not a vector");
 *   }
 *   }}
 *
 * With further usages like {@code CAFFE_ENFORCE_THAT(IsVector(Input(0).dims()))}
 *
 * Convenient wrappers for binary operations like CAFFE_ENFORCE_EQ are provided
 * too. Please use them instead of CHECK_EQ and friends for failures in
 * user-provided input.
 */
// #define CAFFE_ENFORCE_THAT_IMPL(op, lhs, rhs, expr, ...)
//   ::c10::enforce_detail::enforceThatImpl(
//       op, lhs, rhs, __FILE__, __LINE__, expr, nullptr, ##__VA_ARGS__)

// #define CAFFE_ENFORCE_THAT_IMPL_WITH_CALLER(op, lhs, rhs, expr, ...)
//   ::c10::enforce_detail::enforceThatImpl(
//       op, (lhs), (rhs), __FILE__, __LINE__, expr, this, ##__VA_ARGS__)

 // namespace enforce_detail

// #define CAFFE_ENFORCE_THAT(cmp, op, lhs, rhs, ...)
//   CAFFE_ENFORCE_THAT_IMPL(cmp, lhs, rhs, #lhs " " #op " " #rhs, ##__VA_ARGS__)

// #define CAFFE_ENFORCE_BINARY_OP(cmp, op, x, y, ...)
//   CAFFE_ENFORCE_THAT_IMPL(cmp, x, y, #x " " #op " " #y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_EQ(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP(std::equal_to<void>(), ==, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_NE(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP(std::not_equal_to<void>(), !=, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_LE(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP(std::less_equal<void>(), <=, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_LT(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP(std::less<void>(), <, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_GE(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP(std::greater_equal<void>(), >=, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_GT(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP(std::greater<void>(), >, x, y, ##__VA_ARGS__)

// #define CAFFE_ENFORCE_BINARY_OP_WITH_CALLER(cmp, op, x, y, ...)
//   CAFFE_ENFORCE_THAT_IMPL_WITH_CALLER(
//       cmp, x, y, #x " " #op " " #y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_EQ_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP_WITH_CALLER(
//       std::equal_to<void>(), ==, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_NE_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP_WITH_CALLER(
//       std::not_equal_to<void>(), !=, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_LE_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP_WITH_CALLER(
//       std::less_equal<void>(), <=, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_LT_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP_WITH_CALLER(std::less<void>(), <, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_GE_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP_WITH_CALLER(
//       std::greater_equal<void>(), >=, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_GT_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP_WITH_CALLER(
//       std::greater<void>(), >, x, y, ##__VA_ARGS__)

/**
 * Very lightweight logging for the first time API usage. It's beneficial for
 * tracking of individual functionality usage in larger applications.
 *
 * In order to ensure light-weightedness of logging, we utilize static variable
 * trick - LogAPIUsage will be invoked only once and further invocations will
 * just do an atomic check.
 *
 * Example:
 *   // Logs caller info with an arbitrary text event, if there is a usage.
 *   C10_LOG_API_USAGE_ONCE("my_api");
 */
// #define C10_LOG_API_USAGE_ONCE(...)
//   C10_UNUSED static bool C10_ANONYMOUS_VARIABLE(logFlag) =
//       ::c10::detail::LogAPIUsageFakeReturn(__VA_ARGS__);

// API usage logging capabilities
@Namespace("c10") public static native void SetAPIUsageLogger(@ByVal Logger logger);
@Namespace("c10") public static native void LogAPIUsage(@StdString BytePointer context);
@Namespace("c10") public static native void LogAPIUsage(@StdString String context);
// Targeting ../DDPLoggingData.java



@Namespace("c10") public static native void SetPyTorchDDPUsageLogger(
    @ByVal DataLogger logger);
@Namespace("c10") public static native void LogPyTorchDDPUsage(@Const @ByRef DDPLoggingData ddpData);
// Return value is needed to do the static variable initialization trick
@Namespace("c10::detail") public static native @Cast("bool") boolean LogAPIUsageFakeReturn(@StdString BytePointer context);
@Namespace("c10::detail") public static native @Cast("bool") boolean LogAPIUsageFakeReturn(@StdString String context);
 // namespace detail

 // namespace c10

// #endif // C10_UTIL_LOGGING_H_


// Parsed from c10/core/DeviceType.h

// #pragma once

// This is directly synchronized with caffe2/proto/caffe2.proto, but
// doesn't require me to figure out how to get Protobuf headers into
// ATen/core (which would require a lot more build system hacking.)
// If you modify me, keep me synchronized with that file.

// #include <c10/macros/Macros.h>

// #include <functional>
// #include <ostream>

@Namespace("c10") public enum DeviceType {
  CPU((byte)(0)),
  CUDA((byte)(1)), // CUDA.
  MKLDNN((byte)(2)), // Reserved for explicit MKLDNN
  OPENGL((byte)(3)), // OpenGL
  OPENCL((byte)(4)), // OpenCL
  IDEEP((byte)(5)), // IDEEP.
  HIP((byte)(6)), // AMD HIP
  FPGA((byte)(7)), // FPGA
  ORT((byte)(8)), // ONNX Runtime / Microsoft
  XLA((byte)(9)), // XLA / TPU
  Vulkan((byte)(10)), // Vulkan
  Metal((byte)(11)), // Metal
  XPU((byte)(12)), // XPU
  MLC((byte)(13)), // ML Compute / Apple
  Meta((byte)(14)), // Meta (tensors with no data)
  HPU((byte)(15)), // HPU / HABANA
  VE((byte)(16)), // SX-Aurora / NEC
  Lazy((byte)(17)), // Lazy Tensors
  // NB: If you add more devices:
  //  - Change the implementations of DeviceTypeName and isValidDeviceType
  //    in DeviceType.cpp
  //  - Change the number below
  COMPILE_TIME_MAX_DEVICE_TYPES((byte)(18));

    public final byte value;
    private DeviceType(byte v) { this.value = v; }
    private DeviceType(DeviceType e) { this.value = e.value; }
    public DeviceType intern() { for (DeviceType e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") @MemberGetter public static native DeviceType kCPU();
@Namespace("c10") @MemberGetter public static native DeviceType kCUDA();
@Namespace("c10") @MemberGetter public static native DeviceType kHIP();
@Namespace("c10") @MemberGetter public static native DeviceType kFPGA();
@Namespace("c10") @MemberGetter public static native DeviceType kORT();
@Namespace("c10") @MemberGetter public static native DeviceType kXLA();
@Namespace("c10") @MemberGetter public static native DeviceType kMLC();
@Namespace("c10") @MemberGetter public static native DeviceType kMeta();
@Namespace("c10") @MemberGetter public static native DeviceType kVulkan();
@Namespace("c10") @MemberGetter public static native DeviceType kMetal();
@Namespace("c10") @MemberGetter public static native DeviceType kXPU();
@Namespace("c10") @MemberGetter public static native DeviceType kHPU();
@Namespace("c10") @MemberGetter public static native DeviceType kVE();
@Namespace("c10") @MemberGetter public static native DeviceType kLazy();

// define explicit int constant
@Namespace("c10") @MemberGetter public static native int COMPILE_TIME_MAX_DEVICE_TYPES();

@Namespace("c10") public static native @StdString BytePointer DeviceTypeName(DeviceType d, @Cast("bool") boolean lower_case/*=false*/);
@Namespace("c10") public static native @StdString BytePointer DeviceTypeName(DeviceType d);
@Namespace("c10") public static native @StdString String DeviceTypeName(@Cast("c10::DeviceType") byte d, @Cast("bool") boolean lower_case/*=false*/);
@Namespace("c10") public static native @StdString String DeviceTypeName(@Cast("c10::DeviceType") byte d);

@Namespace("c10") public static native @Cast("bool") boolean isValidDeviceType(DeviceType d);
@Namespace("c10") public static native @Cast("bool") boolean isValidDeviceType(@Cast("c10::DeviceType") byte d);



 // namespace c10
 // namespace std



// Parsed from c10/core/Device.h

// #pragma once

// #include <c10/core/DeviceType.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>

// #include <cstddef>
// #include <functional>
// #include <iosfwd>
// #include <string>

/** An index representing a specific device; e.g., the 1 in GPU 1.
 *  A DeviceIndex is not independently meaningful without knowing
 *  the DeviceType it is associated; try to use Device rather than
 *  DeviceIndex directly. */
// Targeting ../Device.java






// Targeting ../DeviceHash.java


 // namespace std


// Parsed from c10/core/DeviceGuard.h

// #pragma once

// #include <c10/core/impl/InlineDeviceGuard.h>
// Targeting ../DeviceGuard.java


// Targeting ../OptionalDeviceGuard.java



// Note [Whither the DeviceGuard boilerplate]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Design note: in principle, we could avoid these wrappers using:
//
// using DeviceGuard = impl::InlineDeviceGuard<impl::VirtualGuardImpl>;
// using OptionalDeviceGuard =
// impl::InlineOptionalDeviceGuard<impl::VirtualGuardImpl>;
//
// But the error messages are worse, and our users can't just look at the
// header file to find out what's going on.  Furthermore, for specializations
// like CUDAStreamGuard, it can be profitable to replace some interfaces with
// refined types (e.g., return CUDAStream instead of Stream).  So, we eat
// the boilerplate and write out the API explicitly.

 // namespace c10


// Parsed from c10/core/DispatchKey.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>
// #include <ostream>
// #include <string>
// #include <vector>

// Semantically, a dispatch key identifies a possible "level" in our
// dispatch, for which a handler may be registered.  Traditional
// backends like CPU and CUDA get dispatch keys; however, so do
// "wrapping" layers like Variable (for autograd handling).
//
// In implementation terms, the dispatch key identifies a specific "bit" in a
// DispatchKeySet.  Higher bit indexes get handled by dispatching first (because
// we "count leading zeros" when we extract the highest priority dispatch
// key.)
//
// NOTE: Keep the list in sync with `DispatchKey` in tools/codegen/model.py
@Namespace("c10") public enum DispatchKey {

  // ~~~~~~~~~~~~~~~~~~~~~~~~~~ UNDEFINED ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ //
  // This is not a "real" tensor id, but it exists to give us a "nullopt"
  // element we can return for cases when a DispatchKeySet contains no elements.
  // You can think a more semantically accurate definition of DispatchKey is:
  //
  //    using DispatchKey = optional<RealDispatchKey>
  //
  // and Undefined == nullopt.  We didn't actually represent
  // it this way because optional<RealDispatchKey> would take two
  // words, when DispatchKey fits in eight bits.

  Undefined((byte)(0)),

  // Define an alias for Undefined to represent CatchAll (long term
  // this will get eliminated, but for now it's convenient)
  CatchAll((byte)(Undefined.value)),

  // ~~~~~~~~~~~~~~~~~~~~~~~~~~ BACKENDS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ //
  // A "backend" is colloquially used to refer to handlers for dispatch
  // which actually implement the numerics of an operation in question.
  //
  // Due to the nature of the enum, these backends are specified in
  // an ordered way, but for most backends this order is not semantically
  // meaningful (e.g., it's valid to reorder these backends without changing
  // semantics).  The only situation when backend ordering is meaningful
  // is when the backend participates in multiple dispatch with another
  // backend; e.g., CPU and SparseCPU (sparse must have
  // higher priority).

  // Here are backends which you think of as traditionally specifying
  // how to implement operations on some device.
  CPU((byte)(Undefined.value + 1)), // registered at build/aten/src/ATen/RegisterCPU.cpp
  CUDA((byte)(Undefined.value + 2)), // registered at build/aten/src/ATen/RegisterCUDA.cpp
  HIP((byte)(Undefined.value + 3)), // NB: I think this is not actually used, due to Note [Masquerading as
  // CUDA]
  FPGA((byte)(Undefined.value + 4)), // Xilinx support lives out of tree at
  // https://gitlab.com/pytorch-complex/vitis_kernels

  // ONNX Runtime, lives out of tree at https://github.com/pytorch/ort and
  // https://github.com/microsoft/onnxruntime, and is also used to test general
  // backend/extension machinery in the core. cf:
  // - test/cpp_extensions/ort_extension.cpp
  // - test/test_torch.py
  // - aten/src/ATen/test/extension_backend_test.cpp
  ORT((byte)(Undefined.value + 5)),

  XLA((byte)(Undefined.value + 6)), // lives out of tree at https://github.com/pytorch/xla
  MLC((byte)(Undefined.value + 7)), // lives out of tree at https://github.com/pytorch/MLCompute
  Vulkan((byte)(Undefined.value + 8)),
  Metal((byte)(Undefined.value + 9)),
  XPU((byte)(Undefined.value + 10)), // For out of tree Intel's heterogeneous computing plug-in
  HPU((byte)(Undefined.value + 11)), // For out of tree & closed source integration of HPU / Habana
  VE((byte)(Undefined.value + 12)), // For out of tree & closed source integration of SX-Aurora / NEC
  Lazy((byte)(Undefined.value + 13)), // For lazy tensor backends

  // A meta tensor is a tensor without any data associated with it.  (They
  // have also colloquially been referred to as tensors on the "null" device).
  // A meta tensor can be used to dry run operators without actually doing any
  // computation, e.g., add on two meta tensors would give you another meta
  // tensor with the output shape and dtype, but wouldn't actually add anything.
  Meta((byte)(Undefined.value + 14)),

  // Here are backends which specify more specialized operators
  // based on the dtype of the tensor.
  QuantizedCPU((byte)(Undefined.value + 15)), // registered at build/aten/src/ATen/RegisterQuantizedCPU.cpp
  QuantizedCUDA((byte)(Undefined.value + 16)), // registered at build/aten/src/ATen/RegisterQuantizedCUDA.cpp
  QuantizedXPU((byte)(Undefined.value + 17)), // For out of tree Intel's heterogeneous computing plug-in

  // This backend is to support custom RNGs; it lets you go
  // to a different kernel if you pass in a generator that is not a
  // traditional CPUGeneratorImpl/CUDAGeneratorImpl.  To make use of this
  // key:
  //  1) set it as a second parameter of at::Generator constructor call in
  //     the user-defined PRNG class.
  //  2) use it as a dispatch key while registering custom kernels
  //     (templatized kernels specialized for user-defined PRNG class)
  // intended for out of tree use; tested by aten/src/ATen/test/rng_test.cpp
  CustomRNGKeyId((byte)(Undefined.value + 18)),

  // Here are backends which specify more specialized operators
  // based on the layout of the tensor.  Note that the sparse backends
  // are one case where ordering matters: sparse multi-dispatches with
  // the corresponding dense tensors, and must be handled before them.
  MkldnnCPU((byte)(Undefined.value + 19)), // registered at build/aten/src/ATen/RegisterMkldnnCPU.cpp
  // NB: not to be confused with MKLDNN, which is Caffe2 only
  SparseCPU((byte)(Undefined.value + 20)), // registered at build/aten/src/ATen/RegisterSparseCPU.cpp
  SparseCUDA((byte)(Undefined.value + 21)), // registered at build/aten/src/ATen/RegisterSparseCUDA.cpp
  SparseHIP((byte)(Undefined.value + 22)), // TODO: I think this is not actually used, due to Note
  // [Masquerading as CUDA]
  SparseXPU((byte)(Undefined.value + 23)), // For out of tree Intel's heterogeneous computing plug-in
  SparseVE((byte)(Undefined.value + 24)), // For out of tree & closed source integration of SX-Aurora / NEC

  SparseCsrCPU((byte)(Undefined.value + 25)),
  SparseCsrCUDA((byte)(Undefined.value + 26)),

  NestedTensor((byte)(Undefined.value + 27)), // lives out of tree at https://github.com/pytorch/nestedtensor

  // Here are reserved backends for user-defined backends, see Note [Private use
  // DispatchKey]
  // To see some example about how to use this, check out ORT
  PrivateUse1((byte)(Undefined.value + 28)),
  PrivateUse2((byte)(Undefined.value + 29)),
  PrivateUse3((byte)(Undefined.value + 30)),

  // Define an alias key to represent end of backend dispatch keys.
  // If you add new backend keys after PrivateUse3, please also update it here.
  // (But you shouldn't: private use keys should have higher precedence than
  // all built-in keys)
  EndOfBackendKeys((byte)(PrivateUse3.value)),

  // In some situations, it is not immediately obvious what the correct
  // backend for function is, because the function in question doesn't
  // have any "tensor" arguments.  In this case, a BackendSelect function
  // can be registered to implement the custom determination of the
  // correct backend.
  BackendSelect((byte)(PrivateUse3.value + 1)),

  Python((byte)(PrivateUse3.value + 2)),
  FuncTorchPython((byte)(PrivateUse3.value + 3)), // See Note [Out-of-tree vmap+grad prototype]

  // The named dispatch key is set for any tensors with named dimensions.
  // Although we have a dispatch key for named tensors, for historical reasons,
  // this dispatch key doesn't do any of the substantive functionality for named
  // tensor (though, hypothetically, it could!)  At the moment, it's just
  // responsible for letting us give good error messages when operations
  // don't support named tensors.
  //
  // NB: If you ever consider moving named tensor functionality into
  // this dispatch key, note that it might be necessary add another dispatch
  // key that triggers before composite operators, in case a composite operator
  // has named dimension propagation that doesn't match that of its
  // constituent parts.
  Named((byte)(PrivateUse3.value + 4)),

  // The Conjugate dispatch key is set for any tensors that need to perform
  // conjugation
  // This is implemented at a dispatch level right before any backends run
  Conjugate((byte)(PrivateUse3.value + 5)),

  // The Negative dispatch key is set for any tensors that need to perform
  // negation
  // This is implemented at a dispatch level right before any backends run
  Negative((byte)(PrivateUse3.value + 6)),

  // See Note [Out-of-tree vmap+grad prototype]. The purpose of this key
  // is to insert code after the "autograd subsystem" runs, so this key should
  // be directly after ADInplaceOrView and all of the autograd keys.
  FuncTorchDynamicLayerBackMode((byte)(PrivateUse3.value + 7)),

  // Note [ADInplaceOrView key]
  // ADInplaceOrView key is used by inplace or view ops to register a kernel
  // that does additional setup for future autograd computation.
  //
  // 1. For inplace ops this kernel does version bump
  // 2. For view ops this kernel does `as_view` setup where we properly setup
  //    DifferentiableViewMeta on the view tensors.
  //
  // For other ops it's fallthrough kernel since there's no extra
  // work to do.
  //
  // Note [Dream: skip VariableType kernel when requires_grad=false]
  //
  // In an ideal world where we can skip VariableType kernel for inputs
  // with requires_grad=false, instead of a fallthrough kernel, we'll
  // register a kernel shown below to all functional ops as well:
  // torch::Tensor my_functional_op(...) {
  //   {
  //     // Note for every op in VariableType, you need to go through
  //     // `AutoDispatchBelowADInplaceOrView` guard exactly once to add the
  //     // key to TLS excluded set. If you don't go through it at all,
  //     // inplace/view ops called through `at::` inside your backend
  //     // kernel will dispatch to ADInplaceOrView kernels and do a lot
  //     // of extra work.
  //     at::AutoDispatchBelowADInplaceOrView guard;
  //     at::redispatch::my_functional_op(...);
  //   }
  // }
  // But this work is currently blocked since it adds an extra dispatch
  // for all ops and it's non-trivial overhead at model level(a few percents).
  // Thus our current approach takes advantage of the fact every kernel go
  // through VariableType kernel first and pulls the
  // `at::AutoDispatchBelowADInplaceOrView` guard of functional ops
  // up to the `VariableType` kernel. Thus we only add the extra dispatch
  // to view/inplace ops to minimize its perf impact to real models.
  ADInplaceOrView((byte)(PrivateUse3.value + 8)),

  // Note [Alias Dispatch Key : Autograd]
  // All backends are oblivious to autograd; autograd is handled as a
  // layer which happens on top of all backends. It inspects the autograd
  // metadata of all inputs, determines what autograd metadata should be
  // constructed by the output, and otherwise defers to the backend to
  // actually do the numeric computation.  Autograd contains
  // the bulk of this logic.

  // Autograd is now an alias dispatch key which by default maps to all
  // backend-specific autograd keys.
  // Backend-specific allow backends to override the default kernel registered
  // to Autograd key as needed.
  // For example, XLA wants to define autograd for einsum directly.
  // Registering a custom autograd implementation at the XLA key won't work
  // because we process Autograd before XLA.  This key has higher priority and
  // gets processed first.  You generally should NOT redispatch after handling
  // autograd here (since that would result in execution of the Autograd
  // operator, which you're trying to skip).  In AutogradXLA implementations,
  // you are responsible for handling autograd yourself, or deferring to other
  // operators which support autograd.

  // Currently we only have backend-specific autograd keys for CPU/CUDA/XLA and
  // reserved user-defined backends. All other in-tree backends share the
  // AutogradOther key. We can add specific autograd key for those backends
  // upon request.
  AutogradOther((byte)(PrivateUse3.value + 9)),
  AutogradCPU((byte)(PrivateUse3.value + 10)),
  AutogradCUDA((byte)(PrivateUse3.value + 11)),
  AutogradXLA((byte)(PrivateUse3.value + 12)),
  AutogradLazy((byte)(PrivateUse3.value + 13)),
  AutogradXPU((byte)(PrivateUse3.value + 14)),
  AutogradMLC((byte)(PrivateUse3.value + 15)),
  AutogradHPU((byte)(PrivateUse3.value + 16)),
  AutogradNestedTensor((byte)(PrivateUse3.value + 17)), // lives out of tree at
  // https://github.com/pytorch/nestedtensor
  // Here are some reserved pre-autograd keys for user-defined backends, see
  // Note [Private use DispatchKey]
  AutogradPrivateUse1((byte)(PrivateUse3.value + 18)),
  AutogradPrivateUse2((byte)(PrivateUse3.value + 19)),
  AutogradPrivateUse3((byte)(PrivateUse3.value + 20)),

  Tracer((byte)(PrivateUse3.value + 21)),

  // Autocasting precedes VariableTypeId, to ensure casts are autograd-exposed
  // and inputs are saved for backward in the post-autocast type.
  AutocastCPU((byte)(PrivateUse3.value + 22)),
  // Naughtily, AutocastCUDA is also being used for XLA.  In the terminal state,
  // it probably should get its own Autocast key
  AutocastCUDA((byte)(PrivateUse3.value + 23)),

  // ~~~~~~~~~~~~~~~~~~~~~~~~~~~ WRAPPERS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ //
  // There are a number of alternative modes which may want to handle before
  // autograd; for example, error checking, tracing, profiling or vmap.  They
  // go here.

  FuncTorchBatched((byte)(PrivateUse3.value + 24)), // See Note [Out-of-tree vmap+grad prototype]
  FuncTorchVmapMode((byte)(PrivateUse3.value + 25)), // See Note [Out-of-tree vmap+grad prototype]

  // This is the dispatch key for BatchedTensorImpl, which is used to implement
  // batching rules for vmap.
  Batched((byte)(PrivateUse3.value + 26)),

  // When we are inside a vmap, all tensors dispatch on this key.
  // See Note: [DispatchKey::VmapMode usage] for more details.
  VmapMode((byte)(PrivateUse3.value + 27)),

  FuncTorchGradWrapper((byte)(PrivateUse3.value + 28)), // See Note [Out-of-tree vmap+grad prototype]
  FuncTorchDynamicLayerFrontMode((byte)(PrivateUse3.value + 29)), // See Note [Out-of-tree vmap+grad prototype]

  // TESTING: This is intended to be a generic testing tensor type id.
  // Don't use it for anything real; its only acceptable use is within a single
  // process test.  Use it by creating a TensorImpl with this DispatchKey, and
  // then registering operators to operate on this type id.  See
  // aten/src/ATen/core/dispatch/backend_fallback_test.cpp for a usage example.
  TESTING_ONLY_GenericWrapper((byte)(PrivateUse3.value + 30)),

  // TESTING: This is intended to be a generic testing tensor type id.
  // Don't use it for anything real; its only acceptable use is within a ingle
  // process test.  Use it by toggling the mode on and off via
  // TESTING_ONLY_tls_generic_mode_set_enabled and then registering operators
  // to operate on this type id.  See
  // aten/src/ATen/core/dispatch/backend_fallback_test.cpp
  // for a usage example
  TESTING_ONLY_GenericMode((byte)(PrivateUse3.value + 31)),

  // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FIN ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ //
  NumDispatchKeys((byte)(PrivateUse3.value + 32)), // Sentinel, end of runtime keys.

  // ~~~~~~~~~~~~~~~~~~~~~~ Alias Dispatch Keys ~~~~~~~~~~~~~~~~~~~~~~~~~~ //
  // Alias dispatch keys are synthetic dispatch keys which map to multiple
  // runtime dispatch keys. Alisa keys have precedence, but they are always
  // lower precedence than runtime keys. You can register a kernel to an
  // alias key, the kernel might be populated to the mapped runtime keys
  // during dispatch table computation.
  // If a runtime dispatch key has multiple kernels from alias keys, which
  // kernel wins is done based on the precedence of alias keys (but runtime
  // keys always have precedence over alias keys).
  // Alias keys won't be directly called during runtime.

  // See Note [Alias Dispatch Key : Autograd]
  Autograd((byte)(PrivateUse3.value + 33)),
  CompositeImplicitAutograd((byte)(PrivateUse3.value + 34)), // registered at
  // build/aten/src/ATen/RegisterCompositeImplicitAutograd.cpp
  CompositeExplicitAutograd((byte)(PrivateUse3.value + 35)), // registered at
  // build/aten/src/ATen/RegisterCompositeExplicitAutograd.cpp

  // Define an alias key to represent end of alias dispatch keys.
  // If you add new alias keys after Autograd, please also update it here.
  EndOfAliasKeys((byte)(CompositeExplicitAutograd.value)), //

  // ~~~~~~~~~~~~~~~~~~~~~~~~~ BC ALIASES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ //
  // The aliases exist for backwards compatibility reasons, they shouldn't
  // be used
  CPUTensorId((byte)(CPU.value)),
  CUDATensorId((byte)(CUDA.value)),
  DefaultBackend((byte)(CompositeExplicitAutograd.value)),
  PrivateUse1_PreAutograd((byte)(AutogradPrivateUse1.value)),
  PrivateUse2_PreAutograd((byte)(AutogradPrivateUse2.value)),
  PrivateUse3_PreAutograd((byte)(AutogradPrivateUse3.value)),
  Autocast((byte)(AutocastCUDA.value));

    public final byte value;
    private DispatchKey(byte v) { this.value = v; }
    private DispatchKey(DispatchKey e) { this.value = e.value; }
    public DispatchKey intern() { for (DispatchKey e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

// Note [Private use DispatchKey]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Private use tensor IDs are preallocated tensor type IDs for use in user
// applications.  Similar to private use fields in HTTP, they can be used
// by end users for experimental or private applications, without needing
// to "standardize" the tensor ID (which would be done by submitting a PR
// to PyTorch to add your type ID).
//
// Private use tensor IDs are appropriate to use if you want to experiment
// with adding a new tensor type (without having to patch PyTorch first) or
// have a private, non-distributed application that needs to make use of a
// new tensor type.  Private use tensor IDs are NOT appropriate to use for
// libraries intended to be distributed to further users: please contact
// the PyTorch developers to get a type ID registered in this case.
//
// We provide two classes of private user tensor id: regular DispatchKeys
// and Autograd DispatchKeys.  DispatchKeys serve the role of ordinary "backend"
// DispatchKeys; if you were adding support for a new type of accelerator, you
// would use a backend DispatchKey, and ideally automatically reuse
// AutogradOther definitions already defined in PyTorch.  AutogradPrivateUse
// DispatchKeys serve as "wrapper" DispatchKeys: they are only necessary for
// tensors that compose multiple internal tensors, and for cases when the
// built-in autograd formulas for operators are not appropriate.

@Namespace("c10") public static native @Cast("const char*") BytePointer toString(DispatchKey arg0);
@Namespace("c10") public static native String toString(@Cast("c10::DispatchKey") byte arg0);


@Namespace("c10") public static native DispatchKey getAutogradKeyFromBackend(DispatchKey t);
@Namespace("c10") public static native @Cast("c10::DispatchKey") byte getAutogradKeyFromBackend(@Cast("c10::DispatchKey") byte t);

// These are some convenience identifiers for dispatch keys which are
// shorter to type than their long counterparts.  Note that some of these
// dispatch keys directly correspond to DeviceType; and most APIs that
// accept DispatchKey also accept DeviceType; e.g.,
// torch::dispatch(torch::kCPU, ...) is also valid.
@Namespace("c10") @MemberGetter public static native DispatchKey kAutograd();

// Check if a DispatchKey is an alias mapping to other runtime keys.
@Namespace("c10") public static native @Cast("bool") boolean isAliasDispatchKey(DispatchKey k);
@Namespace("c10") public static native @Cast("bool") boolean isAliasDispatchKey(@Cast("c10::DispatchKey") byte k);
 // namespace c10
// Expose the constant, but not the TYPE (DispatchKey is an implementation
// detail!)
 // namespace torch

// NB: You really shouldn't use this instance; this enum is guaranteed
// to be pretty small so a regular array should be acceptable.
 // namespace std


// Parsed from c10/core/DispatchKeySet.h

// #pragma once

// #include <c10/core/DispatchKey.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Metaprogramming.h>
// #include <c10/util/llvmMathExtras.h>
// #include <ostream>
// Targeting ../DispatchKeySet.java



@Namespace("c10") public static native @StdString BytePointer toString(@ByVal DispatchKeySet arg0);


// autograd_dispatch_keyset should include all runtime autograd keys.
// Alias key DispatchKey::Autograd maps to autograd_dispatch_keyset.
// NB: keys in this set also get associated with CompositeImplicitAutograd
@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet autograd_dispatch_keyset();

@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet autocast_dispatch_keyset();

// See Note [TLS Initialization]
@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet default_included_set();

@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet default_excluded_set();

@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet autograd_dispatch_keyset_with_ADInplaceOrView();

// backend dispatch keys that map to DispatchKey::AutogradOther
// NB: keys in this set also get associated with CompositeImplicitAutograd
@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet autogradother_backends();

// The set of dispatch keys that come after autograd
// n.b. this relies on the fact that AutogradOther is currently the lowest
// Autograd key
@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet after_autograd_keyset();

// The set of dispatch keys that come after ADInplaceOrView
@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet after_ADInplaceOrView_keyset();

// true if t is a backend dispatch key
@Namespace("c10") public static native @Cast("bool") boolean isBackendDispatchKey(DispatchKey t);
@Namespace("c10") public static native @Cast("bool") boolean isBackendDispatchKey(@Cast("c10::DispatchKey") byte t);

// Resolve alias dispatch key to DispatchKeySet if applicable
@Namespace("c10") public static native @ByVal DispatchKeySet getRuntimeDispatchKeySet(DispatchKey t);
@Namespace("c10") public static native @ByVal DispatchKeySet getRuntimeDispatchKeySet(@Cast("c10::DispatchKey") byte t);

// Returns a DispatchKeySet of all backend keys mapped to Autograd dispatch key
// t, DispatchKeySet is empty if t is not alias of DispatchKey::Autograd.
@Namespace("c10") public static native @ByVal DispatchKeySet getBackendKeySetFromAutograd(DispatchKey t);
@Namespace("c10") public static native @ByVal DispatchKeySet getBackendKeySetFromAutograd(@Cast("c10::DispatchKey") byte t);

// Returns a DispatchKeySet of autograd related keys mapped to backend.
@Namespace("c10") public static native @ByVal DispatchKeySet getAutogradRelatedKeySetFromBackend(DispatchKey t);
@Namespace("c10") public static native @ByVal DispatchKeySet getAutogradRelatedKeySetFromBackend(@Cast("c10::DispatchKey") byte t);

// Returns a DispatchKeySet of autocast related keys mapped to backend.
@Namespace("c10") public static native @ByVal DispatchKeySet getAutocastRelatedKeySetFromBackend(DispatchKey t);
@Namespace("c10") public static native @ByVal DispatchKeySet getAutocastRelatedKeySetFromBackend(@Cast("c10::DispatchKey") byte t);

// This API exists because we have a use case for checking
// getRuntimeDispatchKeySet(alias).has(DispatchKey::Undefined)
// in OperatorEntry.cpp but we disallow it in has() API.
@Namespace("c10") public static native @Cast("bool") boolean isIncludedInAlias(DispatchKey k, DispatchKey alias);
@Namespace("c10") public static native @Cast("bool") boolean isIncludedInAlias(@Cast("c10::DispatchKey") byte k, @Cast("c10::DispatchKey") byte alias);

// Historically, every tensor only had a single DispatchKey, and it was always
// something like CPU, and there wasn't any of this business where TLS
// could cause the DispatchKey of a tensor to change.  But we still have some
// legacy code that is still using DispatchKey for things like instanceof
// checks; if at all possible, refactor the code to stop using DispatchKey in
// those cases.
@Namespace("c10") public static native DispatchKey legacyExtractDispatchKey(@ByVal DispatchKeySet s);

// Given a function type, constructs a function_traits type that drops the first
// parameter type if the first parameter is of type DispatchKeySet. NB:
// DispatchKeySet is currently explicitly hidden from JIT (mainly to avoid
// pushing unnecessary arguments on the stack - see Note [ Plumbing Keys Through
// the Dispatcher] for details). If at any point in the future we need to expose
// this type to JIT, revisit the usage of this type alias.
 // namespace c10


// Parsed from c10/core/Backend.h

// #pragma once

// #include <c10/core/DeviceType.h>
// #include <c10/core/DispatchKey.h>
// #include <c10/core/DispatchKeySet.h>
// #include <c10/util/Exception.h>

// #include <stdexcept>

/**
 * This legacy enum class defines the set of backends supported by old school,
 * code generated Type-based ATen.  A "backend" in this sense roughly
 * corresponds to the cartesian product of (device type, layout), but restricted
 * only to combinations which we actually have kernels for.  Backend does NOT
 * include dtype.
 *
 * The reason we are sunsetting this enum class is because it doesn't allow for
 * open registration; e.g., if you want to add SparseXLA, you'd have to
 * edit this enum; you wouldn't be able to do it out of tree.  DispatchKey is
 * the replacement for Backend which supports open registration.
 *
 * NB: The concept of 'Backend' here disagrees with the notion of backend
 * exposed to users in torch.backends.  Backend here is something like "CPU"
 * or "SparseCUDA"; backend in torch.backends is something like "MKL" or
 * "CUDNN".
 */
@Namespace("c10") public enum Backend {
  CPU(0),
  CUDA(1),
  HIP(2),
  VE(3),
  FPGA(4),
  XPU(5),
  SparseCPU(6),
  SparseCUDA(7),
  SparseCsrCPU(8),
  SparseCsrCUDA(9),
  SparseHIP(10),
  SparseVE(11),
  SparseXPU(12),
  ORT(13),
  XLA(14),
  Vulkan(15),
  Metal(16),
  QuantizedCPU(17),
  QuantizedCUDA(18),
  QuantizedXPU(19),
  Undefined(20),
  MkldnnCPU(21),
  MLC(22),
  HPU(23),
  Lazy(24),
  NumOptions(25);

    public final int value;
    private Backend(int v) { this.value = v; }
    private Backend(Backend e) { this.value = e.value; }
    public Backend intern() { for (Backend e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") public static native Backend dispatchKeyToBackend(DispatchKey t);
@Namespace("c10") public static native @Cast("c10::Backend") int dispatchKeyToBackend(@Cast("c10::DispatchKey") byte t);

@Namespace("c10") public static native DispatchKey backendToDispatchKey(Backend b);
@Namespace("c10") public static native @Cast("c10::DispatchKey") byte backendToDispatchKey(@Cast("c10::Backend") int b);

@Namespace("c10") public static native DeviceType backendToDeviceType(Backend b);
@Namespace("c10") public static native @Cast("c10::DeviceType") byte backendToDeviceType(@Cast("c10::Backend") int b);

// TODO: This probably shouldn't actually be static inline
@Namespace("c10") public static native @Cast("const char*") BytePointer toString(Backend b);
@Namespace("c10") public static native String toString(@Cast("c10::Backend") int b);

@Namespace("c10") public static native @Cast("bool") boolean isSparse(Backend b);
@Namespace("c10") public static native @Cast("bool") boolean isSparse(@Cast("c10::Backend") int b);

@Namespace("c10") public static native @Cast("bool") boolean isSparseCsr(Backend b);
@Namespace("c10") public static native @Cast("bool") boolean isSparseCsr(@Cast("c10::Backend") int b);

 // namespace c10


// Parsed from c10/core/CopyBytes.h

// #pragma once

// #include <c10/core/Device.h>
// Targeting ../CopyBytesFunction.java


// Targeting ../_CopyBytesFunctionRegisterer.java



// #define REGISTER_COPY_BYTES_FUNCTION(from, to, ...)
//   namespace {
//   static _CopyBytesFunctionRegisterer C10_ANONYMOUS_VARIABLE(
//       g_copy_function)(from, to, __VA_ARGS__);
//   }

/*
 * WARNING: Implementations for this function are currently registered from
 * ATen and caffe2, not yet from c10. Don't use this if not either ATen
 * or caffe2 is present as well.
 * We can't move them yet, because the CUDA implementations aren't unified yet
 * between ATen and caffe2.
 * We're planning to move the implementations into c10/backend/xxx
 * to make c10 self contained again.
 */
@Namespace("c10") public static native void CopyBytes(
    @Cast("size_t") long nbytes,
    @Const Pointer src,
    @ByVal Device src_device,
    Pointer dst,
    @ByVal Device dst_device,
    @Cast("bool") boolean async);
 // namespace c10


// Parsed from c10/core/GradMode.h

// #pragma once

// #include <c10/core/AutogradState.h>
// #include <c10/macros/Macros.h>
// Targeting ../GradMode.java


// Targeting ../AutoGradMode.java


// Targeting ../NoGradGuard.java


// Targeting ../AutoFwGradMode.java



 // namespace c10


// Parsed from c10/core/InferenceMode.h

// #pragma once

// #include <c10/core/AutogradState.h>
// #include <c10/core/GradMode.h>
// #include <c10/core/impl/LocalDispatchKeySet.h>
// #include <c10/macros/Macros.h>
// Targeting ../InferenceMode.java


 // namespace c10


// Parsed from c10/core/Layout.h

// #pragma once

// #include <c10/core/Backend.h>
// #include <c10/util/Exception.h>

// #include <ostream>
@Namespace("c10") public enum Layout { Strided((byte)(0)), Sparse((byte)(1)), SparseCsr((byte)(2)), Mkldnn((byte)(3)), NumOptions((byte)(4));

    public final byte value;
    private Layout(byte v) { this.value = v; }
    private Layout(Layout e) { this.value = e.value; }
    public Layout intern() { for (Layout e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") public static native Layout layout_from_backend(Backend backend);
@Namespace("c10") public static native @Cast("c10::Layout") byte layout_from_backend(@Cast("c10::Backend") int backend);



 // namespace c10


// Parsed from c10/core/MemoryFormat.h

// #pragma once

// #include <c10/core/Backend.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>

// #include <ostream>

// Memory format is not the property of a Tensor. It is the way to tell an
// operator how the result should be organized in memory and nothing more. That
// means memory format should never be used as return value for any tensor state
// interrogation functions (internally and externally).
//
// Possible options are:
//  Preserve:
//    If any of the input tensors is in channels_last format, operator output
//    should be in channels_last format
//
//  Contiguous:
//    Regardless of input tensors format, the output should be contiguous
//    Tensor.
//
//  ChannelsLast:
//    Regardless of input tensors format, the output should be in channels_last
//    format.
@Namespace("c10") public enum MemoryFormat {
  Contiguous((byte)(0)),
  Preserve((byte)(1)),
  ChannelsLast((byte)(2)),
  ChannelsLast3d((byte)(3));

    public final byte value;
    private MemoryFormat(byte v) { this.value = v; }
    private MemoryFormat(MemoryFormat e) { this.value = e.value; }
    public MemoryFormat intern() { for (MemoryFormat e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

// If you are seeing this, it means that this call site was not checked if
// the memory format could be preserved, and it was switched to old default
// behaviour of contiguous
// #define LEGACY_CONTIGUOUS_MEMORY_FORMAT c10::get_contiguous_memory_format()

@Namespace("c10") public static native MemoryFormat get_contiguous_memory_format();



// Note: Hardcoded the channel last stride indices here to get better
// performance
@Namespace("c10") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector get_channels_last_strides_2d(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("c10") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector get_channels_last_strides_2d(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sizes);

@Namespace("c10") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector get_channels_last_strides_3d(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("c10") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector get_channels_last_strides_3d(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sizes);

// NOTE:
// Below are Helper functions for is_channels_last_strides_xd.
// 1. Please do not combine these helper functions, each helper function handles
// exactly one case of sizes + memory_format, by doing this, the strides indices
// will be a constant array and we can access it using constant index number,
// the compiler will fully unroll the loop on strides indices to gain a better
// performance.
// 2. No error check in helper function, caller ensures the correctness of the
// input
// 3. All helper functions have similar comments, only 1st helper function is
// commented here.
@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_2d_s4(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides);
@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_2d_s4(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... strides);

@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_3d_s5(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides);
@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_3d_s5(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... strides);

// Note [Ambiguous is_channels_last_strides_xd]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// The flaw of carrying memory_format implicitly through strides is very hard
// to WAR properly. issue #24090
// Without the history of permutation, we can't infer the memory_format of a
// tensor from the snapshot of its size & stride
// e.g.
//
// 1. We can NOT specify the memory_format of N111 tensor through strides in a
//  meaningful way;
//
// 2. Two path that ended up with identical size/stride
//  N11W contiguous tensor sliced at w-dimension becomes [N,1,1,1]@[W,W,W,W]
//  NC11 channels_last tensor sliced at c-dimension becomes [N,1,1,1]@[C,C,C,C]
//    So if we see a tensor [N,1,1,1]@[X,X,X,X], there's no way for us to infer
//    the memory_format of the original tensor.
//
// Due to the limitations, our temporary WAR `is_channels_last_strides` does the
// best effort to infer whether the original memory_format of a tensor is
// at::MemoryFormat::ChannelsLast. The two objectives of this function (ordered
// by their importance):
//   1. Ensure that normal shape manipulation does not accidentally change the
//      MemoryFormat of an existing tensor.
//   2. Allows user to mark MemoryFormat::ChannelsLast to tensors;
//
// The function does so via checking strides of the tensor, including strides of
// size-1 dimensions. Although conventionally PyTorch implies no restriction on
// trivial stride (stride for size-1 dimension).
//
// Note that this approach is a compromise. We did not solve the problem
// completely. Many cases we will not be able to infer the correct memory
// format.
// The implementation of `is_channels_last_strides` is to serve the objectives:
// MemoryFormat::ChannelsLast has to be explicitly opted-in (no accidental
// conversion); Best effort to maintain the ChannelsLast flag.
//
// Due to the fact that this is not a bulletproof solution, through testing
// (aten/src/ATen/test/memory_format_test.cpp)
//   a. we ensure that the common tasks are supported;
//   a. we identify corner cases where the implementation compromises on.
//
// By the time accumulated permutation is enabled to replace implicit
// memory_format through strides, we should be updating our tests and fix the
// issues in our tests.
//
// We use Channels Last 2d as an example above.
// This is a general problem for all the is_channels_last_strides_xd
// implementation. Please check the helper functions
// (is_channels_last_strides_*d_s*) for more details.

@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_2d(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides);
@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_2d(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... strides);

@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_3d(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides);
@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_3d(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... strides);

 // namespace c10


// Parsed from c10/core/QEngine.h

// #pragma once

// #include <c10/core/DeviceType.h>
// #include <c10/core/DispatchKey.h>
// #include <c10/util/Exception.h>

/**
 * QEngine is an enum that is used to select the engine to run quantized ops.
 * Keep this enum in sync with get_qengine_id() in
 * torch/backends/quantized/__init__.py
 */
@Namespace("c10") public enum QEngine {
  NoQEngine((byte)(0)),
  FBGEMM((byte)(1)),
  QNNPACK((byte)(2));

    public final byte value;
    private QEngine(byte v) { this.value = v; }
    private QEngine(QEngine e) { this.value = e.value; }
    public QEngine intern() { for (QEngine e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") public static native @StdString BytePointer toString(QEngine qengine);

 // namespace c10


// Parsed from c10/core/QScheme.h

// #pragma once

// #include <c10/core/DeviceType.h>
// #include <c10/util/Exception.h>

/**
 * QScheme is an enum that specifies the type of quantization. This has a one
 * to one correspondence with Quantizer
 * Please refer to ATen/quantized/Quantizer.h to see the Quantizers classes.
 * Keep this file in sync with torch/nn/_qscheme.py
 */
@Namespace("c10") public enum QScheme {
  PER_TENSOR_AFFINE((byte)(0)),
  PER_CHANNEL_AFFINE((byte)(1)),
  PER_TENSOR_SYMMETRIC((byte)(2)),
  PER_CHANNEL_SYMMETRIC((byte)(3)),
  PER_CHANNEL_AFFINE_FLOAT_QPARAMS((byte)(4)),
  COMPILE_TIME_NUM_QSCHEMES((byte)(5));

    public final byte value;
    private QScheme(byte v) { this.value = v; }
    private QScheme(QScheme e) { this.value = e.value; }
    public QScheme intern() { for (QScheme e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
@Namespace("c10") @MemberGetter public static native int COMPILE_TIME_NUM_QSCHEMES();

@Namespace("c10") public static native @StdString BytePointer toString(QScheme qscheme);

 // namespace c10


// Parsed from c10/core/Stream.h

// #pragma once

// #include <c10/core/Device.h>

/** An index representing a specific stream.  A StreamId is not independently
 *  meaningful without knowing the Device it is associated with; try to
 *  use Stream rather than StreamId directly.
 * 
 *  StreamIds are opaque; they are assigned by some DeviceType-specific
 *  numbering system which is not visible to the user.  HOWEVER, we
 *  guarantee that StreamId 0 is always a valid stream, and corresponds
 *  to some sort of "default" stream. */
// Targeting ../Stream.java






// Targeting ../StreamHash.java


 // namespace std


// Parsed from c10/core/ScalarType.h

// #pragma once

// #include <c10/util/ArrayRef.h>
// #include <c10/util/BFloat16.h>
// #include <c10/util/Half.h>
// #include <c10/util/Optional.h>
// #include <c10/util/complex.h>
// #include <c10/util/qint32.h>
// #include <c10/util/qint8.h>
// #include <c10/util/quint4x2.h>
// #include <c10/util/quint8.h>

// #include <complex>
// #include <cstdint>
// #include <ostream>

// For the macros below:
// NB: If you want to macro some code for all non-QInt scalar types (i.e. types
// with complete information, you probably want one of the
// AT_FORALL_SCALAR_TYPES / AT_FORALL_SCALAR_TYPES_AND
// macros below, which are designed to behave similarly to the Dispatch macros
// with the same name.

// NB: Order matters for this macro; it is relied upon in
// _promoteTypesLookup and the serialization format.
// #define AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS(_)
//   _(uint8_t, Byte) /* 0 */
//   _(int8_t, Char) /* 1 */
//   _(int16_t, Short) /* 2 */
//   _(int, Int) /* 3 */
//   _(int64_t, Long) /* 4 */
//   _(at::Half, Half) /* 5 */
//   _(float, Float) /* 6 */
//   _(double, Double) /* 7 */
//   _(c10::complex<c10::Half>, ComplexHalf) /* 8 */
//   _(c10::complex<float>, ComplexFloat) /* 9 */
//   _(c10::complex<double>, ComplexDouble) /* 10 */
//   _(bool, Bool) /* 11 */
//   _(c10::qint8, QInt8) /* 12 */
//   _(c10::quint8, QUInt8) /* 13 */
//   _(c10::qint32, QInt32) /* 14 */
//   _(at::BFloat16, BFloat16) /* 15 */
//   _(c10::quint4x2, QUInt4x2) /* 16 */

// If you want to support ComplexHalf for real, add ComplexHalf
// into this macro (and change the name).  But beware: convert()
// doesn't work for all the conversions you need...
// #define AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_EXCEPT_COMPLEX_HALF(_)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(at::Half, Half)
//   _(float, Float)
//   _(double, Double)
//   _(c10::complex<float>, ComplexFloat)
//   _(c10::complex<double>, ComplexDouble)
//   _(bool, Bool)
//   _(at::BFloat16, BFloat16)

@Namespace("c10") public enum ScalarType {
  Byte((byte)(0)), /* 0 */
  Char((byte)(1)), /* 1 */
  Short((byte)(2)), /* 2 */
  Int((byte)(3)), /* 3 */
  Long((byte)(4)), /* 4 */
  Half((byte)(5)), /* 5 */
  Float((byte)(6)), /* 6 */
  Double((byte)(7)), /* 7 */
  ComplexHalf((byte)(8)), /* 8 */
  ComplexFloat((byte)(9)), /* 9 */
  ComplexDouble((byte)(10)), /* 10 */
  Bool((byte)(11)), /* 11 */
  QInt8((byte)(12)), /* 12 */
  QUInt8((byte)(13)), /* 13 */
  QInt32((byte)(14)), /* 14 */
  BFloat16((byte)(15)), /* 15 */
  QUInt4x2((byte)(16)),
      Undefined((byte)(17)),
  NumOptions((byte)(18));

    public final byte value;
    private ScalarType(byte v) { this.value = v; }
    private ScalarType(ScalarType e) { this.value = e.value; }
    public ScalarType intern() { for (ScalarType e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") @MemberGetter public static native @Cast("const uint16_t") short NumScalarTypes();

// These are used to map ScalarTypes to C++ types.

// #define SPECIALIZE_ScalarTypeToCPPType(cpp_type, scalar_type)
//   template <>
//   struct ScalarTypeToCPPType<c10::ScalarType::scalar_type> {
//     using type = cpp_type;
// 
//     /* This is a workaround for the CUDA bug which prevents */
//     /* ::detail::ScalarTypeToCType<T>::type being used directly due to */
//     /* ambiguous reference which can't to be resolved. For some reason it */
//     /* cant pick between at::detail and at::cuda::detail. */
//     /* For repro example, please see: */
//     /* https://gist.github.com/izdeby/952ae7cf256ddb740a73776d39a7e7ba */
//     /* TODO: remove once the bug is fixed. */
//     static type t;
//   }; /* 0 */ /* 1 */ /* 2 */ /* 3 */ /* 4 */ /* 5 */ /* 6 */ /* 7 */ /* 8 */ /* 9 */ /* 10 */ /* 11 */ /* 12 */ /* 13 */ /* 14 */ /* 15 */ /* 16 */

// #undef SPECIALIZE_ScalarTypeToCPPType

 // namespace impl

// #define SPECIALIZE_CppTypeToScalarType(cpp_type, scalar_type)
//   template <>
//   struct CppTypeToScalarType<cpp_type>
//       : std::
//             integral_constant<c10::ScalarType, c10::ScalarType::scalar_type> {
//   }; /* 0 */ /* 1 */ /* 2 */ /* 3 */ /* 4 */ /* 5 */ /* 6 */ /* 7 */ /* 8 */ /* 9 */ /* 10 */ /* 11 */ /* 12 */ /* 13 */ /* 14 */ /* 15 */ /* 16 */

// #undef SPECIALIZE_CppTypeToScalarType

// #define AT_FORALL_INT_TYPES(_)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)

// #define AT_FORALL_SCALAR_TYPES(_)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(float, Float)
//   _(double, Double)

// #define AT_FORALL_SCALAR_TYPES_AND(SCALARTYPE, _)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(float, Float)
//   _(double, Double)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE>::t),
//     SCALARTYPE)

// #define AT_FORALL_SCALAR_TYPES_AND2(SCALARTYPE1, SCALARTYPE2, _)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(float, Float)
//   _(double, Double)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE1>::t),
//     SCALARTYPE1)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE2>::t),
//     SCALARTYPE2)

// #define AT_FORALL_SCALAR_TYPES_AND3(SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, _)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(float, Float)
//   _(double, Double)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE1>::t),
//     SCALARTYPE1)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE2>::t),
//     SCALARTYPE2)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE3>::t),
//     SCALARTYPE3)

// #define AT_FORALL_QINT_TYPES(_)
//   _(c10::qint8, QInt8)
//   _(c10::quint8, QUInt8)
//   _(c10::qint32, QInt32)
//   _(c10::quint4x2, QUInt4x2)

// #define AT_FORALL_COMPLEX_TYPES(_)
//   _(c10::complex<float>, ComplexFloat)
//   _(c10::complex<double>, ComplexDouble)

// #define DEFINE_CONSTANT(_, name)
//   constexpr ScalarType k##name = ScalarType::name;

@Namespace("c10") @MemberGetter public static native ScalarType kByte(); /* 0 */
  @Namespace("c10") @MemberGetter public static native ScalarType kChar(); /* 1 */
  @Namespace("c10") @MemberGetter public static native ScalarType kShort(); /* 2 */
  @Namespace("c10") @MemberGetter public static native ScalarType kInt(); /* 3 */
  @Namespace("c10") @MemberGetter public static native ScalarType kLong(); /* 4 */
  @Namespace("c10") @MemberGetter public static native ScalarType kHalf(); /* 5 */
  @Namespace("c10") @MemberGetter public static native ScalarType kFloat(); /* 6 */
  @Namespace("c10") @MemberGetter public static native ScalarType kDouble(); /* 7 */
  @Namespace("c10") @MemberGetter public static native ScalarType kComplexHalf(); /* 8 */
  @Namespace("c10") @MemberGetter public static native ScalarType kComplexFloat(); /* 9 */
  @Namespace("c10") @MemberGetter public static native ScalarType kComplexDouble(); /* 10 */
  @Namespace("c10") @MemberGetter public static native ScalarType kBool(); /* 11 */
  @Namespace("c10") @MemberGetter public static native ScalarType kQInt8(); /* 12 */
  @Namespace("c10") @MemberGetter public static native ScalarType kQUInt8(); /* 13 */
  @Namespace("c10") @MemberGetter public static native ScalarType kQInt32(); /* 14 */
  @Namespace("c10") @MemberGetter public static native ScalarType kBFloat16(); /* 15 */
  @Namespace("c10") @MemberGetter public static native ScalarType kQUInt4x2(); /* 16 */
// #undef DEFINE_CONSTANT

@Namespace("c10") public static native @Cast("const char*") BytePointer toString(ScalarType t);

@Namespace("c10") public static native @Cast("size_t") long elementSize(ScalarType t);

@Namespace("c10") public static native @Cast("bool") @Deprecated boolean isIntegralType(ScalarType t);

@Namespace("c10") public static native @Cast("bool") boolean isIntegralType(ScalarType t, @Cast("bool") boolean includeBool);

@Namespace("c10") public static native @Cast("bool") boolean isFloatingType(ScalarType t);

@Namespace("c10") public static native @Cast("bool") boolean isComplexType(ScalarType t);

@Namespace("c10") public static native @Cast("bool") boolean isQIntType(ScalarType t);

@Namespace("c10") public static native ScalarType toQIntType(ScalarType t);

@Namespace("c10") public static native ScalarType toUnderlying(ScalarType t);

@Namespace("c10") public static native @Cast("bool") boolean isSignedType(ScalarType t);

@Namespace("c10") public static native @Cast("bool") boolean isUnderlying(ScalarType type, ScalarType qtype);

@Namespace("c10") public static native ScalarType toValueType(ScalarType t);

@Namespace("c10") public static native ScalarType toComplexType(ScalarType t);

// see tensor_attributes.rst for detailed explanation and examples
// of casting rules.
@Namespace("c10") public static native @Cast("bool") boolean canCast(ScalarType from, ScalarType to);

@Namespace("c10") public static native ScalarType promoteTypes(ScalarType a, ScalarType b);



// #define AT_FORAUTOCAST_SCALAR_TYPES(_)
//   _(half, Half) /* 0 */
//   _(bfloat16, BFloat16) /* 1 */

 // namespace c10


// Parsed from c10/core/ScalarTypeToTypeMeta.h

// #pragma once

// #include <c10/core/ScalarType.h>
// #include <c10/util/typeid.h>

// these just expose TypeMeta/ScalarType bridge functions in c10
// TODO move to typeid.h (or codemod away) when TypeMeta et al
// are moved from caffe2 to c10 (see note at top of typeid.h)

/**
 * convert ScalarType enum values to TypeMeta handles
 */
@Namespace("c10") public static native @ByVal TypeMeta scalarTypeToTypeMeta(ScalarType scalar_type);

/**
 * convert TypeMeta handles to ScalarType enum values
 */
@Namespace("c10") public static native ScalarType typeMetaToScalarType(@ByVal TypeMeta dtype);

/**
 * typeMetaToScalarType(), lifted to optional
 */
@Namespace("c10") public static native @ByVal ScalarTypeOptional optTypeMetaToScalarType(
    @ByVal TypeMetaOptional type_meta);

/**
 * convenience: equality across TypeMeta/ScalarType conversion
 */








 // namespace c10


// Parsed from c10/core/Scalar.h

// #pragma once

// #include <assert.h>
// #include <stdint.h>
// #include <stdexcept>
// #include <string>
// #include <type_traits>
// #include <utility>

// #include <c10/core/OptionalRef.h>
// #include <c10/core/ScalarType.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Half.h>
// #include <c10/util/TypeCast.h>
// Targeting ../Scalar.java



// define the scalar.to<int64_t>() specializations
// #define DEFINE_TO(T, name)
//   template <>
//   inline T Scalar::to<T>() const {
//     return to##name();
//   }

  
  
  
  
  
  
  
  
  
  
  
// #undef DEFINE_TO

 // namespace c10


// Parsed from c10/core/Allocator.h

// #pragma once

// #include <stddef.h>
// #include <memory>

// #include <c10/core/Device.h>
// #include <c10/util/Exception.h>
// #include <c10/util/ThreadLocalDebugInfo.h>
// #include <c10/util/UniqueVoidPtr.h>
// Targeting ../DataPtr.java



// NB: Device is NOT tested for here; a CUDA nullptr is as much a nullptr as a
// CPU nullptr





// Targeting ../Allocator.java



// This context is used to generate DataPtr which have arbitrary
// std::function deleters associated with them.  In some user facing
// functions, we give a (user-friendly) interface for constructing
// tensors from external data which take an arbitrary std::function
// deleter.  Grep for InefficientStdFunctionContext to find these
// occurrences.
//
// This context is inefficient because we have to do a dynamic
// allocation InefficientStdFunctionContext, on top of the dynamic
// allocation which is implied by std::function itself.

/** Set the allocator for DeviceType {@code t}. The passed in allocator pointer is
 *  expected to have static lifetime; this function does NOT take ownership
 *  of the raw pointer. (The reason for this is to prevent existing pointers
 *  to an allocator of a particular device from being invalidated when
 *  SetAllocator is called.)
 *
 *  Also note that this is not thread-safe, and we assume this function will
 *  only be called during initialization.
 *
 *  The 'priority' flag is introduced when we want to overwrite the default
 *  allocator, since the allocators are set statically. The default priority
 *  is 0, which means the lowest. Only higher or equal priority can overwrite
 *  existing ones.
 */
@Namespace("c10") public static native void SetAllocator(DeviceType t, Allocator alloc, @Cast("uint8_t") byte priority/*=0*/);
@Namespace("c10") public static native void SetAllocator(DeviceType t, Allocator alloc);
@Namespace("c10") public static native void SetAllocator(@Cast("c10::DeviceType") byte t, Allocator alloc, @Cast("uint8_t") byte priority/*=0*/);
@Namespace("c10") public static native void SetAllocator(@Cast("c10::DeviceType") byte t, Allocator alloc);
@Namespace("c10") public static native Allocator GetAllocator(DeviceType t);
@Namespace("c10") public static native Allocator GetAllocator(@Cast("c10::DeviceType") byte t);

// #define REGISTER_ALLOCATOR(t, f)
//   namespace {
//   static AllocatorRegisterer<t> g_allocator_d(f);
//   }
// Targeting ../MemoryReportingInfoBase.java



@Namespace("c10") public static native @Cast("bool") boolean memoryProfilingEnabled();
@Namespace("c10") public static native void reportMemoryUsageToProfiler(
    Pointer ptr,
    @Cast("int64_t") long alloc_size,
    @Cast("int64_t") long total_allocated,
    @Cast("int64_t") long total_reserved,
    @ByVal Device device);

 // namespace c10


// Parsed from c10/core/DefaultDtype.h

// #pragma once

// #include <c10/core/ScalarType.h>
// #include <c10/macros/Macros.h>
 // namespace caffe2
@Namespace("c10") public static native void set_default_dtype(@ByVal TypeMeta dtype);
@Namespace("c10") public static native @Const @ByVal TypeMeta get_default_dtype();
@Namespace("c10") public static native ScalarType get_default_dtype_as_scalartype();
@Namespace("c10") public static native @Const @ByVal TypeMeta get_default_complex_dtype();
 // namespace c10


// Parsed from c10/core/StorageImpl.h

// #pragma once

// #include <c10/core/Allocator.h>
// #include <c10/core/ScalarType.h>

// #include <c10/util/intrusive_ptr.h>
// Targeting ../StorageImpl.java


 // namespace c10


// Parsed from c10/core/Storage.h

// #pragma once

// #include <c10/core/StorageImpl.h>
// Targeting ../Storage.java



 // namespace c10


// Parsed from c10/core/TensorOptions.h

// #pragma once

// #include <c10/core/Backend.h>
// #include <c10/core/DefaultDtype.h>
// #include <c10/core/Device.h>
// #include <c10/core/DispatchKeySet.h>
// #include <c10/core/Layout.h>
// #include <c10/core/MemoryFormat.h>
// #include <c10/core/ScalarType.h>
// #include <c10/core/ScalarTypeToTypeMeta.h>

// #include <c10/macros/Macros.h>
// #include <c10/util/C++17.h>
// #include <c10/util/Optional.h>

// #include <cstddef>
// #include <iosfwd>
// #include <utility>

@Namespace("c10") public static native DispatchKey computeDispatchKey(
    @ByVal ScalarTypeOptional dtype,
    @ByVal LayoutOptional layout,
    @ByVal DeviceOptional device);

@Namespace("c10") public static native ScalarType dtype_or_default(@ByVal ScalarTypeOptional dtype);

@Namespace("c10") public static native @ByVal TypeMeta dtype_or_default(
    @ByVal TypeMetaOptional dtype);

@Namespace("c10") public static native Layout layout_or_default(@ByVal LayoutOptional layout);

@Namespace("c10") public static native @ByVal Device device_or_default(@ByVal DeviceOptional device);


///
///
///
///
///
///
///
///
///
///
///
///
///
///
///
///
///
@Namespace("c10") public static native @Cast("bool") boolean pinned_memory_or_default(@ByVal BoolOptional pinned_memory);
// Targeting ../TensorOptions.java



// We should aspire to fit in one machine-size word; but a size greater than two
// words is too much.  (We are doing terribly on 32-bit archs, where we require
// three machine size words to store tensor options.  Eek!)

/** Convenience function that returns a {@code TensorOptions} object with the {@code dtype}
 *  set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions dtype(@ByVal TypeMeta dtype);

// legacy function to support ScalarType
@Namespace("c10") public static native @ByVal TensorOptions dtype(ScalarType dtype);

/** Convenience function that returns a {@code TensorOptions} object with the {@code layout}
 *  set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions layout(Layout layout);
@Namespace("c10") public static native @ByVal TensorOptions layout(@Cast("c10::Layout") byte layout);

/** Convenience function that returns a {@code TensorOptions} object with the {@code device}
 *  set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions device(@ByVal Device device);

/** Convenience function that returns a {@code TensorOptions} object with the
 *  {@code device} set to CUDA and the {@code device_index} set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions device_index(short device_index);

/** Convenience function that returns a {@code TensorOptions} object with the
 *  {@code requires_grad} set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions requires_grad(@Cast("bool") boolean requires_grad/*=true*/);

/** Convenience function that returns a {@code TensorOptions} object with the
 *  {@code memory_format} set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions memory_format(MemoryFormat memory_format);
@Namespace("c10") public static native @ByVal TensorOptions memory_format(@Cast("c10::MemoryFormat") byte memory_format);



@Namespace("c10") public static native @StdString BytePointer toString(@Const @ByVal TensorOptions options);

// This is intended to be a centralized location by which we can determine
// what an appropriate DispatchKey for a tensor is.

@Namespace("c10") public static native Layout dispatchKeyToLayout(DispatchKey dispatch_key);
@Namespace("c10") public static native @Cast("c10::Layout") byte dispatchKeyToLayout(@Cast("c10::DispatchKey") byte dispatch_key);

@Namespace("c10") public static native DeviceType dispatchKeyToDeviceType(DispatchKey dispatch_key);
@Namespace("c10") public static native @Cast("c10::DeviceType") byte dispatchKeyToDeviceType(@Cast("c10::DispatchKey") byte dispatch_key);

@Namespace("c10") public static native @ByVal TensorOptions dispatchKeyToTensorOptions(DispatchKey dispatch_key);
@Namespace("c10") public static native @ByVal TensorOptions dispatchKeyToTensorOptions(@Cast("c10::DispatchKey") byte dispatch_key);

 // namespace c10


// Parsed from c10/core/TensorImpl.h

// #pragma once

// #include <algorithm>
// #include <atomic>
// #include <memory>
// #include <numeric>

// #include <c10/core/Backend.h>
// #include <c10/core/CopyBytes.h>
// #include <c10/core/DispatchKeySet.h>
// #include <c10/core/InferenceMode.h>
// #include <c10/core/MemoryFormat.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/core/impl/LocalDispatchKeySet.h>
// #include <c10/core/impl/SizesAndStrides.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Flags.h>
// #include <c10/util/Logging.h>
// #include <c10/util/Optional.h>
// #include <c10/util/accumulate.h>
// #include <c10/util/python_stub.h>

// A global boolean variable to control whether we free memory when a Tensor
// is shrunk to a smaller size. As a result, a Tensor is always going to
// keep the memory allocated for its maximum capacity reshaped to so far.
//
// This parameter is respected "upper-case" methods which call Resize()
// (e.g., CopyFrom, ResizeLike); it is NOT respected by Tensor::resize_
// or ShrinkTo, both of which guarantee to never to free memory.


// Since we can have high variance in blob memory allocated across different
// inputs in the same run, we will shrink the blob only if the memory gain
// is larger than this flag in bytes.  This only applies to functions which
// respect caffe2_keep_on_shrink.

 // namespace at
 // namespace c10

 // namespace torch

/**
 * A utility function to convert vector<int> to vector<int64_t>.
 */
@Namespace("c10") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector ToVectorint64_t(@ByVal @Cast("c10::ArrayRef<int>*") IntArrayRef src);

/**
 * Return product of all dimensions starting from k
 */
@Namespace("c10") public static native @Cast("int64_t") long size_from_dim_(int k, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("c10") public static native @Cast("int64_t") long size_from_dim_(int k, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);

// Product of all dims up to k (not including dims[k])
@Namespace("c10") public static native @Cast("int64_t") long size_to_dim_(int k, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("c10") public static native @Cast("int64_t") long size_to_dim_(int k, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);

// Product of all dims between k and l (not including dims[k] and dims[l])
@Namespace("c10") public static native @Cast("int64_t") long size_between_dim_(int k, int l, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("c10") public static native @Cast("int64_t") long size_between_dim_(int k, int l, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);

// Wrap around axis_index if it is negative, s.t., -1 is the last dim
@Namespace("c10") public static native int canonical_axis_index_(int axis_index, int ndims);
// Targeting ../PlacementDtor.java


// Targeting ../PlacementDeleteContext.java


// Targeting ../AutogradMetaInterface.java



// forward declared
// Targeting ../AutogradMetaFactory.java



@Namespace("c10::impl") public static native void SetAutogradMetaFactory(AutogradMetaFactory factory);
@Namespace("c10::impl") public static native AutogradMetaFactory GetAutogradMetaFactory();
// Targeting ../AutogradMetaFactoryRegisterer.java



// Note [Python interpreter tag]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// We store a PyObject on TensorImpl so that we can efficiently translate
// tensors into the Python representations.  However, in some situations
// (torchdeploy) there may be multiple Python interpreters in a single process
// and we must take care not to accidentally mix up PyObjects with the wrong
// interpreters.  Thus, we also tag every TensorImpl with the Python interpreter
// it corresponds to.
//
// With torchdeploy, we have these invariants:
//  - Any given TensorImpl can be associated with AT MOST one Python
//  interpreter.
//    We represent the interpreter tag as a memory address to an instance of
//    a virtual class that is allocated once per interpreter (this is so that
//    we can request the interpreter to perform operations for us, if
//    necessary).
//  - A given TensorImpl's interpreter tag can only go from uninitialized to
//    tagged; once tagged, this is a quiescent state (once tagged to an
//    interpreter, ALWAYS tagged to that interpreter)
//  - A thread may mutate the PyObject field of a TensorImpl if and only if it
//    holds the GIL for the interpreter tagged on the TensorImpl.  (If the
//    TensorImpl is not tagged, it must first atomically claim its tag before it
//    can validly write)

// The PyInterpreter object itself is a class that contains some function
// pointers for interacting with the interpreter.  For now this is just for
// debugging, but if a Tensor can own a PyObject, the interpreter can be used to
// free it.
//
// WARNING: This class has to be written very carefully, because it may be
// possible for a Tensor to have a reference an interpreter corresponding to
// a shared library that has ALREADY BEEN UNLOADED.  This makes blindly calling
// virtual methods very dangerous, because the vtable may be garbage at that
// point (on a good day, you might get "pure virtual method called").
//
// The idea to solve this problem is we always leak PyInterpreters (so they
// always stay live even after dlclose), and disarm the "virtual methods" by
// replacing them with function pointers that just no-op.  This can't be done
// with a traditional C++ vtable, so we have to roll our own.
//
// NB: The downside with representing PyInterpreter tags as full objects is that
// it takes an extra word on TensorImpl.  If tags were instead just integer
// indices, on 64-bit architectures we could pack the tag and PyObject together
// into a single atomic word.  On 32-bit architectures we could simply say that
// only one Python interpreter is supported (erroring if a nontrivial
// interpreter tag is attempted to be set).
//
// The difficulty with this scheme is we need to maintain an out-of-line table
// to get at the PyInterpreters so that we can do virtual method calls on them,
// and registration/deregistration to this table must be done in a thread safe
// manner.  This can be easily done if the number of possible PyInterpreters is
// small enough (e.g., 8-bit integer) by simply preallocating an array of
// sufficient size to hold all possible interpreters.  Surely 128 threads is
// more than enough for anyone!
//
// I didn't decide to do this technique at the moment, because the extra word
// added by the PyInterpreter tag takes us to 24 words, which means that we
// still fit inside three eight word cache lines.  If you need to penny pinch
// another word consider doing this!

// PyInterpreterStatus describes what the state of its interpreter tag
// is, relative to the thread currently holding the GIL.
@Namespace("c10::impl") public enum PyInterpreterStatus {
  // We just allocated the Tensor, it hasn't escaped to other threads,
  // we know that it definitely hasn't been tagged to be associated
  // with an interpreter.
  DEFINITELY_UNINITIALIZED(0),
  // We queried the interpreter field and it looked uninitialized.  But
  // another thread may have raced with us to tag it with some other
  // interpreter id.  So we will have to do a CEX to make sure we can
  // actually nab it.
  MAYBE_UNINITIALIZED(1),
  // We queried the interpreter field and it was tagged to belong to us.
  // This means we have sole write access (as we hold the GIL for this
  // interpreter)
  TAGGED_BY_US(2),
  // Someone else tagged this.  We can't use this TensorImpl from Python.
  TAGGED_BY_OTHER(3);

    public final int value;
    private PyInterpreterStatus(int v) { this.value = v; }
    private PyInterpreterStatus(PyInterpreterStatus e) { this.value = e.value; }
    public PyInterpreterStatus intern() { for (PyInterpreterStatus e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}


// Targeting ../NamedTensorMetaInterface.java


// Targeting ../TorchDispatchTypeObject.java


// Targeting ../VariableVersion.java



/**
 * NOTE: Some TensorImpl methods are small and not overridden in the
 * PyTorch codebase itself, but may theoretically need to be
 * overridden by third-party TensorImpl subclasses. This macro allows
 * users that need maximum performance and don't need these extension
 * points to disable them with a build-time flag. (In particular,
 * XLA's XLATensorImpl currently overrides these methods, so we can't
 * enable this flag by default.)
 */
// #ifdef C10_DISABLE_TENSORIMPL_EXTENSIBILITY
// #define TENSORIMPL_MAYBE_VIRTUAL
// #else
// #define TENSORIMPL_MAYBE_VIRTUAL virtual
// Targeting ../TensorImpl.java



// Note [TensorImpl size constraints]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Changed the size of TensorImpl?  If the size went down, good for
// you!  Adjust the documentation below and the expected size.
// Did it go up?  Read on...
//
// Struct size matters.  In some production systems at Facebook, we have
// 400M live tensors during a training run.  Do the math: every 64-bit
// word you add to Tensor is an extra 3.2 gigabytes in RAM.
//
// If you are a Facebook employee, you can check if the run in question
// has tipped you over the point using the command here:
// https://fburl.com/q5enpv98
//
// For reference, we OOMed at 160 bytes (20 words) per TensorImpl.
// This is not counting overhead from strides out-of-line allocation and
// StorageImpl space and this is from before we inlined sizes and strides
// directly into TensorImpl as SmallVectors.
//
// Our memory usage on 32-bit systems is suboptimal, but we're not checking
// for it at the moment (to help avoid rage inducing cycles when the
// 32-bit number is wrong).
//
// Current breakdown:
//
//    vtable pointer
//    strong refcount           TODO: pack these into one word
//    weak refcount
//    storage pointer
//    autograd metadata pointer
//    named tensor metadata pointer
//    version counter pointer
//    Python interpreter pointer
//    PyObject pointer
//    SizesAndStrides size/pointer
//    SizesAndStrides sizes (pre-allocated 0)
//    SizesAndStrides sizes (pre-allocated 1)
//    SizesAndStrides sizes (pre-allocated 2)
//    SizesAndStrides sizes (pre-allocated 3)
//    SizesAndStrides sizes (pre-allocated 4)
//    SizesAndStrides strides (pre-allocated 0)
//    SizesAndStrides strides (pre-allocated 1)
//    SizesAndStrides strides (pre-allocated 2)
//    SizesAndStrides strides (pre-allocated 3)
//    SizesAndStrides strides (pre-allocated 4)
//    storage offset
//    numel
//    data type, device, is_contiguous, storage_access_should_throw_, bitfields
//    DispatchKeySet
//
 // namespace c10


// Parsed from c10/core/UndefinedTensorImpl.h

// #pragma once

// #include <c10/core/TensorImpl.h>
// Targeting ../UndefinedTensorImpl.java



 // namespace c10


// Parsed from c10/core/WrapDimMinimal.h

// #pragma once

// #include <c10/util/Exception.h>

@Namespace("c10") public static native @Cast("int64_t") long maybe_wrap_dim(
    @Cast("int64_t") long dim,
    @Cast("int64_t") long dim_post_expr,
    @Cast("bool") boolean wrap_scalar/*=true*/);
@Namespace("c10") public static native @Cast("int64_t") long maybe_wrap_dim(
    @Cast("int64_t") long dim,
    @Cast("int64_t") long dim_post_expr);

 // namespace c10


// Parsed from ATen/core/aten_interned_strings.h

// #pragma once

// ATen symbols correspond exactly to operators defined in ATen.  Every
// symbol here corresponds exactly to an ATen operation which is defined
// in Declarations.yaml; attributes are in one-to-one correspondence with
// their ATen name.
//
// To explicitly use interned strings as symbols in your code, you must add
// them to this list.

// #define FORALL_ATEN_BASE_SYMBOLS(_)
// _(aten, __and__)
// _(aten, __iand__)
// _(aten, __ilshift__)
// _(aten, __ior__)
// _(aten, __irshift__)
// _(aten, __ixor__)
// _(aten, __lshift__)
// _(aten, __or__)
// _(aten, __rshift__)
// _(aten, __xor__)
// _(aten, _abs)
// _(aten, _addmv)
// _(aten, _addr)
// _(aten, _amp_foreach_non_finite_check_and_unscale_)
// _(aten, _amp_update_scale_)
// _(aten, _arange)
// _(aten, _argmax)
// _(aten, _argmin)
// _(aten, _baddbmm_mkl)
// _(aten, _cast_Byte)
// _(aten, _cast_Char)
// _(aten, _cast_Double)
// _(aten, _cast_Float)
// _(aten, _cast_Half)
// _(aten, _cast_Int)
// _(aten, _cast_Long)
// _(aten, _cast_Short)
// _(aten, _ceil)
// _(aten, _clamp_max)
// _(aten, _clamp_min)
// _(aten, _convolution)
// _(aten, _convolution_double_backward)
// _(aten, convolution_overrideable)
// _(aten, convolution_backward_overrideable)
// _(aten, _convolution_nogroup)
// _(aten, _copy_ignoring_overlaps)
// _(aten, _cos)
// _(aten, _cosh)
// _(aten, _ctc_loss)
// _(aten, _ctc_loss_backward)
// _(aten, _cudnn_ctc_loss)
// _(aten, _cudnn_init_dropout_state)
// _(aten, _cudnn_rnn)
// _(aten, _cudnn_rnn_backward)
// _(aten, _cudnn_rnn_flatten_weight)
// _(aten, _cufft_clear_plan_cache)
// _(aten, _cufft_get_plan_cache_max_size)
// _(aten, _cufft_get_plan_cache_size)
// _(aten, _cufft_set_plan_cache_max_size)
// _(aten, _denseDims)
// _(aten, _dimI)
// _(aten, _dimV)
// _(aten, _dim_arange)
// _(aten, _dirichlet_grad)
// _(aten, _dot)
// _(aten, _embedding_bag)
// _(aten, _embedding_bag_backward)
// _(aten, _embedding_bag_dense_backward)
// _(aten, _embedding_bag_sparse_backward)
// _(aten, _erf)
// _(aten, _erfc)
// _(aten, _exp)
// _(aten, _exp2)
// _(aten, _expm1)
// _(aten, _fft_with_size)
// _(aten, _fill)
// _(aten, _floor)
// _(aten, _fused_dropout)
// _(aten, _indices)
// _(aten, _ldexp)
// _(aten, _linspace)
// _(aten, _local_scalar)
// _(aten, _local_scalar_dense)
// _(aten, _log)
// _(aten, _log10)
// _(aten, _log1p)
// _(aten, _log2)
// _(aten, _logspace)
// _(aten, _lu_with_info)
// _(aten, _masked_scale)
// _(aten, _mm)
// _(aten, _mv)
// _(aten, _nnz)
// _(aten, _nansum)
// _(aten, _pack_padded_sequence)
// _(aten, _pack_padded_sequence_backward)
// _(aten, _pad_packed_sequence)
// _(aten, _pdist_backward)
// _(aten, _pdist_forward)
// _(aten, _prod)
// _(aten, _prodall)
// _(aten, _range)
// _(aten, _reshape_from_tensor)
// _(aten, _round)
// _(aten, _rsqrt)
// _(aten, _s_where)
// _(aten, _shape_as_tensor)
// _(aten, _sigmoid)
// _(aten, _sigmoid_forward)
// _(aten, _sin)
// _(aten, _sinh)
// _(aten, _sparseDims)
// _(aten, _sparse_add)
// _(aten, _sparse_addmm)
// _(aten, _sparse_coo_tensor_with_dims)
// _(aten, _sparse_coo_tensor_with_dims_and_tensors)
// _(aten, _sparse_coo_tensor_unsafe)
// _(aten, _sparse_csr_tensor_unsafe)
// _(aten, _sparse_dense_add)
// _(aten, _sparse_div_scalar)
// _(aten, _sparse_div_zerodim)
// _(aten, _sparse_mul)
// _(aten, _sparse_mul_scalar)
// _(aten, _sparse_mul_zerodim)
// _(aten, _sparse_sum)
// _(aten, _sqrt)
// _(aten, _square)
// _(aten, _standard_gamma)
// _(aten, _standard_gamma_grad)
// _(aten, _sum)
// _(aten, _sum_cuda)
// _(aten, _tan)
// _(aten, _tanh)
// _(aten, _tanh_forward)
// _(aten, _th_get_device)
// _(aten, _th_kthvalue)
// _(aten, _th_prod)
// _(aten, _th_sigmoid)
// _(aten, _th_std)
// _(aten, _th_sum)
// _(aten, _th_tanh)
// _(aten, _th_var)
// _(aten, _thnn_fused_gru_cell)
// _(aten, _thnn_fused_gru_cell_backward)
// _(aten, _thnn_fused_lstm_cell)
// _(aten, _thnn_fused_lstm_cell_backward)
// _(aten, _trilinear)
// _(aten, _trunc)
// _(aten, _unique)
// _(aten, _unique_dim)
// _(aten, _unsafe_view)
// _(aten, _validate_sparse_coo_tensor_args)
// _(aten, _values)
// _(aten, _weight_norm)
// _(aten, _weight_norm_cuda_interface)
// _(aten, _weight_norm_cuda_interface_backward)
// _(aten, _weight_norm_differentiable_backward)
// _(aten, abs)
// _(aten, adaptive_avg_pool1d)
// _(aten, adaptive_avg_pool2d)
// _(aten, adaptive_avg_pool2d_backward)
// _(aten, adaptive_avg_pool2d_forward)
// _(aten, adaptive_avg_pool3d)
// _(aten, adaptive_avg_pool3d_backward)
// _(aten, adaptive_avg_pool3d_forward)
// _(aten, adaptive_max_pool1d)
// _(aten, adaptive_max_pool2d)
// _(aten, adaptive_max_pool2d_backward)
// _(aten, adaptive_max_pool2d_forward)
// _(aten, adaptive_max_pool3d)
// _(aten, adaptive_max_pool3d_backward)
// _(aten, adaptive_max_pool3d_forward)
// _(aten, add)
// _(aten, add_)
// _(aten, addbmm)
// _(aten, addcdiv)
// _(aten, addcmul)
// _(aten, addmm)
// _(aten, addmv)
// _(aten, addr)
// _(aten, affine_grid_generator)
// _(aten, affine_grid_generator_backward)
// _(aten, alias)
// _(aten, all)
// _(aten, allclose)
// _(aten, alpha_dropout)
// _(aten, any)
// _(aten, arange)
// _(aten, argmax)
// _(aten, argmin)
// _(aten, amax)
// _(aten, amin)
// _(aten, aminmax)
// _(aten, as_strided)
// _(aten, as_tensor)
// _(aten, atan2)
// _(aten, atleast_1d)
// _(aten, atleast_2d)
// _(aten, atleast_3d)
// _(aten, avg_pool1d)
// _(aten, avg_pool2d)
// _(aten, avg_pool2d_backward)
// _(aten, avg_pool2d_forward)
// _(aten, avg_pool3d)
// _(aten, avg_pool3d_backward)
// _(aten, avg_pool3d_forward)
// _(aten, baddbmm)
// _(aten, bartlett_window)
// _(aten, batch_norm)
// _(aten, bernoulli)
// _(aten, bilinear)
// _(aten, binary_cross_entropy)
// _(aten, binary_cross_entropy_backward)
// _(aten, binary_cross_entropy_forward)
// _(aten, binary_cross_entropy_with_logits)
// _(aten, binary_cross_entropy_with_logits_backward)
// _(aten, binary_cross_entropy_with_logits_target_backward)
// _(aten, bincount)
// _(aten, blackman_window)
// _(aten, block_diag)
// _(aten, bmm)
// _(aten, broadcast_tensors)
// _(aten, broadcast_to)
// _(aten, cartesian_prod)
// _(aten, cauchy)
// _(aten, ceil)
// _(aten, celu)
// _(aten, cholesky)
// _(aten, cholesky_inverse)
// _(aten, cholesky_solve)
// _(aten, chunk)
// _(aten, clamp_max)
// _(aten, clamp_min)
// _(aten, clone)
// _(aten, coalesce)
// _(aten, combinations)
// _(aten, _conj)
// _(aten, conj)
// _(aten, conj_physical)
// _(aten, conj_physical_)
// _(aten, resolve_conj)
// _(aten, resolve_neg)
// _(aten, complex)
// _(aten, copysign)
// _(aten, polar)
// _(aten, constant_pad_nd)
// _(aten, contiguous)
// _(aten, conv1d)
// _(aten, conv2d)
// _(aten, conv3d)
// _(aten, conv_tbc)
// _(aten, conv_tbc_backward)
// _(aten, conv_transpose1d)
// _(aten, convolution)
// _(aten, copy_sparse_to_sparse)
// _(aten, corrcoef)
// _(aten, cos)
// _(aten, cosh)
// _(aten, cosine_embedding_loss)
// _(aten, cosine_similarity)
// _(aten, count_nonzero)
// _(aten, cross)
// _(aten, cov)
// _(aten, std_mean)
// _(aten, var_mean)
// _(aten, ctc_loss)
// _(aten, cudnn_affine_grid_generator)
// _(aten, cudnn_affine_grid_generator_backward)
// _(aten, cudnn_batch_norm)
// _(aten, cudnn_batch_norm_backward)
// _(aten, cudnn_convolution)
// _(aten, cudnn_convolution_backward)
// _(aten, cudnn_convolution_backward_bias)
// _(aten, cudnn_convolution_backward_input)
// _(aten, cudnn_convolution_backward_weight)
// _(aten, cudnn_convolution_transpose)
// _(aten, cudnn_convolution_transpose_backward)
// _(aten, cudnn_convolution_transpose_backward_bias)
// _(aten, cudnn_convolution_transpose_backward_input)
// _(aten, cudnn_convolution_transpose_backward_weight)
// _(aten, cudnn_convolution_relu)
// _(aten, cudnn_convolution_add_relu)
// _(aten, cudnn_grid_sampler)
// _(aten, cudnn_grid_sampler_backward)
// _(aten, cudnn_is_acceptable)
// _(aten, cummax)
// _(aten, cummin)
// _(aten, cumprod)
// _(aten, cumsum)
// _(aten, data_ptr)
// _(aten, deg2rad)
// _(aten, detach)
// _(aten, diag)
// _(aten, diag_embed)
// _(aten, diagflat)
// _(aten, diagonal)
// _(aten, fill_diagonal_)
// _(aten, diff)
// _(aten, frexp)
// _(aten, dim)
// _(aten, dist)
// _(aten, dot)
// _(aten, dropout)
// _(aten, dsplit)
// _(aten, dstack)
// _(aten, eig)
// _(aten, einsum)
// _(aten, elu)
// _(aten, elu_backward)
// _(aten, elu_forward)
// _(aten, embedding)
// _(aten, embedding_backward)
// _(aten, embedding_bag)
// _(aten, embedding_dense_backward)
// _(aten, embedding_renorm)
// _(aten, embedding_sparse_backward)
// _(aten, empty)
// _(aten, empty_like)
// _(aten, empty_strided)
// _(aten, special_entr)
// _(aten, eq)
// _(aten, equal)
// _(aten, exp)
// _(aten, expand)
// _(aten, expand_as)
// _(aten, exponential)
// _(aten, eye)
// _(aten, feature_alpha_dropout)
// _(aten, feature_dropout)
// _(aten, fft)
// _(aten, fill)
// _(aten, flatten)
// _(aten, flip)
// _(aten, fliplr)
// _(aten, flipud)
// _(aten, floor)
// _(aten, fmod)
// _(aten, fmod_)
// _(aten, fmax)
// _(aten, fmin)
// _(aten, frac)
// _(aten, fractional_max_pool2d)
// _(aten, fractional_max_pool2d_backward)
// _(aten, fractional_max_pool2d_forward)
// _(aten, frobenius_norm)
// _(aten, full)
// _(aten, full_like)
// _(aten, gather)
// _(aten, gcd)
// _(aten, gelu)
// _(aten, geometric)
// _(aten, geqrf)
// _(aten, get_device)
// _(aten, glu)
// _(aten, glu_backward)
// _(aten, glu_forward)
// _(aten, gradient)
// _(aten, grid_sampler)
// _(aten, grid_sampler_2d)
// _(aten, grid_sampler_2d_backward)
// _(aten, grid_sampler_3d)
// _(aten, grid_sampler_3d_backward)
// _(aten, group_norm)
// _(aten, gru)
// _(aten, gru_cell)
// _(aten, hamming_window)
// _(aten, hann_window)
// _(aten, hardshrink)
// _(aten, hardshrink_backward)
// _(aten, hardsigmoid)
// _(aten, hardsigmoid_backward)
// _(aten, hardtanh)
// _(aten, hardtanh_backward)
// _(aten, hardtanh_forward)
// _(aten, heaviside)
// _(aten, hinge_embedding_loss)
// _(aten, histc)
// _(aten, histogram)
// _(aten, hspmm)
// _(aten, hsplit)
// _(aten, hstack)
// _(aten, hypot)
// _(aten, i0_)
// _(aten, ifft)
// _(aten, index)
// _(aten, index_add)
// _(aten, index_copy)
// _(aten, index_fill)
// _(aten, index_put)
// _(aten, index_select)
// _(aten, indices)
// _(aten, inner)
// _(aten, instance_norm)
// _(aten, inverse)
// _(aten, irfft)
// _(aten, is_coalesced)
// _(aten, is_complex)
// _(aten, is_contiguous)
// _(aten, is_cuda)
// _(aten, is_mlc)
// _(aten, is_ort)
// _(aten, is_distributed)
// _(aten, is_floating_point)
// _(aten, is_inference)
// _(aten, is_nonzero)
// _(aten, is_same_size)
// _(aten, is_set_to)
// _(aten, is_signed)
// _(aten, is_sparse)
// _(aten, is_sparse_csr)
// _(aten, isclose)
// _(aten, isreal)
// _(aten, istft)
// _(aten, isposinf)
// _(aten, isneginf)
// _(aten, kaiser_window)
// _(aten, kl_div)
// _(aten, kl_div_backward)
// _(aten, kthvalue)
// _(aten, l1_loss)
// _(aten, l1_loss_backward)
// _(aten, l1_loss_forward)
// _(aten, layer_norm)
// _(aten, lcm)
// _(aten, leaky_relu)
// _(aten, leaky_relu_backward)
// _(aten, leaky_relu_forward)
// _(aten, lerp)
// _(aten, linear)
// _(aten, linspace)
// _(aten, log)
// _(aten, log10)
// _(aten, log2)
// _(aten, log_normal)
// _(aten, log_sigmoid)
// _(aten, log_sigmoid_backward)
// _(aten, log_sigmoid_forward)
// _(aten, _log_softmax)
// _(aten, _log_softmax_backward_data)
// _(aten, logcumsumexp)
// _(aten, logdet)
// _(aten, logspace)
// _(aten, lstm)
// _(aten, lstm_cell)
// _(aten, lstsq)
// _(aten, lu_solve)
// _(aten, margin_ranking_loss)
// _(aten, masked_fill)
// _(aten, masked_scatter)
// _(aten, masked_select)
// _(aten, matrix_rank)
// _(aten, matrix_exp)
// _(aten, max)
// _(aten, max_pool1d)
// _(aten, max_pool1d_with_indices)
// _(aten, max_pool2d)
// _(aten, max_pool2d_with_indices)
// _(aten, max_pool2d_with_indices_backward)
// _(aten, max_pool2d_with_indices_forward)
// _(aten, max_pool3d)
// _(aten, max_pool3d_with_indices)
// _(aten, max_pool3d_with_indices_backward)
// _(aten, max_pool3d_with_indices_forward)
// _(aten, max_unpool2d)
// _(aten, max_unpool2d_backward)
// _(aten, max_unpool2d_forward)
// _(aten, max_unpool3d)
// _(aten, max_unpool3d_backward)
// _(aten, max_unpool3d_forward)
// _(aten, max_values)
// _(aten, mean)
// _(aten, nanmean)
// _(aten, median)
// _(aten, nanmedian)
// _(aten, meshgrid)
// _(aten, min)
// _(aten, min_values)
// _(aten, miopen_batch_norm)
// _(aten, miopen_batch_norm_backward)
// _(aten, miopen_convolution)
// _(aten, miopen_convolution_backward)
// _(aten, miopen_convolution_backward_bias)
// _(aten, miopen_convolution_backward_input)
// _(aten, miopen_convolution_backward_weight)
// _(aten, miopen_convolution_transpose)
// _(aten, miopen_convolution_transpose_backward)
// _(aten, miopen_convolution_transpose_backward_input)
// _(aten, miopen_convolution_transpose_backward_weight)
// _(aten, miopen_depthwise_convolution)
// _(aten, miopen_depthwise_convolution_backward)
// _(aten, miopen_depthwise_convolution_backward_input)
// _(aten, miopen_depthwise_convolution_backward_weight)
// _(aten, miopen_rnn)
// _(aten, miopen_rnn_backward)
// _(aten, mish)
// _(aten, mkldnn_convolution)
// _(aten, mkldnn_convolution_backward)
// _(aten, mkldnn_convolution_backward_input)
// _(aten, mkldnn_convolution_backward_weights)
// _(aten, mm)
// _(aten, mode)
// _(aten, mse_loss)
// _(aten, mse_loss_backward)
// _(aten, mse_loss_forward)
// _(aten, msort)
// _(aten, multi_margin_loss)
// _(aten, multi_margin_loss_backward)
// _(aten, multi_margin_loss_forward)
// _(aten, multilabel_margin_loss)
// _(aten, multilabel_margin_loss_backward)
// _(aten, multilabel_margin_loss_forward)
// _(aten, multinomial)
// _(aten, mv)
// _(aten, nansum)
// _(aten, nan_to_num)
// _(aten, narrow)
// _(aten, narrow_copy)
// _(aten, native_batch_norm)
// _(aten, native_batch_norm_backward)
// _(aten, native_clone)
// _(aten, native_get_device)
// _(aten, native_norm)
// _(aten, native_pow)
// _(aten, native_resize_as)
// _(aten, native_tensor)
// _(aten, native_zero)
// _(aten, special_ndtr)
// _(aten, nextafter)
// _(aten, special_ndtri)
// _(aten, logical_and)
// _(aten, logical_not)
// _(aten, logical_or)
// _(aten, logical_xor)
// _(aten, bitwise_and)
// _(aten, bitwise_not)
// _(aten, bitwise_or)
// _(aten, bitwise_xor)
// _(aten, element_size)
// _(aten, nll_loss)
// _(aten, nll_loss2d)
// _(aten, nll_loss2d_backward)
// _(aten, nll_loss2d_forward)
// _(aten, nll_loss_backward)
// _(aten, nll_loss_forward)
// _(aten, nonzero)
// _(aten, nonzero_numpy)
// _(aten, norm)
// _(aten, norm_except_dim)
// _(aten, normal)
// _(aten, nuclear_norm)
// _(aten, numel)
// _(aten, ones)
// _(aten, ones_like)
// _(aten, ormqr)
// _(aten, pairwise_distance)
// _(aten, _euclidean_dist)
// _(aten, pdist)
// _(aten, cdist)
// _(aten, permute)
// _(aten, pin_memory)
// _(aten, pinverse)
// _(aten, pixel_shuffle)
// _(aten, pixel_unshuffle)
// _(aten, poisson)
// _(aten, pow)
// _(aten, float_power)
// _(aten, prelu)
// _(aten, prelu_backward)
// _(aten, prod)
// _(aten, put)
// _(aten, qr)
// _(aten, quantile)
// _(aten, nanquantile)
// _(aten, rad2deg)
// _(aten, rand)
// _(aten, rand_like)
// _(aten, randint)
// _(aten, randint_like)
// _(aten, randn)
// _(aten, randn_like)
// _(aten, random)
// _(aten, randperm)
// _(aten, range)
// _(aten, ravel)
// _(aten, reciprocal)
// _(aten, reflection_pad1d)
// _(aten, reflection_pad1d_backward)
// _(aten, reflection_pad1d_forward)
// _(aten, reflection_pad2d)
// _(aten, reflection_pad2d_backward)
// _(aten, reflection_pad2d_forward)
// _(aten, reflection_pad3d)
// _(aten, reflection_pad3d_backward)
// _(aten, reflection_pad3d_forward)
// _(aten, relu)
// _(aten, remainder)
// _(aten, renorm)
// _(aten, repeat)
// _(aten, replication_pad1d)
// _(aten, replication_pad1d_backward)
// _(aten, replication_pad1d_forward)
// _(aten, replication_pad2d)
// _(aten, replication_pad2d_backward)
// _(aten, replication_pad2d_forward)
// _(aten, replication_pad3d)
// _(aten, replication_pad3d_backward)
// _(aten, replication_pad3d_forward)
// _(aten, reshape)
// _(aten, reshape_as)
// _(aten, resize)
// _(aten, resize_)
// _(aten, resize_as)
// _(aten, resize_as_)
// _(aten, rfft)
// _(aten, rnn_relu)
// _(aten, rnn_relu_cell)
// _(aten, rnn_tanh)
// _(aten, rnn_tanh_cell)
// _(aten, rot90)
// _(aten, rrelu)
// _(aten, rrelu_with_noise)
// _(aten, rrelu_with_noise_backward)
// _(aten, rrelu_with_noise_forward)
// _(aten, rsqrt)
// _(aten, scatter)
// _(aten, scatter_add)
// _(aten, segment_reduce)
// _(aten, select)
// _(aten, selu)
// _(aten, set)
// _(aten, sign)
// _(aten, signbit)
// _(aten, silu)
// _(aten, sgn)
// _(aten, sin)
// _(aten, sinh)
// _(aten, size)
// _(aten, sizes)
// _(aten, slice)
// _(aten, slogdet)
// _(aten, smm)
// _(aten, smooth_l1_loss)
// _(aten, smooth_l1_loss_backward)
// _(aten, smooth_l1_loss_forward)
// _(aten, soft_margin_loss)
// _(aten, soft_margin_loss_backward)
// _(aten, soft_margin_loss_forward)
// _(aten, softmax)
// _(aten, _softmax)
// _(aten, _softmax_backward_data)
// _(aten, softplus)
// _(aten, softplus_backward)
// _(aten, softplus_forward)
// _(aten, softshrink)
// _(aten, softshrink_backward)
// _(aten, softshrink_forward)
// _(aten, solve)
// _(aten, sort)
// _(aten, sparse_coo_tensor)
// _(aten, sparse_csr_tensor)
// _(aten, sparse_mask)
// _(aten, sparse_resize)
// _(aten, sparse_resize_and_clear)
// _(aten, split)
// _(aten, split_with_sizes)
// _(aten, sqrt)
// _(aten, square)
// _(aten, squeeze)
// _(aten, sspaddmm)
// _(aten, stack)
// _(aten, std)
// _(aten, stft)
// _(aten, storage_offset)
// _(aten, stride)
// _(aten, strides)
// _(aten, rsub)
// _(aten, sum)
// _(aten, sum_to_size)
// _(aten, svd)
// _(aten, symeig)
// _(aten, t)
// _(aten, take)
// _(aten, take_along_dim)
// _(aten, tan)
// _(aten, tanh)
// _(aten, tanh_)
// _(aten, tensor)
// _(aten, tensordot)
// _(aten, tensor_split)
// _(aten, th_clone)
// _(aten, th_norm)
// _(aten, th_pow)
// _(aten, th_resize_as)
// _(aten, th_tensor)
// _(aten, th_zero)
// _(aten, thnn_conv2d)
// _(aten, _slow_conv2d_backward)
// _(aten, _slow_conv2d_forward)
// _(aten, tile)
// _(aten, slow_conv3d)
// _(aten, slow_conv3d_backward)
// _(aten, slow_conv3d_forward)
// _(aten, thnn_conv_depthwise2d)
// _(aten, thnn_conv_depthwise2d_backward)
// _(aten, thnn_conv_depthwise2d_forward)
// _(aten, slow_conv_dilated2d)
// _(aten, slow_conv_dilated2d_backward)
// _(aten, slow_conv_dilated3d)
// _(aten, slow_conv_dilated3d_backward)
// _(aten, slow_conv_transpose2d)
// _(aten, slow_conv_transpose2d_backward)
// _(aten, slow_conv_transpose3d)
// _(aten, slow_conv_transpose3d_backward)
// _(aten, threshold)
// _(aten, threshold_backward)
// _(aten, to)
// _(aten, to_sparse)
// _(aten, to_dense)
// _(aten, topk)
// _(aten, trace)
// _(aten, triangular_solve)
// _(aten, tril)
// _(aten, triplet_margin_loss)
// _(aten, triu)
// _(aten, type_as)
// _(aten, unbind)
// _(aten, unfold)
// _(aten, uniform)
// _(aten, unsafe_chunk)
// _(aten, unsafe_split)
// _(aten, unsafe_split_with_sizes)
// _(aten, unsqueeze)
// _(aten, upsample_bilinear2d)
// _(aten, upsample_bilinear2d_backward)
// _(aten, upsample_bilinear2d_forward)
// _(aten, upsample_bicubic2d)
// _(aten, upsample_bicubic2d_backward)
// _(aten, upsample_bicubic2d_forward)
// _(aten, upsample_linear1d)
// _(aten, upsample_linear1d_backward)
// _(aten, upsample_linear1d_forward)
// _(aten, upsample_nearest1d)
// _(aten, upsample_nearest1d_backward)
// _(aten, upsample_nearest1d_forward)
// _(aten, upsample_nearest2d)
// _(aten, upsample_nearest2d_backward)
// _(aten, upsample_nearest2d_forward)
// _(aten, upsample_nearest3d)
// _(aten, upsample_nearest3d_backward)
// _(aten, upsample_nearest3d_forward)
// _(aten, upsample_trilinear3d)
// _(aten, upsample_trilinear3d_backward)
// _(aten, upsample_trilinear3d_forward)
// _(aten, values)
// _(aten, vander)
// _(aten, var)
// _(aten, view)
// _(aten, view_as)
// _(aten, vsplit)
// _(aten, where)
// _(aten, zero)
// _(aten, zeros)
// _(aten, zeros_like)
// _(aten, real)
// _(aten, imag)
// _(aten, view_as_real)
// _(aten, view_as_complex) 
/* nothing */

// #define FORALL_ATTR_BASE_SYMBOLS(_)
// _(attr, A)
// _(attr, C)
// _(attr, H)
// _(attr, LU_data)
// _(attr, LU_pivots)
// _(attr, N)
// _(attr, W)
// _(attr, accumulate)
// _(attr, align_corners)
// _(attr, alpha)
// _(attr, anchor)
// _(attr, argmaxes)
// _(attr, atol)
// _(attr, b_hh)
// _(attr, b_ih)
// _(attr, bag_size)
// _(attr, base)
// _(attr, batch1)
// _(attr, batch2)
// _(attr, batch_first)
// _(attr, batch_sizes)
// _(attr, benchmark)
// _(attr, beta)
// _(attr, bias)
// _(attr, bias_defined)
// _(attr, bidirectional)
// _(attr, bins)
// _(attr, blank)
// _(attr, buffer)
// _(attr, ceil_mode)
// _(attr, checked_signal_sizes)
// _(attr, chunks)
// _(attr, columns)
// _(attr, column_stack)
// _(attr, complex_input)
// _(attr, complex_output)
// _(attr, condition)
// _(attr, count_include_pad)
// _(attr, cudnn_enable)
// _(attr, cudnn_enabled)
// _(attr, cx)
// _(attr, cy)
// _(attr, data)
// _(attr, dense_dim)
// _(attr, descending)
// _(attr, deterministic)
// _(attr, device)
// _(attr, diagonal)
// _(attr, dilation)
// _(attr, dim)
// _(attr, dim0)
// _(attr, dim1)
// _(attr, dim2)
// _(attr, dimension)
// _(attr, dims)
// _(attr, dims_other)
// _(attr, dims_self)
// _(attr, divisor_override)
// _(attr, dropout)
// _(attr, dropout_seed)
// _(attr, dropout_state)
// _(attr, dtype)
// _(attr, eigenvectors)
// _(attr, end)
// _(attr, end_dim)
// _(attr, eps)
// _(attr, epsilon)
// _(attr, equal_nan)
// _(attr, equation)
// _(attr, expand1)
// _(attr, expand2)
// _(attr, expand3)
// _(attr, exponent)
// _(attr, exponential_average_factor)
// _(attr, fgrad_input)
// _(attr, fill_value)
// _(attr, finput)
// _(attr, from)
// _(attr, g)
// _(attr, gO)
// _(attr, generator)
// _(attr, ggI)
// _(attr, ggW)
// _(attr, ggb)
// _(attr, grad)
// _(attr, gradOutput)
// _(attr, grad_bias)
// _(attr, grad_cy)
// _(attr, grad_hy)
// _(attr, grad_input)
// _(attr, grad_out)
// _(attr, grad_output)
// _(attr, grad_w)
// _(attr, grad_weight)
// _(attr, grid)
// _(attr, groups)
// _(attr, has_bias)
// _(attr, has_biases)
// _(attr, hidden_bias)
// _(attr, hidden_gates)
// _(attr, hidden_size)
// _(attr, high)
// _(attr, hop_length)
// _(attr, hx)
// _(attr, i1)
// _(attr, i2)
// _(attr, i3)
// _(attr, ignore_index)
// _(attr, implicit)
// _(attr, index)
// _(attr, indices)
// _(attr, info)
// _(attr, input)
// _(attr, input1)
// _(attr, input2)
// _(attr, input3)
// _(attr, input_bias)
// _(attr, input_gates)
// _(attr, input_lengths)
// _(attr, input_scale)
// _(attr, input_size)
// _(attr, interpolation_mode)
// _(attr, inverse)
// _(attr, is_target)
// _(attr, k)
// _(attr, keepdim)
// _(attr, kernel_size)
// _(attr, lambd)
// _(attr, largest)
// _(attr, layout)
// _(attr, left)
// _(attr, length)
// _(attr, lengths)
// _(attr, like)
// _(attr, log_alpha)
// _(attr, log_probs)
// _(attr, low)
// _(attr, lower)
// _(attr, lu)
// _(attr, m)
// _(attr, margin)
// _(attr, mask)
// _(attr, mat)
// _(attr, mat1)
// _(attr, mat2)
// _(attr, max)
// _(attr, max_indices)
// _(attr, max_norm)
// _(attr, max_size)
// _(attr, max_val)
// _(attr, max_values)
// _(attr, maximum_indices)
// _(attr, maxnorm)
// _(attr, maximum)
// _(attr, mean)
// _(attr, median)
// _(attr, nanmedian)
// _(attr, min)
// _(attr, min_indices)
// _(attr, min_val)
// _(attr, minlength)
// _(attr, minimum)
// _(attr, mode)
// _(attr, momentum)
// _(attr, n)
// _(attr, n_fft)
// _(attr, neg_log_likelihood)
// _(attr, negative)
// _(attr, negative_slope)
// _(attr, noise)
// _(attr, non_blocking)
// _(attr, norm_type)
// _(attr, normalized)
// _(attr, normalized_shape)
// _(attr, num_groups)
// _(attr, num_layers)
// _(attr, num_samples)
// _(attr, num_weights)
// _(attr, offset)
// _(attr, offset2bag)
// _(attr, offsets)
// _(attr, ones)
// _(attr, onesided)
// _(attr, options)
// _(attr, other)
// _(attr, output)
// _(attr, output_mask)
// _(attr, output_padding)
// _(attr, output_size)
// _(attr, output_sizes)
// _(attr, p)
// _(attr, pad)
// _(attr, padding)
// _(attr, padding_idx)
// _(attr, padding_mode)
// _(attr, padding_value)
// _(attr, params)
// _(attr, pdist)
// _(attr, cdist)
// _(attr, std_mean)
// _(attr, var_mean)
// _(attr, periodic)
// _(attr, pivot)
// _(attr, pivots)
// _(attr, pooledHeight)
// _(attr, pooledWidth)
// _(attr, positive)
// _(attr, pow)
// _(attr, random_samples)
// _(attr, rcond)
// _(attr, reduction)
// _(attr, repeats)
// _(attr, replacement)
// _(attr, res1)
// _(attr, res2)
// _(attr, res3)
// _(attr, reserve)
// _(attr, result)
// _(attr, return_inverse)
// _(attr, rois)
// _(attr, rtol)
// _(attr, running_mean)
// _(attr, running_var)
// _(attr, save_mean)
// _(attr, save_std)
// _(attr, save_var)
// _(attr, saved_g)
// _(attr, saved_norms)
// _(attr, saved_v)
// _(attr, scale)
// _(attr, scale_grad_by_freq)
// _(attr, self)
// _(attr, self_size)
// _(attr, self_ty)
// _(attr, shape)
// _(attr, sigma)
// _(attr, signal_ndim)
// _(attr, signal_sizes)
// _(attr, size)
// _(attr, solution)
// _(attr, some)
// _(attr, sorted)
// _(attr, source)
// _(attr, sparse)
// _(attr, sparse_dim)
// _(attr, sparse_dtype)
// _(attr, spatialScale)
// _(attr, split_size)
// _(attr, split_sizes)
// _(attr, src)
// _(attr, start)
// _(attr, start_dim)
// _(attr, std)
// _(attr, step)
// _(attr, steps)
// _(attr, storage)
// _(attr, storageOffset)
// _(attr, storage_offset)
// _(attr, stride)
// _(attr, sumdim)
// _(attr, swap)
// _(attr, symmetric)
// _(attr, target)
// _(attr, target_lengths)
// _(attr, targets)
// _(attr, tensor)
// _(attr, tensor1)
// _(attr, tensor2)
// _(attr, tensors)
// _(attr, the_template)
// _(attr, theta)
// _(attr, threshold)
// _(attr, to)
// _(attr, tol)
// _(attr, total)
// _(attr, total_length)
// _(attr, total_weight)
// _(attr, train)
// _(attr, training)
// _(attr, transpose)
// _(attr, transposed)
// _(attr, unbiased)
// _(attr, unitriangular)
// _(attr, unroll_dim)
// _(attr, upper)
// _(attr, upscale_factor)
// _(attr, use_input_stats)
// _(attr, v)
// _(attr, value)
// _(attr, values)
// _(attr, vec)
// _(attr, vec1)
// _(attr, vec2)
// _(attr, w_hh)
// _(attr, w_ih)
// _(attr, weight)
// _(attr, weight_arr)
// _(attr, weight_buf)
// _(attr, weight_size)
// _(attr, weight_stride0)
// _(attr, weights)
// _(attr, win_length)
// _(attr, window)
// _(attr, window_length)
// _(attr, workspace)
// _(attr, x)
// _(attr, x1)
// _(attr, x2)


// Parsed from ATen/core/interned_strings.h

// #pragma once
// #include <vector>
// #include <cstdint>
// #include <string>
// #include <unordered_map>
// #include <algorithm>

// #include <c10/macros/Macros.h>

// #include <ATen/core/aten_interned_strings.h>

// #define FORALL_NS_SYMBOLS(_)
//   _(namespaces, prim)
//   _(namespaces, aten)
//   _(namespaces, cuda)
//   _(namespaces, onnx)
//   _(namespaces, attr)
//   _(namespaces, scope)
//   _(namespaces, user)
//   _(namespaces, _caffe2)
//   _(namespaces, dimname)
//   _(namespaces, namespaces)
//   _(prim, Assign)
//   _(prim, BroadcastingChunk)
//   _(prim, BroadcastSizes)
//   _(prim, ReductionSizes)
//   _(prim, Constant)
//   _(prim, ChunkSizes)
//   _(prim, ConstantMKLDNNTensor)
//   _(prim, BroadcastMKLDNNTensors)
//   _(prim, MKLDNNGroup)
//   _(prim, MKLDNNHardSwish)
//   _(prim, MKLDNNHardSigmoid)
//   _(prim, MKLDNNHardTanh)
//   _(prim, MKLDNNClamp)
//   _(prim, Drop)
//   _(prim, Eval)
//   _(prim, Expand) /* onnx */
//   _(prim, FusionGroup)
//   _(prim, CudaFusionGroup)
//   _(prim, CudaFusionGuard)
//   _(prim, FunctionalGraph)
//   _(prim, DifferentiableGraph)
//   _(prim, TensorExprGroup)
//   _(prim, StaticSubgraph)
//   _(prim, If)
//   _(prim, Jump) /* debug */
//   _(prim, JumpNZ) /* debug */
//   _(prim, JumpZ) /* debug */
//   _(prim, Load)
//   _(prim, Loop)
//   _(prim, Param)
//   _(prim, PackPadded) /* onnx */
//   _(prim, PadPacked) /* onnx */
//   _(prim, Placeholder) /* debug */
//   _(prim, Print)
//   _(prim, PythonOp)
//   _(prim, IgnoredPythonOp)
//   _(prim, Reverse)
//   _(prim, Return)
//   _(prim, ReturnStmt)
//   _(prim, BreakStmt)
//   _(prim, ContinueStmt)
//   _(prim, ComprehensionScope)
//   _(prim, Store)
//   _(prim, AutogradZero)
//   _(prim, AutogradAnyNonZero)
//   _(prim, AutogradAllNonZero)
//   _(prim, AutogradAllZero)
//   _(prim, Starred)
//   _(prim, TupleConstruct)
//   _(prim, TupleUnpack)
//   _(prim, TupleIndex)
//   _(prim, TupleSlice)
//   _(prim, ListConstruct)
//   _(prim, ListUnpack)
//   _(prim, DictConstruct)
//   _(prim, ModuleContainerIndex)
//   _(prim, EnumName)
//   _(prim, EnumValue)
//   _(prim, StringIndex)
//   _(prim, NumToTensor)
//   _(prim, Uninitialized)
//   _(prim, VarConcat)
//   _(prim, VarStack)
//   _(prim, With)
//   _(prim, Enter)
//   _(prim, Exit)
//   _(aten, Bool)
//   _(aten, Int)
//   _(aten, FloatImplicit)
//   _(aten, ComplexImplicit)
//   _(aten, IntImplicit)
//   _(aten, ScalarImplicit)
//   _(aten, Float)
//   _(aten, Complex)
//   _(aten, str)
//   _(aten, is_pinned)
//   _(aten, Delete)
//   _(aten, relu_)
//   _(aten, gelu_)
//   _(aten, relu6)
//   _(aten, relu6_)
//   _(aten, dropout_)
//   _(aten, sigmoid_)
//   _(prim, device)
//   _(prim, dtype)
//   _(prim, layout)
//   _(prim, id)
//   _(prim, requires_grad)
//   _(prim, MakeTestTensor) /* test */
//   _(prim, AutogradAdd)
//   _(prim, GradOf)
//   _(aten, grad)
//   _(aten, backward)
//   _(prim, Guard)
//   _(prim, BailOut)
//   _(prim, TypeCheck)
//   _(prim, RequiresGradCheck)
//   _(prim, FallbackGraph)
//   _(prim, FusedConcat)
//   _(prim, ConstantChunk)
//   _(prim, MMTreeReduce)
//   _(prim, MMBatchSide)
//   _(prim, list)
//   _(prim, dict)
//   _(prim, min)
//   _(prim, max)
//   _(prim, abs)
//   _(aten, divmod)
//   _(prim, zip)
//   _(prim, enumerate)
//   _(prim, range)
//   _(prim, rangelist)
//   _(prim, isinstance)
//   _(prim, tolist)
//   _(prim, unchecked_cast)
//   _(aten, _grad_sum_to_size)
//   _(aten, _size_if_not_equal)
//   _(aten, _ncf_unsqueeze)
//   _(aten, warn)
//   _(aten, sorted)
//   _(aten, floordiv)
//   _(aten, __range_length)
//   _(aten, __derive_index)
//   _(aten, __round_to_zero_floordiv)
//   _(aten, is_scripting)
//   _(aten, _unwrap_optional)
//   _(prim, fork)
//   _(prim, forkClosure)
//   _(prim, RaiseException)
//   _(prim, Closure)
//   _(prim, CreateObject)
//   _(prim, SetAttr)
//   _(prim, GetAttr)
//   _(prim, HasAttr)
//   _(prim, profile)
//   _(prim, profile_ivalue)
//   _(prim, AddStatValue)
//   _(prim, TimePoint)
//   _(prim, CallFunction)
//   _(prim, CallMethod)
//   _(prim, LoopContinuation)
//   _(prim, annotate)
//   _(prim, TracedModuleForward)
//   _(prim, TracedFork)
//   _(prim, TracedAttr)
//   _(prim, rpc_async)
//   _(prim, rpc_sync)
//   _(prim, rpc_remote)
//   _(prim, is_cuda)
//   _(aten, abs_)
//   _(aten, absolute)
//   _(aten, absolute_)
//   _(aten, acos)
//   _(aten, acos_)
//   _(aten, arccos)
//   _(aten, arccos_)
//   _(aten, acosh)
//   _(aten, acosh_)
//   _(aten, arccosh)
//   _(aten, arccosh_)
//   _(aten, asin)
//   _(aten, asin_)
//   _(aten, arcsin)
//   _(aten, arcsin_)
//   _(aten, asinh)
//   _(aten, asinh_)
//   _(aten, arcsinh)
//   _(aten, arcsinh_)
//   _(aten, atan)
//   _(aten, atan_)
//   _(aten, arctan)
//   _(aten, arctan_)
//   _(aten, atanh)
//   _(aten, atanh_)
//   _(aten, arctanh)
//   _(aten, arctanh_)
//   _(aten, clamp)
//   _(aten, clamp_)
//   _(aten, clip)
//   _(aten, clip_)
//   _(aten, det)
//   _(aten, linalg_det)
//   _(aten, matrix_power)
//   _(aten, linalg_matrix_power)
//   _(aten, chain_matmul)
//   _(aten, linalg_multi_dot)
//   _(aten, linalg_norm)
//   _(aten, linalg_vector_norm)
//   _(aten, linalg_matrix_norm)
//   _(aten, matmul)
//   _(aten, linalg_matmul)
//   _(aten, append)
//   _(aten, item)
//   _(aten, format)
//   _(aten, percentFormat)
//   _(aten, __not__)
//   _(aten, __is__)
//   _(aten, __isnot__)
//   _(aten, copy)
//   _(aten, copy_)
//   _(aten, div)
//   _(aten, div_)
//   _(aten, divide)
//   _(aten, divide_)
//   _(aten, true_divide)
//   _(aten, true_divide_)
//   _(aten, t_)
//   _(aten, addbmm_)
//   _(aten, addcdiv_)
//   _(aten, addcmul_)
//   _(aten, addmv_)
//   _(aten, addr_)
//   _(aten, baddbmm_)
//   _(aten, ge)
//   _(aten, ge_)
//   _(aten, greater_equal)
//   _(aten, greater_equal_)
//   _(aten, gt)
//   _(aten, gt_)
//   _(aten, greater)
//   _(aten, greater_)
//   _(aten, le)
//   _(aten, le_)
//   _(aten, less_equal)
//   _(aten, less_equal_)
//   _(aten, lerp_)
//   _(aten, lt)
//   _(aten, lt_)
//   _(aten, less)
//   _(aten, less_)
//   _(aten, isnan)
//   _(aten, mul)
//   _(aten, mul_)
//   _(aten, multiply)
//   _(aten, multiply_)
//   _(aten, ne)
//   _(aten, ne_)
//   _(aten, not_equal)
//   _(aten, not_equal_)
//   _(aten, _ger)
//   _(aten, ger)
//   _(aten, outer)
//   _(aten, orgqr)
//   _(aten, linalg_householder_product)
//   _(aten, transpose)
//   _(aten, transpose_)
//   _(aten, trapz)
//   _(aten, trapezoid)
//   _(aten, cumulative_trapezoid)
//   _(aten, unsqueeze_)
//   _(aten, __getitem__)
//   _(aten, _set_item)
//   _(aten, manual_seed)
//   _(aten, set_)
//   _(aten, index_put_)
//   _(aten, device)
//   _(aten, hash)
//   _(aten, len)
//   _(aten, list)
//   _(aten, dict)
//   _(aten, wait)
//   _(aten, save)
//   _(aten, sub)
//   _(aten, sub_)
//   _(aten, subtract)
//   _(aten, subtract_)
//   _(aten, keys)
//   _(aten, ord)
//   _(aten, chr)
//   _(aten, hex)
//   _(aten, oct)
//   _(aten, clear)
//   _(aten, trunc)
//   _(aten, trunc_)
//   _(aten, fix)
//   _(aten, fix_)
//   _(aten, to_mkldnn)
//   _(aten, positive)
//   _(aten, neg)
//   _(aten, neg_)
//   _(aten, negative)
//   _(aten, negative_)
//   _(aten, setdefault)
//   _(aten, bin)
//   _(aten, pop)
//   _(aten, insert)
//   _(aten, _cat)
//   _(aten, cat)
//   _(aten, concat)
//   _(aten, vstack)
//   _(aten, row_stack)
//   _(prim, unchecked_unwrap_optional)
//   _(aten, __contains__)
//   _(prim, BailoutTemplate)
//   _(prim, grad)
//   _(aten, zero_)
//   _(aten, fill_)
//   _(aten, masked_fill_)
//   _(cuda, _set_device)
//   _(cuda, set_stream)
//   _(cuda, _current_device)
//   _(cuda, synchronize)
//   _(aten, swapaxes)
//   _(aten, swapaxes_)
//   _(aten, swapdims)
//   _(aten, swapdims_)
//   _(aten, movedim)
//   _(aten, moveaxis)
//   _(aten, polygamma)
//   _(aten, special_polygamma)
//   _(aten, lgamma)
//   _(aten, special_gammaln)
//   _(aten, logsumexp)
//   _(aten, special_logsumexp)
//   _(aten, digamma)
//   _(aten, special_psi)
//   _(aten, special_digamma)
//   _(aten, erf)
//   _(aten, special_erf)
//   _(aten, erfc)
//   _(aten, special_erfc)
//   _(aten, special_erfcx)
//   _(aten, erfinv)
//   _(aten, special_erfinv)
//   _(aten, logit)
//   _(aten, special_logit)
//   _(aten, sigmoid)
//   _(aten, special_expit)
//   _(aten, expm1)
//   _(aten, special_expm1)
//   _(aten, exp2)
//   _(aten, special_exp2)
//   _(aten, log1p)
//   _(aten, special_log1p)
//   _(aten, round)
//   _(aten, special_round)
//   _(aten, sinc)
//   _(aten, special_sinc)
//   _(aten, i0)
//   _(aten, special_i0)
//   _(aten, special_i0e)
//   _(aten, special_i1)
//   _(aten, special_i1e)
//   _(aten, xlogy)
//   _(aten, special_xlogy)
//   _(aten, special_xlog1py)
//   _(aten, log_softmax)
//   _(aten, special_log_softmax)
//   _(aten, special_zeta)
//   _(aten, igamma)
//   _(aten, igamma_)
//   _(aten, special_gammainc)
//   _(aten, igammac)
//   _(aten, igammac_)
//   _(aten, special_gammaincc)
//   _(aten, mvlgamma)
//   _(aten, special_multigammaln)
//   _(aten, has_torch_function)
//   _(aten, hardswish)
//   _(aten, hardswish_)
//   _(aten, hardsigmoid_)
//   _(aten, hardtanh_)
//   FORALL_ATEN_BASE_SYMBOLS(_)
//   _(onnx, Add)
//   _(onnx, Concat)
//   _(onnx, Constant)
//   _(onnx, ConstantFill)
//   _(onnx, Div)
//   _(onnx, GRU)
//   _(onnx, Gather)
//   _(onnx, Gemm)
//   _(onnx, LSTM)
//   _(onnx, MatMul)
//   _(onnx, Mul)
//   _(onnx, Pow)
//   _(onnx, RNN)
//   _(onnx, Shape)
//   _(onnx, Size)
//   _(onnx, Slice)
//   _(onnx, Softmax)
//   _(onnx, Squeeze)
//   _(onnx, Sub)
//   _(onnx, Transpose)
//   _(onnx, Unsqueeze)
//   _(onnx, Loop)
//   _(onnx, If)
//   _(onnx, Reshape)
//   _(onnx, Expand)
//   _(onnx, Equal)
//   _(onnx, Greater)
//   _(onnx, GreaterOrEqual)
//   _(onnx, Less)
//   _(onnx, LessOrEqual)
//   _(onnx, Not)
//   _(onnx, ATen)
//   _(onnx, Split)
//   _(onnx, ConstantOfShape)
//   _(onnx, Cast)
//   _(onnx, Mod)
//   _(onnx, Sqrt)
//   _(onnx, SplitToSequence)
//   _(onnx, SequenceAt)
//   _(onnx, SequenceConstruct)
//   _(onnx, SequenceEmpty)
//   _(onnx, SequenceInsert)
//   _(onnx, SequenceErase)
//   _(onnx, ConcatFromSequence)
//   _(onnx, Identity)
//   _(onnx, SoftmaxCrossEntropyLoss)
//   _(onnx, NegativeLogLikelihoodLoss)
//   _(onnx, LogSoftmax)
//   _(onnx, ReduceL1)
//   _(onnx, ReduceL2)
//   _(onnx, Conv)
//   _(onnx, BatchNormalization)
//   _(onnx, ReduceMean)
//   _(onnx, ReduceProd)
//   _(onnx, Relu)
//   _(onnx, Neg)
//   _(onnx, NonZero)
//   _(onnx, Range)
//   _(onnx, Tile)
//   _(onnx, Where)
//   FORALL_ATTR_BASE_SYMBOLS(_)
//   _(attr, Subgraph)
//   _(attr, ReverseSubgraph)
//   _(attr, f_real_outputs)
//   _(attr, df_input_vjps)
//   _(attr, df_input_captured_inputs)
//   _(attr, df_input_captured_outputs)
//   _(attr, df_output_vjps)
//   _(attr, axes)
//   _(attr, axis)
//   _(attr, broadcast)
//   _(attr, direction)
//   _(attr, ends)
//   _(attr, inplace)
//   _(attr, input_as_shape)
//   _(attr, is_zero)
//   _(attr, num_none)
//   _(attr, num_present)
//   _(attr, perm)
//   _(attr, sizes)
//   _(attr, starts)
//   _(attr, profiled_type)
//   _(attr, transA)
//   _(attr, transB)
//   _(attr, name)
//   _(attr, a)
//   _(attr, b)
//   _(attr, beg)
//   _(attr, idx)
//   _(attr, split)
//   _(attr, slot)
//   _(attr, kinds)
//   _(attr, types)
//   _(attr, scope)
//   _(attr, keepdims)
//   _(attr, cache_id)
//   _(attr, new_axis)
//   _(attr, warn_id)
//   _(attr, allowzero)

// 'prim' symbols are synthetic operators that occur only in the IR
// and don't have corresponding implementations in ATen.

// 'onnx' symbols correspond to ONNX operators.  Their semantics
// are defined in https://github.com/onnx/onnx/blob/master/docs/Operators.md
// The particular version we are targeting is specified by '_onnx_opset_version'
// in torch.onnx.symbolic_helper
//
// In general, most ONNX operators won't get an entry here, because they
// are handled from the Python end.  However, you may occasionally need
// to intern an ONNX symbol here so that you can conveniently write an
// optimization on ONNX operations.

// 'attr' symbols are attribute keys.  They are shared between both ONNX and ATen
// operators (you disambiguate their meaning by looking at the operator itself).
// In general, you only need to define attribute keys that are used by
// onnx or prim; ATen attributes are automatically generated in FORALL_ATTR_BASE_SYMBOLS.

// Note [Symbol allocation]
// ~~~~~~~~~~~~~~~~~~~~~~~~
//
//  1. Symbol namespace is split up into namespaces.
//
//  2. The intended access pattern for built-in symbols is onnx::MatMul
//  in the c10 namespace (this is a Symbol).
//

// Built-in constant definition strategy:
// - Enum is the most convenient way to generate a contiguous sequence
//   of numbers for an identifier.
// - However, an enum gives you a fresh type.  We want onnx::MatMul to
//   be type Symbol, not some random enum type!
// - Therefore, after using enums to generate the sequence of integers,
//   we then declare constexpr Symbols to get everything the actual Symbol
//   type we want.  Symbols must be constexpr to be valid to be "case"ed on.


// Targeting ../Symbol.java





@Namespace("c10") public enum _keys {
    namespaces_prim(0),
  namespaces_aten(1),
  namespaces_cuda(2),
  namespaces_onnx(3),
  namespaces_attr(4),
  namespaces_scope(5),
  namespaces_user(6),
  namespaces__caffe2(7),
  namespaces_dimname(8),
  namespaces_namespaces(9),
  prim_Assign(10),
  prim_BroadcastingChunk(11),
  prim_BroadcastSizes(12),
  prim_ReductionSizes(13),
  prim_Constant(14),
  prim_ChunkSizes(15),
  prim_ConstantMKLDNNTensor(16),
  prim_BroadcastMKLDNNTensors(17),
  prim_MKLDNNGroup(18),
  prim_MKLDNNHardSwish(19),
  prim_MKLDNNHardSigmoid(20),
  prim_MKLDNNHardTanh(21),
  prim_MKLDNNClamp(22),
  prim_Drop(23),
  prim_Eval(24),
  prim_Expand(25), /* onnx */
  prim_FusionGroup(26),
  prim_CudaFusionGroup(27),
  prim_CudaFusionGuard(28),
  prim_FunctionalGraph(29),
  prim_DifferentiableGraph(30),
  prim_TensorExprGroup(31),
  prim_StaticSubgraph(32),
  prim_If(33),
  prim_Jump(34), /* debug */
  prim_JumpNZ(35), /* debug */
  prim_JumpZ(36), /* debug */
  prim_Load(37),
  prim_Loop(38),
  prim_Param(39),
  prim_PackPadded(40), /* onnx */
  prim_PadPacked(41), /* onnx */
  prim_Placeholder(42), /* debug */
  prim_Print(43),
  prim_PythonOp(44),
  prim_IgnoredPythonOp(45),
  prim_Reverse(46),
  prim_Return(47),
  prim_ReturnStmt(48),
  prim_BreakStmt(49),
  prim_ContinueStmt(50),
  prim_ComprehensionScope(51),
  prim_Store(52),
  prim_AutogradZero(53),
  prim_AutogradAnyNonZero(54),
  prim_AutogradAllNonZero(55),
  prim_AutogradAllZero(56),
  prim_Starred(57),
  prim_TupleConstruct(58),
  prim_TupleUnpack(59),
  prim_TupleIndex(60),
  prim_TupleSlice(61),
  prim_ListConstruct(62),
  prim_ListUnpack(63),
  prim_DictConstruct(64),
  prim_ModuleContainerIndex(65),
  prim_EnumName(66),
  prim_EnumValue(67),
  prim_StringIndex(68),
  prim_NumToTensor(69),
  prim_Uninitialized(70),
  prim_VarConcat(71),
  prim_VarStack(72),
  prim_With(73),
  prim_Enter(74),
  prim_Exit(75),
  aten_Bool(76),
  aten_Int(77),
  aten_FloatImplicit(78),
  aten_ComplexImplicit(79),
  aten_IntImplicit(80),
  aten_ScalarImplicit(81),
  aten_Float(82),
  aten_Complex(83),
  aten_str(84),
  aten_is_pinned(85),
  aten_Delete(86),
  aten_relu_(87),
  aten_gelu_(88),
  aten_relu6(89),
  aten_relu6_(90),
  aten_dropout_(91),
  aten_sigmoid_(92),
  prim_device(93),
  prim_dtype(94),
  prim_layout(95),
  prim_id(96),
  prim_requires_grad(97),
  prim_MakeTestTensor(98), /* test */
  prim_AutogradAdd(99),
  prim_GradOf(100),
  aten_grad(101),
  aten_backward(102),
  prim_Guard(103),
  prim_BailOut(104),
  prim_TypeCheck(105),
  prim_RequiresGradCheck(106),
  prim_FallbackGraph(107),
  prim_FusedConcat(108),
  prim_ConstantChunk(109),
  prim_MMTreeReduce(110),
  prim_MMBatchSide(111),
  prim_list(112),
  prim_dict(113),
  prim_min(114),
  prim_max(115),
  prim_abs(116),
  aten_divmod(117),
  prim_zip(118),
  prim_enumerate(119),
  prim_range(120),
  prim_rangelist(121),
  prim_isinstance(122),
  prim_tolist(123),
  prim_unchecked_cast(124),
  aten__grad_sum_to_size(125),
  aten__size_if_not_equal(126),
  aten__ncf_unsqueeze(127),
  aten_warn(128),
  aten_sorted(129),
  aten_floordiv(130),
  aten___range_length(131),
  aten___derive_index(132),
  aten___round_to_zero_floordiv(133),
  aten_is_scripting(134),
  aten__unwrap_optional(135),
  prim_fork(136),
  prim_forkClosure(137),
  prim_RaiseException(138),
  prim_Closure(139),
  prim_CreateObject(140),
  prim_SetAttr(141),
  prim_GetAttr(142),
  prim_HasAttr(143),
  prim_profile(144),
  prim_profile_ivalue(145),
  prim_AddStatValue(146),
  prim_TimePoint(147),
  prim_CallFunction(148),
  prim_CallMethod(149),
  prim_LoopContinuation(150),
  prim_annotate(151),
  prim_TracedModuleForward(152),
  prim_TracedFork(153),
  prim_TracedAttr(154),
  prim_rpc_async(155),
  prim_rpc_sync(156),
  prim_rpc_remote(157),
  prim_is_cuda(158),
  aten_abs_(159),
  aten_absolute(160),
  aten_absolute_(161),
  aten_acos(162),
  aten_acos_(163),
  aten_arccos(164),
  aten_arccos_(165),
  aten_acosh(166),
  aten_acosh_(167),
  aten_arccosh(168),
  aten_arccosh_(169),
  aten_asin(170),
  aten_asin_(171),
  aten_arcsin(172),
  aten_arcsin_(173),
  aten_asinh(174),
  aten_asinh_(175),
  aten_arcsinh(176),
  aten_arcsinh_(177),
  aten_atan(178),
  aten_atan_(179),
  aten_arctan(180),
  aten_arctan_(181),
  aten_atanh(182),
  aten_atanh_(183),
  aten_arctanh(184),
  aten_arctanh_(185),
  aten_clamp(186),
  aten_clamp_(187),
  aten_clip(188),
  aten_clip_(189),
  aten_det(190),
  aten_linalg_det(191),
  aten_matrix_power(192),
  aten_linalg_matrix_power(193),
  aten_chain_matmul(194),
  aten_linalg_multi_dot(195),
  aten_linalg_norm(196),
  aten_linalg_vector_norm(197),
  aten_linalg_matrix_norm(198),
  aten_matmul(199),
  aten_linalg_matmul(200),
  aten_append(201),
  aten_item(202),
  aten_format(203),
  aten_percentFormat(204),
  aten___not__(205),
  aten___is__(206),
  aten___isnot__(207),
  aten_copy(208),
  aten_copy_(209),
  aten_div(210),
  aten_div_(211),
  aten_divide(212),
  aten_divide_(213),
  aten_true_divide(214),
  aten_true_divide_(215),
  aten_t_(216),
  aten_addbmm_(217),
  aten_addcdiv_(218),
  aten_addcmul_(219),
  aten_addmv_(220),
  aten_addr_(221),
  aten_baddbmm_(222),
  aten_ge(223),
  aten_ge_(224),
  aten_greater_equal(225),
  aten_greater_equal_(226),
  aten_gt(227),
  aten_gt_(228),
  aten_greater(229),
  aten_greater_(230),
  aten_le(231),
  aten_le_(232),
  aten_less_equal(233),
  aten_less_equal_(234),
  aten_lerp_(235),
  aten_lt(236),
  aten_lt_(237),
  aten_less(238),
  aten_less_(239),
  aten_isnan(240),
  aten_mul(241),
  aten_mul_(242),
  aten_multiply(243),
  aten_multiply_(244),
  aten_ne(245),
  aten_ne_(246),
  aten_not_equal(247),
  aten_not_equal_(248),
  aten__ger(249),
  aten_ger(250),
  aten_outer(251),
  aten_orgqr(252),
  aten_linalg_householder_product(253),
  aten_transpose(254),
  aten_transpose_(255),
  aten_trapz(256),
  aten_trapezoid(257),
  aten_cumulative_trapezoid(258),
  aten_unsqueeze_(259),
  aten___getitem__(260),
  aten__set_item(261),
  aten_manual_seed(262),
  aten_set_(263),
  aten_index_put_(264),
  aten_device(265),
  aten_hash(266),
  aten_len(267),
  aten_list(268),
  aten_dict(269),
  aten_wait(270),
  aten_save(271),
  aten_sub(272),
  aten_sub_(273),
  aten_subtract(274),
  aten_subtract_(275),
  aten_keys(276),
  aten_ord(277),
  aten_chr(278),
  aten_hex(279),
  aten_oct(280),
  aten_clear(281),
  aten_trunc(282),
  aten_trunc_(283),
  aten_fix(284),
  aten_fix_(285),
  aten_to_mkldnn(286),
  aten_positive(287),
  aten_neg(288),
  aten_neg_(289),
  aten_negative(290),
  aten_negative_(291),
  aten_setdefault(292),
  aten_bin(293),
  aten_pop(294),
  aten_insert(295),
  aten__cat(296),
  aten_cat(297),
  aten_concat(298),
  aten_vstack(299),
  aten_row_stack(300),
  prim_unchecked_unwrap_optional(301),
  aten___contains__(302),
  prim_BailoutTemplate(303),
  prim_grad(304),
  aten_zero_(305),
  aten_fill_(306),
  aten_masked_fill_(307),
  cuda__set_device(308),
  cuda_set_stream(309),
  cuda__current_device(310),
  cuda_synchronize(311),
  aten_swapaxes(312),
  aten_swapaxes_(313),
  aten_swapdims(314),
  aten_swapdims_(315),
  aten_movedim(316),
  aten_moveaxis(317),
  aten_polygamma(318),
  aten_special_polygamma(319),
  aten_lgamma(320),
  aten_special_gammaln(321),
  aten_logsumexp(322),
  aten_special_logsumexp(323),
  aten_digamma(324),
  aten_special_psi(325),
  aten_special_digamma(326),
  aten_erf(327),
  aten_special_erf(328),
  aten_erfc(329),
  aten_special_erfc(330),
  aten_special_erfcx(331),
  aten_erfinv(332),
  aten_special_erfinv(333),
  aten_logit(334),
  aten_special_logit(335),
  aten_sigmoid(336),
  aten_special_expit(337),
  aten_expm1(338),
  aten_special_expm1(339),
  aten_exp2(340),
  aten_special_exp2(341),
  aten_log1p(342),
  aten_special_log1p(343),
  aten_round(344),
  aten_special_round(345),
  aten_sinc(346),
  aten_special_sinc(347),
  aten_i0(348),
  aten_special_i0(349),
  aten_special_i0e(350),
  aten_special_i1(351),
  aten_special_i1e(352),
  aten_xlogy(353),
  aten_special_xlogy(354),
  aten_special_xlog1py(355),
  aten_log_softmax(356),
  aten_special_log_softmax(357),
  aten_special_zeta(358),
  aten_igamma(359),
  aten_igamma_(360),
  aten_special_gammainc(361),
  aten_igammac(362),
  aten_igammac_(363),
  aten_special_gammaincc(364),
  aten_mvlgamma(365),
  aten_special_multigammaln(366),
  aten_has_torch_function(367),
  aten_hardswish(368),
  aten_hardswish_(369),
  aten_hardsigmoid_(370),
  aten_hardtanh_(371),
  aten___and__(372),
aten___iand__(373),
aten___ilshift__(374),
aten___ior__(375),
aten___irshift__(376),
aten___ixor__(377),
aten___lshift__(378),
aten___or__(379),
aten___rshift__(380),
aten___xor__(381),
aten__abs(382),
aten__addmv(383),
aten__addr(384),
aten__amp_foreach_non_finite_check_and_unscale_(385),
aten__amp_update_scale_(386),
aten__arange(387),
aten__argmax(388),
aten__argmin(389),
aten__baddbmm_mkl(390),
aten__cast_Byte(391),
aten__cast_Char(392),
aten__cast_Double(393),
aten__cast_Float(394),
aten__cast_Half(395),
aten__cast_Int(396),
aten__cast_Long(397),
aten__cast_Short(398),
aten__ceil(399),
aten__clamp_max(400),
aten__clamp_min(401),
aten__convolution(402),
aten__convolution_double_backward(403),
aten_convolution_overrideable(404),
aten_convolution_backward_overrideable(405),
aten__convolution_nogroup(406),
aten__copy_ignoring_overlaps(407),
aten__cos(408),
aten__cosh(409),
aten__ctc_loss(410),
aten__ctc_loss_backward(411),
aten__cudnn_ctc_loss(412),
aten__cudnn_init_dropout_state(413),
aten__cudnn_rnn(414),
aten__cudnn_rnn_backward(415),
aten__cudnn_rnn_flatten_weight(416),
aten__cufft_clear_plan_cache(417),
aten__cufft_get_plan_cache_max_size(418),
aten__cufft_get_plan_cache_size(419),
aten__cufft_set_plan_cache_max_size(420),
aten__denseDims(421),
aten__dimI(422),
aten__dimV(423),
aten__dim_arange(424),
aten__dirichlet_grad(425),
aten__dot(426),
aten__embedding_bag(427),
aten__embedding_bag_backward(428),
aten__embedding_bag_dense_backward(429),
aten__embedding_bag_sparse_backward(430),
aten__erf(431),
aten__erfc(432),
aten__exp(433),
aten__exp2(434),
aten__expm1(435),
aten__fft_with_size(436),
aten__fill(437),
aten__floor(438),
aten__fused_dropout(439),
aten__indices(440),
aten__ldexp(441),
aten__linspace(442),
aten__local_scalar(443),
aten__local_scalar_dense(444),
aten__log(445),
aten__log10(446),
aten__log1p(447),
aten__log2(448),
aten__logspace(449),
aten__lu_with_info(450),
aten__masked_scale(451),
aten__mm(452),
aten__mv(453),
aten__nnz(454),
aten__nansum(455),
aten__pack_padded_sequence(456),
aten__pack_padded_sequence_backward(457),
aten__pad_packed_sequence(458),
aten__pdist_backward(459),
aten__pdist_forward(460),
aten__prod(461),
aten__prodall(462),
aten__range(463),
aten__reshape_from_tensor(464),
aten__round(465),
aten__rsqrt(466),
aten__s_where(467),
aten__shape_as_tensor(468),
aten__sigmoid(469),
aten__sigmoid_forward(470),
aten__sin(471),
aten__sinh(472),
aten__sparseDims(473),
aten__sparse_add(474),
aten__sparse_addmm(475),
aten__sparse_coo_tensor_with_dims(476),
aten__sparse_coo_tensor_with_dims_and_tensors(477),
aten__sparse_coo_tensor_unsafe(478),
aten__sparse_csr_tensor_unsafe(479),
aten__sparse_dense_add(480),
aten__sparse_div_scalar(481),
aten__sparse_div_zerodim(482),
aten__sparse_mul(483),
aten__sparse_mul_scalar(484),
aten__sparse_mul_zerodim(485),
aten__sparse_sum(486),
aten__sqrt(487),
aten__square(488),
aten__standard_gamma(489),
aten__standard_gamma_grad(490),
aten__sum(491),
aten__sum_cuda(492),
aten__tan(493),
aten__tanh(494),
aten__tanh_forward(495),
aten__th_get_device(496),
aten__th_kthvalue(497),
aten__th_prod(498),
aten__th_sigmoid(499),
aten__th_std(500),
aten__th_sum(501),
aten__th_tanh(502),
aten__th_var(503),
aten__thnn_fused_gru_cell(504),
aten__thnn_fused_gru_cell_backward(505),
aten__thnn_fused_lstm_cell(506),
aten__thnn_fused_lstm_cell_backward(507),
aten__trilinear(508),
aten__trunc(509),
aten__unique(510),
aten__unique_dim(511),
aten__unsafe_view(512),
aten__validate_sparse_coo_tensor_args(513),
aten__values(514),
aten__weight_norm(515),
aten__weight_norm_cuda_interface(516),
aten__weight_norm_cuda_interface_backward(517),
aten__weight_norm_differentiable_backward(518),
aten_abs(519),
aten_adaptive_avg_pool1d(520),
aten_adaptive_avg_pool2d(521),
aten_adaptive_avg_pool2d_backward(522),
aten_adaptive_avg_pool2d_forward(523),
aten_adaptive_avg_pool3d(524),
aten_adaptive_avg_pool3d_backward(525),
aten_adaptive_avg_pool3d_forward(526),
aten_adaptive_max_pool1d(527),
aten_adaptive_max_pool2d(528),
aten_adaptive_max_pool2d_backward(529),
aten_adaptive_max_pool2d_forward(530),
aten_adaptive_max_pool3d(531),
aten_adaptive_max_pool3d_backward(532),
aten_adaptive_max_pool3d_forward(533),
aten_add(534),
aten_add_(535),
aten_addbmm(536),
aten_addcdiv(537),
aten_addcmul(538),
aten_addmm(539),
aten_addmv(540),
aten_addr(541),
aten_affine_grid_generator(542),
aten_affine_grid_generator_backward(543),
aten_alias(544),
aten_all(545),
aten_allclose(546),
aten_alpha_dropout(547),
aten_any(548),
aten_arange(549),
aten_argmax(550),
aten_argmin(551),
aten_amax(552),
aten_amin(553),
aten_aminmax(554),
aten_as_strided(555),
aten_as_tensor(556),
aten_atan2(557),
aten_atleast_1d(558),
aten_atleast_2d(559),
aten_atleast_3d(560),
aten_avg_pool1d(561),
aten_avg_pool2d(562),
aten_avg_pool2d_backward(563),
aten_avg_pool2d_forward(564),
aten_avg_pool3d(565),
aten_avg_pool3d_backward(566),
aten_avg_pool3d_forward(567),
aten_baddbmm(568),
aten_bartlett_window(569),
aten_batch_norm(570),
aten_bernoulli(571),
aten_bilinear(572),
aten_binary_cross_entropy(573),
aten_binary_cross_entropy_backward(574),
aten_binary_cross_entropy_forward(575),
aten_binary_cross_entropy_with_logits(576),
aten_binary_cross_entropy_with_logits_backward(577),
aten_binary_cross_entropy_with_logits_target_backward(578),
aten_bincount(579),
aten_blackman_window(580),
aten_block_diag(581),
aten_bmm(582),
aten_broadcast_tensors(583),
aten_broadcast_to(584),
aten_cartesian_prod(585),
aten_cauchy(586),
aten_ceil(587),
aten_celu(588),
aten_cholesky(589),
aten_cholesky_inverse(590),
aten_cholesky_solve(591),
aten_chunk(592),
aten_clamp_max(593),
aten_clamp_min(594),
aten_clone(595),
aten_coalesce(596),
aten_combinations(597),
aten__conj(598),
aten_conj(599),
aten_conj_physical(600),
aten_conj_physical_(601),
aten_resolve_conj(602),
aten_resolve_neg(603),
aten_complex(604),
aten_copysign(605),
aten_polar(606),
aten_constant_pad_nd(607),
aten_contiguous(608),
aten_conv1d(609),
aten_conv2d(610),
aten_conv3d(611),
aten_conv_tbc(612),
aten_conv_tbc_backward(613),
aten_conv_transpose1d(614),
aten_convolution(615),
aten_copy_sparse_to_sparse(616),
aten_corrcoef(617),
aten_cos(618),
aten_cosh(619),
aten_cosine_embedding_loss(620),
aten_cosine_similarity(621),
aten_count_nonzero(622),
aten_cross(623),
aten_cov(624),
aten_std_mean(625),
aten_var_mean(626),
aten_ctc_loss(627),
aten_cudnn_affine_grid_generator(628),
aten_cudnn_affine_grid_generator_backward(629),
aten_cudnn_batch_norm(630),
aten_cudnn_batch_norm_backward(631),
aten_cudnn_convolution(632),
aten_cudnn_convolution_backward(633),
aten_cudnn_convolution_backward_bias(634),
aten_cudnn_convolution_backward_input(635),
aten_cudnn_convolution_backward_weight(636),
aten_cudnn_convolution_transpose(637),
aten_cudnn_convolution_transpose_backward(638),
aten_cudnn_convolution_transpose_backward_bias(639),
aten_cudnn_convolution_transpose_backward_input(640),
aten_cudnn_convolution_transpose_backward_weight(641),
aten_cudnn_convolution_relu(642),
aten_cudnn_convolution_add_relu(643),
aten_cudnn_grid_sampler(644),
aten_cudnn_grid_sampler_backward(645),
aten_cudnn_is_acceptable(646),
aten_cummax(647),
aten_cummin(648),
aten_cumprod(649),
aten_cumsum(650),
aten_data_ptr(651),
aten_deg2rad(652),
aten_detach(653),
aten_diag(654),
aten_diag_embed(655),
aten_diagflat(656),
aten_diagonal(657),
aten_fill_diagonal_(658),
aten_diff(659),
aten_frexp(660),
aten_dim(661),
aten_dist(662),
aten_dot(663),
aten_dropout(664),
aten_dsplit(665),
aten_dstack(666),
aten_eig(667),
aten_einsum(668),
aten_elu(669),
aten_elu_backward(670),
aten_elu_forward(671),
aten_embedding(672),
aten_embedding_backward(673),
aten_embedding_bag(674),
aten_embedding_dense_backward(675),
aten_embedding_renorm(676),
aten_embedding_sparse_backward(677),
aten_empty(678),
aten_empty_like(679),
aten_empty_strided(680),
aten_special_entr(681),
aten_eq(682),
aten_equal(683),
aten_exp(684),
aten_expand(685),
aten_expand_as(686),
aten_exponential(687),
aten_eye(688),
aten_feature_alpha_dropout(689),
aten_feature_dropout(690),
aten_fft(691),
aten_fill(692),
aten_flatten(693),
aten_flip(694),
aten_fliplr(695),
aten_flipud(696),
aten_floor(697),
aten_fmod(698),
aten_fmod_(699),
aten_fmax(700),
aten_fmin(701),
aten_frac(702),
aten_fractional_max_pool2d(703),
aten_fractional_max_pool2d_backward(704),
aten_fractional_max_pool2d_forward(705),
aten_frobenius_norm(706),
aten_full(707),
aten_full_like(708),
aten_gather(709),
aten_gcd(710),
aten_gelu(711),
aten_geometric(712),
aten_geqrf(713),
aten_get_device(714),
aten_glu(715),
aten_glu_backward(716),
aten_glu_forward(717),
aten_gradient(718),
aten_grid_sampler(719),
aten_grid_sampler_2d(720),
aten_grid_sampler_2d_backward(721),
aten_grid_sampler_3d(722),
aten_grid_sampler_3d_backward(723),
aten_group_norm(724),
aten_gru(725),
aten_gru_cell(726),
aten_hamming_window(727),
aten_hann_window(728),
aten_hardshrink(729),
aten_hardshrink_backward(730),
aten_hardsigmoid(731),
aten_hardsigmoid_backward(732),
aten_hardtanh(733),
aten_hardtanh_backward(734),
aten_hardtanh_forward(735),
aten_heaviside(736),
aten_hinge_embedding_loss(737),
aten_histc(738),
aten_histogram(739),
aten_hspmm(740),
aten_hsplit(741),
aten_hstack(742),
aten_hypot(743),
aten_i0_(744),
aten_ifft(745),
aten_index(746),
aten_index_add(747),
aten_index_copy(748),
aten_index_fill(749),
aten_index_put(750),
aten_index_select(751),
aten_indices(752),
aten_inner(753),
aten_instance_norm(754),
aten_inverse(755),
aten_irfft(756),
aten_is_coalesced(757),
aten_is_complex(758),
aten_is_contiguous(759),
aten_is_cuda(760),
aten_is_mlc(761),
aten_is_ort(762),
aten_is_distributed(763),
aten_is_floating_point(764),
aten_is_inference(765),
aten_is_nonzero(766),
aten_is_same_size(767),
aten_is_set_to(768),
aten_is_signed(769),
aten_is_sparse(770),
aten_is_sparse_csr(771),
aten_isclose(772),
aten_isreal(773),
aten_istft(774),
aten_isposinf(775),
aten_isneginf(776),
aten_kaiser_window(777),
aten_kl_div(778),
aten_kl_div_backward(779),
aten_kthvalue(780),
aten_l1_loss(781),
aten_l1_loss_backward(782),
aten_l1_loss_forward(783),
aten_layer_norm(784),
aten_lcm(785),
aten_leaky_relu(786),
aten_leaky_relu_backward(787),
aten_leaky_relu_forward(788),
aten_lerp(789),
aten_linear(790),
aten_linspace(791),
aten_log(792),
aten_log10(793),
aten_log2(794),
aten_log_normal(795),
aten_log_sigmoid(796),
aten_log_sigmoid_backward(797),
aten_log_sigmoid_forward(798),
aten__log_softmax(799),
aten__log_softmax_backward_data(800),
aten_logcumsumexp(801),
aten_logdet(802),
aten_logspace(803),
aten_lstm(804),
aten_lstm_cell(805),
aten_lstsq(806),
aten_lu_solve(807),
aten_margin_ranking_loss(808),
aten_masked_fill(809),
aten_masked_scatter(810),
aten_masked_select(811),
aten_matrix_rank(812),
aten_matrix_exp(813),
aten_max(814),
aten_max_pool1d(815),
aten_max_pool1d_with_indices(816),
aten_max_pool2d(817),
aten_max_pool2d_with_indices(818),
aten_max_pool2d_with_indices_backward(819),
aten_max_pool2d_with_indices_forward(820),
aten_max_pool3d(821),
aten_max_pool3d_with_indices(822),
aten_max_pool3d_with_indices_backward(823),
aten_max_pool3d_with_indices_forward(824),
aten_max_unpool2d(825),
aten_max_unpool2d_backward(826),
aten_max_unpool2d_forward(827),
aten_max_unpool3d(828),
aten_max_unpool3d_backward(829),
aten_max_unpool3d_forward(830),
aten_max_values(831),
aten_mean(832),
aten_nanmean(833),
aten_median(834),
aten_nanmedian(835),
aten_meshgrid(836),
aten_min(837),
aten_min_values(838),
aten_miopen_batch_norm(839),
aten_miopen_batch_norm_backward(840),
aten_miopen_convolution(841),
aten_miopen_convolution_backward(842),
aten_miopen_convolution_backward_bias(843),
aten_miopen_convolution_backward_input(844),
aten_miopen_convolution_backward_weight(845),
aten_miopen_convolution_transpose(846),
aten_miopen_convolution_transpose_backward(847),
aten_miopen_convolution_transpose_backward_input(848),
aten_miopen_convolution_transpose_backward_weight(849),
aten_miopen_depthwise_convolution(850),
aten_miopen_depthwise_convolution_backward(851),
aten_miopen_depthwise_convolution_backward_input(852),
aten_miopen_depthwise_convolution_backward_weight(853),
aten_miopen_rnn(854),
aten_miopen_rnn_backward(855),
aten_mish(856),
aten_mkldnn_convolution(857),
aten_mkldnn_convolution_backward(858),
aten_mkldnn_convolution_backward_input(859),
aten_mkldnn_convolution_backward_weights(860),
aten_mm(861),
aten_mode(862),
aten_mse_loss(863),
aten_mse_loss_backward(864),
aten_mse_loss_forward(865),
aten_msort(866),
aten_multi_margin_loss(867),
aten_multi_margin_loss_backward(868),
aten_multi_margin_loss_forward(869),
aten_multilabel_margin_loss(870),
aten_multilabel_margin_loss_backward(871),
aten_multilabel_margin_loss_forward(872),
aten_multinomial(873),
aten_mv(874),
aten_nansum(875),
aten_nan_to_num(876),
aten_narrow(877),
aten_narrow_copy(878),
aten_native_batch_norm(879),
aten_native_batch_norm_backward(880),
aten_native_clone(881),
aten_native_get_device(882),
aten_native_norm(883),
aten_native_pow(884),
aten_native_resize_as(885),
aten_native_tensor(886),
aten_native_zero(887),
aten_special_ndtr(888),
aten_nextafter(889),
aten_special_ndtri(890),
aten_logical_and(891),
aten_logical_not(892),
aten_logical_or(893),
aten_logical_xor(894),
aten_bitwise_and(895),
aten_bitwise_not(896),
aten_bitwise_or(897),
aten_bitwise_xor(898),
aten_element_size(899),
aten_nll_loss(900),
aten_nll_loss2d(901),
aten_nll_loss2d_backward(902),
aten_nll_loss2d_forward(903),
aten_nll_loss_backward(904),
aten_nll_loss_forward(905),
aten_nonzero(906),
aten_nonzero_numpy(907),
aten_norm(908),
aten_norm_except_dim(909),
aten_normal(910),
aten_nuclear_norm(911),
aten_numel(912),
aten_ones(913),
aten_ones_like(914),
aten_ormqr(915),
aten_pairwise_distance(916),
aten__euclidean_dist(917),
aten_pdist(918),
aten_cdist(919),
aten_permute(920),
aten_pin_memory(921),
aten_pinverse(922),
aten_pixel_shuffle(923),
aten_pixel_unshuffle(924),
aten_poisson(925),
aten_pow(926),
aten_float_power(927),
aten_prelu(928),
aten_prelu_backward(929),
aten_prod(930),
aten_put(931),
aten_qr(932),
aten_quantile(933),
aten_nanquantile(934),
aten_rad2deg(935),
aten_rand(936),
aten_rand_like(937),
aten_randint(938),
aten_randint_like(939),
aten_randn(940),
aten_randn_like(941),
aten_random(942),
aten_randperm(943),
aten_range(944),
aten_ravel(945),
aten_reciprocal(946),
aten_reflection_pad1d(947),
aten_reflection_pad1d_backward(948),
aten_reflection_pad1d_forward(949),
aten_reflection_pad2d(950),
aten_reflection_pad2d_backward(951),
aten_reflection_pad2d_forward(952),
aten_reflection_pad3d(953),
aten_reflection_pad3d_backward(954),
aten_reflection_pad3d_forward(955),
aten_relu(956),
aten_remainder(957),
aten_renorm(958),
aten_repeat(959),
aten_replication_pad1d(960),
aten_replication_pad1d_backward(961),
aten_replication_pad1d_forward(962),
aten_replication_pad2d(963),
aten_replication_pad2d_backward(964),
aten_replication_pad2d_forward(965),
aten_replication_pad3d(966),
aten_replication_pad3d_backward(967),
aten_replication_pad3d_forward(968),
aten_reshape(969),
aten_reshape_as(970),
aten_resize(971),
aten_resize_(972),
aten_resize_as(973),
aten_resize_as_(974),
aten_rfft(975),
aten_rnn_relu(976),
aten_rnn_relu_cell(977),
aten_rnn_tanh(978),
aten_rnn_tanh_cell(979),
aten_rot90(980),
aten_rrelu(981),
aten_rrelu_with_noise(982),
aten_rrelu_with_noise_backward(983),
aten_rrelu_with_noise_forward(984),
aten_rsqrt(985),
aten_scatter(986),
aten_scatter_add(987),
aten_segment_reduce(988),
aten_select(989),
aten_selu(990),
aten_set(991),
aten_sign(992),
aten_signbit(993),
aten_silu(994),
aten_sgn(995),
aten_sin(996),
aten_sinh(997),
aten_size(998),
aten_sizes(999),
aten_slice(1000),
aten_slogdet(1001),
aten_smm(1002),
aten_smooth_l1_loss(1003),
aten_smooth_l1_loss_backward(1004),
aten_smooth_l1_loss_forward(1005),
aten_soft_margin_loss(1006),
aten_soft_margin_loss_backward(1007),
aten_soft_margin_loss_forward(1008),
aten_softmax(1009),
aten__softmax(1010),
aten__softmax_backward_data(1011),
aten_softplus(1012),
aten_softplus_backward(1013),
aten_softplus_forward(1014),
aten_softshrink(1015),
aten_softshrink_backward(1016),
aten_softshrink_forward(1017),
aten_solve(1018),
aten_sort(1019),
aten_sparse_coo_tensor(1020),
aten_sparse_csr_tensor(1021),
aten_sparse_mask(1022),
aten_sparse_resize(1023),
aten_sparse_resize_and_clear(1024),
aten_split(1025),
aten_split_with_sizes(1026),
aten_sqrt(1027),
aten_square(1028),
aten_squeeze(1029),
aten_sspaddmm(1030),
aten_stack(1031),
aten_std(1032),
aten_stft(1033),
aten_storage_offset(1034),
aten_stride(1035),
aten_strides(1036),
aten_rsub(1037),
aten_sum(1038),
aten_sum_to_size(1039),
aten_svd(1040),
aten_symeig(1041),
aten_t(1042),
aten_take(1043),
aten_take_along_dim(1044),
aten_tan(1045),
aten_tanh(1046),
aten_tanh_(1047),
aten_tensor(1048),
aten_tensordot(1049),
aten_tensor_split(1050),
aten_th_clone(1051),
aten_th_norm(1052),
aten_th_pow(1053),
aten_th_resize_as(1054),
aten_th_tensor(1055),
aten_th_zero(1056),
aten_thnn_conv2d(1057),
aten__slow_conv2d_backward(1058),
aten__slow_conv2d_forward(1059),
aten_tile(1060),
aten_slow_conv3d(1061),
aten_slow_conv3d_backward(1062),
aten_slow_conv3d_forward(1063),
aten_thnn_conv_depthwise2d(1064),
aten_thnn_conv_depthwise2d_backward(1065),
aten_thnn_conv_depthwise2d_forward(1066),
aten_slow_conv_dilated2d(1067),
aten_slow_conv_dilated2d_backward(1068),
aten_slow_conv_dilated3d(1069),
aten_slow_conv_dilated3d_backward(1070),
aten_slow_conv_transpose2d(1071),
aten_slow_conv_transpose2d_backward(1072),
aten_slow_conv_transpose3d(1073),
aten_slow_conv_transpose3d_backward(1074),
aten_threshold(1075),
aten_threshold_backward(1076),
aten_to(1077),
aten_to_sparse(1078),
aten_to_dense(1079),
aten_topk(1080),
aten_trace(1081),
aten_triangular_solve(1082),
aten_tril(1083),
aten_triplet_margin_loss(1084),
aten_triu(1085),
aten_type_as(1086),
aten_unbind(1087),
aten_unfold(1088),
aten_uniform(1089),
aten_unsafe_chunk(1090),
aten_unsafe_split(1091),
aten_unsafe_split_with_sizes(1092),
aten_unsqueeze(1093),
aten_upsample_bilinear2d(1094),
aten_upsample_bilinear2d_backward(1095),
aten_upsample_bilinear2d_forward(1096),
aten_upsample_bicubic2d(1097),
aten_upsample_bicubic2d_backward(1098),
aten_upsample_bicubic2d_forward(1099),
aten_upsample_linear1d(1100),
aten_upsample_linear1d_backward(1101),
aten_upsample_linear1d_forward(1102),
aten_upsample_nearest1d(1103),
aten_upsample_nearest1d_backward(1104),
aten_upsample_nearest1d_forward(1105),
aten_upsample_nearest2d(1106),
aten_upsample_nearest2d_backward(1107),
aten_upsample_nearest2d_forward(1108),
aten_upsample_nearest3d(1109),
aten_upsample_nearest3d_backward(1110),
aten_upsample_nearest3d_forward(1111),
aten_upsample_trilinear3d(1112),
aten_upsample_trilinear3d_backward(1113),
aten_upsample_trilinear3d_forward(1114),
aten_values(1115),
aten_vander(1116),
aten_var(1117),
aten_view(1118),
aten_view_as(1119),
aten_vsplit(1120),
aten_where(1121),
aten_zero(1122),
aten_zeros(1123),
aten_zeros_like(1124),
aten_real(1125),
aten_imag(1126),
aten_view_as_real(1127),
aten_view_as_complex(1128),
/* nothing */
  onnx_Add(1129),
  onnx_Concat(1130),
  onnx_Constant(1131),
  onnx_ConstantFill(1132),
  onnx_Div(1133),
  onnx_GRU(1134),
  onnx_Gather(1135),
  onnx_Gemm(1136),
  onnx_LSTM(1137),
  onnx_MatMul(1138),
  onnx_Mul(1139),
  onnx_Pow(1140),
  onnx_RNN(1141),
  onnx_Shape(1142),
  onnx_Size(1143),
  onnx_Slice(1144),
  onnx_Softmax(1145),
  onnx_Squeeze(1146),
  onnx_Sub(1147),
  onnx_Transpose(1148),
  onnx_Unsqueeze(1149),
  onnx_Loop(1150),
  onnx_If(1151),
  onnx_Reshape(1152),
  onnx_Expand(1153),
  onnx_Equal(1154),
  onnx_Greater(1155),
  onnx_GreaterOrEqual(1156),
  onnx_Less(1157),
  onnx_LessOrEqual(1158),
  onnx_Not(1159),
  onnx_ATen(1160),
  onnx_Split(1161),
  onnx_ConstantOfShape(1162),
  onnx_Cast(1163),
  onnx_Mod(1164),
  onnx_Sqrt(1165),
  onnx_SplitToSequence(1166),
  onnx_SequenceAt(1167),
  onnx_SequenceConstruct(1168),
  onnx_SequenceEmpty(1169),
  onnx_SequenceInsert(1170),
  onnx_SequenceErase(1171),
  onnx_ConcatFromSequence(1172),
  onnx_Identity(1173),
  onnx_SoftmaxCrossEntropyLoss(1174),
  onnx_NegativeLogLikelihoodLoss(1175),
  onnx_LogSoftmax(1176),
  onnx_ReduceL1(1177),
  onnx_ReduceL2(1178),
  onnx_Conv(1179),
  onnx_BatchNormalization(1180),
  onnx_ReduceMean(1181),
  onnx_ReduceProd(1182),
  onnx_Relu(1183),
  onnx_Neg(1184),
  onnx_NonZero(1185),
  onnx_Range(1186),
  onnx_Tile(1187),
  onnx_Where(1188),
  attr_A(1189),
attr_C(1190),
attr_H(1191),
attr_LU_data(1192),
attr_LU_pivots(1193),
attr_N(1194),
attr_W(1195),
attr_accumulate(1196),
attr_align_corners(1197),
attr_alpha(1198),
attr_anchor(1199),
attr_argmaxes(1200),
attr_atol(1201),
attr_b_hh(1202),
attr_b_ih(1203),
attr_bag_size(1204),
attr_base(1205),
attr_batch1(1206),
attr_batch2(1207),
attr_batch_first(1208),
attr_batch_sizes(1209),
attr_benchmark(1210),
attr_beta(1211),
attr_bias(1212),
attr_bias_defined(1213),
attr_bidirectional(1214),
attr_bins(1215),
attr_blank(1216),
attr_buffer(1217),
attr_ceil_mode(1218),
attr_checked_signal_sizes(1219),
attr_chunks(1220),
attr_columns(1221),
attr_column_stack(1222),
attr_complex_input(1223),
attr_complex_output(1224),
attr_condition(1225),
attr_count_include_pad(1226),
attr_cudnn_enable(1227),
attr_cudnn_enabled(1228),
attr_cx(1229),
attr_cy(1230),
attr_data(1231),
attr_dense_dim(1232),
attr_descending(1233),
attr_deterministic(1234),
attr_device(1235),
attr_diagonal(1236),
attr_dilation(1237),
attr_dim(1238),
attr_dim0(1239),
attr_dim1(1240),
attr_dim2(1241),
attr_dimension(1242),
attr_dims(1243),
attr_dims_other(1244),
attr_dims_self(1245),
attr_divisor_override(1246),
attr_dropout(1247),
attr_dropout_seed(1248),
attr_dropout_state(1249),
attr_dtype(1250),
attr_eigenvectors(1251),
attr_end(1252),
attr_end_dim(1253),
attr_eps(1254),
attr_epsilon(1255),
attr_equal_nan(1256),
attr_equation(1257),
attr_expand1(1258),
attr_expand2(1259),
attr_expand3(1260),
attr_exponent(1261),
attr_exponential_average_factor(1262),
attr_fgrad_input(1263),
attr_fill_value(1264),
attr_finput(1265),
attr_from(1266),
attr_g(1267),
attr_gO(1268),
attr_generator(1269),
attr_ggI(1270),
attr_ggW(1271),
attr_ggb(1272),
attr_grad(1273),
attr_gradOutput(1274),
attr_grad_bias(1275),
attr_grad_cy(1276),
attr_grad_hy(1277),
attr_grad_input(1278),
attr_grad_out(1279),
attr_grad_output(1280),
attr_grad_w(1281),
attr_grad_weight(1282),
attr_grid(1283),
attr_groups(1284),
attr_has_bias(1285),
attr_has_biases(1286),
attr_hidden_bias(1287),
attr_hidden_gates(1288),
attr_hidden_size(1289),
attr_high(1290),
attr_hop_length(1291),
attr_hx(1292),
attr_i1(1293),
attr_i2(1294),
attr_i3(1295),
attr_ignore_index(1296),
attr_implicit(1297),
attr_index(1298),
attr_indices(1299),
attr_info(1300),
attr_input(1301),
attr_input1(1302),
attr_input2(1303),
attr_input3(1304),
attr_input_bias(1305),
attr_input_gates(1306),
attr_input_lengths(1307),
attr_input_scale(1308),
attr_input_size(1309),
attr_interpolation_mode(1310),
attr_inverse(1311),
attr_is_target(1312),
attr_k(1313),
attr_keepdim(1314),
attr_kernel_size(1315),
attr_lambd(1316),
attr_largest(1317),
attr_layout(1318),
attr_left(1319),
attr_length(1320),
attr_lengths(1321),
attr_like(1322),
attr_log_alpha(1323),
attr_log_probs(1324),
attr_low(1325),
attr_lower(1326),
attr_lu(1327),
attr_m(1328),
attr_margin(1329),
attr_mask(1330),
attr_mat(1331),
attr_mat1(1332),
attr_mat2(1333),
attr_max(1334),
attr_max_indices(1335),
attr_max_norm(1336),
attr_max_size(1337),
attr_max_val(1338),
attr_max_values(1339),
attr_maximum_indices(1340),
attr_maxnorm(1341),
attr_maximum(1342),
attr_mean(1343),
attr_median(1344),
attr_nanmedian(1345),
attr_min(1346),
attr_min_indices(1347),
attr_min_val(1348),
attr_minlength(1349),
attr_minimum(1350),
attr_mode(1351),
attr_momentum(1352),
attr_n(1353),
attr_n_fft(1354),
attr_neg_log_likelihood(1355),
attr_negative(1356),
attr_negative_slope(1357),
attr_noise(1358),
attr_non_blocking(1359),
attr_norm_type(1360),
attr_normalized(1361),
attr_normalized_shape(1362),
attr_num_groups(1363),
attr_num_layers(1364),
attr_num_samples(1365),
attr_num_weights(1366),
attr_offset(1367),
attr_offset2bag(1368),
attr_offsets(1369),
attr_ones(1370),
attr_onesided(1371),
attr_options(1372),
attr_other(1373),
attr_output(1374),
attr_output_mask(1375),
attr_output_padding(1376),
attr_output_size(1377),
attr_output_sizes(1378),
attr_p(1379),
attr_pad(1380),
attr_padding(1381),
attr_padding_idx(1382),
attr_padding_mode(1383),
attr_padding_value(1384),
attr_params(1385),
attr_pdist(1386),
attr_cdist(1387),
attr_std_mean(1388),
attr_var_mean(1389),
attr_periodic(1390),
attr_pivot(1391),
attr_pivots(1392),
attr_pooledHeight(1393),
attr_pooledWidth(1394),
attr_positive(1395),
attr_pow(1396),
attr_random_samples(1397),
attr_rcond(1398),
attr_reduction(1399),
attr_repeats(1400),
attr_replacement(1401),
attr_res1(1402),
attr_res2(1403),
attr_res3(1404),
attr_reserve(1405),
attr_result(1406),
attr_return_inverse(1407),
attr_rois(1408),
attr_rtol(1409),
attr_running_mean(1410),
attr_running_var(1411),
attr_save_mean(1412),
attr_save_std(1413),
attr_save_var(1414),
attr_saved_g(1415),
attr_saved_norms(1416),
attr_saved_v(1417),
attr_scale(1418),
attr_scale_grad_by_freq(1419),
attr_self(1420),
attr_self_size(1421),
attr_self_ty(1422),
attr_shape(1423),
attr_sigma(1424),
attr_signal_ndim(1425),
attr_signal_sizes(1426),
attr_size(1427),
attr_solution(1428),
attr_some(1429),
attr_sorted(1430),
attr_source(1431),
attr_sparse(1432),
attr_sparse_dim(1433),
attr_sparse_dtype(1434),
attr_spatialScale(1435),
attr_split_size(1436),
attr_split_sizes(1437),
attr_src(1438),
attr_start(1439),
attr_start_dim(1440),
attr_std(1441),
attr_step(1442),
attr_steps(1443),
attr_storage(1444),
attr_storageOffset(1445),
attr_storage_offset(1446),
attr_stride(1447),
attr_sumdim(1448),
attr_swap(1449),
attr_symmetric(1450),
attr_target(1451),
attr_target_lengths(1452),
attr_targets(1453),
attr_tensor(1454),
attr_tensor1(1455),
attr_tensor2(1456),
attr_tensors(1457),
attr_the_template(1458),
attr_theta(1459),
attr_threshold(1460),
attr_to(1461),
attr_tol(1462),
attr_total(1463),
attr_total_length(1464),
attr_total_weight(1465),
attr_train(1466),
attr_training(1467),
attr_transpose(1468),
attr_transposed(1469),
attr_unbiased(1470),
attr_unitriangular(1471),
attr_unroll_dim(1472),
attr_upper(1473),
attr_upscale_factor(1474),
attr_use_input_stats(1475),
attr_v(1476),
attr_value(1477),
attr_values(1478),
attr_vec(1479),
attr_vec1(1480),
attr_vec2(1481),
attr_w_hh(1482),
attr_w_ih(1483),
attr_weight(1484),
attr_weight_arr(1485),
attr_weight_buf(1486),
attr_weight_size(1487),
attr_weight_stride0(1488),
attr_weights(1489),
attr_win_length(1490),
attr_window(1491),
attr_window_length(1492),
attr_workspace(1493),
attr_x(1494),
attr_x1(1495),
attr_x2(1496),
  attr_Subgraph(1497),
  attr_ReverseSubgraph(1498),
  attr_f_real_outputs(1499),
  attr_df_input_vjps(1500),
  attr_df_input_captured_inputs(1501),
  attr_df_input_captured_outputs(1502),
  attr_df_output_vjps(1503),
  attr_axes(1504),
  attr_axis(1505),
  attr_broadcast(1506),
  attr_direction(1507),
  attr_ends(1508),
  attr_inplace(1509),
  attr_input_as_shape(1510),
  attr_is_zero(1511),
  attr_num_none(1512),
  attr_num_present(1513),
  attr_perm(1514),
  attr_sizes(1515),
  attr_starts(1516),
  attr_profiled_type(1517),
  attr_transA(1518),
  attr_transB(1519),
  attr_name(1520),
  attr_a(1521),
  attr_b(1522),
  attr_beg(1523),
  attr_idx(1524),
  attr_split(1525),
  attr_slot(1526),
  attr_kinds(1527),
  attr_types(1528),
  attr_scope(1529),
  attr_keepdims(1530),
  attr_cache_id(1531),
  attr_new_axis(1532),
  attr_warn_id(1533),
  attr_allowzero(1534),
    num_symbols(1535);

    public final int value;
    private _keys(int v) { this.value = v; }
    private _keys(_keys e) { this.value = e.value; }
    public _keys intern() { for (_keys e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

// #define DEFINE_SYMBOL(s)
//   constexpr Symbol s(static_cast<unique_t>(_keys::s));

// #undef DEFINE_SYMBOL

// #define DEFINE_SYMBOL(ns, s)
//   namespace ns { constexpr Symbol s(static_cast<unique_t>(_keys::ns##_##s)); }
@Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol prim(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol aten(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol cuda(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol onnx(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol attr(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol scope(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol user(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol _caffe2(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol dimname(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol namespaces(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Assign(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol BroadcastingChunk(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol BroadcastSizes(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ReductionSizes(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Constant(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ChunkSizes(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ConstantMKLDNNTensor(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol BroadcastMKLDNNTensors(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol MKLDNNGroup(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol MKLDNNHardSwish(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol MKLDNNHardSigmoid(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol MKLDNNHardTanh(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol MKLDNNClamp(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Drop(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Eval(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Expand();  /* onnx */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol FusionGroup(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol CudaFusionGroup(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol CudaFusionGuard(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol FunctionalGraph(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol DifferentiableGraph(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TensorExprGroup(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol StaticSubgraph(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol If(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Jump();  /* debug */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol JumpNZ();  /* debug */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol JumpZ();  /* debug */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Load(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Loop(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Param(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol PackPadded();  /* onnx */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol PadPacked();  /* onnx */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Placeholder();  /* debug */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Print(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol PythonOp(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol IgnoredPythonOp(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Reverse(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Return(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ReturnStmt(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol BreakStmt(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ContinueStmt(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ComprehensionScope(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Store(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol AutogradZero(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol AutogradAnyNonZero(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol AutogradAllNonZero(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol AutogradAllZero(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Starred(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TupleConstruct(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TupleUnpack(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TupleIndex(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TupleSlice(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ListConstruct(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ListUnpack(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol DictConstruct(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ModuleContainerIndex(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol EnumName(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol EnumValue(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol StringIndex(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol NumToTensor(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Uninitialized(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol VarConcat(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol VarStack(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol With(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Enter(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Exit(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol Bool(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol Int(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol FloatImplicit(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ComplexImplicit(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol IntImplicit(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ScalarImplicit(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol Float(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol Complex(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol str(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_pinned(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol Delete(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol relu_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gelu_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol relu6(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol relu6_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol dropout_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sigmoid_(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol device(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol dtype(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol layout(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol id(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol requires_grad(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol MakeTestTensor();  /* test */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol AutogradAdd(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol GradOf(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol grad(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol backward(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Guard(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol BailOut(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TypeCheck(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol RequiresGradCheck(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol FallbackGraph(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol FusedConcat(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ConstantChunk(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol MMTreeReduce(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol MMBatchSide(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol list(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol dict(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol min(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol max(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol abs(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol divmod(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol zip(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol enumerate(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol range(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol rangelist(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol isinstance(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol tolist(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol unchecked_cast(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _grad_sum_to_size(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _size_if_not_equal(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _ncf_unsqueeze(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol warn(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sorted(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol floordiv(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __range_length(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __derive_index(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __round_to_zero_floordiv(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_scripting(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _unwrap_optional(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol fork(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol forkClosure(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol RaiseException(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Closure(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol CreateObject(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol SetAttr(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol GetAttr(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol HasAttr(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol profile(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol profile_ivalue(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol AddStatValue(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TimePoint(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol CallFunction(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol CallMethod(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol LoopContinuation(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol annotate(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TracedModuleForward(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TracedFork(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TracedAttr(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol rpc_async(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol rpc_sync(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol rpc_remote(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol is_cuda(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol abs_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol absolute(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol absolute_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol acos(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol acos_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arccos(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arccos_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol acosh(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol acosh_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arccosh(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arccosh_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol asin(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol asin_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arcsin(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arcsin_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol asinh(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol asinh_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arcsinh(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arcsinh_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atan(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atan_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arctan(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arctan_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atanh(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atanh_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arctanh(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arctanh_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol clamp(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol clamp_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol clip(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol clip_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol det(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_det(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol matrix_power(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_matrix_power(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol chain_matmul(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_multi_dot(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_norm(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_vector_norm(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_matrix_norm(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol matmul(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_matmul(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol append(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol item(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol format(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol percentFormat(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __not__(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __is__(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __isnot__(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol copy(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol copy_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol div(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol div_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol divide(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol divide_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol true_divide(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol true_divide_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol t_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addbmm_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addcdiv_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addcmul_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addmv_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addr_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol baddbmm_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ge(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ge_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol greater_equal(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol greater_equal_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gt(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gt_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol greater(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol greater_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol le(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol le_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol less_equal(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol less_equal_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lerp_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lt(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lt_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol less(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol less_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol isnan(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mul(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mul_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multiply(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multiply_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ne(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ne_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol not_equal(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol not_equal_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _ger(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ger(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol outer(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol orgqr(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_householder_product(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol transpose(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol transpose_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol trapz(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol trapezoid(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cumulative_trapezoid(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unsqueeze_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __getitem__(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _set_item(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol manual_seed(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol set_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_put_();  
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hash(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol len();   
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef @Name("wait") Symbol _wait(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol save(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sub(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sub_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol subtract(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol subtract_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol keys(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ord(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol chr(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hex(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol oct(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol clear(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol trunc(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol trunc_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fix(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fix_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol to_mkldnn(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol positive(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol neg(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol neg_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol negative(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol negative_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol setdefault(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bin(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pop(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol insert(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cat(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cat(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol concat(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol vstack(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol row_stack(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol unchecked_unwrap_optional(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __contains__(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol BailoutTemplate();  
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol zero_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fill_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol masked_fill_(); 
  @Namespace("c10::cuda") @MemberGetter public static native @Const @ByRef Symbol _set_device(); 
  @Namespace("c10::cuda") @MemberGetter public static native @Const @ByRef Symbol set_stream(); 
  @Namespace("c10::cuda") @MemberGetter public static native @Const @ByRef Symbol _current_device(); 
  @Namespace("c10::cuda") @MemberGetter public static native @Const @ByRef Symbol synchronize(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol swapaxes(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol swapaxes_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol swapdims(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol swapdims_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol movedim(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol moveaxis(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol polygamma(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_polygamma(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lgamma(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_gammaln(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logsumexp(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_logsumexp(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol digamma(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_psi(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_digamma(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol erf(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_erf(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol erfc(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_erfc(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_erfcx(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol erfinv(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_erfinv(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logit(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_logit(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sigmoid(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_expit(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol expm1(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_expm1(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol exp2(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_exp2(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log1p(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_log1p(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol round(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_round(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sinc(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_sinc(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol i0(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_i0(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_i0e(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_i1(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_i1e(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol xlogy(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_xlogy(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_xlog1py(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log_softmax(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_log_softmax(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_zeta(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol igamma(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol igamma_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_gammainc(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol igammac(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol igammac_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_gammaincc(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mvlgamma(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_multigammaln(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol has_torch_function(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardswish(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardswish_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardsigmoid_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardtanh_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __and__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __iand__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __ilshift__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __ior__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __irshift__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __ixor__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __lshift__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __or__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __rshift__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __xor__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _abs(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _addmv(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _addr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _amp_foreach_non_finite_check_and_unscale_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _amp_update_scale_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _arange(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _argmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _argmin(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _baddbmm_mkl(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Byte(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Char(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Double(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Float(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Half(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Int(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Long(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Short(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _ceil(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _clamp_max(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _clamp_min(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _convolution(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _convolution_double_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol convolution_overrideable(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol convolution_backward_overrideable(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _convolution_nogroup(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _copy_ignoring_overlaps(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cos(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cosh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _ctc_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _ctc_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cudnn_ctc_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cudnn_init_dropout_state(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cudnn_rnn(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cudnn_rnn_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cudnn_rnn_flatten_weight(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cufft_clear_plan_cache(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cufft_get_plan_cache_max_size(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cufft_get_plan_cache_size(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cufft_set_plan_cache_max_size(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _denseDims(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _dimI(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _dimV(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _dim_arange(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _dirichlet_grad(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _dot(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _embedding_bag(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _embedding_bag_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _embedding_bag_dense_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _embedding_bag_sparse_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _erf(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _erfc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _exp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _exp2(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _expm1(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fft_with_size(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fill(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _floor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fused_dropout(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _indices(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _ldexp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _linspace(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _local_scalar(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _local_scalar_dense(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _log(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _log10(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _log1p(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _log2(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _logspace(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _lu_with_info(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _masked_scale(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _mm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _mv(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _nnz(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _nansum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _pack_padded_sequence(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _pack_padded_sequence_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _pad_packed_sequence(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _pdist_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _pdist_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _prod(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _prodall(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _range(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _reshape_from_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _round(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _rsqrt(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _s_where(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _shape_as_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sigmoid(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sigmoid_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sin(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sinh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparseDims(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_add(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_addmm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_coo_tensor_with_dims(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_coo_tensor_with_dims_and_tensors(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_coo_tensor_unsafe(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_csr_tensor_unsafe(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_dense_add(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_div_scalar(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_div_zerodim(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_mul(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_mul_scalar(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_mul_zerodim(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_sum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sqrt(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _square(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _standard_gamma(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _standard_gamma_grad(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sum_cuda(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _tan(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _tanh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _tanh_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _th_get_device(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _th_kthvalue(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _th_prod(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _th_sigmoid(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _th_std(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _th_sum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _th_tanh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _th_var(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _thnn_fused_gru_cell(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _thnn_fused_gru_cell_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _thnn_fused_lstm_cell(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _thnn_fused_lstm_cell_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _trilinear(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _trunc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _unique(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _unique_dim(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _unsafe_view(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _validate_sparse_coo_tensor_args(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _values(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _weight_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _weight_norm_cuda_interface(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _weight_norm_cuda_interface_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _weight_norm_differentiable_backward();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_avg_pool1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_avg_pool2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_avg_pool2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_avg_pool2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_avg_pool3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_avg_pool3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_avg_pool3d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_max_pool1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_max_pool2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_max_pool2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_max_pool2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_max_pool3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_max_pool3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_max_pool3d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol add(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol add_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addbmm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addcdiv(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addcmul(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addmm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addmv(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol affine_grid_generator(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol affine_grid_generator_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol alias(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol all(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol allclose(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol alpha_dropout(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol any(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arange(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol argmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol argmin(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol amax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol amin(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol aminmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol as_strided(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol as_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atan2(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atleast_1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atleast_2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atleast_3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol avg_pool1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol avg_pool2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol avg_pool2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol avg_pool2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol avg_pool3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol avg_pool3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol avg_pool3d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol baddbmm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bartlett_window(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol batch_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bernoulli(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bilinear(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol binary_cross_entropy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol binary_cross_entropy_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol binary_cross_entropy_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol binary_cross_entropy_with_logits(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol binary_cross_entropy_with_logits_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol binary_cross_entropy_with_logits_target_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bincount(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol blackman_window(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol block_diag(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bmm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol broadcast_tensors(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol broadcast_to(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cartesian_prod(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cauchy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ceil(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol celu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cholesky(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cholesky_inverse(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cholesky_solve(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol chunk(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol clamp_max(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol clamp_min(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef @Name("clone") Symbol _clone(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol coalesce(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol combinations(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _conj(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conj(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conj_physical(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conj_physical_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol resolve_conj(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol resolve_neg(); 
 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol copysign(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol polar(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol constant_pad_nd(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol contiguous(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conv1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conv2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conv3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conv_tbc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conv_tbc_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conv_transpose1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol convolution(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol copy_sparse_to_sparse(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol corrcoef(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cos(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cosh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cosine_embedding_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cosine_similarity(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol count_nonzero(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cross(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cov(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol std_mean(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol var_mean(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ctc_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_affine_grid_generator(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_affine_grid_generator_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_batch_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_batch_norm_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution_backward_bias(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution_backward_input(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution_backward_weight(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution_transpose(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution_transpose_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution_transpose_backward_bias(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution_transpose_backward_input(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution_transpose_backward_weight(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution_relu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution_add_relu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_grid_sampler(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_grid_sampler_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_is_acceptable(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cummax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cummin(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cumprod(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cumsum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol data_ptr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol deg2rad(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol detach(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol diag(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol diag_embed(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol diagflat(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol diagonal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fill_diagonal_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol diff(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol frexp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol dim(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol dist(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol dot(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol dropout(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol dsplit(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol dstack(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol eig(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol einsum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol elu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol elu_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol elu_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol embedding(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol embedding_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol embedding_bag(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol embedding_dense_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol embedding_renorm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol embedding_sparse_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol empty(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol empty_like(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol empty_strided(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_entr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol eq(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol equal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol exp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol expand(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol expand_as(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol exponential(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol eye(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol feature_alpha_dropout(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol feature_dropout(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol flatten(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol flip(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fliplr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol flipud(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol floor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fmod(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fmod_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fmin(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol frac(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fractional_max_pool2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fractional_max_pool2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fractional_max_pool2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol frobenius_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol full(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol full_like(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gather(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gcd(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gelu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol geometric(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol geqrf(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol get_device(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol glu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol glu_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol glu_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gradient(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol grid_sampler(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol grid_sampler_2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol grid_sampler_2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol grid_sampler_3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol grid_sampler_3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol group_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gru(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gru_cell(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hamming_window(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hann_window(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardshrink(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardshrink_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardsigmoid(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardsigmoid_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardtanh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardtanh_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardtanh_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol heaviside(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hinge_embedding_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol histc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol histogram(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hspmm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hsplit(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hstack(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hypot(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol i0_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ifft(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_add(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_fill(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_put(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_select(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol indices(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol inner(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol instance_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol inverse(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol irfft(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_coalesced(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_complex(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_contiguous();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_mlc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_ort(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_distributed(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_floating_point(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_inference(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_nonzero(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_same_size(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_set_to(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_signed(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_sparse(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_sparse_csr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol isclose(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol isreal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol istft(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol isposinf(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol isneginf(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol kaiser_window(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol kl_div(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol kl_div_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol kthvalue(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol l1_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol l1_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol l1_loss_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol layer_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lcm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol leaky_relu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol leaky_relu_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol leaky_relu_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lerp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linear(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linspace(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log10(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log2(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log_normal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log_sigmoid(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log_sigmoid_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log_sigmoid_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _log_softmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _log_softmax_backward_data(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logcumsumexp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logdet(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logspace(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lstm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lstm_cell(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lstsq(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lu_solve(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol margin_ranking_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol masked_fill(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol masked_scatter(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol masked_select(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol matrix_rank(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol matrix_exp();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool1d_with_indices(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool2d_with_indices(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool2d_with_indices_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool2d_with_indices_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool3d_with_indices(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool3d_with_indices_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool3d_with_indices_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_unpool2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_unpool2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_unpool2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_unpool3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_unpool3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_unpool3d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_values(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mean(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nanmean(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol median(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nanmedian(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol meshgrid();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol min_values(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_batch_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_batch_norm_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_convolution(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_convolution_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_convolution_backward_bias(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_convolution_backward_input(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_convolution_backward_weight(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_convolution_transpose(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_convolution_transpose_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_convolution_transpose_backward_input(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_convolution_transpose_backward_weight(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_depthwise_convolution(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_depthwise_convolution_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_depthwise_convolution_backward_input(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_depthwise_convolution_backward_weight(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_rnn(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_rnn_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mish(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mkldnn_convolution(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mkldnn_convolution_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mkldnn_convolution_backward_input(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mkldnn_convolution_backward_weights(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mode(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mse_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mse_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mse_loss_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol msort(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multi_margin_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multi_margin_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multi_margin_loss_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multilabel_margin_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multilabel_margin_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multilabel_margin_loss_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multinomial(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mv(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nansum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nan_to_num(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol narrow(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol narrow_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_batch_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_batch_norm_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_clone(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_get_device(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_pow(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_resize_as(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_zero(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_ndtr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nextafter(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_ndtri(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logical_and(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logical_not(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logical_or(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logical_xor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bitwise_and(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bitwise_not(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bitwise_or(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bitwise_xor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol element_size(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nll_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nll_loss2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nll_loss2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nll_loss2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nll_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nll_loss_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nonzero(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nonzero_numpy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol norm_except_dim(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol normal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nuclear_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol numel(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ones(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ones_like(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ormqr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pairwise_distance(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _euclidean_dist(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pdist(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cdist(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol permute(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pin_memory(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pinverse(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pixel_shuffle(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pixel_unshuffle(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol poisson(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pow(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol float_power(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol prelu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol prelu_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol prod(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol put(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol qr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol quantile(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nanquantile(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rad2deg(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rand(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rand_like(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol randint(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol randint_like(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol randn(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol randn_like(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol random(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol randperm();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ravel(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reciprocal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reflection_pad1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reflection_pad1d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reflection_pad1d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reflection_pad2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reflection_pad2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reflection_pad2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reflection_pad3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reflection_pad3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reflection_pad3d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol relu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol remainder(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol renorm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol repeat(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad1d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad1d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad3d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reshape(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reshape_as(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol resize(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol resize_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol resize_as(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol resize_as_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rfft(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rnn_relu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rnn_relu_cell(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rnn_tanh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rnn_tanh_cell(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rot90(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rrelu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rrelu_with_noise(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rrelu_with_noise_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rrelu_with_noise_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rsqrt(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol scatter(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol scatter_add(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol segment_reduce(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol select(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol selu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol set(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sign(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol signbit(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol silu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sgn(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sin(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sinh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol size(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sizes(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slice(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slogdet(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol smm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol smooth_l1_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol smooth_l1_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol smooth_l1_loss_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol soft_margin_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol soft_margin_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol soft_margin_loss_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol softmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _softmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _softmax_backward_data(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol softplus(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol softplus_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol softplus_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol softshrink(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol softshrink_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol softshrink_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol solve(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sort(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sparse_coo_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sparse_csr_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sparse_mask(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sparse_resize(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sparse_resize_and_clear(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol split(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol split_with_sizes(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sqrt(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol square(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol squeeze(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sspaddmm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol stack(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol std(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol stft(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol storage_offset(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol stride(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol strides(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rsub(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sum_to_size(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol svd(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol symeig(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol t(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol take(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol take_along_dim(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tan(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tanh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tanh_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tensordot(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tensor_split(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol th_clone(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol th_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol th_pow(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol th_resize_as(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol th_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol th_zero(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol thnn_conv2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _slow_conv2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _slow_conv2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tile(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv3d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol thnn_conv_depthwise2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol thnn_conv_depthwise2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol thnn_conv_depthwise2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv_dilated2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv_dilated2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv_dilated3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv_dilated3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv_transpose2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv_transpose2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv_transpose3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv_transpose3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol threshold(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol threshold_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol to(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol to_sparse(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol to_dense(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol topk(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol trace(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol triangular_solve(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tril(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol triplet_margin_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol triu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol type_as(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unbind(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unfold(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol uniform(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unsafe_chunk(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unsafe_split(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unsafe_split_with_sizes(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unsqueeze(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_bilinear2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_bilinear2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_bilinear2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_bicubic2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_bicubic2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_bicubic2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_linear1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_linear1d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_linear1d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest1d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest1d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest3d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_trilinear3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_trilinear3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_trilinear3d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol values(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol vander(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol var(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol view(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol view_as(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol vsplit(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol where(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef @Name("zero") Symbol _zero(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol zeros(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol zeros_like(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol real(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol imag(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol view_as_real(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol view_as_complex(); 
/* nothing */
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Add(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Concat();  
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol ConstantFill(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Div(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol GRU(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Gather(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Gemm(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol LSTM(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol MatMul(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Mul(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Pow(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol RNN(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Shape(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Size(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Slice(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Softmax(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Squeeze(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Sub(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Transpose(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Unsqueeze();   
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Reshape();  
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Equal(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Greater(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol GreaterOrEqual(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Less(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol LessOrEqual(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Not(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol ATen(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Split(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol ConstantOfShape(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Cast(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Mod(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Sqrt(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol SplitToSequence(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol SequenceAt(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol SequenceConstruct(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol SequenceEmpty(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol SequenceInsert(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol SequenceErase(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol ConcatFromSequence(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Identity(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol SoftmaxCrossEntropyLoss(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol NegativeLogLikelihoodLoss(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol LogSoftmax(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol ReduceL1(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol ReduceL2(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Conv(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol BatchNormalization(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol ReduceMean(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol ReduceProd(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Relu(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Neg(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol NonZero(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Range(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Tile(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Where(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol A(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol C(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol H(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol LU_data(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol LU_pivots(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol N(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol W(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol accumulate(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol align_corners(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol alpha(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol anchor(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol argmaxes(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol atol(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol b_hh(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol b_ih(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol bag_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol base(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol batch1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol batch2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol batch_first(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol batch_sizes(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol benchmark(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol beta(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol bias(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol bias_defined(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol bidirectional(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol bins(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol blank(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol buffer(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ceil_mode(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol checked_signal_sizes(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol chunks(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol columns(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol column_stack(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol complex_input(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol complex_output(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol condition(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol count_include_pad(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cudnn_enable(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cudnn_enabled(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cx(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cy(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol data(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dense_dim(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol descending(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol deterministic();   
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dilation();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dim0(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dim1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dim2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dimension(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dims(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dims_other(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dims_self(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol divisor_override();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dropout_seed(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dropout_state();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol eigenvectors(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol end(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol end_dim(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol eps(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol epsilon(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol equal_nan(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol equation(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol expand1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol expand2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol expand3(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol exponent(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol exponential_average_factor(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol fgrad_input(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol fill_value(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol finput(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol from(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol g(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol gO(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol generator(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ggI(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ggW(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ggb();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol gradOutput(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_bias(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_cy(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_hy(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_input(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_out(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_output(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_w(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_weight(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grid(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol groups(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol has_bias(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol has_biases(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol hidden_bias(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol hidden_gates(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol hidden_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol high(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol hop_length(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol hx(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol i1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol i2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol i3(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ignore_index(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol implicit();   
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol info(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input3(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input_bias(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input_gates(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input_lengths(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input_scale(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol interpolation_mode();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol is_target(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol k(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol keepdim(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol kernel_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol lambd(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol largest();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol left(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol length(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol lengths(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol like(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol log_alpha(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol log_probs(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol low(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol lower(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol lu(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol m(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol margin(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol mask(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol mat(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol mat1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol mat2();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol max_indices(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol max_norm(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol max_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol max_val();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol maximum_indices(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol maxnorm(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol maximum();     
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol min_indices(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol min_val(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol minlength(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol minimum();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol momentum(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol n(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol n_fft(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol neg_log_likelihood();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol negative_slope(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol noise(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol non_blocking(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol norm_type(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol normalized(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol normalized_shape(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol num_groups(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol num_layers(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol num_samples(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol num_weights(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol offset(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol offset2bag(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol offsets();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol onesided(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol options(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol other(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol output(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol output_mask(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol output_padding(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol output_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol output_sizes(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol p(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol pad(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol padding(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol padding_idx(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol padding_mode(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol padding_value(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol params();     
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol periodic(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol pivot(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol pivots(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol pooledHeight(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol pooledWidth();   
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol random_samples(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol rcond(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol reduction(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol repeats(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol replacement(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol res1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol res2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol res3(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol reserve(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol result(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol return_inverse(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol rois(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol rtol(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol running_mean(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol running_var(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol save_mean(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol save_std(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol save_var(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol saved_g(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol saved_norms(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol saved_v(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol scale(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol scale_grad_by_freq(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol self(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol self_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol self_ty(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol shape(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol sigma(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol signal_ndim(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol signal_sizes();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol solution(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol some();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol source(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol sparse(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol sparse_dim(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol sparse_dtype(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol spatialScale(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol split_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol split_sizes(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol src(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol start(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol start_dim();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol step(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol steps(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol storage(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol storageOffset();   
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol sumdim(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol swap(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol symmetric(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol target(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol target_lengths(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol targets();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol tensor1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol tensor2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol tensors(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol the_template(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol theta();   
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol tol(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol total(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol total_length(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol total_weight(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol train(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol training();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol transposed(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol unbiased(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol unitriangular(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol unroll_dim(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol upper(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol upscale_factor(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol use_input_stats(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol v(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol value();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol vec(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol vec1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol vec2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol w_hh(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol w_ih(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weight(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weight_arr(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weight_buf(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weight_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weight_stride0(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weights(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol win_length(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol window(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol window_length(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol workspace(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol x(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol x1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol x2(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol Subgraph(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ReverseSubgraph(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol f_real_outputs(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol df_input_vjps(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol df_input_captured_inputs(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol df_input_captured_outputs(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol df_output_vjps(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol axes(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol axis(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol broadcast(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol direction(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ends(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol inplace(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input_as_shape(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol is_zero(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol num_none(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol num_present(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol perm();  
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol starts(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol profiled_type(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol transA(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol transB(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol name(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol a(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol b(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol beg(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol idx();  
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol slot(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol kinds(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol types();  
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol keepdims(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cache_id(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol new_axis(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol warn_id(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol allowzero(); 
// #undef DEFINE_SYMBOL




















// Targeting ../SymbolHash.java





// Parsed from ATen/core/grad_mode.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/core/GradMode.h>



// Parsed from ATen/core/ATenGeneral.h

// #pragma once

// #include <c10/macros/Macros.h>


// Parsed from ATen/core/Dimname.h

// #pragma once

// #include <ATen/core/interned_strings.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Optional.h>
// #include <ostream>

@Namespace("at") public enum NameType { BASIC((byte)(0)), WILDCARD((byte)(1));

    public final byte value;
    private NameType(byte v) { this.value = v; }
    private NameType(NameType e) { this.value = e.value; }
    public NameType intern() { for (NameType e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../Dimname.java



@Namespace("at") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Dimname dimname);

@Namespace("at") public static native @Cast("bool") @Name("operator ==") boolean equals(@Const @ByRef Dimname lhs, @Const @ByRef Dimname rhs);

@Namespace("at") public static native @Cast("bool") @Name("operator !=") boolean notEquals(@Const @ByRef Dimname lhs, @Const @ByRef Dimname rhs);

 // namespace at


// Parsed from ATen/core/DimVector.h

// #pragma once

// #include <c10/util/SmallVector.h>
// #include <stdint.h>

@Namespace("at") @MemberGetter public static native @Cast("const size_t") long kDimVectorStaticSize();

/** A container for sizes or strides */

 // namespace at


// Parsed from ATen/core/Generator.h

// #pragma once

// #include <stdint.h>
// #include <mutex>
// #include <deque>
// #include <atomic>
// #include <typeinfo>
// #include <utility>
// #include <cstddef>

// #include <c10/util/Exception.h>
// #include <c10/util/C++17.h>
// #include <c10/util/intrusive_ptr.h>
// #include <c10/core/Device.h>
// #include <c10/core/DispatchKeySet.h>

// For the record I don't think this is a correct pimpl idiom.
// Including Impl header in interface header defeats the purpose
// because you can't change Impl private members without forcing
// everything that included the interface to rebuild.
// Impl should be forward-declared in the interface header instead.
// #include <c10/core/GeneratorImpl.h>
// Targeting ../Generator.java



/**
 * Helper function for checking the validity of new random generator
 * state. Right now following conditions are checked:
 *
 * - The new state tensor must be a torch.ByteTensor
 * - Data of the new state tensor must be contiguous
 */
@Namespace("at::detail") public static native void check_rng_state(@Const @ByRef TensorImpl new_state);

 // namespace detail

 // namespace at


// Parsed from ATen/core/List.h

// #pragma once

// #include <ATen/core/ivalue_to.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/TypeTraits.h>
// #include <c10/util/TypeList.h>
// #include <c10/util/intrusive_ptr.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Optional.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <vector>

// Targeting ../ListImpl.java







// Targeting ../ListElementConstReferenceTraits.java



// this wraps vector::iterator to make sure user code can't rely
// on it being the type of the underlying vector.
  

// Parsed from ATen/core/NamedTensor.h

// #pragma once

// #include <ATen/core/Dimname.h>
// #include <c10/core/TensorImpl.h>
// #include <c10/util/C++17.h>
// Targeting ../NamedTensorMeta.java


// Targeting ../NamesMode.java


// Targeting ../NoNamesGuard.java






// Sets the names of `tensor` to be `names`.
@Namespace("at") public static native @Const @ByRef TensorBase internal_set_names_inplace(@Const @ByRef TensorBase tensor, @ByVal DimnameListOptional names);
@Namespace("at") public static native @Const @ByRef TensorBase internal_set_names_inplace(@Const @ByRef TensorBase tensor, @StdMove DimnameVector names, @Cast("bool") boolean validate_names);

@Namespace("at") @MemberGetter public static native @Cast("const size_t") long kMaxNamedTensorDim();



// Some helper functions on TensorImpl. Useful for working with names in TH.
// XXX: Ideally these would exist as methods on TensorImpl
@Namespace("at::impl") public static native void internal_set_names_inplace(TensorImpl impl, @ByVal DimnameListOptional names, @Cast("bool") boolean validate_names);
@Namespace("at::impl") public static native void internal_set_names_inplace(TensorImpl impl, @StdMove DimnameVector names, @Cast("bool") boolean validate_names);



// Returns true if the tensor's names exist and are not all 'None'.
// Returns false if the tensor's names don't exist (were not allocated),
// or if all names are 'None'.
// We treat not-allocated-names the same as allocated names that are all 'None'.
@Namespace("at::impl") public static native @Cast("bool") boolean has_names(@Const TensorImpl impl);

// Returns the names of the tensor's dimensions.
// Unnamed tensors are treated as having 'None' in all dimension; this method
// would return a DimnameList of all 'None's for an unnamed tensor.
@Namespace("at::impl") public static native @ByVal DimnameArrayRef get_names(@Const TensorImpl impl);

// This is more of an implementation detail; one should use impl::get_names /
// Tensor::names() whenever possible because it provides a cleaner API.
// Returns the names of the tensor if they have been allocated; returns nullopt
// instead if the haven't been. The names of a tensor are not allocated if a
// tensor is constructed with names=None.
@Namespace("at::impl") public static native @ByVal DimnameListOptional get_opt_names(@Const TensorImpl impl);

 // namespace impl

 // namespace at


// Parsed from ATen/core/Reduction.h

// #pragma once

// NB: Keep this in sync with Reduction class in torch/nn/_reduction.py
// These constants control the reduction behavior of loss functions.
// Ideally, this would be a scoped enum, but jit doesn't support that
@Namespace("at::Reduction") public enum Reduction {
  None(0),             // Do not reduce
  Mean(1),             // (Possibly weighted) mean of losses
  Sum(2),              // Sum losses
  END(3);

    public final int value;
    private Reduction(int v) { this.value = v; }
    private Reduction(Reduction e) { this.value = e.value; }
    public Reduction intern() { for (Reduction e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
 // namespace Reduction
 // namespace at


// Parsed from ATen/core/Scalar.h

// #include <c10/core/Scalar.h>


// Parsed from ATen/core/TensorAccessor.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Exception.h>
// #include <stdint.h>
// #include <cstddef>

// The PtrTraits argument to the TensorAccessor/GenericPackedTensorAccessor
// is used to enable the __restrict__ keyword/modifier for the data
// passed to cuda.

// #if defined(__CUDACC__) || defined(__HIPCC__)
// #endif

// TensorAccessorBase and TensorAccessor are used for both CPU and CUDA tensors.
// For CUDA tensors it is used in device code (only). This means that we restrict ourselves
// to functions and types available there (e.g. IntArrayRef isn't).

// The PtrTraits argument is only relevant to cuda to support `__restrict__` pointers.

// The `TensorAccessor` is typically instantiated for CPU `Tensor`s using
// `Tensor.accessor<T, N>()`.
// For CUDA `Tensor`s, `GenericPackedTensorAccessor` is used on the host and only
// indexing on the device uses `TensorAccessor`s.


// GenericPackedTensorAccessorBase and GenericPackedTensorAccessor are used on for CUDA `Tensor`s on the host
// and as
// In contrast to `TensorAccessor`s, they copy the strides and sizes on instantiation (on the host)
// in order to transfer them on the device when calling kernels.
// On the device, indexing of multidimensional tensors gives to `TensorAccessor`s.
// Use RestrictPtrTraits as PtrTraits if you want the tensor's data pointer to be marked as __restrict__.
// Instantiation from data, sizes, strides is only needed on the host and std::copy isn't available
// on the device, so those functions are host only.


// Can't put this directly into the macro function args because of commas
// #define AT_X GenericPackedTensorAccessor<T, N, PtrTraits, index_t>

// Old name for `GenericPackedTensorAccessor`
// #undef AT_X
 // namespace at


// Parsed from ATen/core/TensorBase.h

// #pragma once

// #include <c10/core/Device.h>
// #include <c10/core/Layout.h>
// #include <c10/core/MemoryFormat.h>
// #include <c10/core/ScalarType.h>
// #include <c10/core/ScalarTypeToTypeMeta.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorImpl.h>
// #include <c10/core/UndefinedTensorImpl.h>
// #include <c10/core/WrapDimMinimal.h>
// #include <c10/util/Exception.h>
// #include <c10/util/MaybeOwned.h>
// #include <c10/util/Optional.h>
// #include <c10/util/intrusive_ptr.h>

// #include <ATen/core/NamedTensor.h>
// #include <ATen/core/QuantizerBase.h>
// #include <ATen/core/TensorAccessor.h>


 // namespace torch::autograd

// Convert Tensor to TensorBase without any need to include Tensor.h
@Namespace("at") public static native @Const @ByRef TensorBase get_tensor_base(@Const @ByRef Tensor t);
@Namespace("at::impl") public static native @Cast("bool") boolean variable_excluded_from_dispatch();

// Targeting ../TensorBase.java








// Helper creator for Tensor class which doesn't requires the users to pass
// in an intrusive_ptr instead it just converts the argument passed to
// requested intrusive_ptr type.

 // namespace detail

@Namespace("at") public static native DispatchKey legacyExtractDispatchKey(@Const @ByRef TensorBase t);


// Targeting ../MaybeOwnedTraits.java


 // namespace c10




 // namespace at


// Parsed from ATen/core/TensorBody.h

// #pragma once

// #include <ATen/Operators.h>
// #include <c10/core/Device.h>
// #include <c10/core/Layout.h>
// #include <c10/core/MemoryFormat.h>
// #include <c10/core/QScheme.h>
// #include <c10/core/Stream.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/ScalarType.h>
// #include <c10/core/ScalarTypeToTypeMeta.h>
// #include <c10/core/Storage.h>
// #include <ATen/core/TensorAccessor.h>
// #include <c10/core/TensorImpl.h>
// #include <c10/core/UndefinedTensorImpl.h>
// #include <c10/core/WrapDimMinimal.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/MaybeOwned.h>
// #include <c10/util/Optional.h>
// #include <c10/util/intrusive_ptr.h>
// #include <ATen/core/DeprecatedTypePropertiesRegistry.h>
// #include <ATen/core/DeprecatedTypeProperties.h>
// #include <ATen/core/NamedTensor.h>
// #include <ATen/core/QuantizerBase.h>
// #include <ATen/core/TensorBase.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>

// Targeting ../DeprecatedTypeProperties.java


 // namespace at
 // namespace indexing
 // namespace at

 // namespace torch::autograd
// Targeting ../Tensor.java


// Helper creator for Tensor class which doesn't requires the users to pass
// in an intrusive_ptr instead it just converts the argument passed to
// requested intrusive_ptr type.

 // namespace detail

 // namespace at

// See Note [Avoiding Include Cycles In Static Dispatch]

// aten::_backward(Tensor self, Tensor[] inputs, Tensor? gradient=None, bool? retain_graph=None, bool create_graph=False) -> ()


// aten::set_data(Tensor(a!) self, Tensor new_data) -> ()


// aten::data(Tensor self) -> Tensor


// aten::is_leaf(Tensor self) -> bool


// aten::output_nr(Tensor self) -> int


// aten::_version(Tensor self) -> int


// aten::requires_grad_(Tensor(a!) self, bool requires_grad=True) -> Tensor(a!)


// aten::retain_grad(Tensor(a!) self) -> ()


// aten::retains_grad(Tensor self) -> bool


// aten::_fw_primal(Tensor(a) self, int level) -> Tensor(a)


// aten::rename_(Tensor(a!) self, Dimname[]? names) -> Tensor(a!)


// aten::rename(Tensor(a) self, Dimname[]? names) -> Tensor(a)


// aten::align_to(Tensor(a) self, Dimname[] names) -> Tensor(a)


// aten::align_to.ellipsis_idx(Tensor(a) self, Dimname[] order, int ellipsis_idx) -> Tensor(a)


// aten::align_as(Tensor self, Tensor other) -> Tensor


// aten::refine_names(Tensor(a) self, Dimname[] names) -> Tensor(a)


// aten::abs(Tensor self) -> Tensor


// aten::abs_(Tensor(a!) self) -> Tensor(a!)


// aten::absolute(Tensor self) -> Tensor


// aten::absolute_(Tensor(a!) self) -> Tensor(a!)


// aten::angle(Tensor self) -> Tensor


// aten::sgn(Tensor self) -> Tensor


// aten::sgn_(Tensor(a!) self) -> Tensor(a!)


// aten::_conj(Tensor(a) self) -> Tensor(a)


// aten::conj(Tensor(a) self) -> Tensor(a)


// aten::_conj_physical(Tensor self) -> Tensor


// aten::conj_physical(Tensor self) -> Tensor


// aten::conj_physical_(Tensor(a!) self) -> Tensor(a!)


// aten::resolve_conj(Tensor(a) self) -> Tensor(a)


// aten::resolve_neg(Tensor(a) self) -> Tensor(a)


// aten::_neg_view(Tensor(a) self) -> Tensor(a)


// aten::acos(Tensor self) -> Tensor


// aten::acos_(Tensor(a!) self) -> Tensor(a!)


// aten::arccos(Tensor self) -> Tensor


// aten::arccos_(Tensor(a!) self) -> Tensor(a!)


// aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor


// aten::add_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)


// aten::add.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor


// aten::add_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)


// aten::addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor


// aten::addmv_(Tensor(a!) self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)


// aten::addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor


// aten::addr_(Tensor(a!) self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)


// aten::all.dim(Tensor self, int dim, bool keepdim=False) -> Tensor


// aten::all.dimname(Tensor self, Dimname dim, bool keepdim=False) -> Tensor


// aten::allclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> bool


// aten::any.dim(Tensor self, int dim, bool keepdim=False) -> Tensor


// aten::any.dimname(Tensor self, Dimname dim, bool keepdim=False) -> Tensor


// aten::argmax(Tensor self, int? dim=None, bool keepdim=False) -> Tensor


// aten::argmin(Tensor self, int? dim=None, bool keepdim=False) -> Tensor


// aten::acosh(Tensor self) -> Tensor


// aten::acosh_(Tensor(a!) self) -> Tensor(a!)


// aten::arccosh(Tensor self) -> Tensor


// aten::arccosh_(Tensor(a!) self) -> Tensor(a!)


// aten::asinh(Tensor self) -> Tensor


// aten::asinh_(Tensor(a!) self) -> Tensor(a!)


// aten::arcsinh(Tensor self) -> Tensor


// aten::arcsinh_(Tensor(a!) self) -> Tensor(a!)


// aten::atanh(Tensor self) -> Tensor


// aten::atanh_(Tensor(a!) self) -> Tensor(a!)


// aten::arctanh(Tensor self) -> Tensor


// aten::arctanh_(Tensor(a!) self) -> Tensor(a!)


// aten::as_strided(Tensor(a) self, int[] size, int[] stride, int? storage_offset=None) -> Tensor(a)


// aten::as_strided_(Tensor(a!) self, int[] size, int[] stride, int? storage_offset=None) -> Tensor(a!)


// aten::asin(Tensor self) -> Tensor


// aten::asin_(Tensor(a!) self) -> Tensor(a!)


// aten::arcsin(Tensor self) -> Tensor


// aten::arcsin_(Tensor(a!) self) -> Tensor(a!)


// aten::atan(Tensor self) -> Tensor


// aten::atan_(Tensor(a!) self) -> Tensor(a!)


// aten::arctan(Tensor self) -> Tensor


// aten::arctan_(Tensor(a!) self) -> Tensor(a!)


// aten::baddbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor


// aten::baddbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)


// aten::bernoulli(Tensor self, *, Generator? generator=None) -> Tensor


// aten::bernoulli_.Tensor(Tensor(a!) self, Tensor p, *, Generator? generator=None) -> Tensor(a!)


// aten::bernoulli_.float(Tensor(a!) self, float p=0.5, *, Generator? generator=None) -> Tensor(a!)


// aten::bernoulli.p(Tensor self, float p, *, Generator? generator=None) -> Tensor


// aten::bincount(Tensor self, Tensor? weights=None, int minlength=0) -> Tensor


// aten::bitwise_not(Tensor self) -> Tensor


// aten::bitwise_not_(Tensor(a!) self) -> Tensor(a!)


// aten::copysign.Tensor(Tensor self, Tensor other) -> Tensor


// aten::copysign_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::copysign.Scalar(Tensor self, Scalar other) -> Tensor


// aten::copysign_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::logical_not(Tensor self) -> Tensor


// aten::logical_not_(Tensor(a!) self) -> Tensor(a!)


// aten::logical_xor(Tensor self, Tensor other) -> Tensor


// aten::logical_xor_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::logical_and(Tensor self, Tensor other) -> Tensor


// aten::logical_and_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::logical_or(Tensor self, Tensor other) -> Tensor


// aten::logical_or_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bmm(Tensor self, Tensor mat2) -> Tensor


// aten::broadcast_to(Tensor(a) self, int[] size) -> Tensor(a)


// aten::ceil(Tensor self) -> Tensor


// aten::ceil_(Tensor(a!) self) -> Tensor(a!)


// aten::unsafe_chunk(Tensor self, int chunks, int dim=0) -> Tensor[]


// aten::chunk(Tensor(a) self, int chunks, int dim=0) -> Tensor(a)[]


// aten::tensor_split.sections(Tensor(a) self, int sections, int dim=0) -> Tensor(a)[]


// aten::tensor_split.indices(Tensor(a) self, int[] indices, int dim=0) -> Tensor(a)[]


// aten::tensor_split.tensor_indices_or_sections(Tensor(a) self, Tensor tensor_indices_or_sections, int dim=0) -> Tensor(a)[]


// aten::clamp(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor


// aten::clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor


// aten::clamp_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)


// aten::clamp_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)


// aten::clamp_max(Tensor self, Scalar max) -> Tensor


// aten::clamp_max.Tensor(Tensor self, Tensor max) -> Tensor


// aten::clamp_max_(Tensor(a!) self, Scalar max) -> Tensor(a!)


// aten::clamp_max_.Tensor(Tensor(a!) self, Tensor max) -> Tensor(a!)


// aten::clamp_min(Tensor self, Scalar min) -> Tensor


// aten::clamp_min.Tensor(Tensor self, Tensor min) -> Tensor


// aten::clamp_min_(Tensor(a!) self, Scalar min) -> Tensor(a!)


// aten::clamp_min_.Tensor(Tensor(a!) self, Tensor min) -> Tensor(a!)


// aten::clip(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor


// aten::clip.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor


// aten::clip_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)


// aten::clip_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)


// aten::contiguous(Tensor(a) self, *, MemoryFormat memory_format=contiguous_format) -> Tensor(a)


// aten::copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)


// aten::cos(Tensor self) -> Tensor


// aten::cos_(Tensor(a!) self) -> Tensor(a!)


// aten::cosh(Tensor self) -> Tensor


// aten::cosh_(Tensor(a!) self) -> Tensor(a!)


// aten::count_nonzero.dim_IntList(Tensor self, int[] dim) -> Tensor


// aten::count_nonzero(Tensor self, int? dim=None) -> Tensor


// aten::cov(Tensor self, *, int correction=1, Tensor? fweights=None, Tensor? aweights=None) -> Tensor


// aten::corrcoef(Tensor self) -> Tensor


// aten::cummax(Tensor self, int dim) -> (Tensor values, Tensor indices)


// aten::cummax.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)


// aten::cummin(Tensor self, int dim) -> (Tensor values, Tensor indices)


// aten::cummin.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)


// aten::cumprod(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor


// aten::cumprod_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -> Tensor(a!)


// aten::cumprod.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor


// aten::cumprod_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -> Tensor(a!)


// aten::cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor


// aten::cumsum_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -> Tensor(a!)


// aten::cumsum.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor


// aten::cumsum_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -> Tensor(a!)


// aten::diag_embed(Tensor self, int offset=0, int dim1=-2, int dim2=-1) -> Tensor


// aten::diagflat(Tensor self, int offset=0) -> Tensor


// aten::diagonal(Tensor(a) self, int offset=0, int dim1=0, int dim2=1) -> Tensor(a)


// aten::diagonal.Dimname(Tensor(a) self, *, Dimname outdim, Dimname dim1, Dimname dim2, int offset=0) -> Tensor(a)


// aten::fill_diagonal_(Tensor(a!) self, Scalar fill_value, bool wrap=False) -> Tensor(a!)


// aten::diff(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None) -> Tensor


// aten::div.Tensor(Tensor self, Tensor other) -> Tensor


// aten::div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::div.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor


// aten::div_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)


// aten::div.Scalar(Tensor self, Scalar other) -> Tensor


// aten::div_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::div.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor


// aten::div_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)


// aten::divide.Tensor(Tensor self, Tensor other) -> Tensor


// aten::divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::divide.Scalar(Tensor self, Scalar other) -> Tensor


// aten::divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::divide.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor


// aten::divide_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)


// aten::divide.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor


// aten::divide_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)


// aten::true_divide.Tensor(Tensor self, Tensor other) -> Tensor


// aten::true_divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::true_divide.Scalar(Tensor self, Scalar other) -> Tensor


// aten::true_divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::dot(Tensor self, Tensor tensor) -> Tensor


// aten::vdot(Tensor self, Tensor other) -> Tensor


// aten::new_empty(Tensor self, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_empty(Tensor self, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_empty_strided(Tensor self, int[] size, int[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_empty_strided(Tensor self, int[] size, int[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_full(Tensor self, int[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_full(Tensor self, int[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_zeros(Tensor self, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_zeros(Tensor self, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_ones(Tensor self, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_ones(Tensor self, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::resize_(Tensor(a!) self, int[] size, *, MemoryFormat? memory_format=None) -> Tensor(a!)


// aten::erf(Tensor self) -> Tensor


// aten::erf_(Tensor(a!) self) -> Tensor(a!)


// aten::erfc(Tensor self) -> Tensor


// aten::erfc_(Tensor(a!) self) -> Tensor(a!)


// aten::exp(Tensor self) -> Tensor


// aten::exp_(Tensor(a!) self) -> Tensor(a!)


// aten::exp2(Tensor self) -> Tensor


// aten::exp2_(Tensor(a!) self) -> Tensor(a!)


// aten::expm1(Tensor self) -> Tensor


// aten::expm1_(Tensor(a!) self) -> Tensor(a!)


// aten::expand(Tensor(a) self, int[] size, *, bool implicit=False) -> Tensor(a)


// aten::expand_as(Tensor(a) self, Tensor other) -> Tensor(a)


// aten::flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -> Tensor(a)


// aten::flatten.named_out_dim(Tensor(a) self, int start_dim, int end_dim, Dimname out_dim) -> Tensor(a)


// aten::flatten.using_names(Tensor(a) self, Dimname start_dim, Dimname end_dim, Dimname out_dim) -> Tensor(a)


// aten::flatten.DimnameList(Tensor(a) self, Dimname[] dims, Dimname out_dim) -> Tensor(a)


// aten::unflatten.int(Tensor(a) self, int dim, int[] sizes, Dimname[]? names=None) -> Tensor(a)


// aten::unflatten.Dimname(Tensor(a) self, Dimname dim, int[] sizes, Dimname[] names) -> Tensor(a)


// aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)


// aten::fill_.Tensor(Tensor(a!) self, Tensor value) -> Tensor(a!)


// aten::floor(Tensor self) -> Tensor


// aten::floor_(Tensor(a!) self) -> Tensor(a!)


// aten::floor_divide(Tensor self, Tensor other) -> Tensor


// aten::floor_divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::floor_divide.Scalar(Tensor self, Scalar other) -> Tensor


// aten::floor_divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::frac(Tensor self) -> Tensor


// aten::frac_(Tensor(a!) self) -> Tensor(a!)


// aten::gcd(Tensor self, Tensor other) -> Tensor


// aten::gcd_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::lcm(Tensor self, Tensor other) -> Tensor


// aten::lcm_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor


// aten::index_copy_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)


// aten::index_copy(Tensor self, int dim, Tensor index, Tensor source) -> Tensor


// aten::index_copy_.dimname(Tensor(a!) self, Dimname dim, Tensor index, Tensor source) -> Tensor(a!)


// aten::index_copy.dimname(Tensor self, Dimname dim, Tensor index, Tensor source) -> Tensor


// aten::index_put_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor(a!)


// aten::index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor


// aten::inverse(Tensor self) -> Tensor


// aten::isclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> Tensor


// aten::isnan(Tensor self) -> Tensor


// aten::is_distributed(Tensor self) -> bool


// aten::is_floating_point(Tensor self) -> bool


// aten::is_complex(Tensor self) -> bool


// aten::is_conj(Tensor self) -> bool


// aten::is_neg(Tensor self) -> bool


// aten::isreal(Tensor self) -> Tensor


// aten::is_nonzero(Tensor self) -> bool


// aten::is_same_size(Tensor self, Tensor other) -> bool


// aten::is_signed(Tensor self) -> bool


// aten::is_inference(Tensor self) -> bool


// aten::kron(Tensor self, Tensor other) -> Tensor


// aten::kthvalue(Tensor self, int k, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::kthvalue.dimname(Tensor self, int k, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::nan_to_num(Tensor self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor


// aten::nan_to_num_(Tensor(a!) self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor(a!)


// aten::ldexp.Tensor(Tensor self, Tensor other) -> Tensor


// aten::ldexp_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::log(Tensor self) -> Tensor


// aten::log_(Tensor(a!) self) -> Tensor(a!)


// aten::log10(Tensor self) -> Tensor


// aten::log10_(Tensor(a!) self) -> Tensor(a!)


// aten::log1p(Tensor self) -> Tensor


// aten::log1p_(Tensor(a!) self) -> Tensor(a!)


// aten::log2(Tensor self) -> Tensor


// aten::log2_(Tensor(a!) self) -> Tensor(a!)


// aten::logaddexp(Tensor self, Tensor other) -> Tensor


// aten::logaddexp2(Tensor self, Tensor other) -> Tensor


// aten::xlogy.Tensor(Tensor self, Tensor other) -> Tensor


// aten::xlogy.Scalar_Other(Tensor self, Scalar other) -> Tensor


// aten::xlogy_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::xlogy_.Scalar_Other(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::logdet(Tensor self) -> Tensor


// aten::log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor


// aten::log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor


// aten::logcumsumexp(Tensor self, int dim) -> Tensor


// aten::logcumsumexp.dimname(Tensor self, Dimname dim) -> Tensor


// aten::logsumexp(Tensor self, int[1] dim, bool keepdim=False) -> Tensor


// aten::logsumexp.names(Tensor self, Dimname[1] dim, bool keepdim=False) -> Tensor


// aten::matmul(Tensor self, Tensor other) -> Tensor


// aten::matrix_power(Tensor self, int n) -> Tensor


// aten::matrix_exp(Tensor self) -> Tensor


// aten::aminmax(Tensor self, *, int? dim=None, bool keepdim=False) -> (Tensor min, Tensor max)


// aten::max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::amax(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor


// aten::mean(Tensor self, *, ScalarType? dtype=None) -> Tensor


// aten::mean.dim(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::mean.names_dim(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::nanmean(Tensor self, int[1] dim=[], bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::median(Tensor self) -> Tensor


// aten::median.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::median.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::nanmedian(Tensor self) -> Tensor


// aten::nanmedian.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::nanmedian.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::amin(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor


// aten::mm(Tensor self, Tensor mat2) -> Tensor


// aten::mode(Tensor self, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::mode.dimname(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::mul.Tensor(Tensor self, Tensor other) -> Tensor


// aten::mul_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::mul.Scalar(Tensor self, Scalar other) -> Tensor


// aten::mul_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::multiply.Tensor(Tensor self, Tensor other) -> Tensor


// aten::multiply_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::multiply.Scalar(Tensor self, Scalar other) -> Tensor


// aten::multiply_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::mv(Tensor self, Tensor vec) -> Tensor


// aten::mvlgamma(Tensor self, int p) -> Tensor


// aten::mvlgamma_(Tensor(a!) self, int p) -> Tensor(a!)


// aten::narrow_copy(Tensor self, int dim, int start, int length) -> Tensor


// aten::narrow(Tensor(a) self, int dim, int start, int length) -> Tensor(a)


// aten::narrow.Tensor(Tensor(a) self, int dim, Tensor start, int length) -> Tensor(a)


// aten::permute(Tensor(a) self, int[] dims) -> Tensor(a)


// aten::movedim.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)


// aten::movedim.int(Tensor(a) self, int source, int destination) -> Tensor(a)


// aten::moveaxis.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)


// aten::moveaxis.int(Tensor(a) self, int source, int destination) -> Tensor(a)


// aten::numpy_T(Tensor(a) self) -> Tensor(a)


// aten::is_pinned(Tensor self, Device? device=None) -> bool


// aten::pin_memory(Tensor(a) self, Device? device=None) -> Tensor(a)


// aten::pinverse(Tensor self, float rcond=1e-15) -> Tensor


// aten::rad2deg(Tensor self) -> Tensor


// aten::rad2deg_(Tensor(a!) self) -> Tensor(a!)


// aten::deg2rad(Tensor self) -> Tensor


// aten::deg2rad_(Tensor(a!) self) -> Tensor(a!)


// aten::ravel(Tensor(a) self) -> Tensor(a)


// aten::reciprocal(Tensor self) -> Tensor


// aten::reciprocal_(Tensor(a!) self) -> Tensor(a!)


// aten::neg(Tensor self) -> Tensor


// aten::neg_(Tensor(a!) self) -> Tensor(a!)


// aten::negative(Tensor self) -> Tensor


// aten::negative_(Tensor(a!) self) -> Tensor(a!)


// aten::repeat(Tensor self, int[] repeats) -> Tensor


// aten::repeat_interleave.self_Tensor(Tensor self, Tensor repeats, int? dim=None, *, int? output_size=None) -> Tensor


// aten::repeat_interleave.self_int(Tensor self, int repeats, int? dim=None, *, int? output_size=None) -> Tensor


// aten::reshape(Tensor(a) self, int[] shape) -> Tensor(a)


// aten::_reshape_alias(Tensor(a) self, int[] size, int[] stride) -> Tensor(a)


// aten::reshape_as(Tensor(a) self, Tensor other) -> Tensor(a)


// aten::round(Tensor self) -> Tensor


// aten::round_(Tensor(a!) self) -> Tensor(a!)


// aten::relu(Tensor self) -> Tensor


// aten::relu_(Tensor(a!) self) -> Tensor(a!)


// aten::prelu(Tensor self, Tensor weight) -> Tensor


// aten::prelu_backward(Tensor grad_output, Tensor self, Tensor weight) -> (Tensor, Tensor)


// aten::hardshrink(Tensor self, Scalar lambd=0.5) -> Tensor


// aten::hardshrink_backward(Tensor grad_out, Tensor self, Scalar lambd) -> Tensor


// aten::rsqrt(Tensor self) -> Tensor


// aten::rsqrt_(Tensor(a!) self) -> Tensor(a!)


// aten::select.Dimname(Tensor(a) self, Dimname dim, int index) -> Tensor(a)


// aten::select.int(Tensor(a) self, int dim, int index) -> Tensor(a)


// aten::sigmoid(Tensor self) -> Tensor


// aten::sigmoid_(Tensor(a!) self) -> Tensor(a!)


// aten::logit(Tensor self, float? eps=None) -> Tensor


// aten::logit_(Tensor(a!) self, float? eps=None) -> Tensor(a!)


// aten::sin(Tensor self) -> Tensor


// aten::sin_(Tensor(a!) self) -> Tensor(a!)


// aten::sinc(Tensor self) -> Tensor


// aten::sinc_(Tensor(a!) self) -> Tensor(a!)


// aten::sinh(Tensor self) -> Tensor


// aten::sinh_(Tensor(a!) self) -> Tensor(a!)


// aten::detach(Tensor(a) self) -> Tensor(a)


// aten::detach_(Tensor(a!) self) -> Tensor(a!)


// aten::size.Dimname(Tensor self, Dimname dim) -> int


// aten::slice.Tensor(Tensor(a) self, int dim=0, int? start=None, int? end=None, int step=1) -> Tensor(a)


// aten::slogdet(Tensor self) -> (Tensor sign, Tensor logabsdet)


// aten::smm(Tensor self, Tensor mat2) -> Tensor


// aten::softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor


// aten::softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor


// aten::unsafe_split.Tensor(Tensor self, int split_size, int dim=0) -> Tensor[]


// aten::split.Tensor(Tensor(a) self, int split_size, int dim=0) -> Tensor(a)[]


// aten::unsafe_split_with_sizes(Tensor self, int[] split_sizes, int dim=0) -> Tensor[]


// aten::split_with_sizes(Tensor(a) self, int[] split_sizes, int dim=0) -> Tensor(a)[]


// aten::hsplit.int(Tensor(a) self, int sections) -> Tensor(a)[]


// aten::hsplit.array(Tensor(a) self, int[] indices) -> Tensor(a)[]


// aten::vsplit.int(Tensor(a) self, int sections) -> Tensor(a)[]


// aten::vsplit.array(Tensor(a) self, int[] indices) -> Tensor(a)[]


// aten::dsplit.int(Tensor(a) self, int sections) -> Tensor(a)[]


// aten::dsplit.array(Tensor(a) self, int[] indices) -> Tensor(a)[]


// aten::squeeze(Tensor(a) self) -> Tensor(a)


// aten::squeeze.dim(Tensor(a) self, int dim) -> Tensor(a)


// aten::squeeze.dimname(Tensor(a) self, Dimname dim) -> Tensor(a)


// aten::squeeze_(Tensor(a!) self) -> Tensor(a!)


// aten::squeeze_.dim(Tensor(a!) self, int dim) -> Tensor(a!)


// aten::squeeze_.dimname(Tensor(a!) self, Dimname dim) -> Tensor(a!)


// aten::sspaddmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor


// aten::stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor


// aten::istft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, bool normalized=False, bool? onesided=None, int? length=None, bool return_complex=False) -> Tensor


// aten::stride.Dimname(Tensor self, Dimname dim) -> int


// aten::sum(Tensor self, *, ScalarType? dtype=None) -> Tensor


// aten::sum.dim_IntList(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::sum.dim_DimnameList(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::nansum(Tensor self, *, ScalarType? dtype=None) -> Tensor


// aten::nansum.dim_IntList(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::sum_to_size(Tensor self, int[] size) -> Tensor


// aten::sqrt(Tensor self) -> Tensor


// aten::sqrt_(Tensor(a!) self) -> Tensor(a!)


// aten::square(Tensor self) -> Tensor


// aten::square_(Tensor(a!) self) -> Tensor(a!)


// aten::std(Tensor self, bool unbiased=True) -> Tensor


// aten::std.dim(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor


// aten::std.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> Tensor


// aten::std.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor


// aten::std.correction_names(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False) -> Tensor


// aten::prod(Tensor self, *, ScalarType? dtype=None) -> Tensor


// aten::prod.dim_int(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::prod.dim_Dimname(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::t(Tensor(a) self) -> Tensor(a)


// aten::t_(Tensor(a!) self) -> Tensor(a!)


// aten::tan(Tensor self) -> Tensor


// aten::tan_(Tensor(a!) self) -> Tensor(a!)


// aten::tanh(Tensor self) -> Tensor


// aten::tanh_(Tensor(a!) self) -> Tensor(a!)


// aten::tile(Tensor self, int[] dims) -> Tensor


// aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)


// aten::transpose.Dimname(Tensor(a) self, Dimname dim0, Dimname dim1) -> Tensor(a)


// aten::transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)


// aten::flip(Tensor self, int[] dims) -> Tensor


// aten::fliplr(Tensor self) -> Tensor


// aten::flipud(Tensor self) -> Tensor


// aten::roll(Tensor self, int[1] shifts, int[1] dims=[]) -> Tensor


// aten::rot90(Tensor self, int k=1, int[] dims=[0,1]) -> Tensor


// aten::trunc(Tensor self) -> Tensor


// aten::trunc_(Tensor(a!) self) -> Tensor(a!)


// aten::fix(Tensor self) -> Tensor


// aten::fix_(Tensor(a!) self) -> Tensor(a!)


// aten::type_as(Tensor self, Tensor other) -> Tensor


// aten::unsqueeze(Tensor(a) self, int dim) -> Tensor(a)


// aten::unsqueeze_(Tensor(a!) self, int dim) -> Tensor(a!)


// aten::var(Tensor self, bool unbiased=True) -> Tensor


// aten::var.dim(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor


// aten::var.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> Tensor


// aten::var.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor


// aten::var.correction_names(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False) -> Tensor


// aten::view_as(Tensor(a) self, Tensor other) -> Tensor(a)


// aten::where.self(Tensor condition, Tensor self, Tensor other) -> Tensor


// aten::norm.ScalarOpt_dtype(Tensor self, Scalar? p, *, ScalarType dtype) -> Tensor


// aten::norm.Scalar(Tensor self, Scalar p=2) -> Tensor


// aten::norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor


// aten::norm.ScalarOpt_dim(Tensor self, Scalar? p, int[1] dim, bool keepdim=False) -> Tensor


// aten::norm.names_ScalarOpt_dim_dtype(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor


// aten::norm.names_ScalarOpt_dim(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False) -> Tensor


// aten::frexp.Tensor(Tensor self) -> (Tensor mantissa, Tensor exponent)


// aten::clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor


// aten::positive(Tensor(a) self) -> Tensor(a)


// aten::resize_as_(Tensor(a!) self, Tensor the_template, *, MemoryFormat? memory_format=None) -> Tensor(a!)


// aten::zero_(Tensor(a!) self) -> Tensor(a!)


// aten::sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor


// aten::sub_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)


// aten::sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor


// aten::sub_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)


// aten::subtract.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor


// aten::subtract_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)


// aten::subtract.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor


// aten::subtract_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)


// aten::heaviside(Tensor self, Tensor values) -> Tensor


// aten::heaviside_(Tensor(a!) self, Tensor values) -> Tensor(a!)


// aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor


// aten::addmm_(Tensor(a!) self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)


// aten::sparse_resize_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -> Tensor(a!)


// aten::sparse_resize_and_clear_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -> Tensor(a!)


// aten::sparse_mask(Tensor self, Tensor mask) -> Tensor


// aten::to_dense(Tensor self, ScalarType? dtype=None) -> Tensor


// aten::sparse_dim(Tensor self) -> int


// aten::_dimI(Tensor self) -> int


// aten::dense_dim(Tensor self) -> int


// aten::_dimV(Tensor self) -> int


// aten::_nnz(Tensor self) -> int


// aten::coalesce(Tensor(a) self) -> Tensor(a)


// aten::is_coalesced(Tensor self) -> bool


// aten::_indices(Tensor(a) self) -> Tensor(a)


// aten::_values(Tensor(a) self) -> Tensor(a)


// aten::_coalesced_(Tensor(a!) self, bool coalesced) -> Tensor(a!)


// aten::indices(Tensor(a) self) -> Tensor(a)


// aten::values(Tensor(a) self) -> Tensor(a)


// aten::crow_indices(Tensor(a) self) -> Tensor(a)


// aten::col_indices(Tensor(a) self) -> Tensor(a)


// aten::unbind.int(Tensor(a) self, int dim=0) -> Tensor(a)[]


// aten::unbind.Dimname(Tensor(a) self, Dimname dim) -> Tensor(a)[]


// aten::to_sparse.sparse_dim(Tensor self, int sparse_dim) -> Tensor


// aten::to_sparse(Tensor self) -> Tensor


// aten::to_mkldnn(Tensor self, ScalarType? dtype=None) -> Tensor


// aten::dequantize.self(Tensor self) -> Tensor


// aten::q_scale(Tensor self) -> float


// aten::q_zero_point(Tensor self) -> int


// aten::q_per_channel_scales(Tensor self) -> Tensor


// aten::q_per_channel_zero_points(Tensor self) -> Tensor


// aten::q_per_channel_axis(Tensor self) -> int


// aten::int_repr(Tensor self) -> Tensor


// aten::qscheme(Tensor self) -> QScheme


// aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)


// aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)


// aten::to.device(Tensor(a) self, Device device, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)


// aten::to.dtype(Tensor(a) self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)


// aten::to.other(Tensor(a) self, Tensor other, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)


// aten::item(Tensor self) -> Scalar


// aten::set_.source_Storage(Tensor(a!) self, Storage source) -> Tensor(a!)


// aten::set_.source_Storage_storage_offset(Tensor(a!) self, Storage source, int storage_offset, int[] size, int[] stride=[]) -> Tensor(a!)


// aten::set_.source_Tensor(Tensor(a!) self, Tensor source) -> Tensor(a!)


// aten::set_(Tensor(a!) self) -> Tensor(a!)


// aten::is_set_to(Tensor self, Tensor tensor) -> bool


// aten::masked_fill_.Scalar(Tensor(a!) self, Tensor mask, Scalar value) -> Tensor(a!)


// aten::masked_fill.Scalar(Tensor self, Tensor mask, Scalar value) -> Tensor


// aten::masked_fill_.Tensor(Tensor(a!) self, Tensor mask, Tensor value) -> Tensor(a!)


// aten::masked_fill.Tensor(Tensor self, Tensor mask, Tensor value) -> Tensor


// aten::masked_scatter_(Tensor(a!) self, Tensor mask, Tensor source) -> Tensor(a!)


// aten::masked_scatter(Tensor self, Tensor mask, Tensor source) -> Tensor


// aten::view(Tensor(a) self, int[] size) -> Tensor(a)


// aten::view.dtype(Tensor(a) self, ScalarType dtype) -> Tensor(a)


// aten::put_(Tensor(a!) self, Tensor index, Tensor source, bool accumulate=False) -> Tensor(a!)


// aten::put(Tensor self, Tensor index, Tensor source, bool accumulate=False) -> Tensor


// aten::index_add_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)


// aten::index_add_.alpha(Tensor(a!) self, int dim, Tensor index, Tensor source, *, Scalar alpha) -> Tensor(a!)


// aten::index_add(Tensor self, int dim, Tensor index, Tensor source) -> Tensor


// aten::index_add.alpha(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha) -> Tensor


// aten::index_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor


// aten::index_fill_.int_Scalar(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)


// aten::index_fill.int_Scalar(Tensor self, int dim, Tensor index, Scalar value) -> Tensor


// aten::index_fill_.int_Tensor(Tensor(a!) self, int dim, Tensor index, Tensor value) -> Tensor(a!)


// aten::index_fill.int_Tensor(Tensor self, int dim, Tensor index, Tensor value) -> Tensor


// aten::index_fill_.Dimname_Scalar(Tensor(a!) self, Dimname dim, Tensor index, Scalar value) -> Tensor(a!)


// aten::index_fill_.Dimname_Tensor(Tensor(a!) self, Dimname dim, Tensor index, Tensor value) -> Tensor(a!)


// aten::index_fill.Dimname_Scalar(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor


// aten::index_fill.Dimname_Tensor(Tensor self, Dimname dim, Tensor index, Tensor value) -> Tensor


// aten::scatter.src(Tensor self, int dim, Tensor index, Tensor src) -> Tensor


// aten::scatter_.src(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)


// aten::scatter.value(Tensor self, int dim, Tensor index, Scalar value) -> Tensor


// aten::scatter_.value(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)


// aten::scatter.reduce(Tensor self, int dim, Tensor index, Tensor src, *, str reduce) -> Tensor


// aten::scatter_.reduce(Tensor(a!) self, int dim, Tensor index, Tensor src, *, str reduce) -> Tensor(a!)


// aten::scatter.value_reduce(Tensor self, int dim, Tensor index, Scalar value, *, str reduce) -> Tensor


// aten::scatter_.value_reduce(Tensor(a!) self, int dim, Tensor index, Scalar value, *, str reduce) -> Tensor(a!)


// aten::scatter.dimname_src(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor


// aten::scatter.dimname_value(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor


// aten::scatter_add(Tensor self, int dim, Tensor index, Tensor src) -> Tensor


// aten::scatter_add_(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)


// aten::scatter_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor


// aten::eq_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::eq_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bitwise_and.Scalar(Tensor self, Scalar other) -> Tensor


// aten::bitwise_and.Tensor(Tensor self, Tensor other) -> Tensor


// aten::bitwise_and_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::bitwise_and_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::__and__.Scalar(Tensor self, Scalar other) -> Tensor


// aten::__and__.Tensor(Tensor self, Tensor other) -> Tensor


// aten::__iand__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::__iand__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bitwise_or.Scalar(Tensor self, Scalar other) -> Tensor


// aten::bitwise_or.Tensor(Tensor self, Tensor other) -> Tensor


// aten::bitwise_or_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::bitwise_or_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::__or__.Scalar(Tensor self, Scalar other) -> Tensor


// aten::__or__.Tensor(Tensor self, Tensor other) -> Tensor


// aten::__ior__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::__ior__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bitwise_xor.Scalar(Tensor self, Scalar other) -> Tensor


// aten::bitwise_xor.Tensor(Tensor self, Tensor other) -> Tensor


// aten::bitwise_xor_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::bitwise_xor_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::__xor__.Scalar(Tensor self, Scalar other) -> Tensor


// aten::__xor__.Tensor(Tensor self, Tensor other) -> Tensor


// aten::__ixor__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::__ixor__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::__lshift__.Scalar(Tensor self, Scalar other) -> Tensor


// aten::__lshift__.Tensor(Tensor self, Tensor other) -> Tensor


// aten::__ilshift__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::__ilshift__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bitwise_left_shift.Tensor(Tensor self, Tensor other) -> Tensor


// aten::bitwise_left_shift_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bitwise_left_shift.Tensor_Scalar(Tensor self, Scalar other) -> Tensor


// aten::bitwise_left_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::__rshift__.Scalar(Tensor self, Scalar other) -> Tensor


// aten::__rshift__.Tensor(Tensor self, Tensor other) -> Tensor


// aten::__irshift__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::__irshift__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bitwise_right_shift.Tensor(Tensor self, Tensor other) -> Tensor


// aten::bitwise_right_shift_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bitwise_right_shift.Tensor_Scalar(Tensor self, Scalar other) -> Tensor


// aten::bitwise_right_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::tril_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)


// aten::triu_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)


// aten::digamma_(Tensor(a!) self) -> Tensor(a!)


// aten::lerp_.Scalar(Tensor(a!) self, Tensor end, Scalar weight) -> Tensor(a!)


// aten::lerp_.Tensor(Tensor(a!) self, Tensor end, Tensor weight) -> Tensor(a!)


// aten::addbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)


// aten::addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor


// aten::random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -> Tensor(a!)


// aten::random_.to(Tensor(a!) self, int to, *, Generator? generator=None) -> Tensor(a!)


// aten::random_(Tensor(a!) self, *, Generator? generator=None) -> Tensor(a!)


// aten::uniform_(Tensor(a!) self, float from=0, float to=1, *, Generator? generator=None) -> Tensor(a!)


// aten::cauchy_(Tensor(a!) self, float median=0, float sigma=1, *, Generator? generator=None) -> Tensor(a!)


// aten::log_normal_(Tensor(a!) self, float mean=1, float std=2, *, Generator? generator=None) -> Tensor(a!)


// aten::exponential_(Tensor(a!) self, float lambd=1, *, Generator? generator=None) -> Tensor(a!)


// aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -> Tensor(a!)


// aten::diag(Tensor self, int diagonal=0) -> Tensor


// aten::cross(Tensor self, Tensor other, int? dim=None) -> Tensor


// aten::triu(Tensor self, int diagonal=0) -> Tensor


// aten::tril(Tensor self, int diagonal=0) -> Tensor


// aten::trace(Tensor self) -> Tensor


// aten::ne.Scalar(Tensor self, Scalar other) -> Tensor


// aten::ne.Tensor(Tensor self, Tensor other) -> Tensor


// aten::ne_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::ne_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::not_equal.Scalar(Tensor self, Scalar other) -> Tensor


// aten::not_equal.Tensor(Tensor self, Tensor other) -> Tensor


// aten::not_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::not_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::eq.Scalar(Tensor self, Scalar other) -> Tensor


// aten::eq.Tensor(Tensor self, Tensor other) -> Tensor


// aten::ge.Scalar(Tensor self, Scalar other) -> Tensor


// aten::ge.Tensor(Tensor self, Tensor other) -> Tensor


// aten::ge_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::ge_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::greater_equal.Scalar(Tensor self, Scalar other) -> Tensor


// aten::greater_equal.Tensor(Tensor self, Tensor other) -> Tensor


// aten::greater_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::greater_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::le.Scalar(Tensor self, Scalar other) -> Tensor


// aten::le.Tensor(Tensor self, Tensor other) -> Tensor


// aten::le_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::le_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::less_equal.Scalar(Tensor self, Scalar other) -> Tensor


// aten::less_equal.Tensor(Tensor self, Tensor other) -> Tensor


// aten::less_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::less_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::gt.Scalar(Tensor self, Scalar other) -> Tensor


// aten::gt.Tensor(Tensor self, Tensor other) -> Tensor


// aten::gt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::gt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::greater.Scalar(Tensor self, Scalar other) -> Tensor


// aten::greater.Tensor(Tensor self, Tensor other) -> Tensor


// aten::greater_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::greater_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::lt.Scalar(Tensor self, Scalar other) -> Tensor


// aten::lt.Tensor(Tensor self, Tensor other) -> Tensor


// aten::lt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::lt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::less.Scalar(Tensor self, Scalar other) -> Tensor


// aten::less.Tensor(Tensor self, Tensor other) -> Tensor


// aten::less_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::less_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::take(Tensor self, Tensor index) -> Tensor


// aten::take_along_dim(Tensor self, Tensor indices, int? dim=None) -> Tensor


// aten::index_select(Tensor self, int dim, Tensor index) -> Tensor


// aten::index_select.dimname(Tensor self, Dimname dim, Tensor index) -> Tensor


// aten::masked_select(Tensor self, Tensor mask) -> Tensor


// aten::nonzero(Tensor self) -> Tensor


// aten::nonzero_numpy(Tensor self) -> Tensor[]


// aten::gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor


// aten::gather.dimname(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False) -> Tensor


// aten::addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor


// aten::addcmul_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)


// aten::addcdiv(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor


// aten::addcdiv_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)


// aten::lstsq(Tensor self, Tensor A) -> (Tensor solution, Tensor QR)


// aten::triangular_solve(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False) -> (Tensor solution, Tensor cloned_coefficient)


// aten::symeig(Tensor self, bool eigenvectors=False, bool upper=True) -> (Tensor eigenvalues, Tensor eigenvectors)


// aten::eig(Tensor self, bool eigenvectors=False) -> (Tensor eigenvalues, Tensor eigenvectors)


// aten::svd(Tensor self, bool some=True, bool compute_uv=True) -> (Tensor U, Tensor S, Tensor V)


// aten::swapaxes(Tensor(a) self, int axis0, int axis1) -> Tensor(a)


// aten::swapaxes_(Tensor(a!) self, int axis0, int axis1) -> Tensor(a!)


// aten::swapdims(Tensor(a) self, int dim0, int dim1) -> Tensor(a)


// aten::swapdims_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)


// aten::cholesky(Tensor self, bool upper=False) -> Tensor


// aten::cholesky_solve(Tensor self, Tensor input2, bool upper=False) -> Tensor


// aten::solve(Tensor self, Tensor A) -> (Tensor solution, Tensor LU)


// aten::cholesky_inverse(Tensor self, bool upper=False) -> Tensor


// aten::qr(Tensor self, bool some=True) -> (Tensor Q, Tensor R)


// aten::geqrf(Tensor self) -> (Tensor a, Tensor tau)


// aten::orgqr(Tensor self, Tensor input2) -> Tensor


// aten::ormqr(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False) -> Tensor


// aten::lu_solve(Tensor self, Tensor LU_data, Tensor LU_pivots) -> Tensor


// aten::multinomial(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None) -> Tensor


// aten::lgamma_(Tensor(a!) self) -> Tensor(a!)


// aten::lgamma(Tensor self) -> Tensor


// aten::digamma(Tensor self) -> Tensor


// aten::polygamma(int n, Tensor self) -> Tensor


// aten::polygamma_(Tensor(a!) self, int n) -> Tensor(a!)


// aten::erfinv(Tensor self) -> Tensor


// aten::erfinv_(Tensor(a!) self) -> Tensor(a!)


// aten::i0(Tensor self) -> Tensor


// aten::i0_(Tensor(a!) self) -> Tensor(a!)


// aten::sign(Tensor self) -> Tensor


// aten::sign_(Tensor(a!) self) -> Tensor(a!)


// aten::signbit(Tensor self) -> Tensor


// aten::dist(Tensor self, Tensor other, Scalar p=2) -> Tensor


// aten::atan2_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::atan2(Tensor self, Tensor other) -> Tensor


// aten::lerp.Scalar(Tensor self, Tensor end, Scalar weight) -> Tensor


// aten::lerp.Tensor(Tensor self, Tensor end, Tensor weight) -> Tensor


// aten::histc(Tensor self, int bins=100, Scalar min=0, Scalar max=0) -> Tensor


// aten::histogram.bins_tensor(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor bin_edges)


// aten::histogram.bin_ct(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor bin_edges)


// aten::fmod.Scalar(Tensor self, Scalar other) -> Tensor


// aten::fmod_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::fmod.Tensor(Tensor self, Tensor other) -> Tensor


// aten::fmod_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::hypot(Tensor self, Tensor other) -> Tensor


// aten::hypot_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::igamma(Tensor self, Tensor other) -> Tensor


// aten::igamma_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::igammac(Tensor self, Tensor other) -> Tensor


// aten::igammac_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::nextafter(Tensor self, Tensor other) -> Tensor


// aten::nextafter_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::remainder.Scalar(Tensor self, Scalar other) -> Tensor


// aten::remainder_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::remainder.Tensor(Tensor self, Tensor other) -> Tensor


// aten::remainder_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::min(Tensor self) -> Tensor


// aten::fmin(Tensor self, Tensor other) -> Tensor


// aten::max(Tensor self) -> Tensor


// aten::fmax(Tensor self, Tensor other) -> Tensor


// aten::maximum(Tensor self, Tensor other) -> Tensor


// aten::max.other(Tensor self, Tensor other) -> Tensor


// aten::minimum(Tensor self, Tensor other) -> Tensor


// aten::min.other(Tensor self, Tensor other) -> Tensor


// aten::quantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False) -> Tensor


// aten::quantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False) -> Tensor


// aten::nanquantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False) -> Tensor


// aten::nanquantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False) -> Tensor


// aten::quantile.new_scalar(Tensor self, float q, int? dim, bool keepdim, *, str interpolation) -> Tensor


// aten::quantile.new(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation) -> Tensor


// aten::nanquantile.new_scalar(Tensor self, float q, int? dim, bool keepdim, *, str interpolation) -> Tensor


// aten::nanquantile.new(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation) -> Tensor


// aten::sort(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)


// aten::sort.stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)


// aten::sort.dimname(Tensor self, Dimname dim, bool descending=False) -> (Tensor values, Tensor indices)


// aten::sort.dimname_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False) -> (Tensor values, Tensor indices)


// aten::msort(Tensor self) -> Tensor


// aten::argsort(Tensor self, int dim=-1, bool descending=False) -> Tensor


// aten::argsort.dimname(Tensor self, Dimname dim, bool descending=False) -> Tensor


// aten::topk(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)


// aten::all(Tensor self) -> Tensor


// aten::any(Tensor self) -> Tensor


// aten::renorm(Tensor self, Scalar p, int dim, Scalar maxnorm) -> Tensor


// aten::renorm_(Tensor(a!) self, Scalar p, int dim, Scalar maxnorm) -> Tensor(a!)


// aten::unfold(Tensor(a) self, int dimension, int size, int step) -> Tensor(a)


// aten::equal(Tensor self, Tensor other) -> bool


// aten::pow.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor


// aten::pow.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor


// aten::pow_.Scalar(Tensor(a!) self, Scalar exponent) -> Tensor(a!)


// aten::pow_.Tensor(Tensor(a!) self, Tensor exponent) -> Tensor(a!)


// aten::float_power.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor


// aten::float_power.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor


// aten::float_power_.Scalar(Tensor(a!) self, Scalar exponent) -> Tensor(a!)


// aten::float_power_.Tensor(Tensor(a!) self, Tensor exponent) -> Tensor(a!)


// aten::normal_(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor(a!)


// aten::alias(Tensor(a) self) -> Tensor(a)


// aten::isfinite(Tensor self) -> Tensor


// aten::isinf(Tensor self) -> Tensor


// aten::record_stream(Tensor(a!) self, Stream s) -> ()


// aten::isposinf(Tensor self) -> Tensor


// aten::isneginf(Tensor self) -> Tensor


// aten::special_polygamma(int n, Tensor self) -> Tensor


// aten::det(Tensor self) -> Tensor


// aten::inner(Tensor self, Tensor other) -> Tensor


// aten::outer(Tensor self, Tensor vec2) -> Tensor


// aten::ger(Tensor self, Tensor vec2) -> Tensor

 // namespace at
 // namespace c10




 // namespace at


// Parsed from ATen/core/Tensor.h

// #pragma once

// #include <ATen/core/TensorBody.h>
// #include <c10/util/Exception.h>
// Targeting ../OptionalTensorRef.java







 // namespace at


// Parsed from ATen/core/Formatting.h

// #pragma once

// #include <c10/core/Scalar.h>
// #include <ATen/core/Tensor.h>
// #include <ostream>



@Namespace("at") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef DeprecatedTypeProperties t);
@Namespace("at") public static native @Cast("std::ostream*") @ByRef Pointer print(
    @Cast("std::ostream*") @ByRef Pointer stream,
    @Const @ByRef Tensor tensor,
    @Cast("int64_t") long linesize);
@Namespace("at") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Tensor t);
@Namespace("at") public static native void print(@Const @ByRef Tensor t, @Cast("int64_t") long linesize/*=80*/);
@Namespace("at") public static native void print(@Const @ByRef Tensor t);

@Namespace("at") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @ByVal Scalar s);




// Parsed from ATen/core/UnsafeFromTH.h

// #pragma once
// #include <ATen/core/Tensor.h>

@Namespace("at") public static native @ByVal Tensor unsafeTensorFromTH(Pointer th_pointer, @Cast("bool") boolean retain);

@Namespace("at") public static native @Cast({"", "c10::Storage&&"}) @StdMove Storage unsafeStorageFromTH(Pointer th_pointer, @Cast("bool") boolean retain);




// Parsed from ATen/core/Variadic.h

// #pragma once

// #include <cstdint>
// #include <tuple>
// #include <type_traits>
// #include <utility>

// #include <c10/util/ArrayRef.h>
// #include <ATen/core/List.h>

// This class allows you to write variadic functions which
// call a (possibly overloaded) function on each argument,
// in order.  This is most commonly used in autogenerated code,
// where it is convenient to have a function that can uniformly
// take arguments of different types.  If your arguments
// are homogenous consider using a std::initializer_list instead.
//
// For examples of this in use, see torch/csrc/utils/variadic.h

 // namespace torch


// Parsed from ATen/core/blob.h

// #pragma once

// #include <cstddef>
// #include <sstream>
// #include <type_traits>
// #include <typeinfo>
// #include <vector>

// #include <c10/util/intrusive_ptr.h>
// #include <c10/util/typeid.h>
// #include <c10/macros/Macros.h>
// Targeting ../Blob.java



@Namespace("caffe2") public static native void swap(@ByRef Blob lhs, @ByRef Blob rhs);

@Namespace("caffe2") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Blob v);

 // namespace caffe2


// Parsed from ATen/core/functional.h

// #pragma once

// #include <vector>
// #include <c10/util/ArrayRef.h>

// The passed in function must take T by value (T), or by
// const reference (const T&); taking T by non-const reference
// will result in an error like:
//
//    error: no type named 'type' in 'class std::result_of<foobar::__lambda(T)>'
//
// No explicit template parameters are required.

// Overload for explicit function and ArrayRef

// C++ forbids taking an address of a constructor, so here's a workaround...
// Overload for constructor (R) application

 // namespace c10


// Parsed from ATen/core/ivalue.h

// #pragma once

// #include <ATen/core/TensorBody.h>
// #include <ATen/core/blob.h>
// #include <ATen/core/ivalue_to.h>
// #include <c10/util/C++17.h>
// #include <c10/util/intrusive_ptr.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <typeindex>
// Targeting ../CustomClassHolder.java


 // namespace jit

// Targeting ../IValueIValueDict.java


// Targeting ../RRefInterface.java



@Namespace("c10") public static native @Cast("bool") boolean _fastEqualsForContainer(@Const @ByRef IValue lhs, @Const @ByRef IValue rhs);

@Namespace("c10") public static native Function checkObjectSortSchema(
    @Const @SharedPtr @ByRef ClassType t,
    @Cast("std::stringstream*") @ByRef Pointer why_not);

// A comparator that checks ordering of two IValues of same type.

@Namespace("c10") public static native @ByVal @Cast("c10::IValueComparator*") Pointer getLessThanComparator(@Const @ByRef IValue v);
@Namespace("c10") public static native @ByVal @Cast("c10::IValueComparator*") Pointer getGreaterThanComparator(@Const @ByRef IValue v);
// Targeting ../Tuple.java


// Targeting ../Future.java


// Targeting ../ConstantString.java


// Targeting ../GenericDict.java


// Targeting ../Object.java


// Targeting ../PyObjectHolder.java


// Targeting ../EnumHolder.java


// Targeting ../ComplexHolder.java



// Targeting ../LongOptionalArray.java


// Targeting ../DoubleOptionalArray.java


// Targeting ../Capsule.java



// IValue is the generic tagged union used by the interpreter to hold
// all value types.
// It is a 16-byte object with an 8-byte payload and an 8-byte tag.
// The tag is currently 4 bytes to determine the type, and 1 byte
// to mark whether that type is a subtype of c10::intrusive_ptr_target and needs
// retain/release calls.


///
///
///
///
///
// #define TORCH_FORALL_TAGS(_)
//   _(None)
//   _(Tensor)
//   _(Storage)
//   _(Double)
//   _(ComplexDouble)
//   _(Int)
//   _(Bool)
//   _(Tuple)
//   _(String)
//   _(Blob)
//   _(GenericList)
//   _(GenericDict)
//   _(Future)
//   _(Device)
//   _(Stream)
//   _(Object)
//   _(PyObject)
//   _(Uninitialized)
//   _(Capsule)
//   _(RRef)
//   _(Quantizer)
//   _(Generator)
//   _(Enum)
// Targeting ../IValue.java


// Targeting ../WeakIValue.java


// Targeting ../StrongTypePtr.java



@Namespace("c10") public static native @ByRef StringFunctionMap getClassConverter();
 // namespace c10

// #include <ATen/core/ivalue_inl.h>


// Parsed from ATen/core/ivalue_to.h

// #pragma once

// #include <string>
 // namespace at
// Targeting ../ivalue_to_const_ref_overload_return.java



 // namespace detail
 // namespace c10


// Parsed from ATen/core/operator_name.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Optional.h>
// #include <c10/util/string_view.h>
// #include <string>
// #include <utility>
// #include <ostream>
// Targeting ../OperatorName.java


// Targeting ../OperatorNameView.java







@Namespace("c10") public static native @StdString BytePointer toString(@Const @ByRef OperatorName opName);


 // namespace c10



// Parsed from ATen/core/qualified_name.h

// #pragma once

// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>
// #include <c10/util/StringUtil.h>
// #include <string>
// Targeting ../QualifiedName.java


 // namespace c10
 // namespace std


// Parsed from ATen/core/stack.h

// #pragma once

// #include <type_traits>

// #include <ATen/core/ivalue.h>
// #include <c10/util/Deprecated.h>

// TODO move this to c10 namespace
// Targeting ../Operation.java



// An operation with N inputs and M outputs pops the last N inputs off
// the stack and pushes its M inputs onto the stack
// before: <other stack items> I0, I1, ... IN <- stack.back()
// after: <other stack items> O0, O1, ... OM
// operations are defined this way so that ownership of inputs can be
// transferred to the operation and it can incrementally drop ownership of
// tensors when they become unneeded. For large operations, like 'run an entire
// subgraph', this functionality is very important for minimizing gpu memory
// usage return value is the relative 'offset' to jump to for the next
// operation:
//   pc += 1 + offset
// so a return value of 0 goes to the next instruction

// treat the last N elements of the stack as a list, looking up
// element i
@Namespace("torch::jit") public static native @ByRef IValue peek(@ByRef IValueVector stack, @Cast("size_t") long i, @Cast("size_t") long N);
// treat the last N elements of the stack as a list, looking up the
// slice starting at index i and having length len
@Namespace("torch::jit") public static native @ByVal IValueArrayRef peekSlice(
    @Const @ByRef IValueVector stack,
    @Cast("size_t") long i,
    @Cast("size_t") long len,
    @Cast("size_t") long N);
@Namespace("torch::jit") public static native @ByVal IValueArrayRef last(@Const @ByRef IValueVector stack, @Cast("size_t") long N);
@Namespace("torch::jit") public static native void drop(@ByRef IValueVector stack, @Cast("size_t") long n);
@Namespace("torch::jit") public static native @ByVal IValue pop(@ByRef IValueVector stack);
@Namespace("torch::jit") public static native @ByVal IValueVector pop(@ByRef IValueVector stack, @Cast("size_t") long n);

// variadic pop:
// int64_t a; at::Tensor b;
// pop(stack, a, b);
// equivalent to:
// b = pop(stack).toTensor();
// a = pop(stack).toInt();

@Namespace("torch::jit") public static native void push_one(@ByRef IValueVector stack, @ByVal TensorOptions options);

// The packer here is carefully written not to make any unnecessary
// copies.

// pack takes the return values of aten functions pushes them onto the stack

 // namespace jit
 // namespace torch


// Parsed from ATen/core/alias_info.h

// #pragma once
// #include <unordered_set>
// #include <vector>
// #include <ATen/core/interned_strings.h>
// #include <c10/util/Exception.h>
// Targeting ../AliasInfo.java





// this does match the way things are represented in the schema

 // namespace c10


// Parsed from ATen/core/jit_type_base.h

// #pragma once

// #include <functional>
// #include <string>
// #include <memory>
// #include <c10/macros/Macros.h>
// #include <c10/util/Optional.h>
// #include <c10/util/Exception.h>
// #include <c10/util/ArrayRef.h>

// #define C10_FORALL_TYPES(_)
//   _(AnyType)
//   _(EnumType)
//   _(AnyEnumType)
//   _(TensorType)
//   _(StorageType)
//   _(TupleType)
//   _(ListType)
//   _(DictType)
//   _(NumberType)
//   _(FloatType)
//   _(ComplexType)
//   _(FutureType)
//   _(RRefType)
//   _(IntType)
//   _(NoneType)
//   _(StringType)
//   _(GeneratorType)
//   _(QuantizerType)
//   _(BoolType)
//   _(OptionalType)
//   _(VarType)
//   _(DeviceObjType)
//   _(StreamObjType)
//   _(FunctionType)
//   _(ClassType)
//   _(PyObjectType)
//   _(CapsuleType)
//   _(InterfaceType)
//   _(QSchemeType)
//   _(LayoutType)
//   _(ScalarTypeType)
//   _(AnyListType)
//   _(AnyTupleType)
//   _(AnyClassType)
//   _(UnionType)

@Namespace("c10") public enum TypeKind {
  AnyType(0),
  EnumType(1),
  AnyEnumType(2),
  TensorType(3),
  StorageType(4),
  TupleType(5),
  ListType(6),
  DictType(7),
  NumberType(8),
  FloatType(9),
  ComplexType(10),
  FutureType(11),
  RRefType(12),
  IntType(13),
  NoneType(14),
  StringType(15),
  GeneratorType(16),
  QuantizerType(17),
  BoolType(18),
  OptionalType(19),
  VarType(20),
  DeviceObjType(21),
  StreamObjType(22),
  FunctionType(23),
  ClassType(24),
  PyObjectType(25),
  CapsuleType(26),
  InterfaceType(27),
  QSchemeType(28),
  LayoutType(29),
  ScalarTypeType(30),
  AnyListType(31),
  AnyTupleType(32),
  AnyClassType(33),
  UnionType(34);

    public final int value;
    private TypeKind(int v) { this.value = v; }
    private TypeKind(TypeKind e) { this.value = e.value; }
    public TypeKind intern() { for (TypeKind e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") public static native @Cast("const char*") BytePointer typeKindToString(TypeKind kind);
@Namespace("c10") public static native String typeKindToString(@Cast("c10::TypeKind") int kind);

// Use this to customize how a Type is printed using `annotation_str()`. If
// c10::nullopt is returned, `annotation_str()` falls through to its default
// implementation.
// Targeting ../Type.java






// Parsed from ATen/core/jit_type.h

// #pragma once

// #include <ATen/core/jit_type_base.h>
// #include <ATen/core/TensorBody.h>
// #include <ATen/core/functional.h>
// #include <ATen/core/interned_strings.h>
// #include <ATen/core/qualified_name.h>
// #include <ATen/core/ivalue.h>
// #include <c10/util/TypeList.h>
// #include <c10/util/Optional.h>

// #include <array>
// #include <memory>
// #include <ostream>
// #include <sstream>
// #include <type_traits>
 // namespace jit
 // namespace torch



// Targeting ../AnyType.java



@Namespace("c10") public static native @StdString BytePointer toString(@SharedPtr @ByVal Type typePtr);


// Targeting ../ListSingleElementType.java


// Targeting ../RRefSingleElementType.java


// Targeting ../FutureSingleElementType.java


// Targeting ../OptionalSingleElementType.java


// Targeting ../UnionType.java


// Targeting ../OptionalType.java


// Targeting ../Stride.java



@Namespace("c10") public static native @ByVal StrideOptional merge_primitive(
    @Const @ByRef StrideOptional a,
    @Const @ByRef StrideOptional b);
// Targeting ../ShapeSymbol.java



@Namespace("c10") public static native @ByVal ShapeSymbol merge_primitive(
    @Const @ByRef ShapeSymbol a,
    @Const @ByRef ShapeSymbol b);
// Targeting ../SymbolicShape.java


@Namespace("c10::detail") public static native @Cast("bool") boolean isComplete(@Const @ByRef Stride s);

// Targeting ../LongVaryingShape.java


// Targeting ../StrideVaryingShape.java


// Targeting ../TensorType.java


// Targeting ../ListType.java


// Targeting ../DictType.java


// Targeting ../FutureType.java


// Targeting ../RRefType.java


// Targeting ../NamedType.java



// Any should never appear in a named type like a class, namedtuple or
// interface. If it does, then dynamic type information will be lost in the
// Pickler, leading to hard-to-track-down bugs that will only occur
// after saving or loading a model. This is because we rely on the
// static types in named types to reconstruct type tags of loaded
// values. Lifting this restriction requires solving the serialization
// problem first.
@Namespace("c10") public static native void checkNoAny(
    @Const @ByRef Type base,
    @Cast("const char*") BytePointer what,
    @StdString BytePointer attrname,
    @Const @SharedPtr @ByRef Type attrtype);
@Namespace("c10") public static native void checkNoAny(
    @Const @ByRef Type base,
    String what,
    @StdString String attrname,
    @Const @SharedPtr @ByRef Type attrtype);
// Targeting ../TupleType.java


// Targeting ../EnumType.java



// the common supertype of all Enums, only used in operator registraion.
// EnumType <: AnyEnumType for all Enums
// Targeting ../AnyEnumType.java


// Targeting ../NumberType.java


// Targeting ../FloatType.java


// Targeting ../ComplexType.java


// Targeting ../IntType.java


// Targeting ../BoolType.java


// Targeting ../StringType.java


// Targeting ../StorageType.java


// Targeting ../FunctionType.java


// Targeting ../NoneType.java


// Targeting ../GeneratorType.java


// Targeting ../QuantizerType.java


// Targeting ../QSchemeType.java


// Targeting ../DeviceObjType.java


// Targeting ../StreamObjType.java


// Targeting ../VarType.java


// Targeting ../CapsuleType.java


// Targeting ../PyObjectType.java



@Namespace("c10") public enum TypeVerbosity {
  None(0),
  Type(1),
  TypeAndStride(2),
  Full(3),
  Symbolic(4),
  Default(Full.value);

    public final int value;
    private TypeVerbosity(int v) { this.value = v; }
    private TypeVerbosity(TypeVerbosity e) { this.value = e.value; }
    public TypeVerbosity intern() { for (TypeVerbosity e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") public static native TypeVerbosity type_verbosity();






// what is the type, ignoring extra size/shape information?
// e.g. Tensor(2x3) -> Dynamic, and Tuple(Tensor(2x3),...) -> Tuple(Dynamic,...)

// `unshapedType` is used to remove Tensor subtypes. We treat all Tensor
// subtypes as simply "Tensor"; we also create a new version of any
// container types in which internal Tensors have undergone the same
// operation. This is used for type comparisons between two Tensor types
// (`unshapedType` means that we don't falsely return `false` for e.g.
// Tensors of different dimensions). It's also used in the alias
// analysis pass.
// Be careful with calls because this can be very slow. If calling this
// on a graph, use `EraseShapeInformation` in shape_analysis.h
@Namespace("c10") public static native @SharedPtr @ByVal Type unshapedType(@Const @SharedPtr @ByRef Type type);




@Namespace("c10") public static native @ByVal ScalarTypeOptional tryScalarTypeFromJitType(@Const @SharedPtr @ByRef Type type);

@Namespace("c10") public static native ScalarType scalarTypeFromJitType(@Const @SharedPtr @ByRef Type type);

// Attempt to find the correct supertype of the two types `t1` and `t2`.
// If no supertype is found, then nullopt will be returned if
// `default_to_union` is false, and `Union[t1, t2]` will be returned
// if it is true. If `t1 == t2`, or `t1` is a type refinement of `t2`,
// then `t2` will be returned (and vice versa).
//
// Two different tensortypes will return dynamic.
//
// Currently we chose not to support returning a NumberType for
// two types from the set of {FloatType, IntType, ComplexType}, because
// there is a lack of operator support for NumberType.
//
// If `type_hint` is an `InterfaceType`, then we can use that as a
// potential supertype for `ClassType`s in the list. Otherwise, we have
// no way to find and use some common interface type
@Namespace("c10") public static native @ByVal TypePtrOptional unifyTypes(
    @Const @SharedPtr @ByRef Type t1,
    @Const @SharedPtr @ByRef Type t2,
    @Cast("bool") boolean default_to_union/*=false*/,
    @SharedPtr @ByVal(nullValue = "c10::TypePtr(nullptr)") Type type_hint);
@Namespace("c10") public static native @ByVal TypePtrOptional unifyTypes(
    @Const @SharedPtr @ByRef Type t1,
    @Const @SharedPtr @ByRef Type t2);

@Namespace("c10") public static native @ByVal TypePtrOptional unifyTypeList(
    @ByVal TypeArrayRef elements,
    @Cast("std::ostream*") @ByRef Pointer why_not,
    @Cast("bool") boolean default_to_union/*=false*/,
    @SharedPtr @ByVal(nullValue = "c10::TypePtr(nullptr)") Type type_hint);
@Namespace("c10") public static native @ByVal TypePtrOptional unifyTypeList(
    @ByVal TypeArrayRef elements,
    @Cast("std::ostream*") @ByRef Pointer why_not);
// Targeting ../getTypePtr_.java


 // namespace detail
// Targeting ../MatchTypeReturn.java



// attempt to match the type variables in formal to actual, adding them to type_env.
// If no match is possible this returns a MatchTypeReturn with r.success() == false
// and a r.reason() that describes why it could not match.
// note: It is possible to successfully match a formal, but for type variables
// in the formal to still not be defined. In particular, None matches Optional[T]
// but does not define the value of T.
@Namespace("c10") public static native @ByVal MatchTypeReturn matchTypeVariables(@SharedPtr @ByVal Type formal, @SharedPtr @ByVal Type actual, @Cast("c10::TypeEnv*") @ByRef IValueIValueMap type_env);

// replace type variables appearing in `type` with the values in
// `type_env`. Returns nullptr if a variable used in `type`
// does not appear in `type_env`
@Namespace("c10") public static native @SharedPtr @ByVal Type tryEvalTypeVariables(@SharedPtr @ByVal Type type, @Cast("c10::TypeEnv*") @ByRef IValueIValueMap type_env);

@Namespace("c10") public static native @Cast("bool") boolean elementTypeCanBeInferredFromMembers(@Const @SharedPtr @ByRef Type elem_type);

// This enumerator represents the 'kind' of an attribute - a buffer, a paramter, or neither.
// This state is mutually exclusive. Buffers and Parameters can only appear on modules.
@Namespace("c10") public enum AttributeKind {
  BUFFER(0),
  PARAMETER(1),
  REGULAR_ATTRIBUTE(2);

    public final int value;
    private AttributeKind(int v) { this.value = v; }
    private AttributeKind(AttributeKind e) { this.value = e.value; }
    public AttributeKind intern() { for (AttributeKind e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../ClassAttribute.java



/**
 * User Defined Types
 */
// Targeting ../ClassType.java


// Targeting ../InterfaceType.java


// Targeting ../LayoutEnumerationType.java


// Targeting ../ScalarTypeEnumerationType.java


// Targeting ../LayoutType.java


// Targeting ../ScalarTypeType.java



// the common supertype of all lists,
// List[T] <: AnyList for all T
// Targeting ../AnyListType.java



// the common supertype of all tuples,
// Tuple[T...] <: AnyTuple for all T
// Targeting ../AnyTupleType.java



// the common supertype of all classes,
// ClassType <: AnyClassType for all classes
// Targeting ../AnyClassType.java
















// Targeting ../InferredType.java



 // namespace c10


// Parsed from ATen/core/function_schema.h

// #pragma once

// #include <c10/util/StringUtil.h>
// #include <c10/util/string_view.h>
// #include <ATen/core/jit_type.h>
// #include <ATen/core/interned_strings.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/alias_info.h>
// #include <ATen/core/operator_name.h>
// #include <ATen/core/dispatch/OperatorOptions.h>
// #include <unordered_map>

// schema as used in the compiler for resolving function calls and reporting
// errors. These objects should be constructed from C10 schema once those
// are available.


// Targeting ../Argument.java






// Targeting ../FunctionSchema.java







// print out Argument, which is compatible with FunctionSchema parser
// full format: Type(alias)? name=default_value




@Namespace("c10") public static native @StdString BytePointer toString(@Const @ByRef FunctionSchema schema);

 // namespace c10

// #include <ATen/core/function_schema_inl.h>


// Parsed from ATen/core/function.h

// #pragma once
// #include <ATen/core/function_schema.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/qualified_name.h>
// #include <mutex>

@Namespace("at") public static native void launch(@ByVal Func func);

// Targeting ../RecursiveMethodCallError.java



@Namespace("torch::jit") public static native void preoptimizeGraph(@SharedPtr @ByRef Graph graph);
// Targeting ../Function.java


 // namespace jit
 // namespace torch


// Parsed from ATen/core/boxing/KernelFunction.h

// #pragma once

// #include <ATen/core/stack.h>
// #include <c10/util/TypeList.h>
// Targeting ../OperatorKernel.java



// This kernel implements the behavior of falling through to the next available
// registered dispatch key.  The implementation of this function is FAST; it is
// no overhead to fallthrough to the next key.  See cpp file for some more
// implementation notes; notably, this does NOT actually go through the
// boxing/unboxing codepath.
@Namespace("c10") public static native void fallthrough_kernel(OperatorKernel arg0, @Const @ByRef OperatorHandle arg1, @ByVal DispatchKeySet arg2, @Cast("c10::Stack*") IValueVector arg3);

// Note [Ambiguity in AutogradOther kernel]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// This error-reporting kernel is registered to the AutogradOther entry in the
// dispatch table when there is both a CompositeImplicitAutograd kernel and a
// backend kernel for ANY backend that maps to AutogradOther.  To see why
// this is necessary in the AutogradOther case, it's helpful to first see
// why everything works out fine for a backend that has a reserved Autograd
// entry (see rule 2.2 in [Note] DispatchTable computation):
//
//    CPU   AutogradCPU
//    reg?  registers with...
//    -------------------------------------------------
//    y     Autograd registration takes precedence
//          over CompositeImplicitAutograd.
//          This is good, because the CPU specific backend
//          implementation is more specialized and typically better;
//          if we used the composite, we would bypass it.
//          (NB: the Autograd key is guaranteed to exist because
//          the autograd codegen requires it!)
//
//    n     CompositeImplicitAutograd takes precedence.
//          This is also good, because the Autograd
//          registration (if it exists) would try to redispatch
//          to the (non-existent) CPU implementation; by
//          using the composite, we ensure the operator
//          actually works.
//
// As you can see, when we have a specific Autograd key (AutogradCPU), we can
// decide whether or not to use the CompositeImplicitAutograd kernel or the
// Autograd kernel based on whether or not the backend kernel exists.
//
// However, for AutogradOther (which is the catchall autograd kernel for
// everything that doesn't have a specific Autograd key), we can't do this
// trick because there isn't any unique backend to peek at to disambiguate;
// if there are some backends that have implementations they prefer Autograd,
// but unimplemented backends would prefer CompositeImplicitAutograd.  Rather
// than arbitrarily pick one or the other, we just register a kernel that raises
// an error and let the user decide how to proceed.
@Namespace("c10") public static native void ambiguous_autogradother_kernel(OperatorKernel arg0, @Const @ByRef OperatorHandle arg1, @ByVal DispatchKeySet arg2, @Cast("c10::Stack*") IValueVector arg3);

// Note [named_not_supported_kernel]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// This kernel implements reporting an error message saying that named tensor is
// not supported.  This kernel doesn't rely on the Stack, and so it is special
// cased in the dispatcher to be triggered before we attempt boxing (so we can
// give a good error message in cases when boxing is not supported).  When
// boxing is universally supported this can be removed.
@Namespace("c10") public static native void named_not_supported_kernel(OperatorKernel arg0, @Const @ByRef OperatorHandle arg1, @ByVal DispatchKeySet arg2, @Cast("c10::Stack*") IValueVector arg3);
// Targeting ../KernelFunction.java





// #include <ATen/core/boxing/KernelFunction_impl.h>


// Parsed from ATen/core/dispatch/CppSignature.h

// #pragma once

// #include <typeindex>
// #include <c10/core/DispatchKeySet.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/Metaprogramming.h>
// #include <c10/util/Type.h>
// Targeting ../CppSignature.java









// Parsed from ATen/core/dispatch/DispatchKeyExtractor.h

// #pragma once

// #include <cstdint>
// #include <ATen/core/function_schema.h>
// #include <ATen/core/jit_type.h>
// #include <c10/util/Bitset.h>
// #include <c10/core/DispatchKeySet.h>
// #include <ATen/core/Variadic.h>
// #include <ATen/core/stack.h>

// Take a DispatchKeySet for a Tensor and determine what the actual dispatch
// DispatchKey should be, taking into account TLS, and skipping backends which
// fall through.
//
// Unlike Tensor::key_set(), the value of this on a tensor can change depending
// on TLS.
//
// NB: If there is no valid dispatch key, this will return Undefined
@Namespace("c10::impl") public static native @ByVal DispatchKeySet computeDispatchKeySet(
    @ByVal DispatchKeySet ks,
    @ByVal DispatchKeySet key_mask
);


  // A small gadget to extract the DispatchKeySet from types which are known
  // to have it.  Used to extract dispatch keys from unboxed calls.

  // NB: take by const reference (Don't do universal forwarding here! You
  // don't want to move into this function!)

// Targeting ../DispatchKeyExtractor.java






// Parsed from ATen/core/dispatch/RegistrationHandleRAII.h

// #pragma once

// #include <functional>
// Targeting ../RegistrationHandleRAII.java






// Parsed from ATen/core/dispatch/OperatorOptions.h

// #pragma once

// #include <cstdint>

@Namespace("c10") public enum AliasAnalysisKind {
  INTERNAL_SPECIAL_CASE((byte)(0)),
  CONSERVATIVE((byte)(1)), // The most conservative alias analysis type, assumes
                // side-effects. This is the default analysis.
  FROM_SCHEMA((byte)(2)),
  PURE_FUNCTION((byte)(3));

    public final byte value;
    private AliasAnalysisKind(byte v) { this.value = v; }
    private AliasAnalysisKind(AliasAnalysisKind e) { this.value = e.value; }
    public AliasAnalysisKind intern() { for (AliasAnalysisKind e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

// #if !defined(_MSC_VER)
@Namespace("c10") public static native @Cast("const char*") BytePointer toString(AliasAnalysisKind aliasAnalysisKind);

 // namespace c10


// Parsed from ATen/core/dispatch/OperatorEntry.h

// #pragma once

// #include <ATen/core/function_schema.h>
// #include <c10/util/Metaprogramming.h>
// #include <c10/util/flat_hash_map.h>
// #include <c10/util/either.h>
// #include <c10/util/Optional.h>
// #include <c10/core/DispatchKey.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/boxing/KernelFunction.h>
// #include <ATen/core/dispatch/DispatchKeyExtractor.h>

// #include <ATen/core/dispatch/OperatorOptions.h>
// #include <ATen/core/dispatch/CppSignature.h>
// #include <ATen/core/dispatch/RegistrationHandleRAII.h>

// #include <list>
// #include <array>

// #ifdef C10_MOBILE
// #define C10_DISPATCHER_ONE_KERNEL_PER_DISPATCH_KEY
// #endif

// This data structure represents a kernel that was registered to us from a
// user.  Unlike KernelFunction, AnnotatedKernel contains some extra metadata
// about the kernel that isn't necessary for actual dispatching (this is why
// we don't put AnnotatedKernel in the actual DispatchTable), but is useful for
// giving good error messages.
// Targeting ../AnnotatedSchema.java



// Internal data structure that records information about a specific operator.
// It's not part of the public API; typically, users will interact with
// OperatorHandle instead.
//
// Concurrent writes to OperatorEntry are protected by the GLOBAL Dispatcher
// lock (this is important because some methods in OperatorEntry access
// dispatcher state)

 // namespace impl
 // namespace c10


// Parsed from ATen/core/dispatch/Dispatcher.h

// #pragma once

// #include <ATen/SequenceNumber.h>
// #include <ATen/core/boxing/KernelFunction.h>
// #include <ATen/core/boxing/impl/boxing.h>
// #include <ATen/core/dispatch/OperatorEntry.h>
// #include <ATen/core/dispatch/CppSignature.h>
// #include <ATen/core/dispatch/RegistrationHandleRAII.h>
// #include <ATen/record_function.h>
// #include <c10/util/Exception.h>
// #include <c10/util/LeftRight.h>
// #include <mutex>
// #include <list>

// #include <ATen/core/grad_mode.h>

// #if C10_MOBILE
// #define C10_DISPATCHER_INLINE_UNLESS_MOBILE inline
// #else
// #endif
// Targeting ../OpRegistrationListener.java


// Targeting ../RegistrationListenerList.java



// Targeting ../SchemaRegistrationHandleRAII.java


// Targeting ../Dispatcher.java


// Targeting ../OperatorHandle.java



/**
 * This is a handle to an operator schema registered with the dispatcher.
 * It holds the same information as an OperatorHandle, but it is templated
 * on the operator arguments and allows calling the operator in an
 * unboxed way.
 */

// CaptureKernelCall is intended to capture return values from Dispatcher
// unboxed kernel calls. A record function may request to get outputs from the
// kernel calls. For boxed kernels, it's straightforward, the returned values
// are in the stack object. The stack can be passed to record functions. For
// unboxed kernels, we need to handle different kinds of return values, cache
// them temporarily, then release the values for the actual function call
// return.

// Handle the lvalue reference differently since it should not be moved.


// Handle case where the kernel returns void.

 // namespace detail

// See [Note: Argument forwarding in the dispatcher] for why Args doesn't use &&


// See [Note: Argument forwarding in the dispatcher] for why Args doesn't use &&


// See [Note: Argument forwarding in the dispatcher] for why Args doesn't use &&






 // namespace c10


// Parsed from ATen/core/op_registration/op_allowlist.h

// #pragma once

// TODO: unify to C10_MOBILE. In theory this header could be used in OSS.
// #ifdef TEMPLATE_SELECTIVE_BUILD
// #include <ATen/selected_mobile_ops.h>
// #endif

/**
 * This header implements functionality to build PyTorch with only a certain
 * set of operators (+ dependencies) included.
 *
 * - Build with -DTORCH_OPERATOR_WHITELIST="aten::add;aten::sub" and only these
 *   two ops will be included in your build.  The allowlist records operators
 *   only, no overloads; if you include aten::add, all overloads of aten::add
 *   will be included.
 *
 * Internally, this is done by removing the operator registration calls
 * using compile time programming, and the linker will then prune all
 * operator functions that weren't registered.
 * See Note [Selective build] for more details
 *
 * WARNING: The allowlist mechanism doesn't work for all ways you could go about
 * registering an operator.  If the dispatch key / operator name is not
 * sufficiently obvious at compile time, then the allowlisting mechanism
 * will fail (and the operator will be included in the binary anyway).
 */

// #include <c10/util/string_view.h>
// #include <c10/core/DispatchKey.h>
// #include <c10/macros/Macros.h>

// returns true iff allowlist contains item
// op_allowlist_contains("a;bc;d", "bc") == true
@Namespace("c10::impl") public static native @Cast("const bool") boolean op_allowlist_contains(@ByVal @Cast("c10::string_view*") Pointer allowlist, @ByVal @Cast("c10::string_view*") Pointer item);

// Returns true iff the given op name is on the allowlist
// and should be registered
@Namespace("c10::impl") public static native @Cast("const bool") boolean op_allowlist_check(@ByVal @Cast("c10::string_view*") Pointer op_name);

// Returns true iff the given schema string is on the allowlist
// and should be registered
@Namespace("c10::impl") public static native @Cast("const bool") boolean schema_allowlist_check(@ByVal @Cast("c10::string_view*") Pointer schema);

// schema_allowlist_check() implicitly depends on a macro, TORCH_OPERATOR_WHITELIST.
// Add this API to pass arbitrary allowlist.
@Namespace("c10::impl") public static native @Cast("const bool") boolean op_allowlist_contains_name_in_schema(@ByVal @Cast("c10::string_view*") Pointer allowlist, @ByVal @Cast("c10::string_view*") Pointer schema);

// Returns true iff the given dispatch key is on the allowlist
// and should be registered.  When we turn this on, the list of valid
// mobile dispatch keys is hard coded (but you need to make sure
// that you have the correct set of dispatch keys for this).
@Namespace("c10::impl") public static native @Cast("const bool") boolean dispatch_key_allowlist_check(DispatchKey k);
@Namespace("c10::impl") public static native @Cast("const bool") boolean dispatch_key_allowlist_check(@Cast("c10::DispatchKey") byte k);

 // namespace impl
 // namespace c10


// Parsed from ATen/record_function.h

// #pragma once

// #include <ATen/core/ivalue.h>
// #include <ATen/core/operator_name.h>
// #include <c10/macros/Export.h>
// #include <c10/util/Optional.h>
// #include <c10/util/SmallVector.h>

// #include <array>
// #include <atomic>
// #include <functional>
// #include <memory>


// Kind of record function scope;
@Namespace("at") public enum RecordScope {
  // c10/ATen ops, autograd nodes
  FUNCTION((byte)(0)),
  // Functions/nodes called from the autograd
  BACKWARD_FUNCTION((byte)(1)),
  // TorchScript functions, methods
  TORCHSCRIPT_FUNCTION((byte)(2)),
  // Kernel Function dtype Tag
  KERNEL_FUNCTION_DTYPE((byte)(3)),
  // Kernel Function dtype Tag
  LITE_INTERPRETER((byte)(4)),
  // User defined scope (e.g. with record_function())
  USER_SCOPE((byte)(5)),
  NUM_SCOPES((byte)(6));// must be the last in the list

    public final byte value;
    private RecordScope(byte v) { this.value = v; }
    private RecordScope(RecordScope e) { this.value = e.value; }
    public RecordScope intern() { for (RecordScope e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

 // namespace at

// Targeting ../StringView.java



// Soft limit on the number of callbacks to use;
@Namespace("at") @MemberGetter public static native @Cast("const std::size_t") long kSoftLimitCallbacks();
// Targeting ../ObserverContext.java


// Targeting ../RecordFunction.java



//
// PyTorch callbacks/observers API:
//

/**
 * RecordFunctionCallback represents a pair of callbacks to be used with
 * RecordFunction, members:
 *   start, end - the callbacks to run when entering and exiting the scope;
 *     optionally, the start callback may return an ObserverContext which will
 *     be passed to the end callback, use appropriate constructor accordingly.
 *   needs_inputs - whether the callbacks need the inputs passed from the observed
 *     function/range; NOTE: passing the inputs incurs an additional overhead;
 *   sampling_probability - if not 1.0, then the callback is probabilistically sampled
 *     to run; NOTE: start and end callbacks always run as a pair and are sampled
 *     together;
 *   scopes - types of scopes to execute the callbacks on (see RecordScope);
 *     passing empty set means the callbacks will be executed for all possible
 *     scope types
 *   should_run - optional function that returns whether this callback should run;
 *     overwrites the effect of setting sampling_probability
 */

// Using macro to minimize inputs copies,
// optional argument - function's seq_no
// #define RECORD_FUNCTION_WITH_SCOPE(scope, fn, inputs, ...)
//   at::RecordFunction guard(scope);
//   if (guard.isActive()) {
//     if (guard.needsInputs()) {
//       guard.before(fn, inputs, ##__VA_ARGS__);
//     } else {
//       guard.before(fn, ##__VA_ARGS__);
//     }
//   }

// #define RECORD_FUNCTION(fn, inputs, ...)
//   RECORD_FUNCTION_WITH_SCOPE(
//     at::RecordScope::FUNCTION,
//     fn, inputs, ##__VA_ARGS__)

// #define RECORD_TORCHSCRIPT_FUNCTION(mn, inputs)
//   RECORD_FUNCTION_WITH_SCOPE(
//     at::RecordScope::TORCHSCRIPT_FUNCTION, mn, inputs)

// Custom user scopes in C++; similar to Python's 'with record_function("..."):'
// #define RECORD_USER_SCOPE(fn)
//   RECORD_FUNCTION_WITH_SCOPE(
//     at::RecordScope::USER_SCOPE, fn, {})

// RECORD_USER_SCOPE with inputs
// #define RECORD_USER_SCOPE_WITH_INPUTS(fn, inputs)
//   RECORD_FUNCTION_WITH_SCOPE(
//     at::RecordScope::USER_SCOPE, fn, inputs)

// Helper macro to pass in debug handle that is used to
// post process events
// #define RECORD_WITH_SCOPE_DEBUG_HANDLE_AND_INPUTS(
//     scope, fn, debug_handle, inputs, ...)
//     at::RecordFunction guard(scope);
//     if (guard.isActive()) {
//       guard.setDebugHandle(debug_handle);
//       if (guard.needsInputs()) {
//         guard.before(fn, inputs, ##__VA_ARGS__);
//       } else {
//         guard.before(fn, ##__VA_ARGS__);
//       }
//     }

// Helper macros to record LITE INTERPETER scope events with debug handles
// #define RECORD_EDGE_SCOPE_WITH_DEBUG_HANDLE_AND_INPUTS(
//     fn, debug_handle, inputs)
//     RECORD_WITH_SCOPE_DEBUG_HANDLE_AND_INPUTS(
//         at::RecordScope::LITE_INTERPRETER, fn, debug_handle, inputs)

// Notes:
//  - two types of callbacks are provided: thread local and global
//     - thread local callbacks are added/removed only for the given thread
//       and are stored locally for each thread and separately from the list
//       of the global callbacks
//     - global callbacks are stored in a single per process list and are
//       invoked by every RecordFunction, in addition to the thread local
//       callbacks specific to the given thread
//  - we allow the added callbacks to be sampled, by specifying a sampling
//    probability for each callback pair, if the start callback is
//    not picked to run, the corresponding end callback won't be called
//  - a typical use case for the global callbacks is passive monitoring
//    in the background (e.g. fleet-wide monitoring), without focusing on
//    the specific peice of code
//  - in contrast, thread local callbacks are enabled locally, on demand,
//    for the specific piece of code (range) and are not sampled
//  - a typical use case for thread local callbacks is profiler and code
//    execution tracer
//  - note, thread local callbacks are automatically propagated with
//    ThreadLocalState across JIT continuations and async tasks (at::launch)
//  - adding/removing global callbacks is not thread safe and should be done
//    only when no other code is running, e.g. during the initialization
// Targeting ../GlobalRecordFunctionCallbacksEntry.java


// Targeting ../ThreadLocalRecordFunctionCallbacksEntry.java



// Holds pairs (callbacks, unique_id)

/**
 * addThreadLocalCallback adds a thread local callback to run with RecordFunction,
 * returns handle to use with removeThreadLocalCallback
 */
@Namespace("at") public static native @Cast("at::CallbackHandle") long addThreadLocalCallback(
    @ByVal @Cast("at::RecordFunctionCallback*") Pointer cb);

/**
 * hasThreadLocalCallbacks returns whether there're callbacks registered
 * with addThreadLocalCallback
 */
@Namespace("at") public static native @Cast("bool") boolean hasThreadLocalCallbacks();

/**
 * clearThreadLocalCallbacks removes all thread local callbacks
 */
@Namespace("at") public static native void clearThreadLocalCallbacks();

/**
 * addGlobalCallback adds a global callback to run with RecordFunction:
 *
 * WARNING: not thread safe, typically addGlobalCallback can be called
 * only during the program initialization
 */
@Namespace("at") public static native @Cast("at::CallbackHandle") long addGlobalCallback(
    @ByVal @Cast("at::RecordFunctionCallback*") Pointer cb);

/**
 * removeCallback removes a callback given the handle returned by
 * addThreadLocalCallback or addGlobalCallback;
 *
 * WARNING: removing a global callback is not thread safe,
 * no other code can run simultaneously
 */
@Namespace("at") public static native void removeCallback(@Cast("at::CallbackHandle") long handle);

/**
 * Prevent the given callback from executing. If handle is invalid,
 * does nothing.
 */
@Namespace("at") public static native void disableCallback(@Cast("at::CallbackHandle") long handle);

/**
 * Allow the given callback, previously disabled with disableCallback, to
 * execute again. If handle is invalid, does nothing.
 */
@Namespace("at") public static native void reenableCallback(@Cast("at::CallbackHandle") long handle);

/**
 * hasGlobalCallbacks returns whether there're global callbacks
 * registered with pushGlobalCallback
 */
@Namespace("at") public static native @Cast("bool") boolean hasGlobalCallbacks();

/**
 * clearGlobalCallbacks removes all global callbacks
 * WARNING: not thread safe
 */
@Namespace("at") public static native void clearGlobalCallbacks();

// for both thread local and global callbacks
@Namespace("at") public static native @Cast("bool") boolean hasCallbacks();
@Namespace("at") public static native void clearCallbacks(); // not thread safe

/**
 * enableRecordFunction enables RecordFunction thread locally
 */
@Namespace("at") public static native void enableRecordFunction(@Cast("bool") boolean enable/*=true*/);
@Namespace("at") public static native void enableRecordFunction();

/**
 * isRecordFunctionEnabled returns whether RecordFunction
 * is enabled thread locally
 */
@Namespace("at") public static native @Cast("bool") boolean isRecordFunctionEnabled();
// Targeting ../RecordFunctionGuard.java


// Targeting ../DisableRecordFunctionGuard.java


// Targeting ../RecordFunctionTLS.java



@Namespace("at") public static native @Const @ByRef RecordFunctionTLS get_record_function_tls_();

@Namespace("at") public static native void set_record_function_tls_(@Const @ByRef RecordFunctionTLS tls);

// Checks whether RecordFunction should be called,
// sets boolean pointed by the argument to whether pre-sampling was used
@Namespace("at") public static native @Cast("bool") boolean shouldRunRecordFunction(@Cast("bool*") BoolPointer arg0);
@Namespace("at") public static native @Cast("bool") boolean shouldRunRecordFunction(@Cast("bool*") boolean[] arg0);

// The following functions are used to disable/enable pre-sampling of RecordFunction
// when high-frequency/non-sampled callbacks are added/removed.
// Note: every call to bumpRecordAllFunctions() is supposed to be matched with
// the corresponding releaseRecordAllFunctions() call.
// Note: disabling pre-sampling of RecordFunction incurs an extra overhead, since
// RecordFunction will be created for each operator call.
@Namespace("at") public static native void bumpRecordAllFunctions();
@Namespace("at") public static native void releaseRecordAllFunctions();
@Namespace("at") public static native @Cast("bool") boolean checkRecordAllFunctions();

 // namespace at


// Parsed from ATen/ThreadLocalState.h

// #pragma once

// #include <c10/core/InferenceMode.h>
// #include <c10/core/impl/LocalDispatchKeySet.h>
// #include <c10/util/Exception.h>
// #include <c10/util/ThreadLocalDebugInfo.h>

// #include <ATen/record_function.h>
// #include <ATen/core/PythonModeTLS.h>
// Targeting ../ThreadLocalState.java


// Targeting ../ThreadLocalStateGuard.java



 // namespace at


// Parsed from ATen/ATen.h

// #pragma once

// #if !defined(_MSC_VER) && __cplusplus < 201402L
// #error C++14 or later compatible compiler is required to use ATen.
// #endif

// #include <c10/core/Allocator.h>
// #include <ATen/core/ATenGeneral.h>
// #include <ATen/Context.h>
// #include <ATen/Device.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/DimVector.h>
// #include <ATen/Dispatch.h>
// #include <ATen/Formatting.h>
// #include <ATen/Functions.h>
// #include <ATen/NamedTensor.h>
// #include <ATen/ScalarOps.h>
// #include <ATen/Tensor.h>
// #include <ATen/TensorGeometry.h>
// #include <ATen/TensorIndexing.h>
// #include <ATen/TensorOperators.h>
// #include <ATen/Version.h>
// #include <ATen/core/ATenGeneral.h>
// #include <ATen/core/Generator.h>
// #include <c10/core/Layout.h>
// #include <ATen/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <ATen/core/Reduction.h>
// #include <c10/util/Exception.h>
// #include <ATen/core/UnsafeFromTH.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/jit_type.h>
// #include <c10/core/InferenceMode.h>


// Parsed from ATen/Config.h

// #pragma once

// Test these using #if AT_MKL_ENABLED(), not #ifdef, so that it's
// obvious if you forgot to include Config.h
//    c.f. https://stackoverflow.com/questions/33759787/generating-an-error-if-checked-boolean-macro-is-not-defined
//
// DO NOT put the macros for CUDA libraries in this file; they belong in cuda/CUDAConfig.h

// #define AT_MKLDNN_ENABLED() 0
// #define AT_MKL_ENABLED() 0
// #define AT_FFTW_ENABLED() 1
// #define AT_POCKETFFT_ENABLED() 1
// #define AT_NNPACK_ENABLED() 1
// #define CAFFE2_STATIC_LINK_CUDA() 0
// #define AT_BUILD_WITH_BLAS() 1
// #define AT_BUILD_WITH_LAPACK() 1
public static final int AT_PARALLEL_OPENMP = 1;
public static final int AT_PARALLEL_NATIVE = 0;
public static final int AT_PARALLEL_NATIVE_TBB = 0;
// #define AT_BLAS_F2C() 0
// #define AT_BLAS_USE_CBLAS_DOT() 1


// Parsed from ATen/Device.h

// #pragma once
// #include <c10/core/Device.h>


// Parsed from ATen/DeviceGuard.h

// #pragma once

// #include <c10/core/DeviceGuard.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/ScalarType.h> // TensorList whyyyyy

// Are you here because you're wondering why DeviceGuard(tensor) no
// longer works?  For code organization reasons, we have temporarily(?)
// removed this constructor from DeviceGuard.  The new way to
// spell it is:
//
//    OptionalDeviceGuard guard(device_of(tensor));

/** Return the Device of a Tensor, if the Tensor is defined. */
@Namespace("at") public static native @ByVal DeviceOptional device_of(@Const @ByRef Tensor t);

@Namespace("at") public static native @ByVal DeviceOptional device_of(@Const @ByRef TensorOptional t);

/** Return the Device of a TensorList, if the list is non-empty and
 *  the first Tensor is defined.  (This function implicitly assumes
 *  that all tensors in the list have the same device.) */
@Namespace("at") public static native @ByVal DeviceOptional device_of(@ByVal TensorArrayRef t);

 // namespace at


// Parsed from ATen/DimVector.h

// #pragma once
// #include <ATen/core/DimVector.h>


// Parsed from ATen/Dispatch.h

// #pragma once

// #include <ATen/core/DeprecatedTypeProperties.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Half.h>
// #include <c10/util/Metaprogramming.h>
// #include <c10/util/complex.h>
// #include <c10/util/string_view.h>

// #ifdef TEMPLATE_SELECTIVE_BUILD
// #include <ATen/selected_mobile_ops.h>
// #else
/**
 * The method should_include_kernel_dtype() returns true/false
 * based on whether the switching code for a specific dtype should be
 * included based on build time constants generated from tracing model
 * execution. This method will be implmeneted via code-generation and
 * included in this file when code-gen is ready.
 */
@Namespace("at") public static native @Cast("const bool") boolean should_include_kernel_dtype(
  @Cast("const char*") BytePointer kernel_tag_str,
  ScalarType scalar_type
);
@Namespace("at") public static native @Cast("const bool") boolean should_include_kernel_dtype(
  String kernel_tag_str,
  ScalarType scalar_type
);

// #endif

/**
 * In the Facebook internal build (using BUCK), this macro is enabled by
 * passing in -c pt.enable_record_kernel_dtype=1 when building the tracer
 * binary.
 */
// #if defined ENABLE_RECORD_KERNEL_FUNCTION_DTYPE
// #else
// #define RECORD_KERNEL_FUNCTION_DTYPE(NAME, enum_type)
// #endif

// #if defined __cpp_if_constexpr
// #define AT_PRIVATE_CASE_TYPE_USING_HINT(NAME, enum_type, type, HINT, ...)
//   case enum_type: {
//     if constexpr (!at::should_include_kernel_dtype(NAME, enum_type)) {
//       AT_ERROR("dtype '", toString(enum_type), "' not selected for kernel tag ", #NAME);
//     }
//     using HINT = type;
//     return __VA_ARGS__();
//   }
// #else
// #define AT_PRIVATE_CASE_TYPE_USING_HINT(NAME, enum_type, type, HINT, ...)
//   case enum_type: {
//     at::guts::if_constexpr<(!at::should_include_kernel_dtype(NAME, enum_type))>(
//       [] {
//         AT_ERROR("dtype '" #enum_type "' not selected for kernel tag " #NAME);
//       }
//     );
//     using HINT = type;
//     return __VA_ARGS__();
//   }
// #endif
// 

// #define AT_PRIVATE_CASE_TYPE(NAME, enum_type, type, ...)
//   AT_PRIVATE_CASE_TYPE_USING_HINT(NAME, enum_type, type, scalar_t, __VA_ARGS__)

// Workaround for C10_UNUSED because CUDA 10.1 and below fails to handle unused
// attribute in the type aliasing context. Keep name long and verbose to avoid
// macro collisions.
// #if defined(__CUDACC__) && CUDA_VERSION <= 10100
// #define C10_UNUSED_DISPATCH_CUDA_WORKAROUND
// #else
// #define C10_UNUSED_DISPATCH_CUDA_WORKAROUND C10_UNUSED
// #endif // defined(__CUDACC__) && CUDA_VERSION <= 10100

// #if defined __cpp_if_constexpr
// #define AT_QINT_PRIVATE_CASE_TYPE(
//     NAME, enum_type, type, underlying_enum, underlying_type, ...)
//   case enum_type: {
//     if constexpr (!at::should_include_kernel_dtype(NAME, enum_type)) {
//       AT_ERROR("dtype '", toString(enum_type), "' not selected for kernel tag ", #NAME);
//     }
//     using scalar_t = type;
//     using underlying_t C10_UNUSED_DISPATCH_CUDA_WORKAROUND =
//         scalar_t::underlying;
//     const auto& SCALAR_TYPE C10_UNUSED_DISPATCH_CUDA_WORKAROUND = enum_type;
//     const auto& UNDERLYING_TYPE C10_UNUSED_DISPATCH_CUDA_WORKAROUND =
//         toUnderlying(enum_type);
//     (void)SCALAR_TYPE;  /* Suppress unused-var compiler warning */
//     /* TODO: Use [[maybe-unused]] when C++17 becomes the standard */
//     return __VA_ARGS__();
//   }
// #else
// #define AT_QINT_PRIVATE_CASE_TYPE(
//     NAME, enum_type, type, underlying_enum, underlying_type, ...)
//   case enum_type: {
//     at::guts::if_constexpr<(!at::should_include_kernel_dtype(NAME, enum_type))>(
//       [] {
//         AT_ERROR("dtype '" #enum_type "' not selected for kernel tag " #NAME);
//       }
//     );
//     using scalar_t = type;
//     using underlying_t C10_UNUSED_DISPATCH_CUDA_WORKAROUND =
//         scalar_t::underlying;
//     const auto& SCALAR_TYPE C10_UNUSED_DISPATCH_CUDA_WORKAROUND = enum_type;
//     const auto& UNDERLYING_TYPE C10_UNUSED_DISPATCH_CUDA_WORKAROUND =
//         toUnderlying(enum_type);
//     (void)SCALAR_TYPE;  /* Suppress unused-var compiler warning */
//     /* TODO: Use [[maybe-unused]] when C++17 becomes the standard */
//     return __VA_ARGS__();
//   }
// #endif

// #if defined __cpp_if_constexpr
// #define AT_QINT_SUB_BYTE_PRIVATE_CASE_TYPE(
//     NAME, enum_type, type, underlying_type, bitwidth, qmin, qmax, ...)
//   case enum_type: {
//       if constexpr (!at::should_include_kernel_dtype(NAME, enum_type)) {
//       AT_ERROR("dtype '", toString(enum_type), "' not selected for kernel tag ", #NAME);
//     }
//     using scalar_t = type;
//     using underlying_t C10_UNUSED_DISPATCH_CUDA_WORKAROUND =
//         scalar_t::underlying;
//     const auto& SCALAR_TYPE C10_UNUSED_DISPATCH_CUDA_WORKAROUND = enum_type;
//     const auto& UNDERLYING_TYPE C10_UNUSED_DISPATCH_CUDA_WORKAROUND =
//         toUnderlying(enum_type);
//     int bit_width = bitwidth;
//     int64_t quant_min = qmin;
//     int64_t quant_max = qmax;
//     (void)bit_width; /* Suppress unused variable warning */
//     (void)quant_min; /* Suppress unused variable warning */
//     (void)quant_max; /* Suppress unused variable warning */
//     return __VA_ARGS__();
//   }
// #else
// #define AT_QINT_SUB_BYTE_PRIVATE_CASE_TYPE(
//     NAME, enum_type, type, underlying_type, bitwidth, qmin, qmax, ...)
//   case enum_type: {
//       at::guts::if_constexpr<(!at::should_include_kernel_dtype(NAME, enum_type))>(
//       [] {
//         AT_ERROR("dtype '" #enum_type "' not selected for kernel tag " #NAME);
//       }
//     );
//     using scalar_t = type;
//     using underlying_t C10_UNUSED_DISPATCH_CUDA_WORKAROUND =
//         scalar_t::underlying;
//     const auto& SCALAR_TYPE C10_UNUSED_DISPATCH_CUDA_WORKAROUND = enum_type;
//     const auto& UNDERLYING_TYPE C10_UNUSED_DISPATCH_CUDA_WORKAROUND =
//         toUnderlying(enum_type);
//     int bit_width = bitwidth;
//     int64_t quant_min = qmin;
//     int64_t quant_max = qmax;
//     (void)bit_width; /* Suppress unused variable warning */
//     (void)quant_min; /* Suppress unused variable warning */
//     (void)quant_max; /* Suppress unused variable warning */
//     return __VA_ARGS__();
//   }
// #endif

@Namespace("detail") public static native ScalarType scalar_type(ScalarType s);

@Namespace("detail") public static native @Deprecated ScalarType scalar_type(@Const @ByRef DeprecatedTypeProperties t);

@Namespace("detail") public static native @Deprecated void deprecated_AT_DISPATCH_ALL_TYPES_AND_HALF();

@Namespace("detail") public static native @Deprecated void deprecated_AT_DISPATCH_ALL_TYPES_AND_HALF_AND_COMPLEX();

 // namespace detail

// The AT_DISPATCH_* family of macros provides the ability to
// conveniently generate specializations of a kernel over all of the
// dtypes we care about in PyTorch.  We call it "dispatch" because
// we are "dispatching" to the correct, dtype-specific kernel.
//
// A standard usage looks like:
//
//      AT_DISPATCH_ALL_TYPES(self.scalar_type(), "op_name", [&] {
//          // Your code here, with 'scalar_t' now defined to
//          // be the dtype in question
//      })
//
// There are many variations of this macro, so it's important to
// understand exactly /which/ dtypes you want to get instantiated, as
// well as what the "default" set is.
//
// The default set of dtypes that are instantiated (e.g., by
// AT_DISPATCH_ALL_TYPES) are floating point types (float, double),
// and integral types (int32_t, int64_t, int16_t, int8_t, uint8_t),
// but NOT booleans (bool), half-precision floats (Half) or
// complex number (c10::complex<float>, c10::complex<double>).
// This "cut" is somewhat historical (the default types are the
// ones that TH historically supported), but it also reflects the
// fact that the non-default types are "poorly" behaved (booleans
// are NOT integers mod 2, half precision operations ~essentially
// don't exist on CPU, complex numbers are an experimental application).
//
// Here are the questions you should generally ask to decide which
// dispatch you want:
//
// 1. Is this an integral or floating point specific operation?
//    (If so, you'll want one of the FLOATING or INTEGRAL macros.)
//
// 2. Should half be supported?  (If you're on CPU, the answer is almost
//    definitely no.  If you do want support, use one of the AND_HALF
//    macros)
//
// Much rarer situations:
//
// 3. Should bool be supported?  (You often have to write your kernel
//    differently if arithmetic operations are involved.)  If so,
//    Use AT_DISPATCH_ALL_TYPES_AND along with ScalarType::Bool
//
// 4. Should complex be supported?  The answer is almost always no,
//    unless you are working on "generic" code that should work on
//    all dtypes.
//
// Parameters:
// -----------
//
// 1. The NAME argument is a "tag" that is used to trace and then
//    conditionally compile fragments of the case statements such
//    that the kernel functions are specialized only for the dtypes
//    that are needed. The NAME parameter *must* be a build time
//    cons char* (can't be std::string, etc...)
//
// Please ensure that the NAME is unique for every implementation
// or you run the risk of over-including code for the kernel
// functions. There is no risk of missing out on any code, so
// it's mostly a risk of a Type-2 error, and not a Type-1 error.
//

// NB: the the_type variable is not used, but we have kept it for
// backwards compatibility.  It's probably not used by anyone though;
// but we're just being safe (and it doesn't hurt.)  Note we must
// use it to shut up warnings about unused store.

// #define AT_DISPATCH_FLOATING_TYPES(TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_FLOATING_TYPES_AND_HALF(TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_FLOATING_TYPES_AND(SCALARTYPE, TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME,
//           SCALARTYPE,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE>::t),
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(TYPE), "'");
//     }
//   }()

// #define AT_DISPATCH_FLOATING_TYPES_AND2(
//     SCALARTYPE1, SCALARTYPE2, TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE1,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE1>::t),
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE2,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE2>::t),
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(TYPE), "'");
//     }
//   }()

// #define AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES(TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           at::ScalarType::ComplexDouble,
//           c10::complex<double>,
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           at::ScalarType::ComplexFloat,
//           c10::complex<float>,
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(
//     SCALARTYPE, TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           at::ScalarType::ComplexDouble, c10::complex<double>, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           at::ScalarType::ComplexFloat, c10::complex<float>, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE>::t),
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(
//     SCALARTYPE1, SCALARTYPE2, TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           at::ScalarType::ComplexDouble,
//           c10::complex<double>,
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           at::ScalarType::ComplexFloat,
//           c10::complex<float>,
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE1,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE1>::t),
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE2,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE2>::t),
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_INTEGRAL_TYPES(TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Byte, uint8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Char, int8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Int, int32_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Long, int64_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Short, int16_t, __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_INTEGRAL_TYPES_AND(SCALARTYPE, TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Byte, uint8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Char, int8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Int, int32_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Long, int64_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Short, int16_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME,
//           SCALARTYPE,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE>::t),
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_ALL_TYPES(TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op  */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Byte, uint8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Char, int8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Int, int32_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Long, int64_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Short, int16_t, __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_COMPLEX_TYPES(TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           at::ScalarType::ComplexFloat,
//           c10::complex<float>,
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           at::ScalarType::ComplexDouble,
//           c10::complex<double>,
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_QINT_TYPES(TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_QINT_PRIVATE_CASE_TYPE(
//           NAME, at::kQInt8, at::qint8, at::kChar, int8_t, __VA_ARGS__)
//       AT_QINT_PRIVATE_CASE_TYPE(
//           NAME, at::kQUInt8, at::quint8, at::kByte, uint8_t, __VA_ARGS__)
//       AT_QINT_PRIVATE_CASE_TYPE(
//           NAME, at::kQInt32, at::qint32, at::kInt, int, __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(TYPE), "'");
//     }
//   }()

// #define AT_DISPATCH_QINT_AND_SUB_BYTE_TYPES(TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_QINT_SUB_BYTE_PRIVATE_CASE_TYPE(
//           NAME, at::kQInt8, at::qint8, int8_t, CHAR_BIT, SCHAR_MIN, SCHAR_MAX, __VA_ARGS__)
//       AT_QINT_SUB_BYTE_PRIVATE_CASE_TYPE(
//           NAME, at::kQUInt8, at::quint8, uint8_t, CHAR_BIT, 0, UCHAR_MAX, __VA_ARGS__)
//       AT_QINT_SUB_BYTE_PRIVATE_CASE_TYPE(
//           NAME, at::kQInt32, at::qint32, int, CHAR_BIT * sizeof(int), INT_MIN, INT_MAX, __VA_ARGS__)
//       AT_QINT_SUB_BYTE_PRIVATE_CASE_TYPE(
//           NAME, at::kQUInt4x2, at::quint4x2, uint8_t, 4, 0, 15, __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(TYPE), "'");
//     }
//   }()

// #define AT_DISPATCH_ALL_TYPES_AND_COMPLEX(TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op*/
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Byte, uint8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Char, int8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Int, int32_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Long, int64_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Short, int16_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME,
//           at::ScalarType::ComplexFloat, c10::complex<float>, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME,
//           at::ScalarType::ComplexDouble, c10::complex<double>, __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_ALL_TYPES_AND(SCALARTYPE, TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op*/
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Byte, uint8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Char, int8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Int, int32_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Long, int64_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Short, int16_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE>::t),
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND(SCALARTYPE, TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op*/
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Byte, uint8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Char, int8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Int, int32_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Long, int64_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Short, int16_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           at::ScalarType::ComplexFloat,
//           c10::complex<float>,
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           at::ScalarType::ComplexDouble,
//           c10::complex<double>,
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE>::t),
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_ALL_TYPES_AND2(SCALARTYPE1, SCALARTYPE2, TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op*/
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Byte, uint8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Char, int8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Int, int32_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Long, int64_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Short, int16_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE1,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE1>::t),
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE2,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE2>::t),
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(
//     SCALARTYPE1, SCALARTYPE2, TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op*/
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Byte, uint8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Char, int8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Int, int32_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Long, int64_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Short, int16_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME, at::ScalarType::ComplexFloat, c10::complex<float>, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME, at::ScalarType::ComplexDouble, c10::complex<double>, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE1,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE1>::t),
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE2,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE2>::t),
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_ALL_TYPES_AND3(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op*/
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Byte, uint8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Char, int8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Int, int32_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Long, int64_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Short, int16_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE1,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE1>::t),
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE2,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE2>::t),
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE3,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE3>::t),
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op*/
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Byte, uint8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Char, int8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Int, int32_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Long, int64_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Short, int16_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME, at::ScalarType::ComplexFloat, c10::complex<float>, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME, at::ScalarType::ComplexDouble, c10::complex<double>, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE1,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE1>::t),
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE2,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE2>::t),
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE3,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE3>::t),
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_INDEX_TYPES(TYPE, NAME, ...)
//   [&] {
//     const auto& the_index_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _it = ::detail::scalar_type(the_index_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _it)
//     switch (_it) {
//       AT_PRIVATE_CASE_TYPE_USING_HINT(NAME, at::ScalarType::Int, int32_t, index_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE_USING_HINT(NAME, at::ScalarType::Long, int64_t, index_t, __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_it), "'");
//     }
//   }()

// ----------------------------------------------------------------------------
// DEPRECATED MACROS, DON'T USE THESE
// ----------------------------------------------------------------------------

// #define AT_DISPATCH_ALL_TYPES_AND_HALF(TYPE, NAME, ...)
//   [&] {
//     detail::deprecated_AT_DISPATCH_ALL_TYPES_AND_HALF();
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Byte, uint8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Char, int8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Int, int32_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Long, int64_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Short, int16_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()


// Parsed from ATen/Formatting.h

// #include <ATen/core/Formatting.h>


// Parsed from ATen/Generator.h

// #pragma once
// #include <ATen/core/Generator.h>


// Parsed from ATen/Parallel.h

// #pragma once
// #include <ATen/Config.h>
// #include <ATen/core/ivalue.h>
// #include <c10/macros/Macros.h>

@Namespace("at") public static native @Cast("int64_t") long divup(@Cast("int64_t") long x, @Cast("int64_t") long y);

// Called during new thread initialization
@Namespace("at") public static native void init_num_threads();

// Sets the number of threads to be used in parallel region
@Namespace("at") public static native void set_num_threads(int arg0);

// Returns the maximum number of threads that may be used in a parallel region
@Namespace("at") public static native int get_num_threads();

// Returns the current thread number (starting from 0)
// in the current parallel region, or 0 in the sequential region
@Namespace("at") public static native int get_thread_num();

// Checks whether the code runs in parallel region
@Namespace("at") public static native @Cast("bool") boolean in_parallel_region();

// Initialise num_threads lazily at first parallel call
@Namespace("at::internal") public static native void lazy_init_num_threads();

@Namespace("at::internal") public static native void set_thread_num(int arg0);
// Targeting ../ThreadIdGuard.java



  // namespace internal

/*
parallel_for

begin: index at which to start applying user function

end: index at which to stop applying user function

grain_size: number of elements per chunk. impacts the degree of parallelization

f: user function applied in parallel to the chunks, signature:
  void f(int64_t begin, int64_t end)

Warning: parallel_for does NOT copy thread local
states from the current thread to the worker threads.
This means for example that Tensor operations CANNOT be used in the
body of your function, only data pointers.
*/

/*
parallel_reduce

begin: index at which to start applying reduction

end: index at which to stop applying reduction

grain_size: number of elements per chunk. impacts number of elements in
intermediate results tensor and degree of parallelization.

ident: identity for binary combination function sf. sf(ident, x) needs to return
x.

f: function for reduction over a chunk. f needs to be of signature scalar_t
f(int64_t partial_begin, int64_t partial_end, scalar_t identifiy)

sf: function to combine two partial results. sf needs to be of signature
scalar_t sf(scalar_t x, scalar_t y)

For example, you might have a tensor of 10000 entires and want to sum together
all the elements. Parallel_reduce with a grain_size of 2500 will then allocate
an intermediate result tensor with 4 elements. Then it will execute the function
"f" you provide and pass the beginning and end index of these chunks, so
0-2499, 2500-4999, etc. and the combination identity. It will then write out
the result from each of these chunks into the intermediate result tensor. After
that it'll reduce the partial results from each chunk into a single number using
the combination function sf and the identity ident. For a total summation this
would be "+" and 0 respectively. This is similar to tbb's approach [1], where
you need to provide a function to accumulate a subrange, a function to combine
two partial results and an identity.

Warning: parallel_reduce does NOT copy thread local
states from the current thread to the worker threads.
This means for example that Tensor operations CANNOT be used in the
body of your function, only data pointers.

[1] https://software.intel.com/en-us/node/506154
*/

// Returns a detailed string describing parallelization settings
@Namespace("at") public static native @StdString BytePointer get_parallel_info();

// Sets number of threads used for inter-op parallelism
@Namespace("at") public static native void set_num_interop_threads(int arg0);

// Returns the number of threads used for inter-op parallelism
@Namespace("at") public static native int get_num_interop_threads();

// Launches inter-op parallel task

 // namespace internal

// Launches intra-op parallel task
@Namespace("at") public static native void intraop_launch(@ByVal Func func);

// Launches intra-op parallel task, returns a future

// Returns number of intra-op threads used by default
@Namespace("at") public static native int intraop_default_num_threads();

 // namespace at

// #if AT_PARALLEL_OPENMP
// #include <ATen/ParallelOpenMP.h>
// #elif AT_PARALLEL_NATIVE
// #include <ATen/ParallelNative.h>
// #elif AT_PARALLEL_NATIVE_TBB
// #include <ATen/ParallelNativeTBB.h>
// #endif


// Parsed from ATen/Utils.h

// #pragma once

// #include <ATen/core/ATenGeneral.h>
// #include <ATen/core/Generator.h>
// #include <ATen/Formatting.h>
// #include <c10/core/ScalarType.h>
// #include <c10/core/StorageImpl.h>
// #include <c10/core/UndefinedTensorImpl.h>
// #include <c10/util/accumulate.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>
// #include <c10/util/irange.h>

// #include <algorithm>
// #include <sstream>
// #include <typeinfo>
// #include <numeric>
// #include <memory>

// #define AT_DISALLOW_COPY_AND_ASSIGN(TypeName)
//   TypeName(const TypeName&) = delete;
//   void operator=(const TypeName&) = delete

@Namespace("at") public static native int _crash_if_asan(int arg0);

// TODO: This unwrapping code is ONLY used for TH bindings; once TH goes
// away, we can delete this function
@Namespace("at") public static native TensorImpl checked_dense_tensor_unwrap(@Const @ByRef Tensor expr, @Cast("const char*") BytePointer name, int pos, @Cast("const char*") BytePointer api, @Cast("bool") boolean allowNull, DeviceType device_type, ScalarType scalar_type);
@Namespace("at") public static native TensorImpl checked_dense_tensor_unwrap(@Const @ByRef Tensor expr, String name, int pos, String api, @Cast("bool") boolean allowNull, @Cast("c10::DeviceType") byte device_type, ScalarType scalar_type);

// Converts a TensorList (i.e. ArrayRef<Tensor> to vector of TensorImpl*)
// NB: This is ONLY used by legacy TH bindings, and ONLY used by cat.
// Once cat is ported entirely to ATen this can be deleted!
@Namespace("at") public static native @ByVal TensorImplVector checked_dense_tensor_list_unwrap(@ByVal TensorArrayRef tensors, @Cast("const char*") BytePointer name, int pos, DeviceType device_type, ScalarType scalar_type);
@Namespace("at") public static native @ByVal TensorImplVector checked_dense_tensor_list_unwrap(@ByVal TensorArrayRef tensors, String name, int pos, @Cast("c10::DeviceType") byte device_type, ScalarType scalar_type);

/**
 * Utility function to static cast input Generator* to
 * the backend generator type (CPU/CUDAGeneratorImpl etc.)
 */

/**
 * Utility function used in tensor implementations, which
 * supplies the default generator to tensors, if an input generator
 * is not supplied. The input Generator* is also static casted to
 * the backend generator type (CPU/CUDAGeneratorImpl etc.)
 */

@Namespace("at") public static native void check_size_nonnegative(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native void check_size_nonnegative(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at::detail") public static native @ByVal Tensor empty_cpu(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype_opt, @ByVal LayoutOptional layout_opt,
                 @ByVal DeviceOptional device_opt, @ByVal BoolOptional pin_memory_opt, @ByVal MemoryFormatOptional memory_format_opt);
@Namespace("at::detail") public static native @ByVal Tensor empty_cpu(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype_opt, @ByVal LayoutOptional layout_opt,
                 @ByVal DeviceOptional device_opt, @ByVal BoolOptional pin_memory_opt, @ByVal MemoryFormatOptional memory_format_opt);

@Namespace("at::detail") public static native @ByVal Tensor empty_generic(
  @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size,
  Allocator allocator,
  DispatchKey dispatch_key,
  ScalarType dtype,
  @ByVal Device device,
  @ByVal MemoryFormatOptional memory_format
);
@Namespace("at::detail") public static native @ByVal Tensor empty_generic(
  @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size,
  Allocator allocator,
  @Cast("c10::DispatchKey") byte dispatch_key,
  ScalarType dtype,
  @ByVal Device device,
  @ByVal MemoryFormatOptional memory_format
);
 // namespace detail


 // at


// Parsed from ATen/TracerMode.h

// #pragma once

// #include <c10/core/impl/LocalDispatchKeySet.h>
// #include <c10/macros/Macros.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>

// NOTE [Tracing Mode Switches]
//
// Historically, tracing function was controlled by two switches:
//
// - `AutoDispatchBelowADInplaceOrView` guard
//
//    Tracing function used to be script-generated inside `VariableType_*.cpp`
//    kernels, sharing the same `Autograd` dispatch key with autograd function.
//    Therefore, before tracing function was moved out of VariableType,
//    `AutoDispatchBelowADInplaceOrView` guard can also disable tracing as a side effect
//    of disabling `Autograd` dispatching.
//
// - `setTracingState()` API in `torch/csrc/jit/frontend/tracer.h`
//
//    It stores tracing data in a `TracingState` object in TLS. If the
//    `TracingState` object in TLS is `null`, then tracing is paused.
//
//    The `TracingState` object is created in `tracer::trace()` - the main
//    entrance of tracing function. It's temporarily set to `null` inside
//    generated VariableType (now TraceType) to bypass tracing for intermediate
//    ops (ops being called by other ops). After the intermediate op call
//    finishes it's set back to the original `TracingState` object.
//
//    The `TracingState` obect in TLS can also be read/written via its Python
//    binding in `python_tracer.cpp`, and `get/setTracingState()` C++ APIs,
//    which are also exposed as `TORCH_API`.
//
// Two new switches were introduced since tracing function was moved out of
// VariableType:
//
// - `tracer::impl::set_dispatch_enabled()` API
//
//    Unlike the special `Autograd` dispatch key which is included in dispatch
//    key set by default, `Tracer` dispatch key is off by default. The
//    dispatching switch can be toggled via this new API.
//
// - `tracer::impl::NoTracerDispatchMode` guard
//
//    It's used to cover the old semantics of `AutoDispatchBelowADInplaceOrView` after
//    tracing was moved out of VariableType.
//
// Before tracing function was moved out of VariableType, tracing was enabled
// when the following conditions are satisfied:
//
//    1) `TracingState` object in TLS != null;
//       - Either inside the execution scope of `tracer::trace()`, or
//       - Eagerly called `setTracingState()` with non-null object.
//    2) Not inside `AutoDispatchBelowADInplaceOrView` scope;
//
// After:
//
//    1) `TracingState` object in TLS != null;
//    2) Has called `tracer::impl::set_dispatch_enabled(true)`;
//    3) Not inside `tracer::impl::NonDispatchGuard` scope;
//
// [TODOs]
//
// - `setTracingState()` v.s. `tracer::impl::set_dispatch_enabled()`
//
//   Currently `set_dispatch_enabled()` is set/unset inside `setTracingState()`
//   to keep the semantics exactly the same as before - it's confusing to keep
//   both switches, though. We should consider simplifying/limiting the exposed
//   `setTracingState()` Python/C++ APIs (and other APIs calling it) so that
//   these two can be unified.
//
// - `AutoDispatchBelowADInplaceOrView` v.s. `tracer::impl::NoTracerDispatchMode`
//
//   We don't need to always set both guards together to keep semantics
//   unchanged. For the follow use cases of `AutoDispatchBelowADInplaceOrView` we don't
//   need set the new tracer guard:
//
//   * Script-generated VariableType kernels. The guard is not necessary as
//     tracing is already disabled explicitly by `setTracingState(null)` in
//     generated TraceType kernels - we could keep it as is or use the new guard
//     instead.
//
//   * Custom ops. Will be handled by fallback kernel for `Tracer`.
//
//   * Functions that are not likely to be called in tracing context (no python
//     binding / not an operator), e.g.: all mobile forward() wrappers, test
//     binaries, and etc.
//
//   * Where new threads are spawned, e.g.: ATen/native/ConvolutionMM2d.cpp.
//     It's not necessary as tracing is off by default.
//
//   For the rest of cases we might need have both:
//
//   * Functions that might be reachable from eager mode python (especially
//     factory methods), e.g.:
//     `internal_new_from_data()` in `torch/csrc/utils/tensor_new.cpp`.
//     Without the new guard it will add `aten::empty` to the traced graph.
//
//   * Some manually maintained functions, e.g.:
//     `torch/csrc/autograd/VariableTypeManual.cpp`.
//     Set the new guard if it's not obvious whether `setTracingState(null)`
//     has been called before it reaches the `AutoDispatchBelowADInplaceOrView` guard.
//
//   We might need tweak the usage of the new guard to optimize/fix things.
//   It should only affect the correctness of tracing function, because the
//   guard is essentially no-op when the master `setTracingState()` switch is
//   off.
// TODO: move this from `at::` to `jit::torch::` after
// `aten/src/ATen/cpp_custom_type_hack.h` is removed.

@Namespace("at::tracer::impl") public static native @Cast("bool") boolean is_dispatch_enabled();

@Namespace("at::tracer::impl") public static native void set_dispatch_enabled(@Cast("bool") boolean enabled);
// Targeting ../NoTracerDispatchMode.java



 // namespace impl
 // namespace tracer
 // namespace at


// Parsed from ATen/WrapDimUtils.h

// #pragma once

// #include <c10/core/WrapDimMinimal.h>
// #include <c10/core/TensorImpl.h>
// #include <ATen/core/Tensor.h>

@Namespace("at") public static native @Cast("int64_t") long maybe_wrap_dim(@Cast("int64_t") long dim, TensorImpl tensor);

@Namespace("at") public static native @Cast("int64_t") long maybe_wrap_dim(@Cast("int64_t") long dim, @ByVal TensorArrayRef tensors);

@Namespace("at") public static native @Cast("int64_t") long maybe_wrap_dim(@Cast("int64_t") long dim, @Cast("std::vector<int64_t>*") @StdVector LongVector tensor_sizes);

// wrap each dim in the dims array, taking dim_post_expr as the true number of dimensions
@Namespace("at") public static native void maybe_wrap_dims_n(@Cast("int64_t*") LongPointer dims, @Cast("int64_t") long ndims, @Cast("int64_t") long dim_post_expr);
@Namespace("at") public static native void maybe_wrap_dims_n(@Cast("int64_t*") LongBuffer dims, @Cast("int64_t") long ndims, @Cast("int64_t") long dim_post_expr);
@Namespace("at") public static native void maybe_wrap_dims_n(@Cast("int64_t*") long[] dims, @Cast("int64_t") long ndims, @Cast("int64_t") long dim_post_expr);

// Wrap each dim in a contiguous container, taking dim_post_expr as the true number of dimensions
// E.g. could also be std::array or c10::SmallVector

// previously, size [0] tensors were the only possible empty tensors; thus, it wasn't possible
// to cat empty tensors unless all the other tensors were 1-dimensional, so we allowed these tensors
// to be "skipped" (both for wrap dimension behavior and dimension size checking).
// We maintain this behavior for backwards compatibility, but only for this specific size
// (i.e. other empty sizes are not skipped).
@Namespace("at") public static native @Cast("int64_t") long legacy_cat_wrap_dim(@Cast("int64_t") long dim, @Cast("std::vector<int64_t>*") @StdVector LongVector tensor_sizes);

@Namespace("at") public static native @Cast("int64_t") long legacy_cat_wrap_dim(@Cast("int64_t") long dim, @ByVal TensorArrayRef tensors);

// wrap negative dims in a vector
@Namespace("at") public static native void wrap_all_dims(@Cast("std::vector<int64_t>*") @ByRef LongVector dims_to_wrap, @Cast("int64_t") long tensor_total_dims);




// Parsed from ATen/Tensor.h

// #pragma once

// #include <ATen/core/TensorBody.h>


// Parsed from ATen/TensorGeometry.h

// #pragma once

// #include <ATen/WrapDimUtils.h>
// #include <ATen/core/Tensor.h>
// Targeting ../TensorGeometry.java



 // namespace at


// Parsed from ATen/TensorNames.h

// #pragma once

// #include <ATen/WrapDimUtils.h>
// Targeting ../TensorName.java


// Targeting ../TensorNames.java




 // namespace at::namedinference


// Parsed from ATen/TensorUtils.h

// #pragma once

// #include <ATen/DimVector.h>
// #include <ATen/Tensor.h>
// #include <ATen/TensorGeometry.h>
// #include <ATen/Utils.h>

// These functions are NOT in Utils.h, because this file has a dep on Tensor.h
// Targeting ../TensorArg.java


// Targeting ../TensorGeometryArg.java



// A string describing which function did checks on its input
// arguments.
// TODO: Consider generalizing this into a call stack.

// The undefined convention: singular operators assume their arguments
// are defined, but functions which take multiple tensors will
// implicitly filter out undefined tensors (to make it easier to perform
// tests which should apply if the tensor is defined, and should not
// otherwise.)
//
// NB: This means that the n-ary operators take lists of TensorArg,
// not TensorGeometryArg, because the Tensor to TensorGeometry
// conversion will blow up if you have undefined tensors.

@Namespace("at") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @ByVal TensorGeometryArg t);
@Namespace("at") public static native void checkDim(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef Tensor tensor,
    @Cast("const char*") BytePointer name,
    int pos,
    @Cast("int64_t") long dim);
@Namespace("at") public static native void checkDim(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef Tensor tensor,
    String name,
    int pos,
    @Cast("int64_t") long dim);
@Namespace("at") public static native void checkDim(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim);
@Namespace("at") public static native void checkDim(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim);
// NB: this is an inclusive-exclusive range
@Namespace("at") public static native void checkDimRange(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim_start,
    @Cast("int64_t") long dim_end);
@Namespace("at") public static native void checkDimRange(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim_start,
    @Cast("int64_t") long dim_end);
@Namespace("at") public static native void checkSameDim(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t1,
    @Const @ByRef TensorGeometryArg t2);
@Namespace("at") public static native void checkSameDim(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t1,
    @Const @ByRef TensorGeometryArg t2);
@Namespace("at") public static native void checkContiguous(@Cast("at::CheckedFrom") BytePointer c, @Const @ByRef TensorGeometryArg t);
@Namespace("at") public static native void checkContiguous(@Cast("at::CheckedFrom") String c, @Const @ByRef TensorGeometryArg t);
@Namespace("at") public static native void checkAllContiguous(@Cast("at::CheckedFrom") BytePointer c, @ByVal TensorArgArrayRef ts);
@Namespace("at") public static native void checkAllContiguous(@Cast("at::CheckedFrom") String c, @ByVal TensorArgArrayRef ts);
@Namespace("at") public static native void checkSize(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("at") public static native void checkSize(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sizes);
@Namespace("at") public static native void checkSize(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim,
    @Cast("int64_t") long size);
@Namespace("at") public static native void checkSize(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim,
    @Cast("int64_t") long size);
@Namespace("at") public static native void checkNumel(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long numel);
@Namespace("at") public static native void checkNumel(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long numel);

@Namespace("at") public static native void checkAllSameNumel(@Cast("at::CheckedFrom") BytePointer c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkAllSameNumel(@Cast("at::CheckedFrom") String c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkScalarType(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorArg t,
    ScalarType s);
@Namespace("at") public static native void checkScalarType(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorArg t,
    ScalarType s);
@Namespace("at") public static native void checkScalarTypes(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorArg t,
    @ByVal ScalarTypeArrayRef l);
@Namespace("at") public static native void checkScalarTypes(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorArg t,
    @ByVal ScalarTypeArrayRef l);
@Namespace("at") public static native void checkSameGPU(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkSameGPU(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkAllSameGPU(@Cast("at::CheckedFrom") BytePointer c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkAllSameGPU(@Cast("at::CheckedFrom") String c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkSameType(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkSameType(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkAllSameType(@Cast("at::CheckedFrom") BytePointer c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkAllSameType(@Cast("at::CheckedFrom") String c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkSameSize(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkSameSize(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkDefined(@Cast("at::CheckedFrom") BytePointer c, @Const @ByRef TensorArg t);
@Namespace("at") public static native void checkDefined(@Cast("at::CheckedFrom") String c, @Const @ByRef TensorArg t);
@Namespace("at") public static native void checkAllDefined(@Cast("at::CheckedFrom") BytePointer c, @ByVal TensorArgArrayRef t);
@Namespace("at") public static native void checkAllDefined(@Cast("at::CheckedFrom") String c, @ByVal TensorArgArrayRef t);

// FixMe: does TensorArg slow things down?
@Namespace("at") public static native void checkBackend(
    @Cast("at::CheckedFrom") BytePointer c,
    @ByVal TensorArrayRef t,
    @ByVal Backend backend);
@Namespace("at") public static native void checkBackend(
    @Cast("at::CheckedFrom") String c,
    @ByVal TensorArrayRef t,
    @ByVal Backend backend);

@Namespace("at") public static native void checkDeviceType(
    @Cast("at::CheckedFrom") BytePointer c,
    @ByVal TensorArrayRef tensors,
    @ByVal DeviceType device_type);
@Namespace("at") public static native void checkDeviceType(
    @Cast("at::CheckedFrom") String c,
    @ByVal TensorArrayRef tensors,
    @ByVal DeviceType device_type);

@Namespace("at") public static native void checkLayout(@Cast("at::CheckedFrom") BytePointer c, @Const @ByRef Tensor t, Layout layout);
@Namespace("at") public static native void checkLayout(@Cast("at::CheckedFrom") String c, @Const @ByRef Tensor t, @Cast("c10::Layout") byte layout);

@Namespace("at") public static native void checkLayout(@Cast("at::CheckedFrom") BytePointer c, @ByVal TensorArrayRef tensors, @ByVal Layout layout);
@Namespace("at") public static native void checkLayout(@Cast("at::CheckedFrom") String c, @ByVal TensorArrayRef tensors, @ByVal Layout layout);

// Methods for getting data_ptr if tensor is defined
@Namespace("at") public static native Pointer maybe_data_ptr(@Const @ByRef Tensor tensor);
@Namespace("at") public static native Pointer maybe_data_ptr(@Const @ByRef TensorArg tensor);

// Return if the tensor geometry represented by `sizes` and `strides` is contiguous
// Although we cache is_contiguous in tensor now, this is till useful because it
// allows checking if a particular geometry is contiguous without explicitly
// constructing a tensor, e.g., when you want to choose a kernel strategy based
// on whether a subgeometry is contiguous.
@Namespace("at") public static native @Cast("bool") boolean geometry_is_contiguous(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides);
@Namespace("at") public static native @Cast("bool") boolean geometry_is_contiguous(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... strides);

@Namespace("at") public static native void check_dim_size(
    @Const @ByRef Tensor tensor,
    @Cast("int64_t") long dim,
    @Cast("int64_t") long dim_size,
    @Cast("int64_t") long size);
@Namespace("at::detail") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector defaultStrides(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("at::detail") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector defaultStrides(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sizes);
@Namespace("at::detail") public static native @Cast("size_t") long computeStorageNbytes(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides, @Cast("size_t") long itemsize);
@Namespace("at::detail") public static native @Cast("size_t") long computeStorageNbytes(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] strides, @Cast("size_t") long itemsize);

@Namespace("at::detail") public static native @ByVal LongVectorOptional computeStride(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef oldshape,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef oldstride,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef newshape);
@Namespace("at::detail") public static native @ByVal LongVectorOptional computeStride(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] oldshape,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] oldstride,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... newshape);

@Namespace("at::detail") public static native @ByVal DimVectorOptional computeStride(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef oldshape,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef oldstride,
    @Const @ByRef DimVector newshape);
@Namespace("at::detail") public static native @ByVal DimVectorOptional computeStride(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] oldshape,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] oldstride,
    @Const @ByRef DimVector newshape);

 // namespace detail
 // namespace at


// Parsed from ATen/Context.h

// #pragma once

// #include <ATen/core/ATenGeneral.h>
// #include <ATen/Tensor.h>
// #include <ATen/Utils.h>
// #include <ATen/core/ATenGeneral.h>
// #include <ATen/core/Generator.h>
// #include <ATen/CPUGeneratorImpl.h>
// #include <ATen/core/LegacyTypeDispatch.h>
// #include <ATen/detail/CUDAHooksInterface.h>
// #include <ATen/detail/HIPHooksInterface.h>
// #include <ATen/detail/ORTHooksInterface.h>
// #include <c10/util/Exception.h>
// #include <c10/core/impl/DeviceGuardImplInterface.h>
// #include <c10/core/QEngine.h>

// #include <memory>
// #include <mutex>
// #include <cstdint>
// Targeting ../Context.java



@Namespace("at") public static native @ByRef Context globalContext();

@Namespace("at") public static native void init();

@Namespace("at") public static native Allocator getCPUAllocator();

@Namespace("at") public static native @ByRef DeprecatedTypeProperties getDeprecatedTypeProperties(Backend p, ScalarType s);
@Namespace("at") public static native @ByRef DeprecatedTypeProperties getDeprecatedTypeProperties(@Cast("c10::Backend") int p, ScalarType s);

@Namespace("at") public static native @ByRef DeprecatedTypeProperties CPU(ScalarType s);

@Namespace("at") public static native @ByRef DeprecatedTypeProperties CUDA(ScalarType s);

@Namespace("at") public static native @ByRef DeprecatedTypeProperties HIP(ScalarType s);

@Namespace("at") public static native @Cast("bool") boolean hasCUDA();

@Namespace("at") public static native @Cast("bool") boolean hasHIP();

@Namespace("at") public static native @Cast("bool") boolean hasXLA();

@Namespace("at") public static native @Cast("bool") boolean hasMLC();

@Namespace("at") public static native @Cast("bool") boolean hasORT();

// Despite its name, this function returns the number of *CUDA* GPUs.
@Namespace("at") public static native @Cast("size_t") long getNumGPUs();

@Namespace("at") public static native @Cast("bool") boolean hasOpenMP();

@Namespace("at") public static native @Cast("bool") boolean hasMKL();

@Namespace("at") public static native @Cast("bool") boolean hasLAPACK();

@Namespace("at") public static native @Cast("bool") boolean hasMAGMA();

@Namespace("at") public static native @Cast("bool") boolean hasMKLDNN();

@Namespace("at") public static native void manual_seed(@Cast("uint64_t") long seed);
// Targeting ../NoTF32Guard.java



 // namespace at


// Parsed from ATen/ExpandUtils.h

// #pragma once

// #include <ATen/core/DimVector.h>
// #include <ATen/Tensor.h>
// #include <c10/util/Exception.h>
// #include <c10/util/MaybeOwned.h>

// #include <functional>
// #include <sstream>
// #include <tuple>

@Namespace("at") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector infer_size(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef a, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef b);
@Namespace("at") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector infer_size(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] a, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... b);
@Namespace("at") public static native @ByVal DimVector infer_size_dimvector(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef a, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef b);
@Namespace("at") public static native @ByVal DimVector infer_size_dimvector(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] a, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... b);
// Targeting ../DimVectorInferExpandGeometryResult.java



@Namespace("at") public static native @ByVal @Cast("std::tuple<std::vector<int64_t>,std::vector<int64_t> >*") LongVector inferExpandGeometry(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef tensor_sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef tensor_strides,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("at") public static native @ByVal @Cast("std::tuple<std::vector<int64_t>,std::vector<int64_t> >*") LongVector inferExpandGeometry(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] tensor_sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] tensor_strides,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sizes);

@Namespace("at") public static native @ByVal DimVectorInferExpandGeometryResult inferExpandGeometry_dimvector(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef tensor_sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef tensor_strides,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("at") public static native @ByVal DimVectorInferExpandGeometryResult inferExpandGeometry_dimvector(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] tensor_sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] tensor_strides,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sizes);

@Namespace("at") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector infer_dense_strides(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef tensor_sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef tensor_strides);
@Namespace("at") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector infer_dense_strides(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] tensor_sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... tensor_strides);

// True if input shapes are expandable
// NOTE: infer_size did a similar check, please keep them sync if change is needed
@Namespace("at") public static native @Cast("bool") boolean are_expandable(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape1, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape2);
@Namespace("at") public static native @Cast("bool") boolean are_expandable(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] shape1, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... shape2);

// avoid copy-construction of Tensor by using a reference_wrapper.

// NOTE [ ExpandUtils Borrowing ]
//
// Functions in ExpandUtils return `c10::MaybeOwned<Tensor>` because
// expansion may not actually be needed, in which case we can improve
// efficiency by returning
// `c10::MaybeOwned<Tensor>::borrowed(to_expand)`. However, this means
// that you need to be careful: the returned `c10::MaybeOwned<Tensor>`
// must not outlive the original `Tensor` object that `to_expand`
// referred to! The deleted rvalue reference overloads of these
// functions help with this by preventing trivial use of a temporary
// resulting from a function call, but it is still possible to make a
// mistake.

@Namespace("at") public static native @Cast({"", "c10::MaybeOwned<at::Tensor>&&"}) @StdMove TensorMaybeOwned expand_inplace(@Const @ByRef Tensor tensor, @Const @ByRef Tensor to_expand);



@Namespace("at") public static native @Cast({"", "c10::MaybeOwned<at::Tensor>&&"}) @StdMove TensorMaybeOwned expand_inplace(@Const @ByRef Tensor tensor, @Const @ByRef Tensor to_expand, @Cast("const char*") BytePointer api_name);
@Namespace("at") public static native @Cast({"", "c10::MaybeOwned<at::Tensor>&&"}) @StdMove TensorMaybeOwned expand_inplace(@Const @ByRef Tensor tensor, @Const @ByRef Tensor to_expand, String api_name);



@Namespace("at") public static native @ByVal TensorMaybeOwnedTensorMaybeOwnedTuple expand_inplace(@Const @ByRef Tensor tensor, @Const @ByRef Tensor to_expand1, @Const @ByRef Tensor to_expand2);





@Namespace("at") public static native @ByVal TensorMaybeOwnedTensorMaybeOwnedTuple expand_inplace(@Const @ByRef Tensor tensor, @Const @ByRef Tensor to_expand1, @Const @ByRef Tensor to_expand2,
                                                 @Cast("const char*") BytePointer api_name);
@Namespace("at") public static native @ByVal TensorMaybeOwnedTensorMaybeOwnedTuple expand_inplace(@Const @ByRef Tensor tensor, @Const @ByRef Tensor to_expand1, @Const @ByRef Tensor to_expand2,
                                                 String api_name);





// See NOTE [ ExpandUtils Borrowing ] above for `MaybeOwned` explanation.
@Namespace("at") public static native @ByVal TensorMaybeOwnedTensorMaybeOwnedTuple expand_outplace(@Const @ByRef Tensor to_expand1, @Const @ByRef Tensor to_expand2);





@Namespace("at") public static native @ByVal TensorMaybeOwnedTensorMaybeOwnedTuple expand_outplace(@Const @ByRef Tensor to_expand1, @Const @ByRef Tensor to_expand2, @Cast("const char*") BytePointer api_name);
@Namespace("at") public static native @ByVal TensorMaybeOwnedTensorMaybeOwnedTuple expand_outplace(@Const @ByRef Tensor to_expand1, @Const @ByRef Tensor to_expand2, String api_name);






@Namespace("at") public static native @ByVal TensorMaybeOwnedTensorMaybeOwnedTensorMaybeOwnedTuple expand_outplace(@Const @ByRef Tensor to_expand1,
                @Const @ByRef Tensor to_expand2,
                @Const @ByRef Tensor to_expand3);









@Namespace("at") public static native @ByVal TensorMaybeOwnedTensorMaybeOwnedTensorMaybeOwnedTuple expand_outplace(@Const @ByRef Tensor to_expand1,
                @Const @ByRef Tensor to_expand2,
                @Const @ByRef Tensor to_expand3,
                @Cast("const char*") BytePointer api_name);
@Namespace("at") public static native @ByVal TensorMaybeOwnedTensorMaybeOwnedTensorMaybeOwnedTuple expand_outplace(@Const @ByRef Tensor to_expand1,
                @Const @ByRef Tensor to_expand2,
                @Const @ByRef Tensor to_expand3,
                String api_name);









@Namespace("at") public static native @Cast({"", "c10::MaybeOwned<at::Tensor>&&"}) @StdMove TensorMaybeOwned expand_size(@Const @ByRef Tensor to_expand, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("at") public static native @Cast({"", "c10::MaybeOwned<at::Tensor>&&"}) @StdMove TensorMaybeOwned expand_size(@Const @ByRef Tensor to_expand, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sizes);




@Namespace("at") public static native @Cast({"", "c10::MaybeOwned<at::Tensor>&&"}) @StdMove TensorMaybeOwned expand_size(@Const @ByRef Tensor to_expand, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes, @Cast("const char*") BytePointer api_name);
@Namespace("at") public static native @Cast({"", "c10::MaybeOwned<at::Tensor>&&"}) @StdMove TensorMaybeOwned expand_size(@Const @ByRef Tensor to_expand, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes, String api_name);



@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector expand_outplace(@ByVal TensorArrayRef to_expand);

// Sums `tensor` repeatedly to produce a tensor of shape `shape`.
// Precondition: is_expandable_to(shape, tensor.sizes()) must be true
@Namespace("at") public static native @ByVal Tensor sum_to(@ByVal Tensor tensor, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape);
@Namespace("at") public static native @ByVal Tensor sum_to(@ByVal Tensor tensor, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... shape);

// True if `shape` can be broadcasted to `desired`
@Namespace("at") public static native @Cast("bool") boolean is_expandable_to(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef desired);
@Namespace("at") public static native @Cast("bool") boolean is_expandable_to(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] shape, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... desired);




// Parsed from ATen/Functions.h

// #pragma once

// @generated by tools/codegen/gen.py from Functions.h

// #include <c10/core/Scalar.h>
// #include <ATen/Tensor.h>
// #include <c10/core/Storage.h>
// #include <ATen/core/Generator.h>
// #include <c10/util/Deprecated.h>
// #include <ATen/DeviceGuard.h>
// #include <c10/core/TensorOptions.h>
// #include <ATen/core/Reduction.h>
// #include <c10/util/Optional.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/Context.h>
// #include <ATen/TracerMode.h>
// #include <ATen/Operators.h>

// These functions are defined in ATen/Utils.cpp.
// #define TENSOR(T, S)
//   TORCH_API Tensor tensor(ArrayRef<T> values, const TensorOptions& options);
//   inline Tensor tensor(
//       std::initializer_list<T> values, const TensorOptions& options) {
//     return at::tensor(ArrayRef<T>(values), options);
//   }
//   inline Tensor tensor(T value, const TensorOptions& options) {
//     return at::tensor(ArrayRef<T>(value), options);
//   }
//   inline Tensor tensor(ArrayRef<T> values) {
//     return at::tensor(std::move(values), at::dtype(k##S));
//   }
//   inline Tensor tensor(std::initializer_list<T> values) {
//     return at::tensor(ArrayRef<T>(values));
//   }
//   inline Tensor tensor(T value) {
//     return at::tensor(ArrayRef<T>(value));
//   }
@Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<uint8_t>*") ByteArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("uint8_t") byte value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<uint8_t>*") ByteArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("uint8_t") byte value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<int16_t>*") ShortArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(short value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<int16_t>*") ShortArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(short value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<int>*") IntArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(int value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<int>*") IntArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(int value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("int64_t") long value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... values);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("int64_t") long value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal FloatArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(float value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal FloatArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(float value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal DoubleArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(double value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal DoubleArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(double value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BoolArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("decltype(::c10::impl::ScalarTypeToCPPType<::c10::ScalarType::Bool>::t)") boolean value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BoolArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("decltype(::c10::impl::ScalarTypeToCPPType<::c10::ScalarType::Bool>::t)") boolean value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal HalfArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal Half value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal HalfArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal Half value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BFloat16ArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BFloat16 value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BFloat16ArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BFloat16 value);
@Namespace("at") public static native @ByVal Tensor tensor(@ByVal FloatComplexrrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal FloatComplexrrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal DoubleComplexrrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal DoubleComplexrrayRef values);
// #undef TENSOR


// aten::_cast_Byte(Tensor self, bool non_blocking=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cast_Byte(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Byte(@Const @ByRef Tensor self);

// aten::_cast_Char(Tensor self, bool non_blocking=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cast_Char(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Char(@Const @ByRef Tensor self);

// aten::_cast_Double(Tensor self, bool non_blocking=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cast_Double(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Double(@Const @ByRef Tensor self);

// aten::_cast_Float(Tensor self, bool non_blocking=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cast_Float(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Float(@Const @ByRef Tensor self);

// aten::_cast_Int(Tensor self, bool non_blocking=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cast_Int(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Int(@Const @ByRef Tensor self);

// aten::_cast_Long(Tensor self, bool non_blocking=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cast_Long(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Long(@Const @ByRef Tensor self);

// aten::_cast_Short(Tensor self, bool non_blocking=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cast_Short(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Short(@Const @ByRef Tensor self);

// aten::_cast_Half(Tensor self, bool non_blocking=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cast_Half(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Half(@Const @ByRef Tensor self);

// aten::_make_dual(Tensor(a) primal, Tensor tangent, int level) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor _make_dual(@Const @ByRef Tensor primal, @Const @ByRef Tensor tangent, @Cast("int64_t") long level);

// aten::_unpack_dual(Tensor(a) dual, int level) -> (Tensor(a) primal, Tensor tangent)
@Namespace("at") public static native @ByVal TensorTensorTuple _unpack_dual(@Const @ByRef Tensor dual, @Cast("int64_t") long level);

// aten::align_tensors(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector align_tensors(@ByVal TensorArrayRef tensors);

// aten::_assert_async(Tensor self) -> ()
@Namespace("at") public static native void _assert_async(@Const @ByRef Tensor self);

// aten::_use_cudnn_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank) -> bool
@Namespace("at") public static native @Cast("bool") boolean _use_cudnn_ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Cast("int64_t") long blank);
@Namespace("at") public static native @Cast("bool") boolean _use_cudnn_ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Cast("int64_t") long blank);

// aten::_cudnn_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank, bool deterministic, bool zero_infinity) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _cudnn_ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Cast("int64_t") long blank, @Cast("bool") boolean deterministic, @Cast("bool") boolean zero_infinity);
@Namespace("at") public static native @ByVal TensorTensorTuple _cudnn_ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Cast("int64_t") long blank, @Cast("bool") boolean deterministic, @Cast("bool") boolean zero_infinity);

// aten::_use_cudnn_rnn_flatten_weight() -> bool
@Namespace("at") public static native @Cast("bool") boolean _use_cudnn_rnn_flatten_weight();

// aten::_cudnn_rnn_flatten_weight(Tensor[] weight_arr, int weight_stride0, int input_size, int mode, int hidden_size, int proj_size, int num_layers, bool batch_first, bool bidirectional) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cudnn_rnn_flatten_weight(@ByVal TensorArrayRef weight_arr, @Cast("int64_t") long weight_stride0, @Cast("int64_t") long input_size, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, @Cast("bool") boolean bidirectional);

// aten::_cudnn_rnn(Tensor input, Tensor[] weight, int weight_stride0, Tensor? weight_buf, Tensor hx, Tensor? cx, int mode, int hidden_size, int proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTensorTuple _cudnn_rnn(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef TensorOptional weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state);
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTensorTuple _cudnn_rnn(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef TensorOptional weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Const @ByRef TensorOptional dropout_state);

// aten::_cudnn_rnn_backward(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, int hidden_size, int proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask) -> (Tensor, Tensor, Tensor, Tensor[])
@Namespace("at") public static native @ByVal TensorTensorTensorTensorVectorTuple _cudnn_rnn_backward(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTensorVectorTuple _cudnn_rnn_backward(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask);

// aten::_cudnn_init_dropout_state(float dropout, bool train, int dropout_seed, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cudnn_init_dropout_state(double dropout, @Cast("bool") boolean train, @Cast("int64_t") long dropout_seed, @ByVal TensorOptions options);

// aten::_cudnn_init_dropout_state(float dropout, bool train, int dropout_seed, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cudnn_init_dropout_state(double dropout, @Cast("bool") boolean train, @Cast("int64_t") long dropout_seed, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::_debug_has_internal_overlap(Tensor self) -> int
@Namespace("at") public static native @Cast("int64_t") long _debug_has_internal_overlap(@Const @ByRef Tensor self);

// aten::_fused_dropout(Tensor self, float p, Generator? generator=None) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _fused_dropout(@Const @ByRef Tensor self, double p, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal TensorTensorTuple _fused_dropout(@Const @ByRef Tensor self, double p);

// aten::_masked_scale(Tensor self, Tensor mask, float scale) -> Tensor
@Namespace("at") public static native @ByVal Tensor _masked_scale(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, double scale);

// aten::_sobol_engine_draw(Tensor quasi, int n, Tensor sobolstate, int dimension, int num_generated, ScalarType? dtype) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _sobol_engine_draw(@Const @ByRef Tensor quasi, @Cast("int64_t") long n, @Const @ByRef Tensor sobolstate, @Cast("int64_t") long dimension, @Cast("int64_t") long num_generated, @ByVal ScalarTypeOptional dtype);

// aten::_sobol_engine_ff_(Tensor(a!) self, int n, Tensor sobolstate, int dimension, int num_generated) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sobol_engine_ff_(@ByRef Tensor self, @Cast("int64_t") long n, @Const @ByRef Tensor sobolstate, @Cast("int64_t") long dimension, @Cast("int64_t") long num_generated);

// aten::_sobol_engine_scramble_(Tensor(a!) self, Tensor ltm, int dimension) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sobol_engine_scramble_(@ByRef Tensor self, @Const @ByRef Tensor ltm, @Cast("int64_t") long dimension);

// aten::_sobol_engine_initialize_state_(Tensor(a!) self, int dimension) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sobol_engine_initialize_state_(@ByRef Tensor self, @Cast("int64_t") long dimension);

// aten::_reshape_from_tensor(Tensor self, Tensor shape) -> Tensor
@Namespace("at") public static native @ByVal Tensor _reshape_from_tensor(@Const @ByRef Tensor self, @Const @ByRef Tensor shape);

// aten::_shape_as_tensor(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _shape_as_tensor(@Const @ByRef Tensor self);

// aten::dropout(Tensor input, float p, bool train) -> Tensor
@Namespace("at") public static native @ByVal Tensor dropout(@Const @ByRef Tensor input, double p, @Cast("bool") boolean train);

// aten::dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor dropout_(@ByRef Tensor self, double p, @Cast("bool") boolean train);

// aten::feature_dropout(Tensor input, float p, bool train) -> Tensor
@Namespace("at") public static native @ByVal Tensor feature_dropout(@Const @ByRef Tensor input, double p, @Cast("bool") boolean train);

// aten::feature_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor feature_dropout_(@ByRef Tensor self, double p, @Cast("bool") boolean train);

// aten::alpha_dropout(Tensor input, float p, bool train) -> Tensor
@Namespace("at") public static native @ByVal Tensor alpha_dropout(@Const @ByRef Tensor input, double p, @Cast("bool") boolean train);

// aten::alpha_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor alpha_dropout_(@ByRef Tensor self, double p, @Cast("bool") boolean train);

// aten::feature_alpha_dropout(Tensor input, float p, bool train) -> Tensor
@Namespace("at") public static native @ByVal Tensor feature_alpha_dropout(@Const @ByRef Tensor input, double p, @Cast("bool") boolean train);

// aten::feature_alpha_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor feature_alpha_dropout_(@ByRef Tensor self, double p, @Cast("bool") boolean train);

// aten::abs(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor abs(@Const @ByRef Tensor self);

// aten::abs_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor abs_(@ByRef Tensor self);

// aten::abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor abs_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor abs_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::absolute(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor absolute(@Const @ByRef Tensor self);

// aten::absolute.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor absolute_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::absolute.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor absolute_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::angle(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor angle(@Const @ByRef Tensor self);

// aten::angle.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor angle_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::angle.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor angle_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::view_as_real(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor view_as_real(@Const @ByRef Tensor self);

// aten::view_as_complex(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor view_as_complex(@Const @ByRef Tensor self);

// aten::sgn(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor sgn(@Const @ByRef Tensor self);

// aten::sgn.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sgn_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::sgn.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sgn_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::real(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor real(@Const @ByRef Tensor self);

// aten::imag(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor imag(@Const @ByRef Tensor self);

// aten::_conj(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor _conj(@Const @ByRef Tensor self);

// aten::conj(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor __dispatch_conj(@Const @ByRef Tensor self);

// aten::_conj_physical(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _conj_physical(@Const @ByRef Tensor self);

// aten::conj_physical(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor conj_physical(@Const @ByRef Tensor self);

// aten::conj_physical.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor conj_physical_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::conj_physical.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor conj_physical_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::conj_physical_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor conj_physical_(@ByRef Tensor self);

// aten::resolve_conj(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor resolve_conj(@Const @ByRef Tensor self);

// aten::resolve_neg(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor resolve_neg(@Const @ByRef Tensor self);

// aten::_neg_view(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor _neg_view(@Const @ByRef Tensor self);

// aten::acos(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor acos(@Const @ByRef Tensor self);

// aten::acos_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor acos_(@ByRef Tensor self);

// aten::acos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor acos_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::acos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor acos_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::arccos(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor arccos(@Const @ByRef Tensor self);

// aten::arccos_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arccos_(@ByRef Tensor self);

// aten::arccos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arccos_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::arccos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arccos_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::avg_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, bool ceil_mode=False, bool count_include_pad=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor avg_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/);
@Namespace("at") public static native @ByVal Tensor avg_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor avg_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/);
@Namespace("at") public static native @ByVal Tensor avg_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::adaptive_avg_pool1d(Tensor self, int[1] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::adaptive_max_pool1d(Tensor self, int[1] output_size) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple adaptive_max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal TensorTensorTuple adaptive_max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor add(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor add(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor add_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor add_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor add_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::_add_relu.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _add_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor _add_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::_add_relu_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _add_relu_(@ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor _add_relu_(@ByRef Tensor self, @Const @ByRef Tensor other);

// aten::_add_relu.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _add_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor _add_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::_add_relu.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _add_relu_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::_add_relu.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _add_relu(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor _add_relu(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::_add_relu_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _add_relu_(@ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor _add_relu_(@ByRef Tensor self, @Const @ByRef Scalar other);

// aten::add.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor add(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor add(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor addmv(@Const @ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor addmv(@Const @ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec);

// aten::addmv_(Tensor(a!) self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addmv_(@ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor addmv_(@ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec);

// aten::addmv.out(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addmv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor addmv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec);

// aten::addmv.out(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addmv_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec, @Const @ByRef Scalar beta, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor addr(@Const @ByRef Tensor self, @Const @ByRef Tensor vec1, @Const @ByRef Tensor vec2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor addr(@Const @ByRef Tensor self, @Const @ByRef Tensor vec1, @Const @ByRef Tensor vec2);

// aten::addr.out(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor vec1, @Const @ByRef Tensor vec2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor addr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor vec1, @Const @ByRef Tensor vec2);

// aten::addr.out(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addr_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor vec1, @Const @ByRef Tensor vec2, @Const @ByRef Scalar beta, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::affine_grid_generator(Tensor theta, int[] size, bool align_corners) -> Tensor
@Namespace("at") public static native @ByVal Tensor affine_grid_generator(@Const @ByRef Tensor theta, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor affine_grid_generator(@Const @ByRef Tensor theta, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Cast("bool") boolean align_corners);

// aten::affine_grid_generator_backward(Tensor grad, int[] size, bool align_corners) -> Tensor
@Namespace("at") public static native @ByVal Tensor affine_grid_generator_backward(@Const @ByRef Tensor grad, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor affine_grid_generator_backward(@Const @ByRef Tensor grad, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Cast("bool") boolean align_corners);

// aten::all.dim(Tensor self, int dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor all(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor all(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::all.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor all_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor all_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::all.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor all_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::all.dimname(Tensor self, Dimname dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor all(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor all(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::all.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor all_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor all_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::all.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor all_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::allclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> bool
@Namespace("at") public static native @Cast("bool") boolean allclose(@Const @ByRef Tensor self, @Const @ByRef Tensor other, double rtol/*=1e-05*/, double atol/*=1e-08*/, @Cast("bool") boolean equal_nan/*=false*/);
@Namespace("at") public static native @Cast("bool") boolean allclose(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::any.dim(Tensor self, int dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor any(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor any(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::any.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor any_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor any_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::any.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor any_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::any.dimname(Tensor self, Dimname dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor any(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor any(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::any.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor any_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor any_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::any.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor any_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar end, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar end);

// aten::arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar end, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end);

// aten::arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::arange.start_step(Scalar start, Scalar end, Scalar step, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step);

// aten::arange.start_step(Scalar start, Scalar end, Scalar step, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::arange.out(Scalar end, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arange_out(@ByRef Tensor out, @Const @ByRef Scalar end);

// aten::arange.out(Scalar end, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arange_outf(@Const @ByRef Scalar end, @ByRef Tensor out);

// aten::arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arange_out(@ByRef Tensor out, @Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar step);
@Namespace("at") public static native @ByRef Tensor arange_out(@ByRef Tensor out, @Const @ByRef Scalar start, @Const @ByRef Scalar end);

// aten::arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arange_outf(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step, @ByRef Tensor out);

// aten::_dim_arange(Tensor like, int dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor _dim_arange(@Const @ByRef Tensor like, @Cast("int64_t") long dim);

// aten::argmax(Tensor self, int? dim=None, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor argmax(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor argmax(@Const @ByRef Tensor self);

// aten::argmax.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor argmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor argmax_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::argmax.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor argmax_outf(@Const @ByRef Tensor self, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::argmin(Tensor self, int? dim=None, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor argmin(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor argmin(@Const @ByRef Tensor self);

// aten::argmin.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor argmin_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor argmin_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::argmin.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor argmin_outf(@Const @ByRef Tensor self, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::acosh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor acosh(@Const @ByRef Tensor self);

// aten::acosh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor acosh_(@ByRef Tensor self);

// aten::acosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor acosh_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::acosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor acosh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::arccosh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor arccosh(@Const @ByRef Tensor self);

// aten::arccosh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arccosh_(@ByRef Tensor self);

// aten::arccosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arccosh_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::arccosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arccosh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::asinh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor asinh(@Const @ByRef Tensor self);

// aten::asinh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor asinh_(@ByRef Tensor self);

// aten::asinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor asinh_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::asinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor asinh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::arcsinh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor arcsinh(@Const @ByRef Tensor self);

// aten::arcsinh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arcsinh_(@ByRef Tensor self);

// aten::arcsinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arcsinh_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::arcsinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arcsinh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::atanh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor atanh(@Const @ByRef Tensor self);

// aten::atanh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atanh_(@ByRef Tensor self);

// aten::atanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atanh_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::atanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atanh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::arctanh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor arctanh(@Const @ByRef Tensor self);

// aten::arctanh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arctanh_(@ByRef Tensor self);

// aten::arctanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arctanh_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::arctanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arctanh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::as_strided(Tensor(a) self, int[] size, int[] stride, int? storage_offset=None) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor as_strided(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByVal Tensor as_strided(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor as_strided(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByVal Tensor as_strided(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);

// aten::as_strided_(Tensor(a!) self, int[] size, int[] stride, int? storage_offset=None) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor as_strided_(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @Const @ByRef Tensor as_strided_(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @Const @ByRef Tensor as_strided_(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @Const @ByRef Tensor as_strided_(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);

// aten::asin(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor asin(@Const @ByRef Tensor self);

// aten::asin_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor asin_(@ByRef Tensor self);

// aten::asin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor asin_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::asin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor asin_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::arcsin(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor arcsin(@Const @ByRef Tensor self);

// aten::arcsin_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arcsin_(@ByRef Tensor self);

// aten::arcsin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arcsin_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::arcsin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arcsin_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::atan(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor atan(@Const @ByRef Tensor self);

// aten::atan_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atan_(@ByRef Tensor self);

// aten::atan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atan_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::atan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atan_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::arctan(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor arctan(@Const @ByRef Tensor self);

// aten::arctan_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arctan_(@ByRef Tensor self);

// aten::arctan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arctan_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::arctan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arctan_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::atleast_1d(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor atleast_1d(@Const @ByRef Tensor self);

// aten::atleast_1d.Sequence(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector atleast_1d(@ByVal TensorArrayRef tensors);

// aten::atleast_2d(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor atleast_2d(@Const @ByRef Tensor self);

// aten::atleast_2d.Sequence(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector atleast_2d(@ByVal TensorArrayRef tensors);

// aten::atleast_3d(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor atleast_3d(@Const @ByRef Tensor self);

// aten::atleast_3d.Sequence(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector atleast_3d(@ByVal TensorArrayRef tensors);

// aten::baddbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor baddbmm(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor baddbmm(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2);

// aten::_baddbmm_mkl_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _baddbmm_mkl_(@ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor _baddbmm_mkl_(@ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2);

// aten::baddbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor baddbmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor baddbmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2);

// aten::baddbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor baddbmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @Const @ByRef Scalar beta, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::bartlett_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length);

// aten::bartlett_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::bartlett_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);

// aten::bartlett_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -> Tensor
@Namespace("at") public static native @ByVal Tensor batch_norm(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double momentum, double eps, @Cast("bool") boolean cudnn_enabled);

// aten::quantized_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantized_batch_norm(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef Tensor mean, @Const @ByRef Tensor var, double eps, double output_scale, @Cast("int64_t") long output_zero_point);

// aten::_batch_norm_impl_index(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -> (Tensor, Tensor, Tensor, Tensor, int)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorLongTuple _batch_norm_impl_index(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double momentum, double eps, @Cast("bool") boolean cudnn_enabled);

// aten::_batch_norm_impl_index_backward(int impl_index, Tensor input, Tensor grad_output, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var_transform, bool train, float eps, bool[3] output_mask, Tensor reservedSpace) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _batch_norm_impl_index_backward(@Cast("int64_t") long impl_index, @Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_var_transform, @Cast("bool") boolean train, double eps, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @Const @ByRef Tensor reservedSpace);

// aten::bernoulli(Tensor self, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor bernoulli(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor bernoulli(@Const @ByRef Tensor self);

// aten::bernoulli.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bernoulli_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor bernoulli_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::bernoulli.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bernoulli_outf(@Const @ByRef Tensor self, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::bernoulli.p(Tensor self, float p, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor bernoulli(@Const @ByRef Tensor self, double p, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor bernoulli(@Const @ByRef Tensor self, double p);

// aten::bilinear(Tensor input1, Tensor input2, Tensor weight, Tensor? bias) -> Tensor
@Namespace("at") public static native @ByVal Tensor bilinear(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias);

// aten::binary_cross_entropy(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy(@Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::binary_cross_entropy.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::binary_cross_entropy.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByRef Tensor out);

// aten::binary_cross_entropy_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::binary_cross_entropy_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::binary_cross_entropy_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByRef Tensor grad_input);

// aten::binary_cross_entropy_with_logits(Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy_with_logits(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional pos_weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy_with_logits(@Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::binary_cross_entropy_with_logits_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy_with_logits_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional pos_weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy_with_logits_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::bincount(Tensor self, Tensor? weights=None, int minlength=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor bincount(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weights, @Cast("int64_t") long minlength/*=0*/);
@Namespace("at") public static native @ByVal Tensor bincount(@Const @ByRef Tensor self);

// aten::bitwise_not(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_not(@Const @ByRef Tensor self);

// aten::bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_not_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_not_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::copysign.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor copysign_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::copysign.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor copysign_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::copysign.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor copysign(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::copysign.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor copysign(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::copysign.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor copysign_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::copysign.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor copysign_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::logical_not(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor logical_not(@Const @ByRef Tensor self);

// aten::logical_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_not_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::logical_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_not_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::logical_xor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor logical_xor(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::logical_xor.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_xor_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::logical_xor.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_xor_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::logical_and(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor logical_and(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::logical_and.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_and_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::logical_and.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_and_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::logical_or(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor logical_or(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::logical_or.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_or_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::logical_or.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_or_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::blackman_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length);

// aten::blackman_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::blackman_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);

// aten::blackman_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::bmm(Tensor self, Tensor mat2) -> Tensor
@Namespace("at") public static native @ByVal Tensor bmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat2);

// aten::bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat2);

// aten::bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat2, @ByRef Tensor out);

// aten::broadcast_tensors(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector broadcast_tensors(@ByVal TensorArrayRef tensors);

// aten::broadcast_to(Tensor(a) self, int[] size) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor broadcast_to(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor broadcast_to(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::cat(Tensor[] tensors, int dim=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor cat(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor cat(@ByVal TensorArrayRef tensors);

// aten::cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cat_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByRef Tensor cat_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);

// aten::cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cat_outf(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim, @ByRef Tensor out);

// aten::cat.names(Tensor[] tensors, Dimname dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor cat(@ByVal TensorArrayRef tensors, @ByVal Dimname dim);

// aten::cat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cat_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @ByVal Dimname dim);

// aten::cat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cat_outf(@ByVal TensorArrayRef tensors, @ByVal Dimname dim, @ByRef Tensor out);

// aten::concat(Tensor[] tensors, int dim=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor concat(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor concat(@ByVal TensorArrayRef tensors);

// aten::concat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor concat_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByRef Tensor concat_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);

// aten::concat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor concat_outf(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim, @ByRef Tensor out);

// aten::concat.names(Tensor[] tensors, Dimname dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor concat(@ByVal TensorArrayRef tensors, @ByVal Dimname dim);

// aten::concat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor concat_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @ByVal Dimname dim);

// aten::concat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor concat_outf(@ByVal TensorArrayRef tensors, @ByVal Dimname dim, @ByRef Tensor out);

// aten::block_diag(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor block_diag(@ByVal TensorArrayRef tensors);

// aten::ceil(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor ceil(@Const @ByRef Tensor self);

// aten::ceil_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ceil_(@ByRef Tensor self);

// aten::ceil.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ceil_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::ceil.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ceil_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::chain_matmul(Tensor[] matrices) -> Tensor
@Namespace("at") public static native @ByVal Tensor chain_matmul(@ByVal TensorArrayRef matrices);

// aten::chain_matmul.out(Tensor[] matrices, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor chain_matmul_out(@ByRef Tensor out, @ByVal TensorArrayRef matrices);

// aten::chain_matmul.out(Tensor[] matrices, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor chain_matmul_outf(@ByVal TensorArrayRef matrices, @ByRef Tensor out);

// aten::unsafe_chunk(Tensor self, int chunks, int dim=0) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unsafe_chunk(@Const @ByRef Tensor self, @Cast("int64_t") long chunks, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unsafe_chunk(@Const @ByRef Tensor self, @Cast("int64_t") long chunks);

// aten::chunk(Tensor(a) self, int chunks, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector chunk(@Const @ByRef Tensor self, @Cast("int64_t") long chunks, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector chunk(@Const @ByRef Tensor self, @Cast("int64_t") long chunks);

// aten::tensor_split.sections(Tensor(a) self, int sections, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @Cast("int64_t") long sections, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @Cast("int64_t") long sections);

// aten::tensor_split.indices(Tensor(a) self, int[] indices, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef indices, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef indices);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] indices, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... indices);

// aten::tensor_split.tensor_indices_or_sections(Tensor(a) self, Tensor tensor_indices_or_sections, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor_indices_or_sections, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor_indices_or_sections);

// aten::clamp(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor clamp(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByVal Tensor clamp(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional min);

// aten::clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor clamp(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional min, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional max);
@Namespace("at") public static native @ByVal Tensor clamp(@Const @ByRef Tensor self);

// aten::clamp_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_(@ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByRef Tensor clamp_(@ByRef Tensor self, @Const @ByRef ScalarOptional min);

// aten::clamp_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_(@ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional min, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional max);
@Namespace("at") public static native @ByRef Tensor clamp_(@ByRef Tensor self);

// aten::clamp.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByRef Tensor clamp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional min);

// aten::clamp.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef ScalarOptional max, @ByRef Tensor out);

// aten::clamp.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional min, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional max);
@Namespace("at") public static native @ByRef Tensor clamp_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::clamp.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_outf(@Const @ByRef Tensor self, @Const @ByRef TensorOptional min, @Const @ByRef TensorOptional max, @ByRef Tensor out);

// aten::clamp_max(Tensor self, Scalar max) -> Tensor
@Namespace("at") public static native @ByVal Tensor clamp_max(@Const @ByRef Tensor self, @Const @ByRef Scalar max);

// aten::clamp_max.Tensor(Tensor self, Tensor max) -> Tensor
@Namespace("at") public static native @ByVal Tensor clamp_max(@Const @ByRef Tensor self, @Const @ByRef Tensor max);

// aten::clamp_max_(Tensor(a!) self, Scalar max) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_max_(@ByRef Tensor self, @Const @ByRef Scalar max);

// aten::clamp_max_.Tensor(Tensor(a!) self, Tensor max) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_max_(@ByRef Tensor self, @Const @ByRef Tensor max);

// aten::clamp_max.out(Tensor self, Scalar max, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_max_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar max);

// aten::clamp_max.out(Tensor self, Scalar max, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_max_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar max, @ByRef Tensor out);

// aten::clamp_max.Tensor_out(Tensor self, Tensor max, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_max_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor max);

// aten::clamp_max.Tensor_out(Tensor self, Tensor max, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_max_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor max, @ByRef Tensor out);

// aten::clamp_min(Tensor self, Scalar min) -> Tensor
@Namespace("at") public static native @ByVal Tensor clamp_min(@Const @ByRef Tensor self, @Const @ByRef Scalar min);

// aten::clamp_min.Tensor(Tensor self, Tensor min) -> Tensor
@Namespace("at") public static native @ByVal Tensor clamp_min(@Const @ByRef Tensor self, @Const @ByRef Tensor min);

// aten::clamp_min_(Tensor(a!) self, Scalar min) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_min_(@ByRef Tensor self, @Const @ByRef Scalar min);

// aten::clamp_min_.Tensor(Tensor(a!) self, Tensor min) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_min_(@ByRef Tensor self, @Const @ByRef Tensor min);

// aten::clamp_min.out(Tensor self, Scalar min, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_min_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar min);

// aten::clamp_min.out(Tensor self, Scalar min, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_min_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar min, @ByRef Tensor out);

// aten::clamp_min.Tensor_out(Tensor self, Tensor min, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_min_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor min);

// aten::clamp_min.Tensor_out(Tensor self, Tensor min, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_min_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor min, @ByRef Tensor out);

// aten::clip(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor clip(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByVal Tensor clip(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional min);

// aten::clip.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor clip(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional min, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional max);
@Namespace("at") public static native @ByVal Tensor clip(@Const @ByRef Tensor self);

// aten::clip_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clip_(@ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByRef Tensor clip_(@ByRef Tensor self, @Const @ByRef ScalarOptional min);

// aten::clip_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clip_(@ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional min, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional max);
@Namespace("at") public static native @ByRef Tensor clip_(@ByRef Tensor self);

// aten::clip.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clip_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByRef Tensor clip_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional min);

// aten::clip.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clip_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef ScalarOptional max, @ByRef Tensor out);

// aten::clip.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clip_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional min, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional max);
@Namespace("at") public static native @ByRef Tensor clip_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::clip.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clip_outf(@Const @ByRef Tensor self, @Const @ByRef TensorOptional min, @Const @ByRef TensorOptional max, @ByRef Tensor out);

// aten::cudnn_is_acceptable(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean cudnn_is_acceptable(@Const @ByRef Tensor self);

// aten::complex(Tensor real, Tensor imag) -> Tensor


// aten::complex.out(Tensor real, Tensor imag, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor complex_out(@ByRef Tensor out, @Const @ByRef Tensor real, @Const @ByRef Tensor imag);

// aten::complex.out(Tensor real, Tensor imag, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor complex_outf(@Const @ByRef Tensor real, @Const @ByRef Tensor imag, @ByRef Tensor out);

// aten::polar(Tensor abs, Tensor angle) -> Tensor
@Namespace("at") public static native @ByVal Tensor polar(@Const @ByRef Tensor abs, @Const @ByRef Tensor angle);

// aten::polar.out(Tensor abs, Tensor angle, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor polar_out(@ByRef Tensor out, @Const @ByRef Tensor abs, @Const @ByRef Tensor angle);

// aten::polar.out(Tensor abs, Tensor angle, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor polar_outf(@Const @ByRef Tensor abs, @Const @ByRef Tensor angle, @ByRef Tensor out);

// aten::constant_pad_nd(Tensor self, int[] pad, Scalar value=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor constant_pad_nd(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef pad, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar value);
@Namespace("at") public static native @ByVal Tensor constant_pad_nd(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef pad);
@Namespace("at") public static native @ByVal Tensor constant_pad_nd(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] pad, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar value);
@Namespace("at") public static native @ByVal Tensor constant_pad_nd(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... pad);

// aten::convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups);

// aten::convolution_overrideable(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor convolution_overrideable(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor convolution_overrideable(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups);

// aten::convolution_backward_overrideable(Tensor grad_output, Tensor input, Tensor weight, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple convolution_backward_overrideable(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple convolution_backward_overrideable(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::_convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32) -> Tensor
@Namespace("at") public static native @ByVal Tensor _convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor _convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled, @Cast("bool") boolean allow_tf32);

// aten::_convolution.deprecated(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled) -> Tensor
@Namespace("at") public static native @ByVal Tensor _convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled);
@Namespace("at") public static native @ByVal Tensor _convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled);

// aten::_convolution_mode(Tensor input, Tensor weight, Tensor? bias, int[] stride, str padding, int[] dilation, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor _convolution_mode(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::string_view*") Pointer padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor _convolution_mode(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast("c10::string_view*") Pointer padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups);

// aten::_convolution_nogroup(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor _convolution_nogroup(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding);
@Namespace("at") public static native @ByVal Tensor _convolution_nogroup(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_padding);

// aten::_convolution_double_backward(Tensor? ggI, Tensor? ggW, Tensor? ggb, Tensor gO, Tensor weight, Tensor self, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _convolution_double_backward(@Const @ByRef TensorOptional ggI, @Const @ByRef TensorOptional ggW, @Const @ByRef TensorOptional ggb, @Const @ByRef Tensor gO, @Const @ByRef Tensor weight, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled, @Cast("bool") boolean allow_tf32, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _convolution_double_backward(@Const @ByRef TensorOptional ggI, @Const @ByRef TensorOptional ggW, @Const @ByRef TensorOptional ggb, @Const @ByRef Tensor gO, @Const @ByRef Tensor weight, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled, @Cast("bool") boolean allow_tf32, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::conv1d(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, int[1] padding=0, int[1] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups/*=1*/);

// aten::conv2d(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups/*=1*/);

// aten::conv3d(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups/*=1*/);

// aten::conv1d.padding(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, str padding="valid", int[1] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::string_view*") Pointer padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::string_view*") Pointer padding);
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast("c10::string_view*") Pointer padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast("c10::string_view*") Pointer padding);

// aten::conv2d.padding(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, str padding="valid", int[2] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::string_view*") Pointer padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::string_view*") Pointer padding);
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast("c10::string_view*") Pointer padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast("c10::string_view*") Pointer padding);

// aten::conv3d.padding(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, str padding="valid", int[3] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::string_view*") Pointer padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::string_view*") Pointer padding);
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast("c10::string_view*") Pointer padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast("c10::string_view*") Pointer padding);

// aten::conv_tbc(Tensor self, Tensor weight, Tensor bias, int pad=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv_tbc(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor bias, @Cast("int64_t") long pad/*=0*/);
@Namespace("at") public static native @ByVal Tensor conv_tbc(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor bias);

// aten::conv_tbc_backward(Tensor self, Tensor input, Tensor weight, Tensor bias, int pad) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple conv_tbc_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef Tensor bias, @Cast("int64_t") long pad);

// aten::conv_transpose1d(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, int[1] padding=0, int[1] output_padding=0, int groups=1, int[1] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv_transpose1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor conv_transpose1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv_transpose1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);

// aten::conv_transpose2d.input(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int groups=1, int[2] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv_transpose2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor conv_transpose2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv_transpose2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);

// aten::conv_transpose3d.input(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] output_padding=0, int groups=1, int[3] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv_transpose3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor conv_transpose3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv_transpose3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);

// aten::_copy_from(Tensor self, Tensor dst, bool non_blocking=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _copy_from(@Const @ByRef Tensor self, @Const @ByRef Tensor dst, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _copy_from(@Const @ByRef Tensor self, @Const @ByRef Tensor dst);

// aten::_copy_from_and_resize(Tensor self, Tensor dst) -> Tensor
@Namespace("at") public static native @ByVal Tensor _copy_from_and_resize(@Const @ByRef Tensor self, @Const @ByRef Tensor dst);

// aten::cos(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor cos(@Const @ByRef Tensor self);

// aten::cos_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cos_(@ByRef Tensor self);

// aten::cos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cos_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::cos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cos_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::cosh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor cosh(@Const @ByRef Tensor self);

// aten::cosh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cosh_(@ByRef Tensor self);

// aten::cosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cosh_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::cosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cosh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::cosine_embedding_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor cosine_embedding_loss(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor target, double margin/*=0.0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor cosine_embedding_loss(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor target);

// aten::count_nonzero.dim_IntList(Tensor self, int[] dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor count_nonzero(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor count_nonzero(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::count_nonzero(Tensor self, int? dim=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor count_nonzero(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByVal Tensor count_nonzero(@Const @ByRef Tensor self);

// aten::cov(Tensor self, *, int correction=1, Tensor? fweights=None, Tensor? aweights=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor cov(@Const @ByRef Tensor self, @Cast("int64_t") long correction/*=1*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional fweights, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional aweights);
@Namespace("at") public static native @ByVal Tensor cov(@Const @ByRef Tensor self);

// aten::corrcoef(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor corrcoef(@Const @ByRef Tensor self);

// aten::cudnn_affine_grid_generator(Tensor theta, int N, int C, int H, int W) -> Tensor grid
@Namespace("at") public static native @ByVal Tensor cudnn_affine_grid_generator(@Const @ByRef Tensor theta, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long H, @Cast("int64_t") long W);

// aten::cudnn_affine_grid_generator_backward(Tensor grad, int N, int C, int H, int W) -> Tensor grad_theta
@Namespace("at") public static native @ByVal Tensor cudnn_affine_grid_generator_backward(@Const @ByRef Tensor grad, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long H, @Cast("int64_t") long W);

// aten::cudnn_batch_norm(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon) -> (Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple cudnn_batch_norm(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double exponential_average_factor, double epsilon);

// aten::cudnn_batch_norm_backward(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon, Tensor reserveSpace) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple cudnn_batch_norm_backward(@Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_var, double epsilon, @Const @ByRef Tensor reserveSpace);

// aten::cudnn_convolution.deprecated(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor cudnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);

// aten::cudnn_convolution.deprecated2(Tensor self, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor cudnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);

// aten::cudnn_convolution(Tensor self, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
@Namespace("at") public static native @ByVal Tensor cudnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);

// aten::cudnn_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_backward_input(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef self_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_backward_input(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] self_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);

// aten::cudnn_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32, bool[2] output_mask) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple cudnn_convolution_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTuple cudnn_convolution_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);

// aten::cudnn_convolution_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_backward_weight(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_backward_weight(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);

// aten::cudnn_convolution_transpose.deprecated(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);

// aten::cudnn_convolution_transpose.deprecated2(Tensor self, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);

// aten::cudnn_convolution_transpose(Tensor self, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);

// aten::cudnn_convolution_transpose_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32, bool[2] output_mask) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple cudnn_convolution_transpose_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTuple cudnn_convolution_transpose_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);

// aten::cudnn_convolution_transpose_backward_input(Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose_backward_input(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose_backward_input(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);

// aten::cudnn_convolution_transpose_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose_backward_weight(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose_backward_weight(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);

// aten::cudnn_convolution_relu(Tensor self, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups);

// aten::cudnn_convolution_add_relu(Tensor self, Tensor weight, Tensor z, Scalar? alpha, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_add_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor z, @Const @ByRef ScalarOptional alpha, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_add_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor z, @Const @ByRef ScalarOptional alpha, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups);

// aten::cudnn_grid_sampler(Tensor self, Tensor grid) -> Tensor output
@Namespace("at") public static native @ByVal Tensor cudnn_grid_sampler(@Const @ByRef Tensor self, @Const @ByRef Tensor grid);

// aten::cudnn_grid_sampler_backward(Tensor self, Tensor grid, Tensor grad_output) -> (Tensor grad_self, Tensor grad_grid)
@Namespace("at") public static native @ByVal TensorTensorTuple cudnn_grid_sampler_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grid, @Const @ByRef Tensor grad_output);

// aten::cummax(Tensor self, int dim) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple cummax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::cummax.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummax_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::cummax.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummax_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::cummax.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple cummax(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::cummax.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummax_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::cummax.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummax_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::_cummax_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()
@Namespace("at") public static native void _cummax_helper(@Const @ByRef Tensor self, @ByRef Tensor values, @ByRef Tensor indices, @Cast("int64_t") long dim);

// aten::cummin(Tensor self, int dim) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple cummin(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::cummin.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummin_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::cummin.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummin_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::cummin.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple cummin(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::cummin.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummin_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::cummin.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummin_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::_cummin_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()
@Namespace("at") public static native void _cummin_helper(@Const @ByRef Tensor self, @ByRef Tensor values, @ByRef Tensor indices, @Cast("int64_t") long dim);

// aten::cummaxmin_backward(Tensor grad, Tensor input, Tensor indices, int dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor cummaxmin_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor input, @Const @ByRef Tensor indices, @Cast("int64_t") long dim);

// aten::cumprod(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor cumprod(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor cumprod(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::cumprod.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumprod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor cumprod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::cumprod.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumprod_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::cumprod.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor cumprod(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor cumprod(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::cumprod.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumprod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor cumprod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::cumprod.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumprod_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::cumprod_backward(Tensor grad, Tensor input, int dim, Tensor output) -> Tensor
@Namespace("at") public static native @ByVal Tensor cumprod_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor input, @Cast("int64_t") long dim, @Const @ByRef Tensor output);

// aten::cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor cumsum(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor cumsum(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::cumsum.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumsum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor cumsum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::cumsum.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumsum_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::cumsum.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor cumsum(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor cumsum(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::cumsum.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumsum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor cumsum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::cumsum.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumsum_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::cumulative_trapezoid.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor cumulative_trapezoid(@Const @ByRef Tensor y, @Const @ByRef Tensor x, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor cumulative_trapezoid(@Const @ByRef Tensor y, @Const @ByRef Tensor x);

// aten::cumulative_trapezoid.dx(Tensor y, *, Scalar dx=1, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor cumulative_trapezoid(@Const @ByRef Tensor y, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar dx, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor cumulative_trapezoid(@Const @ByRef Tensor y);

// aten::ctc_loss.IntList(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Cast("int64_t") long blank/*=0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths);
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Cast("int64_t") long blank/*=0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... target_lengths);

// aten::ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @Const @ByRef Tensor input_lengths, @Const @ByRef Tensor target_lengths, @Cast("int64_t") long blank/*=0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @Const @ByRef Tensor input_lengths, @Const @ByRef Tensor target_lengths);

// aten::_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, bool zero_infinity=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Cast("int64_t") long blank/*=0*/, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple _ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths);
@Namespace("at") public static native @ByVal TensorTensorTuple _ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Cast("int64_t") long blank/*=0*/, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple _ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... target_lengths);

// aten::_ctc_loss_backward(Tensor grad, Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, bool zero_infinity=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _ctc_loss_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Const @ByRef Tensor neg_log_likelihood, @Const @ByRef Tensor log_alpha, @Cast("int64_t") long blank, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal Tensor _ctc_loss_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Const @ByRef Tensor neg_log_likelihood, @Const @ByRef Tensor log_alpha, @Cast("int64_t") long blank);
@Namespace("at") public static native @ByVal Tensor _ctc_loss_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Const @ByRef Tensor neg_log_likelihood, @Const @ByRef Tensor log_alpha, @Cast("int64_t") long blank, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal Tensor _ctc_loss_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Const @ByRef Tensor neg_log_likelihood, @Const @ByRef Tensor log_alpha, @Cast("int64_t") long blank);

// aten::diag_embed(Tensor self, int offset=0, int dim1=-2, int dim2=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor diag_embed(@Const @ByRef Tensor self, @Cast("int64_t") long offset/*=0*/, @Cast("int64_t") long dim1/*=-2*/, @Cast("int64_t") long dim2/*=-1*/);
@Namespace("at") public static native @ByVal Tensor diag_embed(@Const @ByRef Tensor self);

// aten::diagflat(Tensor self, int offset=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor diagflat(@Const @ByRef Tensor self, @Cast("int64_t") long offset/*=0*/);
@Namespace("at") public static native @ByVal Tensor diagflat(@Const @ByRef Tensor self);

// aten::diagonal(Tensor(a) self, int offset=0, int dim1=0, int dim2=1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor diagonal(@Const @ByRef Tensor self, @Cast("int64_t") long offset/*=0*/, @Cast("int64_t") long dim1/*=0*/, @Cast("int64_t") long dim2/*=1*/);
@Namespace("at") public static native @ByVal Tensor diagonal(@Const @ByRef Tensor self);

// aten::diagonal.Dimname(Tensor(a) self, *, Dimname outdim, Dimname dim1, Dimname dim2, int offset=0) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor diagonal(@Const @ByRef Tensor self, @ByVal Dimname outdim, @ByVal Dimname dim1, @ByVal Dimname dim2, @Cast("int64_t") long offset/*=0*/);
@Namespace("at") public static native @ByVal Tensor diagonal(@Const @ByRef Tensor self, @ByVal Dimname outdim, @ByVal Dimname dim1, @ByVal Dimname dim2);

// aten::diagonal_backward(Tensor grad_output, int[] input_sizes, int offset, int dim1, int dim2) -> Tensor
@Namespace("at") public static native @ByVal Tensor diagonal_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_sizes, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2);
@Namespace("at") public static native @ByVal Tensor diagonal_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_sizes, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2);

// aten::diff(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor diff(@Const @ByRef Tensor self, @Cast("int64_t") long n/*=1*/, @Cast("int64_t") long dim/*=-1*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional prepend, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional append);
@Namespace("at") public static native @ByVal Tensor diff(@Const @ByRef Tensor self);

// aten::diff.out(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diff_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long n/*=1*/, @Cast("int64_t") long dim/*=-1*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional prepend, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional append);
@Namespace("at") public static native @ByRef Tensor diff_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::diff.out(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diff_outf(@Const @ByRef Tensor self, @Cast("int64_t") long n, @Cast("int64_t") long dim, @Const @ByRef TensorOptional prepend, @Const @ByRef TensorOptional append, @ByRef Tensor out);

// aten::gradient.scalarint(Tensor self, *, Scalar? spacing=None, int? dim=None, int edge_order=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional spacing, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self);

// aten::gradient.scalararray(Tensor self, *, Scalar spacing, int[] dim, int edge_order=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @Const @ByRef Scalar spacing, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @Const @ByRef Scalar spacing, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @Const @ByRef Scalar spacing, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @Const @ByRef Scalar spacing, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::gradient.array(Tensor self, *, int[] dim, int edge_order=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::gradient.scalarrayint(Tensor self, *, Scalar[] spacing, int? dim=None, int edge_order=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal ScalarArrayRef spacing, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal ScalarArrayRef spacing);

// aten::gradient.scalarrayarray(Tensor self, *, Scalar[] spacing, int[] dim, int edge_order=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal ScalarArrayRef spacing, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal ScalarArrayRef spacing, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal ScalarArrayRef spacing, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal ScalarArrayRef spacing, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::gradient.tensorarrayint(Tensor self, *, Tensor[] spacing, int? dim=None, int edge_order=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal TensorArrayRef spacing, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal TensorArrayRef spacing);

// aten::gradient.tensorarray(Tensor self, *, Tensor[] spacing, int[] dim, int edge_order=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal TensorArrayRef spacing, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal TensorArrayRef spacing, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal TensorArrayRef spacing, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal TensorArrayRef spacing, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::div.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor div(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor div_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor div_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::div.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor
@Namespace("at") public static native @ByVal Tensor div(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer rounding_mode);

// aten::div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor div_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer rounding_mode);

// aten::div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor div_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer rounding_mode, @ByRef Tensor out);

// aten::div.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor div(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::div.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor
@Namespace("at") public static native @ByVal Tensor div(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer rounding_mode);

// aten::divide.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor divide(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor divide_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor divide_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::divide.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor divide(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::divide.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor
@Namespace("at") public static native @ByVal Tensor divide(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer rounding_mode);

// aten::divide.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor divide_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer rounding_mode);

// aten::divide.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor divide_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer rounding_mode, @ByRef Tensor out);

// aten::divide.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor
@Namespace("at") public static native @ByVal Tensor divide(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer rounding_mode);

// aten::true_divide.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor true_divide(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::true_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor true_divide_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::true_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor true_divide_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::true_divide.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor true_divide(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::dot(Tensor self, Tensor tensor) -> Tensor
@Namespace("at") public static native @ByVal Tensor dot(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor);

// aten::dot.out(Tensor self, Tensor tensor, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor dot_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor tensor);

// aten::dot.out(Tensor self, Tensor tensor, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor dot_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor, @ByRef Tensor out);

// aten::vdot(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor vdot(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::vdot.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor vdot_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::vdot.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor vdot_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::einsum(str equation, Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor einsum(@ByVal @Cast("c10::string_view*") Pointer equation, @ByVal TensorArrayRef tensors);

// aten::embedding(Tensor weight, Tensor indices, int padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor embedding(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Cast("int64_t") long padding_idx/*=-1*/, @Cast("bool") boolean scale_grad_by_freq/*=false*/, @Cast("bool") boolean sparse/*=false*/);
@Namespace("at") public static native @ByVal Tensor embedding(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices);

// aten::embedding_backward(Tensor grad, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq, bool sparse) -> Tensor
@Namespace("at") public static native @ByVal Tensor embedding_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Cast("int64_t") long num_weights, @Cast("int64_t") long padding_idx, @Cast("bool") boolean scale_grad_by_freq, @Cast("bool") boolean sparse);

// aten::embedding_dense_backward(Tensor grad_output, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq) -> Tensor
@Namespace("at") public static native @ByVal Tensor embedding_dense_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor indices, @Cast("int64_t") long num_weights, @Cast("int64_t") long padding_idx, @Cast("bool") boolean scale_grad_by_freq);

// aten::embedding_renorm_(Tensor(a!) self, Tensor indices, float max_norm, float norm_type) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor embedding_renorm_(@ByRef Tensor self, @Const @ByRef Tensor indices, double max_norm, double norm_type);

// aten::embedding_sparse_backward(Tensor grad, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq) -> Tensor
@Namespace("at") public static native @ByVal Tensor embedding_sparse_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Cast("int64_t") long num_weights, @Cast("int64_t") long padding_idx, @Cast("bool") boolean scale_grad_by_freq);

// aten::_embedding_bag_forward_only(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple _embedding_bag_forward_only(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Cast("bool") boolean scale_grad_by_freq/*=false*/, @Cast("int64_t") long mode/*=0*/, @Cast("bool") boolean sparse/*=false*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional per_sample_weights, @Cast("bool") boolean include_last_offset/*=false*/, @Cast("int64_t") long padding_idx/*=-1*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple _embedding_bag_forward_only(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets);

// aten::_rowwise_prune(Tensor weight, Tensor mask, ScalarType compressed_indices_dtype) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _rowwise_prune(@Const @ByRef Tensor weight, @Const @ByRef Tensor mask, ScalarType compressed_indices_dtype);

// aten::row_stack(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor row_stack(@ByVal TensorArrayRef tensors);

// aten::row_stack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor row_stack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);

// aten::row_stack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor row_stack_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);

// aten::embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False) -> (Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple embedding_bag(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Cast("bool") boolean scale_grad_by_freq/*=false*/, @Cast("int64_t") long mode/*=0*/, @Cast("bool") boolean sparse/*=false*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional per_sample_weights, @Cast("bool") boolean include_last_offset/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple embedding_bag(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets);

// aten::embedding_bag.padding_idx(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, bool include_last_offset, int? padding_idx) -> (Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple embedding_bag(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Cast("bool") boolean sparse, @Const @ByRef TensorOptional per_sample_weights, @Cast("bool") boolean include_last_offset, @ByVal LongOptional padding_idx);

// aten::_embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple _embedding_bag(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Cast("bool") boolean scale_grad_by_freq/*=false*/, @Cast("int64_t") long mode/*=0*/, @Cast("bool") boolean sparse/*=false*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional per_sample_weights, @Cast("bool") boolean include_last_offset/*=false*/, @Cast("int64_t") long padding_idx/*=-1*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple _embedding_bag(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets);

// aten::_embedding_bag_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, int num_weights, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, int padding_idx=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _embedding_bag_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Const @ByRef Tensor maximum_indices, @Cast("int64_t") long num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Cast("bool") boolean sparse, @Const @ByRef TensorOptional per_sample_weights, @Cast("int64_t") long padding_idx/*=-1*/);
@Namespace("at") public static native @ByVal Tensor _embedding_bag_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Const @ByRef Tensor maximum_indices, @Cast("int64_t") long num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Cast("bool") boolean sparse, @Const @ByRef TensorOptional per_sample_weights);

// aten::_embedding_bag_sparse_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, int num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _embedding_bag_sparse_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Cast("int64_t") long num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Const @ByRef TensorOptional per_sample_weights, @Cast("int64_t") long padding_idx/*=-1*/);
@Namespace("at") public static native @ByVal Tensor _embedding_bag_sparse_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Cast("int64_t") long num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Const @ByRef TensorOptional per_sample_weights);

// aten::_embedding_bag_dense_backward(Tensor grad, Tensor indices, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, int num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _embedding_bag_dense_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Const @ByRef Tensor maximum_indices, @Cast("int64_t") long num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Const @ByRef TensorOptional per_sample_weights, @Cast("int64_t") long padding_idx/*=-1*/);
@Namespace("at") public static native @ByVal Tensor _embedding_bag_dense_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Const @ByRef Tensor maximum_indices, @Cast("int64_t") long num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Const @ByRef TensorOptional per_sample_weights);

// aten::_embedding_bag_per_sample_weights_backward(Tensor grad, Tensor weight, Tensor indices, Tensor offsets, Tensor offset2bag, int mode, int padding_idx=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _embedding_bag_per_sample_weights_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Const @ByRef Tensor offset2bag, @Cast("int64_t") long mode, @Cast("int64_t") long padding_idx/*=-1*/);
@Namespace("at") public static native @ByVal Tensor _embedding_bag_per_sample_weights_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Const @ByRef Tensor offset2bag, @Cast("int64_t") long mode);

// aten::empty.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);

// aten::empty.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::empty.memory_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::empty.memory_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::_empty_affine_quantized(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format) -> Tensor
@Namespace("at") public static native @ByVal Tensor _empty_affine_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, double scale/*=1*/, @Cast("int64_t") long zero_point/*=0*/, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor _empty_affine_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor _empty_affine_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, double scale/*=1*/, @Cast("int64_t") long zero_point/*=0*/, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor _empty_affine_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::_empty_affine_quantized(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format) -> Tensor
@Namespace("at") public static native @ByVal Tensor _empty_affine_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, double scale, @Cast("int64_t") long zero_point, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor _empty_affine_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, double scale, @Cast("int64_t") long zero_point, @ByVal MemoryFormatOptional memory_format);

// aten::_empty_per_channel_affine_quantized(int[] size, *, Tensor scales, Tensor zero_points, int axis, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=contiguous_format) -> Tensor
@Namespace("at") public static native @ByVal Tensor _empty_per_channel_affine_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor _empty_per_channel_affine_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis);
@Namespace("at") public static native @ByVal Tensor _empty_per_channel_affine_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor _empty_per_channel_affine_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis);

// aten::_empty_per_channel_affine_quantized(int[] size, *, Tensor scales, Tensor zero_points, int axis, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=contiguous_format) -> Tensor
@Namespace("at") public static native @ByVal Tensor _empty_per_channel_affine_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor _empty_per_channel_affine_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::empty_quantized(int[] size, Tensor qtensor, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor qtensor, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor qtensor);
@Namespace("at") public static native @ByVal Tensor empty_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor qtensor, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor qtensor);

// aten::empty_quantized(int[] size, Tensor qtensor, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor qtensor, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor qtensor, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::empty.out(int[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::empty.out(int[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor empty_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);

// aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty_like(@Const @ByRef Tensor self);

// aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_like(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::empty_strided(int[] size, int[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);

// aten::empty_strided(int[] size, int[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::erf(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor erf(@Const @ByRef Tensor self);

// aten::erf_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erf_(@ByRef Tensor self);

// aten::erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erf_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erf_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::erfc(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor erfc(@Const @ByRef Tensor self);

// aten::erfc_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erfc_(@ByRef Tensor self);

// aten::erfc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erfc_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::erfc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erfc_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::exp(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor exp(@Const @ByRef Tensor self);

// aten::exp_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor exp_(@ByRef Tensor self);

// aten::exp.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor exp_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::exp.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor exp_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::exp2(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor exp2(@Const @ByRef Tensor self);

// aten::exp2_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor exp2_(@ByRef Tensor self);

// aten::exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor exp2_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor exp2_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::expm1(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor expm1(@Const @ByRef Tensor self);

// aten::expm1_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor expm1_(@ByRef Tensor self);

// aten::expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor expm1_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor expm1_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::eye(int n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n);

// aten::eye(int n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::eye.m(int n, int m, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n, @Cast("int64_t") long m, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n, @Cast("int64_t") long m);

// aten::eye.m(int n, int m, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n, @Cast("int64_t") long m, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::eye.out(int n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eye_out(@ByRef Tensor out, @Cast("int64_t") long n);

// aten::eye.out(int n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eye_outf(@Cast("int64_t") long n, @ByRef Tensor out);

// aten::eye.m_out(int n, int m, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eye_out(@ByRef Tensor out, @Cast("int64_t") long n, @Cast("int64_t") long m);

// aten::eye.m_out(int n, int m, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eye_outf(@Cast("int64_t") long n, @Cast("int64_t") long m, @ByRef Tensor out);

// aten::flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor flatten(@Const @ByRef Tensor self, @Cast("int64_t") long start_dim/*=0*/, @Cast("int64_t") long end_dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor flatten(@Const @ByRef Tensor self);

// aten::flatten.named_out_dim(Tensor(a) self, int start_dim, int end_dim, Dimname out_dim) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor flatten(@Const @ByRef Tensor self, @Cast("int64_t") long start_dim, @Cast("int64_t") long end_dim, @ByVal Dimname out_dim);

// aten::flatten.using_names(Tensor(a) self, Dimname start_dim, Dimname end_dim, Dimname out_dim) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor flatten(@Const @ByRef Tensor self, @ByVal Dimname start_dim, @ByVal Dimname end_dim, @ByVal Dimname out_dim);

// aten::flatten.DimnameList(Tensor(a) self, Dimname[] dims, Dimname out_dim) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor flatten(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dims, @ByVal Dimname out_dim);

// aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fill_(@ByRef Tensor self, @Const @ByRef Scalar value);

// aten::fill_.Tensor(Tensor(a!) self, Tensor value) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fill_(@ByRef Tensor self, @Const @ByRef Tensor value);

// aten::floor(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor floor(@Const @ByRef Tensor self);

// aten::floor_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor floor_(@ByRef Tensor self);

// aten::floor.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor floor_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::floor.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor floor_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::floor_divide(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor floor_divide(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::floor_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor floor_divide_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::floor_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor floor_divide_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::floor_divide.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor floor_divide(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::frac(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor frac(@Const @ByRef Tensor self);

// aten::frac_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor frac_(@ByRef Tensor self);

// aten::frac.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor frac_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::frac.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor frac_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::full.names(int[] size, Scalar fill_value, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names);

// aten::full.names(int[] size, Scalar fill_value, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::full(int[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value);

// aten::full(int[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::full.out(int[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor full_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value);
@Namespace("at") public static native @ByRef Tensor full_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value);

// aten::full.out(int[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor full_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor full_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value, @ByRef Tensor out);

// aten::full_like(Tensor self, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor full_like(@Const @ByRef Tensor self, @Const @ByRef Scalar fill_value, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor full_like(@Const @ByRef Tensor self, @Const @ByRef Scalar fill_value);

// aten::full_like(Tensor self, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor full_like(@Const @ByRef Tensor self, @Const @ByRef Scalar fill_value, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::from_file(str filename, bool? shared=None, int? size=0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor from_file(@ByVal @Cast("c10::string_view*") Pointer filename, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional shared, @ByVal(nullValue = "c10::optional<int64_t>(0)") LongOptional size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_file(@ByVal @Cast("c10::string_view*") Pointer filename);

// aten::from_file(str filename, bool? shared=None, int? size=0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor from_file(@ByVal @Cast("c10::string_view*") Pointer filename, @ByVal BoolOptional shared, @ByVal LongOptional size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::gcd.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gcd_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::gcd.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gcd_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::gcd(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor gcd(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::gcd_(Tensor(a!) self, Tensor other) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gcd_(@ByRef Tensor self, @Const @ByRef Tensor other);

// aten::lcm.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lcm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::lcm.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lcm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::lcm(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor lcm(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::lcm_(Tensor(a!) self, Tensor other) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lcm_(@ByRef Tensor self, @Const @ByRef Tensor other);

// aten::grid_sampler(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
@Namespace("at") public static native @ByVal Tensor grid_sampler(@Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);

// aten::grid_sampler_2d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
@Namespace("at") public static native @ByVal Tensor grid_sampler_2d(@Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);

// aten::grid_sampler_2d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple grid_sampler_2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);

// aten::_grid_sampler_2d_cpu_fallback(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
@Namespace("at") public static native @ByVal Tensor _grid_sampler_2d_cpu_fallback(@Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);

// aten::_grid_sampler_2d_cpu_fallback_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _grid_sampler_2d_cpu_fallback_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);

// aten::grid_sampler_3d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
@Namespace("at") public static native @ByVal Tensor grid_sampler_3d(@Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);

// aten::grid_sampler_3d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple grid_sampler_3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);

// aten::hann_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length);

// aten::hann_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::hann_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);

// aten::hann_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::hamming_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length);

// aten::hamming_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::hamming_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);

// aten::hamming_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::hamming_window.periodic_alpha(int window_length, bool periodic, float alpha, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha);

// aten::hamming_window.periodic_alpha(int window_length, bool periodic, float alpha, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::hamming_window.periodic_alpha_beta(int window_length, bool periodic, float alpha, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, double beta, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, double beta);

// aten::hamming_window.periodic_alpha_beta(int window_length, bool periodic, float alpha, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, double beta, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::kaiser_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length);

// aten::kaiser_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::kaiser_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);

// aten::kaiser_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::kaiser_window.beta(int window_length, bool periodic, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double beta, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double beta);

// aten::kaiser_window.beta(int window_length, bool periodic, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double beta, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::hinge_embedding_loss(Tensor self, Tensor target, float margin=1.0, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor hinge_embedding_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, double margin/*=1.0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor hinge_embedding_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::group_norm(Tensor input, int num_groups, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enabled=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor group_norm(@Const @ByRef Tensor input, @Cast("int64_t") long num_groups, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, double eps/*=1e-05*/, @Cast("bool") boolean cudnn_enabled/*=true*/);
@Namespace("at") public static native @ByVal Tensor group_norm(@Const @ByRef Tensor input, @Cast("int64_t") long num_groups);

// aten::native_group_norm(Tensor input, Tensor? weight, Tensor? bias, int N, int C, int HxW, int group, float eps) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple native_group_norm(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long HxW, @Cast("int64_t") long group, double eps);

// aten::native_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, int N, int C, int HxW, int group, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple native_group_norm_backward(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long HxW, @Cast("int64_t") long group, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::_fft_r2c(Tensor self, int[] dim, int normalization, bool onesided) -> Tensor
@Namespace("at") public static native @ByVal Tensor _fft_r2c(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("bool") boolean onesided);
@Namespace("at") public static native @ByVal Tensor _fft_r2c(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("bool") boolean onesided);

// aten::_fft_r2c.out(Tensor self, int[] dim, int normalization, bool onesided, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _fft_r2c_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("bool") boolean onesided);
@Namespace("at") public static native @ByRef Tensor _fft_r2c_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("bool") boolean onesided);

// aten::_fft_r2c.out(Tensor self, int[] dim, int normalization, bool onesided, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _fft_r2c_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("bool") boolean onesided, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _fft_r2c_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("bool") boolean onesided, @ByRef Tensor out);

// aten::_fft_c2r(Tensor self, int[] dim, int normalization, int last_dim_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor _fft_c2r(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("int64_t") long last_dim_size);
@Namespace("at") public static native @ByVal Tensor _fft_c2r(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("int64_t") long last_dim_size);

// aten::_fft_c2r.out(Tensor self, int[] dim, int normalization, int last_dim_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _fft_c2r_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("int64_t") long last_dim_size);
@Namespace("at") public static native @ByRef Tensor _fft_c2r_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("int64_t") long last_dim_size);

// aten::_fft_c2r.out(Tensor self, int[] dim, int normalization, int last_dim_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _fft_c2r_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("int64_t") long last_dim_size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _fft_c2r_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("int64_t") long last_dim_size, @ByRef Tensor out);

// aten::_fft_c2c(Tensor self, int[] dim, int normalization, bool forward) -> Tensor
@Namespace("at") public static native @ByVal Tensor _fft_c2c(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("bool") boolean forward);
@Namespace("at") public static native @ByVal Tensor _fft_c2c(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("bool") boolean forward);

// aten::_fft_c2c.out(Tensor self, int[] dim, int normalization, bool forward, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _fft_c2c_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("bool") boolean forward);
@Namespace("at") public static native @ByRef Tensor _fft_c2c_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("bool") boolean forward);

// aten::_fft_c2c.out(Tensor self, int[] dim, int normalization, bool forward, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _fft_c2c_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("bool") boolean forward, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _fft_c2c_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("bool") boolean forward, @ByRef Tensor out);

// aten::_cufft_get_plan_cache_size(int device_index) -> int
@Namespace("at") public static native @Cast("int64_t") long _cufft_get_plan_cache_size(@Cast("int64_t") long device_index);

// aten::_cufft_get_plan_cache_max_size(int device_index) -> int
@Namespace("at") public static native @Cast("int64_t") long _cufft_get_plan_cache_max_size(@Cast("int64_t") long device_index);

// aten::_cufft_set_plan_cache_max_size(int device_index, int max_size) -> ()
@Namespace("at") public static native void _cufft_set_plan_cache_max_size(@Cast("int64_t") long device_index, @Cast("int64_t") long max_size);

// aten::_cufft_clear_plan_cache(int device_index) -> ()
@Namespace("at") public static native void _cufft_clear_plan_cache(@Cast("int64_t") long device_index);

// aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor

// aten::index_copy(Tensor self, int dim, Tensor index, Tensor source) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_copy(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source);

// aten::index_copy.dimname(Tensor self, Dimname dim, Tensor index, Tensor source) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_copy(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source);

// aten::index_put_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor(a!)

// aten::index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor

// aten::_index_put_impl_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False, bool unsafe=False) -> Tensor(a!)

// aten::instance_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool use_input_stats, float momentum, float eps, bool cudnn_enabled) -> Tensor
@Namespace("at") public static native @ByVal Tensor instance_norm(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean use_input_stats, double momentum, double eps, @Cast("bool") boolean cudnn_enabled);

// aten::inverse(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor inverse(@Const @ByRef Tensor self);

// aten::inverse.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor inverse_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::inverse.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor inverse_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::_inverse_helper(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _inverse_helper(@Const @ByRef Tensor self);

// aten::isclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor isclose(@Const @ByRef Tensor self, @Const @ByRef Tensor other, double rtol/*=1e-05*/, double atol/*=1e-08*/, @Cast("bool") boolean equal_nan/*=false*/);
@Namespace("at") public static native @ByVal Tensor isclose(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::isin.Tensor_Tensor_out(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isin_out(@ByRef Tensor out, @Const @ByRef Tensor elements, @Const @ByRef Tensor test_elements, @Cast("bool") boolean assume_unique/*=false*/, @Cast("bool") boolean invert/*=false*/);
@Namespace("at") public static native @ByRef Tensor isin_out(@ByRef Tensor out, @Const @ByRef Tensor elements, @Const @ByRef Tensor test_elements);

// aten::isin.Tensor_Tensor_out(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isin_outf(@Const @ByRef Tensor elements, @Const @ByRef Tensor test_elements, @Cast("bool") boolean assume_unique, @Cast("bool") boolean invert, @ByRef Tensor out);

// aten::isin.Tensor_Tensor(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor isin(@Const @ByRef Tensor elements, @Const @ByRef Tensor test_elements, @Cast("bool") boolean assume_unique/*=false*/, @Cast("bool") boolean invert/*=false*/);
@Namespace("at") public static native @ByVal Tensor isin(@Const @ByRef Tensor elements, @Const @ByRef Tensor test_elements);

// aten::isin.Tensor_Scalar_out(Tensor elements, Scalar test_element, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isin_out(@ByRef Tensor out, @Const @ByRef Tensor elements, @Const @ByRef Scalar test_element, @Cast("bool") boolean assume_unique/*=false*/, @Cast("bool") boolean invert/*=false*/);
@Namespace("at") public static native @ByRef Tensor isin_out(@ByRef Tensor out, @Const @ByRef Tensor elements, @Const @ByRef Scalar test_element);

// aten::isin.Tensor_Scalar_out(Tensor elements, Scalar test_element, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isin_outf(@Const @ByRef Tensor elements, @Const @ByRef Scalar test_element, @Cast("bool") boolean assume_unique, @Cast("bool") boolean invert, @ByRef Tensor out);

// aten::isin.Tensor_Scalar(Tensor elements, Scalar test_element, *, bool assume_unique=False, bool invert=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor isin(@Const @ByRef Tensor elements, @Const @ByRef Scalar test_element, @Cast("bool") boolean assume_unique/*=false*/, @Cast("bool") boolean invert/*=false*/);
@Namespace("at") public static native @ByVal Tensor isin(@Const @ByRef Tensor elements, @Const @ByRef Scalar test_element);

// aten::isin.Scalar_Tensor_out(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isin_out(@ByRef Tensor out, @Const @ByRef Scalar element, @Const @ByRef Tensor test_elements, @Cast("bool") boolean assume_unique/*=false*/, @Cast("bool") boolean invert/*=false*/);
@Namespace("at") public static native @ByRef Tensor isin_out(@ByRef Tensor out, @Const @ByRef Scalar element, @Const @ByRef Tensor test_elements);

// aten::isin.Scalar_Tensor_out(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isin_outf(@Const @ByRef Scalar element, @Const @ByRef Tensor test_elements, @Cast("bool") boolean assume_unique, @Cast("bool") boolean invert, @ByRef Tensor out);

// aten::isin.Scalar_Tensor(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor isin(@Const @ByRef Scalar element, @Const @ByRef Tensor test_elements, @Cast("bool") boolean assume_unique/*=false*/, @Cast("bool") boolean invert/*=false*/);
@Namespace("at") public static native @ByVal Tensor isin(@Const @ByRef Scalar element, @Const @ByRef Tensor test_elements);

// aten::isnan(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor isnan(@Const @ByRef Tensor self);

// aten::is_distributed(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean is_distributed(@Const @ByRef Tensor self);

// aten::is_floating_point(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean __dispatch_is_floating_point(@Const @ByRef Tensor self);

// aten::is_complex(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean __dispatch_is_complex(@Const @ByRef Tensor self);

// aten::is_conj(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean __dispatch_is_conj(@Const @ByRef Tensor self);

// aten::is_neg(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean __dispatch_is_neg(@Const @ByRef Tensor self);

// aten::isreal(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor isreal(@Const @ByRef Tensor self);

// aten::is_nonzero(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean is_nonzero(@Const @ByRef Tensor self);

// aten::is_same_size(Tensor self, Tensor other) -> bool
@Namespace("at") public static native @Cast("bool") boolean is_same_size(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::is_signed(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean __dispatch_is_signed(@Const @ByRef Tensor self);

// aten::is_inference(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean __dispatch_is_inference(@Const @ByRef Tensor self);

// aten::kl_div(Tensor self, Tensor target, int reduction=Mean, *, bool log_target=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor kl_div(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("bool") boolean log_target/*=false*/);
@Namespace("at") public static native @ByVal Tensor kl_div(@Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::kl_div_backward(Tensor grad_output, Tensor self, Tensor target, int reduction=Mean, *, bool log_target=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor kl_div_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("bool") boolean log_target/*=false*/);
@Namespace("at") public static native @ByVal Tensor kl_div_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::kron(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor kron(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::kron.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor kron_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::kron.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor kron_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::kthvalue(Tensor self, int k, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple kthvalue(@Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple kthvalue(@Const @ByRef Tensor self, @Cast("int64_t") long k);

// aten::kthvalue.values(Tensor self, int k, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> kthvalue_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> kthvalue_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k);

// aten::kthvalue.values(Tensor self, int k, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> kthvalue_outf(@Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::kthvalue.dimname(Tensor self, int k, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple kthvalue(@Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple kthvalue(@Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal Dimname dim);

// aten::kthvalue.dimname_out(Tensor self, int k, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> kthvalue_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> kthvalue_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal Dimname dim);

// aten::kthvalue.dimname_out(Tensor self, int k, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> kthvalue_outf(@Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::layer_norm(Tensor input, int[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enable=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor layer_norm(@Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef normalized_shape, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, double eps/*=1e-05*/, @Cast("bool") boolean cudnn_enable/*=true*/);
@Namespace("at") public static native @ByVal Tensor layer_norm(@Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef normalized_shape);
@Namespace("at") public static native @ByVal Tensor layer_norm(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] normalized_shape, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, double eps/*=1e-05*/, @Cast("bool") boolean cudnn_enable/*=true*/);
@Namespace("at") public static native @ByVal Tensor layer_norm(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... normalized_shape);

// aten::native_layer_norm(Tensor input, int[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple native_layer_norm(@Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef normalized_shape, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, double eps);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple native_layer_norm(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] normalized_shape, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, double eps);

// aten::native_layer_norm_backward(Tensor grad_out, Tensor input, int[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple native_layer_norm_backward(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef normalized_shape, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple native_layer_norm_backward(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] normalized_shape, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::nan_to_num(Tensor self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor nan_to_num(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional nan, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional posinf, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional neginf);
@Namespace("at") public static native @ByVal Tensor nan_to_num(@Const @ByRef Tensor self);

// aten::nan_to_num_(Tensor(a!) self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nan_to_num_(@ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional nan, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional posinf, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional neginf);
@Namespace("at") public static native @ByRef Tensor nan_to_num_(@ByRef Tensor self);

// aten::nan_to_num.out(Tensor self, float? nan=None, float? posinf=None, float? neginf=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nan_to_num_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional nan, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional posinf, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional neginf);
@Namespace("at") public static native @ByRef Tensor nan_to_num_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::nan_to_num.out(Tensor self, float? nan=None, float? posinf=None, float? neginf=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nan_to_num_outf(@Const @ByRef Tensor self, @ByVal DoubleOptional nan, @ByVal DoubleOptional posinf, @ByVal DoubleOptional neginf, @ByRef Tensor out);

// aten::linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linear(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias);
@Namespace("at") public static native @ByVal Tensor linear(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);

// aten::linear.out(Tensor input, Tensor weight, Tensor? bias=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linear_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias);
@Namespace("at") public static native @ByRef Tensor linear_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight);

// aten::linear.out(Tensor input, Tensor weight, Tensor? bias=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linear_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByRef Tensor out);

// aten::mkldnn_linear(Tensor self, Tensor weight, Tensor? bias=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_linear(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias);
@Namespace("at") public static native @ByVal Tensor mkldnn_linear(@Const @ByRef Tensor self, @Const @ByRef Tensor weight);

// aten::mkldnn_linear_backward_input(int[] input_size, Tensor grad_output, Tensor weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_linear_backward_input(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor mkldnn_linear_backward_input(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight);

// aten::mkldnn_linear_backward_weights(Tensor grad_output, Tensor input, Tensor weight, bool bias_defined) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple mkldnn_linear_backward_weights(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Cast("bool") boolean bias_defined);

// aten::mkldnn_linear_backward(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple mkldnn_linear_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::fbgemm_linear_int8_weight_fp32_activation(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -> Tensor
@Namespace("at") public static native @ByVal Tensor fbgemm_linear_int8_weight_fp32_activation(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef Tensor packed, @Const @ByRef Tensor col_offsets, @Const @ByRef Scalar weight_scale, @Const @ByRef Scalar weight_zero_point, @Const @ByRef Tensor bias);

// aten::fbgemm_linear_int8_weight(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -> Tensor
@Namespace("at") public static native @ByVal Tensor fbgemm_linear_int8_weight(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef Tensor packed, @Const @ByRef Tensor col_offsets, @Const @ByRef Scalar weight_scale, @Const @ByRef Scalar weight_zero_point, @Const @ByRef Tensor bias);

// aten::fbgemm_linear_quantize_weight(Tensor input) -> (Tensor, Tensor, float, int)
@Namespace("at") public static native @ByVal TensorTensorDoubleLongTuple fbgemm_linear_quantize_weight(@Const @ByRef Tensor input);

// aten::fbgemm_pack_gemm_matrix_fp16(Tensor input) -> Tensor
@Namespace("at") public static native @ByVal Tensor fbgemm_pack_gemm_matrix_fp16(@Const @ByRef Tensor input);

// aten::fbgemm_linear_fp16_weight_fp32_activation(Tensor input, Tensor packed_weight, Tensor bias) -> Tensor
@Namespace("at") public static native @ByVal Tensor fbgemm_linear_fp16_weight_fp32_activation(@Const @ByRef Tensor input, @Const @ByRef Tensor packed_weight, @Const @ByRef Tensor bias);

// aten::fbgemm_linear_fp16_weight(Tensor input, Tensor packed_weight, Tensor bias) -> Tensor
@Namespace("at") public static native @ByVal Tensor fbgemm_linear_fp16_weight(@Const @ByRef Tensor input, @Const @ByRef Tensor packed_weight, @Const @ByRef Tensor bias);

// aten::fbgemm_pack_quantized_matrix(Tensor input) -> Tensor
@Namespace("at") public static native @ByVal Tensor fbgemm_pack_quantized_matrix(@Const @ByRef Tensor input);

// aten::fbgemm_pack_quantized_matrix.KN(Tensor input, int K, int N) -> Tensor
@Namespace("at") public static native @ByVal Tensor fbgemm_pack_quantized_matrix(@Const @ByRef Tensor input, @Cast("int64_t") long K, @Cast("int64_t") long N);

// aten::ldexp.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor ldexp(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::ldexp_(Tensor(a!) self, Tensor other) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ldexp_(@ByRef Tensor self, @Const @ByRef Tensor other);

// aten::ldexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ldexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::ldexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ldexp_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::linspace(Scalar start, Scalar end, int? steps=None, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional steps, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor linspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end);

// aten::linspace(Scalar start, Scalar end, int? steps=None, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal LongOptional steps, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::linspace.out(Scalar start, Scalar end, int? steps=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linspace_out(@ByRef Tensor out, @Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional steps);
@Namespace("at") public static native @ByRef Tensor linspace_out(@ByRef Tensor out, @Const @ByRef Scalar start, @Const @ByRef Scalar end);

// aten::linspace.out(Scalar start, Scalar end, int? steps=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linspace_outf(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal LongOptional steps, @ByRef Tensor out);

// aten::log(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor log(@Const @ByRef Tensor self);

// aten::log_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_(@ByRef Tensor self);

// aten::log.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::log.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::log10(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor log10(@Const @ByRef Tensor self);

// aten::log10_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log10_(@ByRef Tensor self);

// aten::log10.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log10_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::log10.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log10_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::log1p(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor log1p(@Const @ByRef Tensor self);

// aten::log1p_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log1p_(@ByRef Tensor self);

// aten::log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log1p_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log1p_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::log2(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor log2(@Const @ByRef Tensor self);

// aten::log2_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log2_(@ByRef Tensor self);

// aten::log2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log2_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::log2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log2_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::logaddexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logaddexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::logaddexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logaddexp_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::logaddexp(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor logaddexp(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::logaddexp2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logaddexp2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::logaddexp2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logaddexp2_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::logaddexp2(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor logaddexp2(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::xlogy.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor xlogy(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::xlogy.Scalar_Self(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor xlogy(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::xlogy.Scalar_Other(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor xlogy(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::xlogy_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_(@ByRef Tensor self, @Const @ByRef Tensor other);

// aten::xlogy_.Scalar_Other(Tensor(a!) self, Scalar other) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_(@ByRef Tensor self, @Const @ByRef Scalar other);

// aten::xlogy.OutTensor(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::xlogy.OutTensor(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::xlogy.OutScalar_Self(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::xlogy.OutScalar_Self(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::xlogy.OutScalar_Other(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::xlogy.OutScalar_Other(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::logdet(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor logdet(@Const @ByRef Tensor self);

// aten::logspace(Scalar start, Scalar end, int? steps=None, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor logspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional steps, double base/*=10.0*/, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor logspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end);

// aten::logspace(Scalar start, Scalar end, int? steps=None, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor logspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal LongOptional steps, double base, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::logspace.out(Scalar start, Scalar end, int? steps=None, float base=10.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logspace_out(@ByRef Tensor out, @Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional steps, double base/*=10.0*/);
@Namespace("at") public static native @ByRef Tensor logspace_out(@ByRef Tensor out, @Const @ByRef Scalar start, @Const @ByRef Scalar end);

// aten::logspace.out(Scalar start, Scalar end, int? steps=None, float base=10.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logspace_outf(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal LongOptional steps, double base, @ByRef Tensor out);

// aten::log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor log_softmax(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor log_softmax(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
@Namespace("at") public static native @ByVal Tensor _log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float);

// aten::_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _log_softmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float);

// aten::_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _log_softmax_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float, @ByRef Tensor out);

// aten::_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _log_softmax_backward_data(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, @Const @ByRef Tensor self);

// aten::_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _log_softmax_backward_data_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, @Const @ByRef Tensor self);

// aten::_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _log_softmax_backward_data_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, @Const @ByRef Tensor self, @ByRef Tensor out);

// aten::_logcumsumexp(Tensor self, int dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor _logcumsumexp(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::_logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _logcumsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::_logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _logcumsumexp_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor out);

// aten::logcumsumexp(Tensor self, int dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor logcumsumexp(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logcumsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logcumsumexp_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor out);

// aten::logcumsumexp.dimname(Tensor self, Dimname dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor logcumsumexp(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::logcumsumexp.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logcumsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::logcumsumexp.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logcumsumexp_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByRef Tensor out);

// aten::logsumexp(Tensor self, int[1] dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logsumexp_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor logsumexp_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::logsumexp.names(Tensor self, Dimname[1] dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);

// aten::logsumexp.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);

// aten::logsumexp.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logsumexp_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::margin_ranking_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor margin_ranking_loss(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor target, double margin/*=0.0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor margin_ranking_loss(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor target);

// aten::matmul(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor matmul(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor matmul_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor matmul_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::matrix_rank.tol(Tensor self, float tol, bool symmetric=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor matrix_rank(@Const @ByRef Tensor self, double tol, @Cast("bool") boolean symmetric/*=false*/);
@Namespace("at") public static native @ByVal Tensor matrix_rank(@Const @ByRef Tensor self, double tol);

// aten::matrix_rank(Tensor self, bool symmetric=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor matrix_rank(@Const @ByRef Tensor self, @Cast("bool") boolean symmetric/*=false*/);
@Namespace("at") public static native @ByVal Tensor matrix_rank(@Const @ByRef Tensor self);

// aten::matrix_power(Tensor self, int n) -> Tensor
@Namespace("at") public static native @ByVal Tensor matrix_power(@Const @ByRef Tensor self, @Cast("int64_t") long n);

// aten::matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor matrix_power_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long n);

// aten::matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor matrix_power_outf(@Const @ByRef Tensor self, @Cast("int64_t") long n, @ByRef Tensor out);

// aten::matrix_exp(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor matrix_exp(@Const @ByRef Tensor self);

// aten::matrix_exp_backward(Tensor self, Tensor grad) -> Tensor
@Namespace("at") public static native @ByVal Tensor matrix_exp_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad);

// aten::_aminmax(Tensor self) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _aminmax(@Const @ByRef Tensor self);

// aten::_aminmax.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _aminmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple _aminmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::aminmax(Tensor self, *, int? dim=None, bool keepdim=False) -> (Tensor min, Tensor max)
@Namespace("at") public static native @ByVal TensorTensorTuple aminmax(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple aminmax(@Const @ByRef Tensor self);

// aten::aminmax.out(Tensor self, *, int? dim=None, bool keepdim=False, Tensor(a!) min, Tensor(b!) max) -> (Tensor(a!) min, Tensor(b!) max)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> aminmax_out(@ByRef Tensor min, @ByRef Tensor max, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> aminmax_out(@ByRef Tensor min, @ByRef Tensor max, @Const @ByRef Tensor self);

// aten::aminmax.out(Tensor self, *, int? dim=None, bool keepdim=False, Tensor(a!) min, Tensor(b!) max) -> (Tensor(a!) min, Tensor(b!) max)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> aminmax_outf(@Const @ByRef Tensor self, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByRef Tensor min, @ByRef Tensor max);

// aten::_compute_linear_combination(Tensor input, Tensor coefficients) -> Tensor
@Namespace("at") public static native @ByVal Tensor _compute_linear_combination(@Const @ByRef Tensor input, @Const @ByRef Tensor coefficients);

// aten::_compute_linear_combination.out(Tensor input, Tensor coefficients, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _compute_linear_combination_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor coefficients);

// aten::_compute_linear_combination.out(Tensor input, Tensor coefficients, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _compute_linear_combination_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor coefficients, @ByRef Tensor out);

// aten::max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple max(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple max(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::max.dim_max(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_out(@ByRef Tensor max, @ByRef Tensor max_values, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_out(@ByRef Tensor max, @ByRef Tensor max_values, @Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::max.dim_max(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor max, @ByRef Tensor max_values);

// aten::max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple max(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple max(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::max.names_dim_max(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_out(@ByRef Tensor max, @ByRef Tensor max_values, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_out(@ByRef Tensor max, @ByRef Tensor max_values, @Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::max.names_dim_max(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor max, @ByRef Tensor max_values);

// aten::value_selecting_reduction_backward(Tensor grad, int dim, Tensor indices, int[] sizes, bool keepdim) -> Tensor
@Namespace("at") public static native @ByVal Tensor value_selecting_reduction_backward(@Const @ByRef Tensor grad, @Cast("int64_t") long dim, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes, @Cast("bool") boolean keepdim);
@Namespace("at") public static native @ByVal Tensor value_selecting_reduction_backward(@Const @ByRef Tensor grad, @Cast("int64_t") long dim, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes, @Cast("bool") boolean keepdim);

// aten::amax(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor amax(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor amax(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor amax(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);

// aten::amax.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor amax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor amax_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor amax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);

// aten::amax.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor amax_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor amax_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::max_pool1d_with_indices(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool1d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool1d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool1d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool1d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::mkldnn_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::mkldnn_max_pool2d_backward(Tensor grad_output, Tensor output, Tensor input, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::mkldnn_max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::mkldnn_max_pool3d_backward(Tensor grad_output, Tensor output, Tensor input, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::quantized_max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantized_max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::quantized_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantized_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::mean(Tensor self, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self);

// aten::mean.dim(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::mean.out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::mean.out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mean_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mean_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::mean.names_dim(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);

// aten::mean.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);

// aten::mean.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mean_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::nanmean(Tensor self, int[1] dim=[], bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor nanmean(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor nanmean(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor nanmean(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);

// aten::nanmean.out(Tensor self, int[1] dim=[], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanmean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor nanmean_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor nanmean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);

// aten::nanmean.out(Tensor self, int[1] dim=[], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanmean_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor nanmean_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::median(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor median(@Const @ByRef Tensor self);

// aten::median.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple median(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple median(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::median.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> median_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> median_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::median.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> median_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::median.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple median(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple median(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::median.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> median_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> median_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::median.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> median_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::nanmedian(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor nanmedian(@Const @ByRef Tensor self);

// aten::nanmedian.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple nanmedian(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple nanmedian(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::nanmedian.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nanmedian_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nanmedian_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::nanmedian.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nanmedian_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::nanmedian.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple nanmedian(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple nanmedian(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::nanmedian.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nanmedian_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nanmedian_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::nanmedian.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nanmedian_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple min(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple min(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::min.dim_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> min_out(@ByRef Tensor min, @ByRef Tensor min_indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> min_out(@ByRef Tensor min, @ByRef Tensor min_indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::min.dim_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> min_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor min, @ByRef Tensor min_indices);

// aten::min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple min(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple min(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::min.names_dim_min(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> min_out(@ByRef Tensor min, @ByRef Tensor min_indices, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> min_out(@ByRef Tensor min, @ByRef Tensor min_indices, @Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::min.names_dim_min(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> min_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor min, @ByRef Tensor min_indices);

// aten::amin(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor amin(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor amin(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor amin(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);

// aten::amin.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor amin_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor amin_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor amin_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);

// aten::amin.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor amin_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor amin_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::mkldnn_convolution(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor mkldnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups);

// aten::mkldnn_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool bias_defined) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_convolution_backward_input(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef self_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean bias_defined);
@Namespace("at") public static native @ByVal Tensor mkldnn_convolution_backward_input(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] self_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean bias_defined);

// aten::mkldnn_convolution_backward_weights(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool bias_defined) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple mkldnn_convolution_backward_weights(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean bias_defined);
@Namespace("at") public static native @ByVal TensorTensorTuple mkldnn_convolution_backward_weights(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean bias_defined);

// aten::mkldnn_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple mkldnn_convolution_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple mkldnn_convolution_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::miopen_batch_norm(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple miopen_batch_norm(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double exponential_average_factor, double epsilon);

// aten::miopen_batch_norm_backward(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple miopen_batch_norm_backward(@Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_var, double epsilon);

// aten::miopen_convolution(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);

// aten::miopen_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_convolution_backward_input(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef self_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_backward_input(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] self_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);

// aten::miopen_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple miopen_convolution_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple miopen_convolution_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::miopen_convolution_backward_bias(Tensor grad_output) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_convolution_backward_bias(@Const @ByRef Tensor grad_output);

// aten::miopen_convolution_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_convolution_backward_weight(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_backward_weight(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);

// aten::miopen_convolution_transpose(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);

// aten::miopen_convolution_transpose_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple miopen_convolution_transpose_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple miopen_convolution_transpose_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::miopen_convolution_transpose_backward_input(Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_convolution_transpose_backward_input(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_transpose_backward_input(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);

// aten::miopen_convolution_transpose_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_convolution_transpose_backward_weight(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_transpose_backward_weight(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);

// aten::miopen_depthwise_convolution(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_depthwise_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_depthwise_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);

// aten::miopen_depthwise_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_depthwise_convolution_backward_input(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef self_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_depthwise_convolution_backward_input(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] self_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);

// aten::miopen_depthwise_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple miopen_depthwise_convolution_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple miopen_depthwise_convolution_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::miopen_depthwise_convolution_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_depthwise_convolution_backward_weight(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_depthwise_convolution_backward_weight(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);

// aten::miopen_rnn(Tensor input, Tensor[] weight, int weight_stride0, Tensor hx, Tensor? cx, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTensorTuple miopen_rnn(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state);
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTensorTuple miopen_rnn(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Const @ByRef TensorOptional dropout_state);

// aten::miopen_rnn_backward(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask) -> (Tensor, Tensor, Tensor, Tensor[])
@Namespace("at") public static native @ByVal TensorTensorTensorTensorVectorTuple miopen_rnn_backward(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTensorVectorTuple miopen_rnn_backward(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask);

// aten::mm(Tensor self, Tensor mat2) -> Tensor
@Namespace("at") public static native @ByVal Tensor mm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat2);

// aten::mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat2);

// aten::mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat2, @ByRef Tensor out);

// aten::_sparse_mm(Tensor sparse, Tensor dense) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_mm(@Const @ByRef Tensor sparse, @Const @ByRef Tensor dense);

// aten::_sparse_sparse_matmul(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_sparse_matmul(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::_sparse_mask_helper(Tensor t, Tensor mask_indices) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_mask_helper(@Const @ByRef Tensor t, @Const @ByRef Tensor mask_indices);

// aten::mode(Tensor self, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple mode(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple mode(@Const @ByRef Tensor self);

// aten::mode.values(Tensor self, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mode_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mode_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self);

// aten::mode.values(Tensor self, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mode_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::mode.dimname(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple mode(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple mode(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::mode.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mode_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mode_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::mode.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mode_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::mul.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor mul(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mul_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mul_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::mul.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor mul(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::multiply.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor multiply(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::multiply.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multiply_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::multiply.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multiply_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::multiply.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor multiply(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::mv(Tensor self, Tensor vec) -> Tensor
@Namespace("at") public static native @ByVal Tensor mv(@Const @ByRef Tensor self, @Const @ByRef Tensor vec);

// aten::mv.out(Tensor self, Tensor vec, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor vec);

// aten::mv.out(Tensor self, Tensor vec, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mv_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor vec, @ByRef Tensor out);

// aten::mvlgamma.out(Tensor self, int p, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mvlgamma_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long p);

// aten::mvlgamma.out(Tensor self, int p, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mvlgamma_outf(@Const @ByRef Tensor self, @Cast("int64_t") long p, @ByRef Tensor out);

// aten::mvlgamma(Tensor self, int p) -> Tensor
@Namespace("at") public static native @ByVal Tensor mvlgamma(@Const @ByRef Tensor self, @Cast("int64_t") long p);

// aten::narrow_copy(Tensor self, int dim, int start, int length) -> Tensor
@Namespace("at") public static native @ByVal Tensor narrow_copy(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long length);

// aten::narrow_copy.out(Tensor self, int dim, int start, int length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor narrow_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long length);

// aten::narrow_copy.out(Tensor self, int dim, int start, int length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor narrow_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long length, @ByRef Tensor out);

// aten::narrow(Tensor(a) self, int dim, int start, int length) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor narrow(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long length);

// aten::narrow.Tensor(Tensor(a) self, int dim, Tensor start, int length) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor narrow(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor start, @Cast("int64_t") long length);

// aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple native_batch_norm(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double momentum, double eps);

// aten::native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_batch_norm_out(@ByRef Tensor out, @ByRef Tensor save_mean, @ByRef Tensor save_invstd, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double momentum, double eps);

// aten::native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_batch_norm_outf(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double momentum, double eps, @ByRef Tensor out, @ByRef Tensor save_mean, @ByRef Tensor save_invstd);

// aten::batch_norm_stats(Tensor input, float eps) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple batch_norm_stats(@Const @ByRef Tensor input, double eps);

// aten::batch_norm_elemt(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps) -> Tensor
@Namespace("at") public static native @ByVal Tensor batch_norm_elemt(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, double eps);

// aten::batch_norm_elemt.out(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor batch_norm_elemt_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, double eps);

// aten::batch_norm_elemt.out(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor batch_norm_elemt_outf(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, double eps, @ByRef Tensor out);

// aten::batch_norm_gather_stats(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, int count) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple batch_norm_gather_stats(@Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum, double eps, @Cast("int64_t") long count);

// aten::batch_norm_gather_stats_with_counts(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple batch_norm_gather_stats_with_counts(@Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum, double eps, @Const @ByRef Tensor counts);

// aten::native_batch_norm_backward(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple native_batch_norm_backward(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_invstd, @Cast("bool") boolean train, double eps, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::batch_norm_backward_reduce(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g) -> (Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple batch_norm_backward_reduce(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional weight, @Cast("bool") boolean input_g, @Cast("bool") boolean weight_g, @Cast("bool") boolean bias_g);

// aten::batch_norm_backward_elemt(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, Tensor mean_dy, Tensor mean_dy_xmu, Tensor count) -> Tensor
@Namespace("at") public static native @ByVal Tensor batch_norm_backward_elemt(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional weight, @Const @ByRef Tensor mean_dy, @Const @ByRef Tensor mean_dy_xmu, @Const @ByRef Tensor count);

// aten::batch_norm_update_stats(Tensor input, Tensor? running_mean, Tensor? running_var, float momentum) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple batch_norm_update_stats(@Const @ByRef Tensor input, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum);

// aten::is_vulkan_available() -> bool
@Namespace("at") public static native @Cast("bool") boolean is_vulkan_available();

// aten::_nnpack_available() -> bool
@Namespace("at") public static native @Cast("bool") boolean _nnpack_available();

// aten::_nnpack_spatial_convolution(Tensor input, Tensor weight, Tensor? bias, int[2] padding, int[2] stride=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _nnpack_spatial_convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor _nnpack_spatial_convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor _nnpack_spatial_convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);
@Namespace("at") public static native @ByVal Tensor _nnpack_spatial_convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::_nnpack_spatial_convolution_backward(Tensor input, Tensor grad_output, Tensor weight, int[2] padding, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _nnpack_spatial_convolution_backward(@Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _nnpack_spatial_convolution_backward(@Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::_nnpack_spatial_convolution_backward_input(Tensor input, Tensor grad_output, Tensor weight, int[2] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor _nnpack_spatial_convolution_backward_input(@Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor _nnpack_spatial_convolution_backward_input(@Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::_nnpack_spatial_convolution_backward_weight(Tensor input, int[] weightsize, Tensor grad_output, int[2] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor _nnpack_spatial_convolution_backward_weight(@Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef weightsize, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor _nnpack_spatial_convolution_backward_weight(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] weightsize, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::ones.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);

// aten::ones.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::ones(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::ones(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::ones.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ones_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor ones_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::ones.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ones_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor ones_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);

// aten::ones_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor ones_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor ones_like(@Const @ByRef Tensor self);

// aten::ones_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor ones_like(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::pairwise_distance(Tensor x1, Tensor x2, float p=2, float eps=1e-06, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor pairwise_distance(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2, double p/*=2*/, double eps/*=1e-06*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor pairwise_distance(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2);

// aten::cdist(Tensor x1, Tensor x2, float p=2, int? compute_mode=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor cdist(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2, double p/*=2*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional compute_mode);
@Namespace("at") public static native @ByVal Tensor cdist(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2);

// aten::_euclidean_dist(Tensor x1, Tensor x2) -> Tensor
@Namespace("at") public static native @ByVal Tensor _euclidean_dist(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2);

// aten::_cdist_forward(Tensor x1, Tensor x2, float p, int? compute_mode) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cdist_forward(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2, double p, @ByVal LongOptional compute_mode);

// aten::_cdist_backward(Tensor grad, Tensor x1, Tensor x2, float p, Tensor cdist) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cdist_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor x1, @Const @ByRef Tensor x2, double p, @Const @ByRef Tensor cdist);

// aten::pdist(Tensor self, float p=2) -> Tensor
@Namespace("at") public static native @ByVal Tensor pdist(@Const @ByRef Tensor self, double p/*=2*/);
@Namespace("at") public static native @ByVal Tensor pdist(@Const @ByRef Tensor self);

// aten::_pdist_forward(Tensor self, float p=2) -> Tensor
@Namespace("at") public static native @ByVal Tensor _pdist_forward(@Const @ByRef Tensor self, double p/*=2*/);
@Namespace("at") public static native @ByVal Tensor _pdist_forward(@Const @ByRef Tensor self);

// aten::_pdist_backward(Tensor grad, Tensor self, float p, Tensor pdist) -> Tensor
@Namespace("at") public static native @ByVal Tensor _pdist_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, double p, @Const @ByRef Tensor pdist);

// aten::cosine_similarity(Tensor x1, Tensor x2, int dim=1, float eps=1e-08) -> Tensor
@Namespace("at") public static native @ByVal Tensor cosine_similarity(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2, @Cast("int64_t") long dim/*=1*/, double eps/*=1e-08*/);
@Namespace("at") public static native @ByVal Tensor cosine_similarity(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2);

// aten::permute(Tensor(a) self, int[] dims) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor permute(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("at") public static native @ByVal Tensor permute(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);

// aten::movedim.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor movedim(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef source, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef destination);
@Namespace("at") public static native @ByVal Tensor movedim(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] source, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... destination);

// aten::movedim.int(Tensor(a) self, int source, int destination) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor movedim(@Const @ByRef Tensor self, @Cast("int64_t") long source, @Cast("int64_t") long destination);

// aten::moveaxis.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor moveaxis(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef source, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef destination);
@Namespace("at") public static native @ByVal Tensor moveaxis(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] source, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... destination);

// aten::moveaxis.int(Tensor(a) self, int source, int destination) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor moveaxis(@Const @ByRef Tensor self, @Cast("int64_t") long source, @Cast("int64_t") long destination);

// aten::pixel_shuffle(Tensor self, int upscale_factor) -> Tensor
@Namespace("at") public static native @ByVal Tensor pixel_shuffle(@Const @ByRef Tensor self, @Cast("int64_t") long upscale_factor);

// aten::pixel_unshuffle(Tensor self, int downscale_factor) -> Tensor
@Namespace("at") public static native @ByVal Tensor pixel_unshuffle(@Const @ByRef Tensor self, @Cast("int64_t") long downscale_factor);

// aten::channel_shuffle(Tensor self, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor channel_shuffle(@Const @ByRef Tensor self, @Cast("int64_t") long groups);

// aten::_pin_memory(Tensor self, Device? device=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _pin_memory(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("at") public static native @ByVal Tensor _pin_memory(@Const @ByRef Tensor self);

// aten::pinverse(Tensor self, float rcond=1e-15) -> Tensor
@Namespace("at") public static native @ByVal Tensor pinverse(@Const @ByRef Tensor self, double rcond/*=1e-15*/);
@Namespace("at") public static native @ByVal Tensor pinverse(@Const @ByRef Tensor self);

// aten::poisson_nll_loss(Tensor input, Tensor target, bool log_input, bool full, float eps, int reduction) -> Tensor
@Namespace("at") public static native @ByVal Tensor poisson_nll_loss(@Const @ByRef Tensor input, @Const @ByRef Tensor target, @Cast("bool") boolean log_input, @Cast("bool") boolean full, double eps, @Cast("int64_t") long reduction);

// aten::rad2deg(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor rad2deg(@Const @ByRef Tensor self);

// aten::rad2deg_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rad2deg_(@ByRef Tensor self);

// aten::rad2deg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rad2deg_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::rad2deg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rad2deg_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::deg2rad(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor deg2rad(@Const @ByRef Tensor self);

// aten::deg2rad_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor deg2rad_(@ByRef Tensor self);

// aten::deg2rad.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor deg2rad_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::deg2rad.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor deg2rad_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::scalar_tensor(Scalar s, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor scalar_tensor(@Const @ByRef Scalar s, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor scalar_tensor(@Const @ByRef Scalar s);

// aten::scalar_tensor(Scalar s, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor scalar_tensor(@Const @ByRef Scalar s, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::rand.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);

// aten::rand.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::rand.generator_with_names(int[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);

// aten::rand.generator_with_names(int[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::rand(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::rand(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::rand.generator(int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);

// aten::rand.generator(int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::rand.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::rand.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);

// aten::rand.generator_out(int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);

// aten::rand.generator_out(int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::rand_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor rand_like(@Const @ByRef Tensor self);

// aten::rand_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand_like(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::randint(int high, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::randint(int high, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::randint.generator(int high, int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);

// aten::randint.generator(int high, int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::randint.low(int low, int high, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::randint.low(int low, int high, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::randint.low_generator(int low, int high, int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);

// aten::randint.low_generator(int low, int high, int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::randint.out(int high, int[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::randint.out(int high, int[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);

// aten::randint.generator_out(int high, int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);

// aten::randint.generator_out(int high, int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::randint.low_out(int low, int high, int[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::randint.low_out(int low, int high, int[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);

// aten::randint.low_generator_out(int low, int high, int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);

// aten::randint.low_generator_out(int low, int high, int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::randint_like(Tensor self, int high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long high, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long high);

// aten::randint_like(Tensor self, int high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long high, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::randint_like.low_dtype(Tensor self, int low, int high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long low, @Cast("int64_t") long high);

// aten::randint_like.low_dtype(Tensor self, int low, int high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::randn(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::randn(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::randn.generator(int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);

// aten::randn.generator(int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::randn.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);

// aten::randn.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::randn.generator_with_names(int[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);

// aten::randn.generator_with_names(int[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::randn.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::randn.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);

// aten::randn.generator_out(int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);

// aten::randn.generator_out(int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::randn_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor randn_like(@Const @ByRef Tensor self);

// aten::randn_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn_like(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::randperm(int n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n);

// aten::randperm(int n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::randperm.generator(int n, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n, @ByVal GeneratorOptional generator);

// aten::randperm.generator(int n, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::randperm.out(int n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randperm_out(@ByRef Tensor out, @Cast("int64_t") long n);

// aten::randperm.out(int n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randperm_outf(@Cast("int64_t") long n, @ByRef Tensor out);

// aten::randperm.generator_out(int n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randperm_out(@ByRef Tensor out, @Cast("int64_t") long n, @ByVal GeneratorOptional generator);

// aten::randperm.generator_out(int n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randperm_outf(@Cast("int64_t") long n, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::range.step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor range(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar step, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);

// aten::range.step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor range(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::range(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor range(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);

// aten::range(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor range(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::range.out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor range_out(@ByRef Tensor out, @Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar step);
@Namespace("at") public static native @ByRef Tensor range_out(@ByRef Tensor out, @Const @ByRef Scalar start, @Const @ByRef Scalar end);

// aten::range.out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor range_outf(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step, @ByRef Tensor out);

// aten::ravel(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor ravel(@Const @ByRef Tensor self);

// aten::reciprocal(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor reciprocal(@Const @ByRef Tensor self);

// aten::reciprocal_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reciprocal_(@ByRef Tensor self);

// aten::reciprocal.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reciprocal_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::reciprocal.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reciprocal_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::neg(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor neg(@Const @ByRef Tensor self);

// aten::neg_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor neg_(@ByRef Tensor self);

// aten::neg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor neg_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::neg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor neg_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::negative(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor negative(@Const @ByRef Tensor self);

// aten::negative_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor negative_(@ByRef Tensor self);

// aten::negative.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor negative_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::negative.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor negative_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::repeat_interleave.Tensor(Tensor repeats, *, int? output_size=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor repeat_interleave(@Const @ByRef Tensor repeats, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional output_size);
@Namespace("at") public static native @ByVal Tensor repeat_interleave(@Const @ByRef Tensor repeats);

// aten::repeat_interleave.self_Tensor(Tensor self, Tensor repeats, int? dim=None, *, int? output_size=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor repeat_interleave(@Const @ByRef Tensor self, @Const @ByRef Tensor repeats, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional output_size);
@Namespace("at") public static native @ByVal Tensor repeat_interleave(@Const @ByRef Tensor self, @Const @ByRef Tensor repeats);

// aten::repeat_interleave.self_int(Tensor self, int repeats, int? dim=None, *, int? output_size=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor repeat_interleave(@Const @ByRef Tensor self, @Cast("int64_t") long repeats, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional output_size);
@Namespace("at") public static native @ByVal Tensor repeat_interleave(@Const @ByRef Tensor self, @Cast("int64_t") long repeats);

// aten::reshape(Tensor(a) self, int[] shape) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor reshape(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape);
@Namespace("at") public static native @ByVal Tensor reshape(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... shape);

// aten::_reshape_alias(Tensor(a) self, int[] size, int[] stride) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor _reshape_alias(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor _reshape_alias(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);

// aten::_mkldnn_reshape(Tensor self, int[] shape) -> Tensor
@Namespace("at") public static native @ByVal Tensor _mkldnn_reshape(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape);
@Namespace("at") public static native @ByVal Tensor _mkldnn_reshape(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... shape);

// aten::round(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor round(@Const @ByRef Tensor self);

// aten::round_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor round_(@ByRef Tensor self);

// aten::round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor round_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor round_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::rrelu(Tensor self, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rrelu(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.125)") Scalar lower, @Const @ByRef(nullValue = "at::Scalar(0.3333333333333333)") Scalar upper, @Cast("bool") boolean training/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor rrelu(@Const @ByRef Tensor self);

// aten::rrelu_(Tensor(a!) self, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rrelu_(@ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.125)") Scalar lower, @Const @ByRef(nullValue = "at::Scalar(0.3333333333333333)") Scalar upper, @Cast("bool") boolean training/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor rrelu_(@ByRef Tensor self);

// aten::relu(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor relu(@Const @ByRef Tensor self);

// aten::relu_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor relu_(@ByRef Tensor self);

// aten::relu6(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor relu6(@Const @ByRef Tensor self);

// aten::relu6_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor relu6_(@ByRef Tensor self);

// aten::prelu(Tensor self, Tensor weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor prelu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight);

// aten::prelu_backward(Tensor grad_output, Tensor self, Tensor weight) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple prelu_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight);

// aten::gelu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gelu_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::gelu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gelu_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::gelu(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor gelu(@Const @ByRef Tensor self);

// aten::gelu_backward.grad_input(Tensor grad, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gelu_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad, @Const @ByRef Tensor self);

// aten::gelu_backward.grad_input(Tensor grad, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gelu_backward_outf(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @ByRef Tensor grad_input);

// aten::gelu_backward(Tensor grad, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor gelu_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self);

// aten::infinitely_differentiable_gelu_backward(Tensor grad, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor infinitely_differentiable_gelu_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self);

// aten::hardshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardshrink_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.5)") Scalar lambd);
@Namespace("at") public static native @ByRef Tensor hardshrink_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::hardshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardshrink_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar lambd, @ByRef Tensor out);

// aten::hardshrink(Tensor self, Scalar lambd=0.5) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardshrink(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.5)") Scalar lambd);
@Namespace("at") public static native @ByVal Tensor hardshrink(@Const @ByRef Tensor self);

// aten::hardshrink_backward.grad_input(Tensor grad_out, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardshrink_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_out, @Const @ByRef Tensor self, @Const @ByRef Scalar lambd);

// aten::hardshrink_backward.grad_input(Tensor grad_out, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardshrink_backward_outf(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor self, @Const @ByRef Scalar lambd, @ByRef Tensor grad_input);

// aten::hardshrink_backward(Tensor grad_out, Tensor self, Scalar lambd) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardshrink_backward(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor self, @Const @ByRef Scalar lambd);

// aten::rsqrt(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor rsqrt(@Const @ByRef Tensor self);

// aten::rsqrt_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rsqrt_(@ByRef Tensor self);

// aten::rsqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rsqrt_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::rsqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rsqrt_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::select.Dimname(Tensor(a) self, Dimname dim, int index) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor select(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("int64_t") long index);

// aten::select.int(Tensor(a) self, int dim, int index) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor select(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long index);

// aten::select_backward(Tensor grad_output, int[] input_sizes, int dim, int index) -> Tensor
@Namespace("at") public static native @ByVal Tensor select_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long index);
@Namespace("at") public static native @ByVal Tensor select_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long index);

// aten::selu(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor selu(@Const @ByRef Tensor self);

// aten::selu_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor selu_(@ByRef Tensor self);

// aten::celu(Tensor self, Scalar alpha=1.0) -> Tensor
@Namespace("at") public static native @ByVal Tensor celu(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(1.0)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor celu(@Const @ByRef Tensor self);

// aten::celu_(Tensor(a!) self, Scalar alpha=1.0) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor celu_(@ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(1.0)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor celu_(@ByRef Tensor self);

// aten::silu(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor silu(@Const @ByRef Tensor self);

// aten::silu_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor silu_(@ByRef Tensor self);

// aten::silu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor silu_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::silu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor silu_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::silu_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor silu_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);

// aten::silu_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor silu_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByRef Tensor grad_input);

// aten::silu_backward(Tensor grad_output, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor silu_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);

// aten::mish(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor mish(@Const @ByRef Tensor self);

// aten::mish_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mish_(@ByRef Tensor self);

// aten::mish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mish_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::mish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mish_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::mish_backward(Tensor grad_output, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor mish_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);

// aten::sigmoid(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor sigmoid(@Const @ByRef Tensor self);

// aten::sigmoid_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sigmoid_(@ByRef Tensor self);

// aten::sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sigmoid_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sigmoid_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::logit(Tensor self, float? eps=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor logit(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByVal Tensor logit(@Const @ByRef Tensor self);

// aten::logit_(Tensor(a!) self, float? eps=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logit_(@ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByRef Tensor logit_(@ByRef Tensor self);

// aten::logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logit_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByRef Tensor logit_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logit_outf(@Const @ByRef Tensor self, @ByVal DoubleOptional eps, @ByRef Tensor out);

// aten::sin(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor sin(@Const @ByRef Tensor self);

// aten::sin_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sin_(@ByRef Tensor self);

// aten::sin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sin_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::sin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sin_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::sinc(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor sinc(@Const @ByRef Tensor self);

// aten::sinc_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sinc_(@ByRef Tensor self);

// aten::sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sinc_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sinc_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::sinh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor sinh(@Const @ByRef Tensor self);

// aten::sinh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sinh_(@ByRef Tensor self);

// aten::sinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sinh_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::sinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sinh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::detach(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor detach(@Const @ByRef Tensor self);

// aten::detach_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor detach_(@ByRef Tensor self);

// aten::size.int(Tensor self, int dim) -> int
@Namespace("at") public static native @Cast("int64_t") long __dispatch_size(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::size.Dimname(Tensor self, Dimname dim) -> int
@Namespace("at") public static native @Cast("int64_t") long size(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::slice.Tensor(Tensor(a) self, int dim=0, int? start=None, int? end=None, int step=1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor slice(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=0*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional start, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional end, @Cast("int64_t") long step/*=1*/);
@Namespace("at") public static native @ByVal Tensor slice(@Const @ByRef Tensor self);

// aten::slice_backward(Tensor grad_output, int[] input_sizes, int dim, int start, int end, int step) -> Tensor
@Namespace("at") public static native @ByVal Tensor slice_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long end, @Cast("int64_t") long step);
@Namespace("at") public static native @ByVal Tensor slice_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long end, @Cast("int64_t") long step);

// aten::slogdet(Tensor self) -> (Tensor sign, Tensor logabsdet)
@Namespace("at") public static native @ByVal TensorTensorTuple slogdet(@Const @ByRef Tensor self);

// aten::smm(Tensor self, Tensor mat2) -> Tensor
@Namespace("at") public static native @ByVal Tensor smm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat2);

// aten::softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor softmax(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor softmax(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
@Namespace("at") public static native @ByVal Tensor _softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float);

// aten::_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _softmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float);

// aten::_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _softmax_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float, @ByRef Tensor out);

// aten::_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _softmax_backward_data(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, @Const @ByRef Tensor self);

// aten::_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _softmax_backward_data_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, @Const @ByRef Tensor self);

// aten::_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _softmax_backward_data_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, @Const @ByRef Tensor self, @ByRef Tensor grad_input);

// aten::unsafe_split.Tensor(Tensor self, int split_size, int dim=0) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unsafe_split(@Const @ByRef Tensor self, @Cast("int64_t") long split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unsafe_split(@Const @ByRef Tensor self, @Cast("int64_t") long split_size);

// aten::split.Tensor(Tensor(a) self, int split_size, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split(@Const @ByRef Tensor self, @Cast("int64_t") long split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split(@Const @ByRef Tensor self, @Cast("int64_t") long split_size);

// aten::unsafe_split_with_sizes(Tensor self, int[] split_sizes, int dim=0) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unsafe_split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unsafe_split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef split_sizes);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unsafe_split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unsafe_split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... split_sizes);

// aten::split_with_sizes(Tensor(a) self, int[] split_sizes, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef split_sizes);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... split_sizes);

// aten::hsplit.int(Tensor(a) self, int sections) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector hsplit(@Const @ByRef Tensor self, @Cast("int64_t") long sections);

// aten::hsplit.array(Tensor(a) self, int[] indices) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector hsplit(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef indices);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector hsplit(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... indices);

// aten::vsplit.int(Tensor(a) self, int sections) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector vsplit(@Const @ByRef Tensor self, @Cast("int64_t") long sections);

// aten::vsplit.array(Tensor(a) self, int[] indices) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector vsplit(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef indices);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector vsplit(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... indices);

// aten::dsplit.int(Tensor(a) self, int sections) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector dsplit(@Const @ByRef Tensor self, @Cast("int64_t") long sections);

// aten::dsplit.array(Tensor(a) self, int[] indices) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector dsplit(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef indices);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector dsplit(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... indices);

// aten::squeeze(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor squeeze(@Const @ByRef Tensor self);

// aten::squeeze.dim(Tensor(a) self, int dim) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor squeeze(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::squeeze.dimname(Tensor(a) self, Dimname dim) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor squeeze(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::sspaddmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor sspaddmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor sspaddmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);

// aten::sspaddmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sspaddmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor sspaddmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);

// aten::sspaddmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sspaddmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef Scalar beta, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::stack(Tensor[] tensors, int dim=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor stack(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor stack(@ByVal TensorArrayRef tensors);

// aten::stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor stack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByRef Tensor stack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);

// aten::stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor stack_outf(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim, @ByRef Tensor out);

// aten::_stack(Tensor[] tensors, int dim=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor _stack(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor _stack(@ByVal TensorArrayRef tensors);

// aten::_stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _stack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByRef Tensor _stack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);

// aten::_stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _stack_outf(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim, @ByRef Tensor out);

// aten::hstack(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor hstack(@ByVal TensorArrayRef tensors);

// aten::hstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hstack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);

// aten::hstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hstack_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);

// aten::vstack(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor vstack(@ByVal TensorArrayRef tensors);

// aten::vstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor vstack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);

// aten::vstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor vstack_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);

// aten::dstack(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor dstack(@ByVal TensorArrayRef tensors);

// aten::dstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor dstack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);

// aten::dstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor dstack_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);

// aten::stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor stft(@Const @ByRef Tensor self, @Cast("int64_t") long n_fft, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional hop_length, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional win_length, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional window, @Cast("bool") boolean normalized/*=false*/, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional onesided, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional return_complex);
@Namespace("at") public static native @ByVal Tensor stft(@Const @ByRef Tensor self, @Cast("int64_t") long n_fft);

// aten::istft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, bool normalized=False, bool? onesided=None, int? length=None, bool return_complex=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor istft(@Const @ByRef Tensor self, @Cast("int64_t") long n_fft, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional hop_length, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional win_length, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional window, @Cast("bool") boolean center/*=true*/, @Cast("bool") boolean normalized/*=false*/, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional onesided, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional length, @Cast("bool") boolean return_complex/*=false*/);
@Namespace("at") public static native @ByVal Tensor istft(@Const @ByRef Tensor self, @Cast("int64_t") long n_fft);

// aten::stride.int(Tensor self, int dim) -> int
@Namespace("at") public static native @Cast("int64_t") long __dispatch_stride(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::stride.Dimname(Tensor self, Dimname dim) -> int
@Namespace("at") public static native @Cast("int64_t") long stride(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::sum(Tensor self, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self);

// aten::sum.dim_IntList(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::sum.dim_DimnameList(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);

// aten::sum.IntList_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::sum.IntList_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sum_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor sum_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::sum.DimnameList_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);

// aten::sum.DimnameList_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sum_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::nansum(Tensor self, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor nansum(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor nansum(@Const @ByRef Tensor self);

// aten::nansum.dim_IntList(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor nansum(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor nansum(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor nansum(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor nansum(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::nansum.IntList_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nansum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor nansum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor nansum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor nansum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::nansum.IntList_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nansum_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor nansum_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::sqrt(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor sqrt(@Const @ByRef Tensor self);

// aten::sqrt_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sqrt_(@ByRef Tensor self);

// aten::sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sqrt_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sqrt_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::square(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor square(@Const @ByRef Tensor self);

// aten::square_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor square_(@ByRef Tensor self);

// aten::square.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor square_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::square.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor square_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::std(Tensor self, bool unbiased=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @Cast("bool") boolean unbiased/*=true*/);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self);

// aten::std.dim(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::std.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @ByVal LongOptional correction);

// aten::std_mean(Tensor self, bool unbiased=True) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, @Cast("bool") boolean unbiased/*=true*/);
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self);

// aten::std_mean.dim(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::std_mean.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @ByVal LongOptional correction);

// aten::std_mean.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);

// aten::std_mean.correction_names(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @ByVal LongOptional correction);

// aten::std.out(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::std.out(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor std_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::std.correction_out(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @ByVal LongOptional correction);

// aten::std.correction_out(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::std.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);

// aten::std.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);

// aten::std.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::std.correction_names(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @ByVal LongOptional correction);

// aten::std.correction_names_out(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @ByVal LongOptional correction);

// aten::std.correction_names_out(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::prod(Tensor self, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self);

// aten::prod.dim_int(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::prod.int_out(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor prod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor prod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::prod.int_out(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor prod_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::prod.dim_Dimname(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::prod.Dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor prod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor prod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::prod.Dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor prod_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::t(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor t(@Const @ByRef Tensor self);

// aten::tan(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor tan(@Const @ByRef Tensor self);

// aten::tan_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tan_(@ByRef Tensor self);

// aten::tan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tan_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::tan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tan_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::tanh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor tanh(@Const @ByRef Tensor self);

// aten::tanh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tanh_(@ByRef Tensor self);

// aten::tanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tanh_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::tanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tanh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::tensordot(Tensor self, Tensor other, int[] dims_self, int[] dims_other) -> Tensor
@Namespace("at") public static native @ByVal Tensor tensordot(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims_self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims_other);
@Namespace("at") public static native @ByVal Tensor tensordot(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dims_self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims_other);

// aten::tensordot.out(Tensor self, Tensor other, int[] dims_self, int[] dims_other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tensordot_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims_self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims_other);
@Namespace("at") public static native @ByRef Tensor tensordot_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dims_self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims_other);

// aten::tensordot.out(Tensor self, Tensor other, int[] dims_self, int[] dims_other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tensordot_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims_self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims_other, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor tensordot_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dims_self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dims_other, @ByRef Tensor out);

// aten::threshold(Tensor self, Scalar threshold, Scalar value) -> Tensor
@Namespace("at") public static native @ByVal Tensor threshold(@Const @ByRef Tensor self, @Const @ByRef Scalar threshold, @Const @ByRef Scalar value);

// aten::threshold_(Tensor(a!) self, Scalar threshold, Scalar value) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor threshold_(@ByRef Tensor self, @Const @ByRef Scalar threshold, @Const @ByRef Scalar value);

// aten::threshold.out(Tensor self, Scalar threshold, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor threshold_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar threshold, @Const @ByRef Scalar value);

// aten::threshold.out(Tensor self, Scalar threshold, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor threshold_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar threshold, @Const @ByRef Scalar value, @ByRef Tensor out);

// aten::threshold_backward.grad_input(Tensor grad_output, Tensor self, Scalar threshold, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor threshold_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar threshold);

// aten::threshold_backward.grad_input(Tensor grad_output, Tensor self, Scalar threshold, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor threshold_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar threshold, @ByRef Tensor grad_input);

// aten::threshold_backward(Tensor grad_output, Tensor self, Scalar threshold) -> Tensor
@Namespace("at") public static native @ByVal Tensor threshold_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar threshold);

// aten::tile(Tensor self, int[] dims) -> Tensor
@Namespace("at") public static native @ByVal Tensor tile(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("at") public static native @ByVal Tensor tile(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);

// aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor transpose(@Const @ByRef Tensor self, @Cast("int64_t") long dim0, @Cast("int64_t") long dim1);

// aten::transpose.Dimname(Tensor(a) self, Dimname dim0, Dimname dim1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor transpose(@Const @ByRef Tensor self, @ByVal Dimname dim0, @ByVal Dimname dim1);

// aten::_mkldnn_transpose(Tensor self, int dim0, int dim1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _mkldnn_transpose(@Const @ByRef Tensor self, @Cast("int64_t") long dim0, @Cast("int64_t") long dim1);

// aten::_mkldnn_transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _mkldnn_transpose_(@ByRef Tensor self, @Cast("int64_t") long dim0, @Cast("int64_t") long dim1);

// aten::one_hot(Tensor self, int num_classes=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor one_hot(@Const @ByRef Tensor self, @Cast("int64_t") long num_classes/*=-1*/);
@Namespace("at") public static native @ByVal Tensor one_hot(@Const @ByRef Tensor self);

// aten::flip(Tensor self, int[] dims) -> Tensor
@Namespace("at") public static native @ByVal Tensor flip(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("at") public static native @ByVal Tensor flip(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);

// aten::fliplr(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor fliplr(@Const @ByRef Tensor self);

// aten::flipud(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor flipud(@Const @ByRef Tensor self);

// aten::roll(Tensor self, int[1] shifts, int[1] dims=[]) -> Tensor
@Namespace("at") public static native @ByVal Tensor roll(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shifts, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("at") public static native @ByVal Tensor roll(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shifts);
@Namespace("at") public static native @ByVal Tensor roll(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] shifts, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);
@Namespace("at") public static native @ByVal Tensor roll(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... shifts);

// aten::rot90(Tensor self, int k=1, int[] dims=[0,1]) -> Tensor
@Namespace("at") public static native @ByVal Tensor rot90(@Const @ByRef Tensor self, @Cast("int64_t") long k/*=1*/, @ByVal(nullValue = "at::IntArrayRef({0,1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("at") public static native @ByVal Tensor rot90(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor rot90(@Const @ByRef Tensor self, @Cast("int64_t") long k/*=1*/, @ByVal(nullValue = "at::IntArrayRef({0,1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);

// aten::trapezoid.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor trapezoid(@Const @ByRef Tensor y, @Const @ByRef Tensor x, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor trapezoid(@Const @ByRef Tensor y, @Const @ByRef Tensor x);

// aten::trapezoid.dx(Tensor y, *, Scalar dx=1, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor trapezoid(@Const @ByRef Tensor y, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar dx, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor trapezoid(@Const @ByRef Tensor y);

// aten::trapz.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor trapz(@Const @ByRef Tensor y, @Const @ByRef Tensor x, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor trapz(@Const @ByRef Tensor y, @Const @ByRef Tensor x);

// aten::trapz.dx(Tensor y, *, float dx=1, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor trapz(@Const @ByRef Tensor y, double dx/*=1*/, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor trapz(@Const @ByRef Tensor y);

// aten::_trilinear(Tensor i1, Tensor i2, Tensor i3, int[] expand1, int[] expand2, int[] expand3, int[] sumdim, int unroll_dim=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _trilinear(@Const @ByRef Tensor i1, @Const @ByRef Tensor i2, @Const @ByRef Tensor i3, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand1, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand2, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand3, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sumdim, @Cast("int64_t") long unroll_dim/*=1*/);
@Namespace("at") public static native @ByVal Tensor _trilinear(@Const @ByRef Tensor i1, @Const @ByRef Tensor i2, @Const @ByRef Tensor i3, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand1, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand2, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand3, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sumdim);
@Namespace("at") public static native @ByVal Tensor _trilinear(@Const @ByRef Tensor i1, @Const @ByRef Tensor i2, @Const @ByRef Tensor i3, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] expand1, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] expand2, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] expand3, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sumdim, @Cast("int64_t") long unroll_dim/*=1*/);
@Namespace("at") public static native @ByVal Tensor _trilinear(@Const @ByRef Tensor i1, @Const @ByRef Tensor i2, @Const @ByRef Tensor i3, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] expand1, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] expand2, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] expand3, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sumdim);

// aten::triplet_margin_loss(Tensor anchor, Tensor positive, Tensor negative, float margin=1.0, float p=2, float eps=1e-06, bool swap=False, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor triplet_margin_loss(@Const @ByRef Tensor anchor, @Const @ByRef Tensor positive, @Const @ByRef Tensor negative, double margin/*=1.0*/, double p/*=2*/, double eps/*=1e-06*/, @Cast("bool") boolean swap/*=false*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor triplet_margin_loss(@Const @ByRef Tensor anchor, @Const @ByRef Tensor positive, @Const @ByRef Tensor negative);

// aten::trunc(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor trunc(@Const @ByRef Tensor self);

// aten::trunc_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor trunc_(@ByRef Tensor self);

// aten::trunc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor trunc_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::trunc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor trunc_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::fix(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor fix(@Const @ByRef Tensor self);

// aten::fix_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fix_(@ByRef Tensor self);

// aten::fix.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fix_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::fix.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fix_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool
@Namespace("at") public static native @Cast("bool") boolean _has_compatible_shallow_copy_type(@Const @ByRef Tensor self, @Const @ByRef Tensor from);

// aten::_unique(Tensor self, bool sorted=True, bool return_inverse=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _unique(@Const @ByRef Tensor self, @Cast("bool") boolean sorted/*=true*/, @Cast("bool") boolean return_inverse/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple _unique(@Const @ByRef Tensor self);

// aten::unique_dim(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple unique_dim(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean sorted/*=true*/, @Cast("bool") boolean return_inverse/*=false*/, @Cast("bool") boolean return_counts/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple unique_dim(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::unique_consecutive(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple unique_consecutive(@Const @ByRef Tensor self, @Cast("bool") boolean return_inverse/*=false*/, @Cast("bool") boolean return_counts/*=false*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple unique_consecutive(@Const @ByRef Tensor self);

// aten::unique_dim_consecutive(Tensor self, int dim, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple unique_dim_consecutive(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean return_inverse/*=false*/, @Cast("bool") boolean return_counts/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple unique_dim_consecutive(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::_unique2(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _unique2(@Const @ByRef Tensor self, @Cast("bool") boolean sorted/*=true*/, @Cast("bool") boolean return_inverse/*=false*/, @Cast("bool") boolean return_counts/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _unique2(@Const @ByRef Tensor self);

// aten::_unsafe_view(Tensor self, int[] size) -> Tensor
@Namespace("at") public static native @ByVal Tensor _unsafe_view(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor _unsafe_view(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::unsqueeze(Tensor(a) self, int dim) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor unsqueeze(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::vander(Tensor x, int? N=None, bool increasing=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor vander(@Const @ByRef Tensor x, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional N, @Cast("bool") boolean increasing/*=false*/);
@Namespace("at") public static native @ByVal Tensor vander(@Const @ByRef Tensor x);

// aten::var(Tensor self, bool unbiased=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @Cast("bool") boolean unbiased/*=true*/);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self);

// aten::var.dim(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::var.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @ByVal LongOptional correction);

// aten::var.out(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::var.out(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor var_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::var.correction_out(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @ByVal LongOptional correction);

// aten::var.correction_out(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::var.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);

// aten::var.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);

// aten::var.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::var.correction_names(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @ByVal LongOptional correction);

// aten::var.correction_names_out(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @ByVal LongOptional correction);

// aten::var.correction_names_out(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::var_mean(Tensor self, bool unbiased=True) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, @Cast("bool") boolean unbiased/*=true*/);
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self);

// aten::var_mean.dim(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::var_mean.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @ByVal LongOptional correction);

// aten::var_mean.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);

// aten::var_mean.correction_names(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @ByVal LongOptional correction);

// aten::where.self(Tensor condition, Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor where(@Const @ByRef Tensor condition, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::where.ScalarSelf(Tensor condition, Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor where(@Const @ByRef Tensor condition, @Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::where.ScalarOther(Tensor condition, Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor where(@Const @ByRef Tensor condition, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::where.Scalar(Tensor condition, Scalar self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor where(@Const @ByRef Tensor condition, @Const @ByRef Scalar self, @Const @ByRef Scalar other);

// aten::where(Tensor condition) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector where(@Const @ByRef Tensor condition);

// aten::_s_where(Tensor condition, Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor _s_where(@Const @ByRef Tensor condition, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::norm_except_dim(Tensor v, int pow=2, int dim=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor norm_except_dim(@Const @ByRef Tensor v, @Cast("int64_t") long pow/*=2*/, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor norm_except_dim(@Const @ByRef Tensor v);

// aten::_weight_norm(Tensor v, Tensor g, int dim=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor _weight_norm(@Const @ByRef Tensor v, @Const @ByRef Tensor g, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor _weight_norm(@Const @ByRef Tensor v, @Const @ByRef Tensor g);

// aten::_weight_norm_cuda_interface(Tensor v, Tensor g, int dim=0) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _weight_norm_cuda_interface(@Const @ByRef Tensor v, @Const @ByRef Tensor g, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal TensorTensorTuple _weight_norm_cuda_interface(@Const @ByRef Tensor v, @Const @ByRef Tensor g);

// aten::_weight_norm_cuda_interface_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _weight_norm_cuda_interface_backward(@Const @ByRef Tensor grad_w, @Const @ByRef Tensor saved_v, @Const @ByRef Tensor saved_g, @Const @ByRef Tensor saved_norms, @Cast("int64_t") long dim);

// aten::_weight_norm_differentiable_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _weight_norm_differentiable_backward(@Const @ByRef Tensor grad_w, @Const @ByRef Tensor saved_v, @Const @ByRef Tensor saved_g, @Const @ByRef Tensor saved_norms, @Cast("int64_t") long dim);

// aten::zeros.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);

// aten::zeros.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::zeros(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::zeros(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::zeros.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zeros_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor zeros_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::zeros.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zeros_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor zeros_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);

// aten::zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor zeros_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor zeros_like(@Const @ByRef Tensor self);

// aten::zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor zeros_like(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::_standard_gamma_grad(Tensor self, Tensor output) -> Tensor
@Namespace("at") public static native @ByVal Tensor _standard_gamma_grad(@Const @ByRef Tensor self, @Const @ByRef Tensor output);

// aten::_standard_gamma(Tensor self, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _standard_gamma(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor _standard_gamma(@Const @ByRef Tensor self);

// aten::_dirichlet_grad(Tensor x, Tensor alpha, Tensor total) -> Tensor
@Namespace("at") public static native @ByVal Tensor _dirichlet_grad(@Const @ByRef Tensor x, @Const @ByRef Tensor alpha, @Const @ByRef Tensor total);

// aten::_sample_dirichlet(Tensor self, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sample_dirichlet(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor _sample_dirichlet(@Const @ByRef Tensor self);

// aten::poisson(Tensor self, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor poisson(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor poisson(@Const @ByRef Tensor self);

// aten::binomial(Tensor count, Tensor prob, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor binomial(@Const @ByRef Tensor count, @Const @ByRef Tensor prob, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor binomial(@Const @ByRef Tensor count, @Const @ByRef Tensor prob);

// aten::native_norm(Tensor self, Scalar p=2) -> Tensor
@Namespace("at") public static native @ByVal Tensor native_norm(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar p);
@Namespace("at") public static native @ByVal Tensor native_norm(@Const @ByRef Tensor self);

// aten::native_norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, ScalarType? dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor native_norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor native_norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype);

// aten::_sparse_sum(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_sum(@Const @ByRef Tensor self);

// aten::_sparse_sum.dtype(Tensor self, *, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_sum(@Const @ByRef Tensor self, ScalarType dtype);

// aten::_sparse_sum.dim(Tensor self, int[1] dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_sum(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor _sparse_sum(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::_sparse_sum.dim_dtype(Tensor self, int[1] dim, *, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_sum(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, ScalarType dtype);
@Namespace("at") public static native @ByVal Tensor _sparse_sum(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, ScalarType dtype);

// aten::_sparse_sum_backward(Tensor grad, Tensor self, int[] dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_sum_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor _sparse_sum_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::_sparse_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor _sparse_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::_sparse_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_softmax(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor _sparse_softmax(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::_sparse_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float);

// aten::_sparse_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_softmax_backward_data(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, @Const @ByRef Tensor self);

// aten::_sparse_log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor _sparse_log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::_sparse_log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_log_softmax(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor _sparse_log_softmax(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::_sparse_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float);

// aten::_sparse_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_log_softmax_backward_data(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, @Const @ByRef Tensor self);

// aten::norm.ScalarOpt_dtype(Tensor self, Scalar? p, *, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, ScalarType dtype);

// aten::norm.Scalar(Tensor self, Scalar p=2) -> Tensor
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar p);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self);

// aten::norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, ScalarType dtype);

// aten::norm.ScalarOpt_dim(Tensor self, Scalar? p, int[1] dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::norm.dtype_out(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, ScalarType dtype);

// aten::norm.dtype_out(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, ScalarType dtype, @ByRef Tensor out);

// aten::norm.out(Tensor self, Scalar? p, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::norm.out(Tensor self, Scalar? p, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::norm.names_ScalarOpt_dim_dtype(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype);

// aten::norm.names_ScalarOpt_dim(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim);

// aten::norm.names_dtype_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype);

// aten::norm.names_dtype_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype, @ByRef Tensor out);

// aten::norm.names_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim);

// aten::norm.names_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::frexp.Tensor(Tensor self) -> (Tensor mantissa, Tensor exponent)
@Namespace("at") public static native @ByVal TensorTensorTuple frexp(@Const @ByRef Tensor self);

// aten::frexp.Tensor_out(Tensor self, *, Tensor(a!) mantissa, Tensor(b!) exponent) -> (Tensor(a!) mantissa, Tensor(b!) exponent)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> frexp_out(@ByRef Tensor mantissa, @ByRef Tensor exponent, @Const @ByRef Tensor self);

// aten::frexp.Tensor_out(Tensor self, *, Tensor(a!) mantissa, Tensor(b!) exponent) -> (Tensor(a!) mantissa, Tensor(b!) exponent)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> frexp_outf(@Const @ByRef Tensor self, @ByRef Tensor mantissa, @ByRef Tensor exponent);

// aten::frobenius_norm(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor frobenius_norm(@Const @ByRef Tensor self);

// aten::frobenius_norm.dim(Tensor self, int[1] dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor frobenius_norm(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor frobenius_norm(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor frobenius_norm(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor frobenius_norm(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::frobenius_norm.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor frobenius_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor frobenius_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor frobenius_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor frobenius_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::frobenius_norm.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor frobenius_norm_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor frobenius_norm_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::nuclear_norm(Tensor self, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self);

// aten::nuclear_norm.out(Tensor self, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::nuclear_norm.out(Tensor self, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nuclear_norm_outf(@Const @ByRef Tensor self, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::nuclear_norm.dim(Tensor self, int[2] dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::nuclear_norm.dim_out(Tensor self, int[2] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::nuclear_norm.dim_out(Tensor self, int[2] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nuclear_norm_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor clone(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor clone(@Const @ByRef Tensor self);

// aten::positive(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor positive(@Const @ByRef Tensor self);

// aten::resize_as_(Tensor(a!) self, Tensor the_template, *, MemoryFormat? memory_format=None) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor resize_as_(@Const @ByRef Tensor self, @Const @ByRef Tensor the_template, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @Const @ByRef Tensor resize_as_(@Const @ByRef Tensor self, @Const @ByRef Tensor the_template);

// aten::resize_as_sparse_(Tensor(a!) self, Tensor the_template) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor resize_as_sparse_(@Const @ByRef Tensor self, @Const @ByRef Tensor the_template);

// aten::zero_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zero_(@ByRef Tensor self);

// aten::sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sub_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor sub_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sub_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor sub(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor sub(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor sub(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor sub(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::subtract.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor subtract_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor subtract_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::subtract.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor subtract_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::subtract.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor subtract(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor subtract(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::subtract.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor subtract(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor subtract(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::rsub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor rsub(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor rsub(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::heaviside.out(Tensor self, Tensor values, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor heaviside_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor values);

// aten::heaviside.out(Tensor self, Tensor values, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor heaviside_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor values, @ByRef Tensor out);

// aten::heaviside(Tensor self, Tensor values) -> Tensor
@Namespace("at") public static native @ByVal Tensor heaviside(@Const @ByRef Tensor self, @Const @ByRef Tensor values);

// aten::rsub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor rsub(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor rsub(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::_sparse_addmm(Tensor self, Tensor sparse, Tensor dense, *, Scalar beta=1, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_addmm(@Const @ByRef Tensor self, @Const @ByRef Tensor sparse, @Const @ByRef Tensor dense, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor _sparse_addmm(@Const @ByRef Tensor self, @Const @ByRef Tensor sparse, @Const @ByRef Tensor dense);

// aten::addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor addmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);

// aten::addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef Scalar beta, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor addmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor addmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);

// aten::sparse_csr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal TensorOptions options);

// aten::sparse_csr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::sparse_csr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);

// aten::sparse_csr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::_sparse_csr_tensor_unsafe(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_csr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_csr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor _sparse_csr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_csr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::_sparse_csr_tensor_unsafe(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_csr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor _sparse_csr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::sparse_coo_tensor.size(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal TensorOptions options);

// aten::sparse_coo_tensor.size(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::sparse_coo_tensor.indices(Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values);

// aten::sparse_coo_tensor.indices(Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::sparse_coo_tensor.indices_size(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::sparse_coo_tensor.indices_size(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::_sparse_coo_tensor_unsafe(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::_sparse_coo_tensor_unsafe(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::_validate_sparse_coo_tensor_args(Tensor indices, Tensor values, int[] size) -> ()
@Namespace("at") public static native void _validate_sparse_coo_tensor_args(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native void _validate_sparse_coo_tensor_args(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::_validate_sparse_csr_tensor_args(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size) -> ()
@Namespace("at") public static native void _validate_sparse_csr_tensor_args(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native void _validate_sparse_csr_tensor_args(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::_sparse_coo_tensor_with_dims(int sparse_dim, int dense_dim, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal TensorOptions options);

// aten::_sparse_coo_tensor_with_dims(int sparse_dim, int dense_dim, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::_sparse_coo_tensor_with_dims_and_tensors(int sparse_dim, int dense_dim, int[] size, Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims_and_tensors(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims_and_tensors(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);

// aten::_sparse_coo_tensor_with_dims_and_tensors(int sparse_dim, int dense_dim, int[] size, Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims_and_tensors(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims_and_tensors(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::_to_cpu(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _to_cpu(@ByVal TensorArrayRef tensors);

// aten::to_dense_backward(Tensor grad, Tensor input) -> Tensor
@Namespace("at") public static native @ByVal Tensor to_dense_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor input);

// aten::_coalesce(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _coalesce(@Const @ByRef Tensor self);

// aten::hspmm.out(Tensor mat1, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hspmm_out(@ByRef Tensor out, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);

// aten::hspmm.out(Tensor mat1, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hspmm_outf(@Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @ByRef Tensor out);

// aten::hspmm(Tensor mat1, Tensor mat2) -> Tensor
@Namespace("at") public static native @ByVal Tensor hspmm(@Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);

// aten::copy_sparse_to_sparse_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor copy_sparse_to_sparse_(@ByRef Tensor self, @Const @ByRef Tensor src, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByRef Tensor copy_sparse_to_sparse_(@ByRef Tensor self, @Const @ByRef Tensor src);

// aten::unbind.int(Tensor(a) self, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unbind(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unbind(@Const @ByRef Tensor self);

// aten::unbind.Dimname(Tensor(a) self, Dimname dim) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unbind(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::mkldnn_reorder_conv2d_weight(Tensor self, int[2] padding=0, int[2] stride=1, int[2] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv2d_weight(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv2d_weight(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv2d_weight(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups/*=1*/);

// aten::mkldnn_reorder_conv3d_weight(Tensor self, int[3] padding=0, int[3] stride=1, int[3] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv3d_weight(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv3d_weight(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv3d_weight(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups/*=1*/);

// aten::to_mkldnn_backward(Tensor grad, Tensor input) -> Tensor
@Namespace("at") public static native @ByVal Tensor to_mkldnn_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor input);

// aten::quantize_per_tensor(Tensor self, float scale, int zero_point, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantize_per_tensor(@Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point, ScalarType dtype);

// aten::quantize_per_tensor.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantize_per_tensor(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, ScalarType dtype);

// aten::quantize_per_tensor.tensors(Tensor[] tensors, Tensor scales, Tensor zero_points, ScalarType dtype) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector quantize_per_tensor(@ByVal TensorArrayRef tensors, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, ScalarType dtype);

// aten::quantize_per_channel(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantize_per_channel(@Const @ByRef Tensor self, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, ScalarType dtype);

// aten::dequantize.self(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor dequantize(@Const @ByRef Tensor self);

// aten::dequantize.tensors(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector dequantize(@ByVal TensorArrayRef tensors);

// aten::q_scale(Tensor self) -> float
@Namespace("at") public static native double q_scale(@Const @ByRef Tensor self);

// aten::q_zero_point(Tensor self) -> int
@Namespace("at") public static native @Cast("int64_t") long q_zero_point(@Const @ByRef Tensor self);

// aten::q_per_channel_scales(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor q_per_channel_scales(@Const @ByRef Tensor self);

// aten::q_per_channel_zero_points(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor q_per_channel_zero_points(@Const @ByRef Tensor self);

// aten::q_per_channel_axis(Tensor self) -> int
@Namespace("at") public static native @Cast("int64_t") long q_per_channel_axis(@Const @ByRef Tensor self);

// aten::int_repr(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor int_repr(@Const @ByRef Tensor self);

// aten::_make_per_tensor_quantized_tensor(Tensor self, float scale, int zero_point) -> Tensor
@Namespace("at") public static native @ByVal Tensor _make_per_tensor_quantized_tensor(@Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point);

// aten::_make_per_channel_quantized_tensor(Tensor self, Tensor scale, Tensor zero_point, int axis) -> Tensor
@Namespace("at") public static native @ByVal Tensor _make_per_channel_quantized_tensor(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis);

// aten::fake_quantize_per_tensor_affine(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -> Tensor
@Namespace("at") public static native @ByVal Tensor fake_quantize_per_tensor_affine(@Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);

// aten::fake_quantize_per_tensor_affine.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max) -> Tensor
@Namespace("at") public static native @ByVal Tensor fake_quantize_per_tensor_affine(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);

// aten::fake_quantize_per_tensor_affine_cachemask(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -> (Tensor output, Tensor mask)
@Namespace("at") public static native @ByVal TensorTensorTuple fake_quantize_per_tensor_affine_cachemask(@Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);

// aten::_fake_quantize_per_tensor_affine_cachemask_tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, Tensor fake_quant_enabled, int quant_min, int quant_max) -> (Tensor output, Tensor mask)
@Namespace("at") public static native @ByVal TensorTensorTuple _fake_quantize_per_tensor_affine_cachemask_tensor_qparams(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Const @ByRef Tensor fake_quant_enabled, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);

// aten::fake_quantize_per_tensor_affine_cachemask_backward(Tensor grad, Tensor mask) -> Tensor
@Namespace("at") public static native @ByVal Tensor fake_quantize_per_tensor_affine_cachemask_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor mask);

// aten::_fake_quantize_learnable_per_tensor_affine(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.0) -> Tensor
@Namespace("at") public static native @ByVal Tensor _fake_quantize_learnable_per_tensor_affine(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, double grad_factor/*=1.0*/);
@Namespace("at") public static native @ByVal Tensor _fake_quantize_learnable_per_tensor_affine(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);

// aten::_fake_quantize_learnable_per_tensor_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.0) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _fake_quantize_learnable_per_tensor_affine_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, double grad_factor/*=1.0*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _fake_quantize_learnable_per_tensor_affine_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);

// aten::fake_quantize_per_channel_affine(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max) -> Tensor
@Namespace("at") public static native @ByVal Tensor fake_quantize_per_channel_affine(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);

// aten::fake_quantize_per_channel_affine_cachemask(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max) -> (Tensor output, Tensor mask)
@Namespace("at") public static native @ByVal TensorTensorTuple fake_quantize_per_channel_affine_cachemask(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);

// aten::fake_quantize_per_channel_affine_cachemask_backward(Tensor grad, Tensor mask) -> Tensor
@Namespace("at") public static native @ByVal Tensor fake_quantize_per_channel_affine_cachemask_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor mask);

// aten::_fake_quantize_learnable_per_channel_affine(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -> Tensor
@Namespace("at") public static native @ByVal Tensor _fake_quantize_learnable_per_channel_affine(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, double grad_factor/*=1.0*/);
@Namespace("at") public static native @ByVal Tensor _fake_quantize_learnable_per_channel_affine(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);

// aten::_fake_quantize_learnable_per_channel_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _fake_quantize_learnable_per_channel_affine_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, double grad_factor/*=1.0*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _fake_quantize_learnable_per_channel_affine_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);

// aten::fused_moving_avg_obs_fake_quant(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor fused_moving_avg_obs_fake_quant(@Const @ByRef Tensor self, @Const @ByRef Tensor observer_on, @Const @ByRef Tensor fake_quant_on, @ByRef Tensor running_min, @ByRef Tensor running_max, @ByRef Tensor scale, @ByRef Tensor zero_point, double averaging_const, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, @Cast("int64_t") long ch_axis, @Cast("bool") boolean per_row_fake_quant/*=false*/, @Cast("bool") boolean symmetric_quant/*=false*/);
@Namespace("at") public static native @ByVal Tensor fused_moving_avg_obs_fake_quant(@Const @ByRef Tensor self, @Const @ByRef Tensor observer_on, @Const @ByRef Tensor fake_quant_on, @ByRef Tensor running_min, @ByRef Tensor running_max, @ByRef Tensor scale, @ByRef Tensor zero_point, double averaging_const, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, @Cast("int64_t") long ch_axis);

// aten::_fused_moving_avg_obs_fq_helper(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -> (Tensor output, Tensor mask)
@Namespace("at") public static native @ByVal TensorTensorTuple _fused_moving_avg_obs_fq_helper(@Const @ByRef Tensor self, @Const @ByRef Tensor observer_on, @Const @ByRef Tensor fake_quant_on, @ByRef Tensor running_min, @ByRef Tensor running_max, @ByRef Tensor scale, @ByRef Tensor zero_point, double averaging_const, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, @Cast("int64_t") long ch_axis, @Cast("bool") boolean per_row_fake_quant/*=false*/, @Cast("bool") boolean symmetric_quant/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple _fused_moving_avg_obs_fq_helper(@Const @ByRef Tensor self, @Const @ByRef Tensor observer_on, @Const @ByRef Tensor fake_quant_on, @ByRef Tensor running_min, @ByRef Tensor running_max, @ByRef Tensor scale, @ByRef Tensor zero_point, double averaging_const, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, @Cast("int64_t") long ch_axis);

// aten::_choose_qparams_per_tensor(Tensor self, bool reduce_range=False) -> (float, int)
@Namespace("at") public static native @ByVal @Cast("std::tuple<double,int64_t>*") LongPointer _choose_qparams_per_tensor(@Const @ByRef Tensor self, @Cast("bool") boolean reduce_range/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<double,int64_t>*") LongPointer _choose_qparams_per_tensor(@Const @ByRef Tensor self);

// aten::_saturate_weight_to_fp16(Tensor weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor _saturate_weight_to_fp16(@Const @ByRef Tensor weight);

// aten::choose_qparams_optimized(Tensor input, int numel, int n_bins, float ratio, int bit_width) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple choose_qparams_optimized(@Const @ByRef Tensor input, @Cast("int64_t") long numel, @Cast("int64_t") long n_bins, double ratio, @Cast("int64_t") long bit_width);

// aten::_to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _to_copy(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @Cast("bool") boolean non_blocking/*=false*/, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor _to_copy(@Const @ByRef Tensor self);

// aten::_to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _to_copy(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @Cast("bool") boolean non_blocking, @ByVal MemoryFormatOptional memory_format);

// aten::meshgrid(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector meshgrid(@ByVal TensorArrayRef tensors);

// aten::meshgrid.indexing(Tensor[] tensors, *, str indexing) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector meshgrid(@ByVal TensorArrayRef tensors, @ByVal @Cast("c10::string_view*") Pointer indexing);

// aten::cartesian_prod(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor cartesian_prod(@ByVal TensorArrayRef tensors);

// aten::combinations(Tensor self, int r=2, bool with_replacement=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor combinations(@Const @ByRef Tensor self, @Cast("int64_t") long r/*=2*/, @Cast("bool") boolean with_replacement/*=false*/);
@Namespace("at") public static native @ByVal Tensor combinations(@Const @ByRef Tensor self);

// aten::result_type.Tensor(Tensor tensor, Tensor other) -> ScalarType
@Namespace("at") public static native ScalarType result_type(@Const @ByRef Tensor tensor, @Const @ByRef Tensor other);

// aten::result_type.Scalar(Tensor tensor, Scalar other) -> ScalarType
@Namespace("at") public static native ScalarType result_type(@Const @ByRef Tensor tensor, @Const @ByRef Scalar other);

// aten::result_type.Scalar_Tensor(Scalar scalar, Tensor tensor) -> ScalarType
@Namespace("at") public static native ScalarType result_type(@Const @ByRef Scalar scalar, @Const @ByRef Tensor tensor);

// aten::result_type.Scalar_Scalar(Scalar scalar1, Scalar scalar2) -> ScalarType
@Namespace("at") public static native ScalarType result_type(@Const @ByRef Scalar scalar1, @Const @ByRef Scalar scalar2);

// aten::can_cast(ScalarType from, ScalarType to) -> bool
@Namespace("at") public static native @Cast("bool") boolean can_cast(ScalarType from, ScalarType to);

// aten::promote_types(ScalarType type1, ScalarType type2) -> ScalarType
@Namespace("at") public static native ScalarType promote_types(ScalarType type1, ScalarType type2);

// aten::_local_scalar_dense(Tensor self) -> Scalar
@Namespace("at") public static native @ByVal Scalar _local_scalar_dense(@Const @ByRef Tensor self);

// aten::_thnn_fused_lstm_cell(Tensor input_gates, Tensor hidden_gates, Tensor cx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _thnn_fused_lstm_cell(@Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef Tensor cx, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional input_bias, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional hidden_bias);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _thnn_fused_lstm_cell(@Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef Tensor cx);

// aten::_thnn_fused_lstm_cell_backward(Tensor? grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, Tensor workspace, bool has_bias) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTensorTuple _thnn_fused_lstm_cell_backward(@Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Const @ByRef Tensor cx, @Const @ByRef Tensor cy, @Const @ByRef Tensor workspace, @Cast("bool") boolean has_bias);

// aten::_thnn_differentiable_lstm_cell_backward(Tensor? grad_hy, Tensor? grad_cy, Tensor input_gates, Tensor hidden_gates, Tensor? input_bias, Tensor? hidden_bias, Tensor cx, Tensor cy) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTensorTuple _thnn_differentiable_lstm_cell_backward(@Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef TensorOptional input_bias, @Const @ByRef TensorOptional hidden_bias, @Const @ByRef Tensor cx, @Const @ByRef Tensor cy);

// aten::_thnn_fused_gru_cell(Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _thnn_fused_gru_cell(@Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef Tensor hx, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional input_bias, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional hidden_bias);
@Namespace("at") public static native @ByVal TensorTensorTuple _thnn_fused_gru_cell(@Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef Tensor hx);

// aten::_thnn_fused_gru_cell_backward(Tensor grad_hy, Tensor workspace, bool has_bias) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTensorTuple _thnn_fused_gru_cell_backward(@Const @ByRef Tensor grad_hy, @Const @ByRef Tensor workspace, @Cast("bool") boolean has_bias);

// aten::_thnn_differentiable_gru_cell_backward(Tensor grad_hy, Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias, Tensor? hidden_bias) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTensorTuple _thnn_differentiable_gru_cell_backward(@Const @ByRef Tensor grad_hy, @Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional input_bias, @Const @ByRef TensorOptional hidden_bias);

// aten::lstm.input(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple lstm(@Const @ByRef Tensor input, @ByVal TensorArrayRef hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);

// aten::lstm.data(Tensor data, Tensor batch_sizes, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple lstm(@Const @ByRef Tensor data, @Const @ByRef Tensor batch_sizes, @ByVal TensorArrayRef hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional);

// aten::gru.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple gru(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);

// aten::gru.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple gru(@Const @ByRef Tensor data, @Const @ByRef Tensor batch_sizes, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional);

// aten::rnn_tanh.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple rnn_tanh(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);

// aten::rnn_tanh.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple rnn_tanh(@Const @ByRef Tensor data, @Const @ByRef Tensor batch_sizes, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional);

// aten::rnn_relu.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple rnn_relu(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);

// aten::rnn_relu.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple rnn_relu(@Const @ByRef Tensor data, @Const @ByRef Tensor batch_sizes, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional);

// aten::lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple lstm_cell(@Const @ByRef Tensor input, @ByVal TensorArrayRef hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_ih, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_hh);
@Namespace("at") public static native @ByVal TensorTensorTuple lstm_cell(@Const @ByRef Tensor input, @ByVal TensorArrayRef hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh);

// aten::gru_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor gru_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_ih, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_hh);
@Namespace("at") public static native @ByVal Tensor gru_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh);

// aten::rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rnn_tanh_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_ih, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_hh);
@Namespace("at") public static native @ByVal Tensor rnn_tanh_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh);

// aten::rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rnn_relu_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_ih, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_hh);
@Namespace("at") public static native @ByVal Tensor rnn_relu_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh);

// aten::quantized_lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple quantized_lstm_cell(@Const @ByRef Tensor input, @ByVal TensorArrayRef hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef Tensor b_ih, @Const @ByRef Tensor b_hh, @Const @ByRef Tensor packed_ih, @Const @ByRef Tensor packed_hh, @Const @ByRef Tensor col_offsets_ih, @Const @ByRef Tensor col_offsets_hh, @Const @ByRef Scalar scale_ih, @Const @ByRef Scalar scale_hh, @Const @ByRef Scalar zero_point_ih, @Const @ByRef Scalar zero_point_hh);

// aten::quantized_gru_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantized_gru_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef Tensor b_ih, @Const @ByRef Tensor b_hh, @Const @ByRef Tensor packed_ih, @Const @ByRef Tensor packed_hh, @Const @ByRef Tensor col_offsets_ih, @Const @ByRef Tensor col_offsets_hh, @Const @ByRef Scalar scale_ih, @Const @ByRef Scalar scale_hh, @Const @ByRef Scalar zero_point_ih, @Const @ByRef Scalar zero_point_hh);

// aten::quantized_rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantized_rnn_relu_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef Tensor b_ih, @Const @ByRef Tensor b_hh, @Const @ByRef Tensor packed_ih, @Const @ByRef Tensor packed_hh, @Const @ByRef Tensor col_offsets_ih, @Const @ByRef Tensor col_offsets_hh, @Const @ByRef Scalar scale_ih, @Const @ByRef Scalar scale_hh, @Const @ByRef Scalar zero_point_ih, @Const @ByRef Scalar zero_point_hh);

// aten::quantized_rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantized_rnn_tanh_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef Tensor b_ih, @Const @ByRef Tensor b_hh, @Const @ByRef Tensor packed_ih, @Const @ByRef Tensor packed_hh, @Const @ByRef Tensor col_offsets_ih, @Const @ByRef Tensor col_offsets_hh, @Const @ByRef Scalar scale_ih, @Const @ByRef Scalar scale_hh, @Const @ByRef Scalar zero_point_ih, @Const @ByRef Scalar zero_point_hh);

// aten::_pack_padded_sequence(Tensor input, Tensor lengths, bool batch_first) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _pack_padded_sequence(@Const @ByRef Tensor input, @Const @ByRef Tensor lengths, @Cast("bool") boolean batch_first);

// aten::_pack_padded_sequence_backward(Tensor grad, int[] input_size, Tensor batch_sizes, bool batch_first) -> Tensor
@Namespace("at") public static native @ByVal Tensor _pack_padded_sequence_backward(@Const @ByRef Tensor grad, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Const @ByRef Tensor batch_sizes, @Cast("bool") boolean batch_first);
@Namespace("at") public static native @ByVal Tensor _pack_padded_sequence_backward(@Const @ByRef Tensor grad, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Const @ByRef Tensor batch_sizes, @Cast("bool") boolean batch_first);

// aten::_pad_packed_sequence(Tensor data, Tensor batch_sizes, bool batch_first, Scalar padding_value, int total_length) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _pad_packed_sequence(@Const @ByRef Tensor data, @Const @ByRef Tensor batch_sizes, @Cast("bool") boolean batch_first, @Const @ByRef Scalar padding_value, @Cast("int64_t") long total_length);

// aten::masked_fill.Scalar(Tensor self, Tensor mask, Scalar value) -> Tensor
@Namespace("at") public static native @ByVal Tensor masked_fill(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @Const @ByRef Scalar value);

// aten::masked_fill.Tensor(Tensor self, Tensor mask, Tensor value) -> Tensor
@Namespace("at") public static native @ByVal Tensor masked_fill(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @Const @ByRef Tensor value);

// aten::masked_scatter(Tensor self, Tensor mask, Tensor source) -> Tensor
@Namespace("at") public static native @ByVal Tensor masked_scatter(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @Const @ByRef Tensor source);

// aten::put(Tensor self, Tensor index, Tensor source, bool accumulate=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor put(@Const @ByRef Tensor self, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @Cast("bool") boolean accumulate/*=false*/);
@Namespace("at") public static native @ByVal Tensor put(@Const @ByRef Tensor self, @Const @ByRef Tensor index, @Const @ByRef Tensor source);

// aten::index_add(Tensor self, int dim, Tensor index, Tensor source) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_add(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source);

// aten::index_add.alpha(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_add(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @Const @ByRef Scalar alpha);

// aten::index_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_add(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor index_add(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source);

// aten::index_fill.int_Scalar(Tensor self, int dim, Tensor index, Scalar value) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_fill(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value);

// aten::index_fill.int_Tensor(Tensor self, int dim, Tensor index, Tensor value) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_fill(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor value);

// aten::index_fill.Dimname_Scalar(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_fill(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value);

// aten::index_fill.Dimname_Tensor(Tensor self, Dimname dim, Tensor index, Tensor value) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_fill(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Tensor value);

// aten::scatter.src(Tensor self, int dim, Tensor index, Tensor src) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src);

// aten::scatter.src_out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src);

// aten::scatter.src_out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @ByRef Tensor out);

// aten::scatter.value(Tensor self, int dim, Tensor index, Scalar value) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value);

// aten::scatter.value_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value);

// aten::scatter.value_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value, @ByRef Tensor out);

// aten::scatter.reduce(Tensor self, int dim, Tensor index, Tensor src, *, str reduce) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @ByVal @Cast("c10::string_view*") Pointer reduce);

// aten::scatter.reduce_out(Tensor self, int dim, Tensor index, Tensor src, *, str reduce, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @ByVal @Cast("c10::string_view*") Pointer reduce);

// aten::scatter.reduce_out(Tensor self, int dim, Tensor index, Tensor src, *, str reduce, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @ByVal @Cast("c10::string_view*") Pointer reduce, @ByRef Tensor out);

// aten::scatter.value_reduce(Tensor self, int dim, Tensor index, Scalar value, *, str reduce) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value, @ByVal @Cast("c10::string_view*") Pointer reduce);

// aten::scatter.value_reduce_out(Tensor self, int dim, Tensor index, Scalar value, *, str reduce, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value, @ByVal @Cast("c10::string_view*") Pointer reduce);

// aten::scatter.value_reduce_out(Tensor self, int dim, Tensor index, Scalar value, *, str reduce, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value, @ByVal @Cast("c10::string_view*") Pointer reduce, @ByRef Tensor out);

// aten::scatter.dimname_src(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src);

// aten::scatter.dimname_value(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value);

// aten::scatter_add(Tensor self, int dim, Tensor index, Tensor src) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter_add(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src);

// aten::scatter_add.out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_add_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src);

// aten::scatter_add.out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_add_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @ByRef Tensor out);

// aten::scatter_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter_add(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src);

// aten::bitwise_and.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_and_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::bitwise_and.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_and_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::bitwise_and.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_and_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::bitwise_and.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_and_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::bitwise_and.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_and(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::bitwise_and.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_and(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::__and__.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __and__(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::__and__.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __and__(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::bitwise_or.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_or_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::bitwise_or.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_or_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::bitwise_or.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_or_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::bitwise_or.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_or_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::bitwise_or.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_or(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::bitwise_or.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_or(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::__or__.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __or__(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::__or__.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __or__(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::bitwise_xor.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_xor_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::bitwise_xor.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_xor_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::bitwise_xor.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_xor_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::bitwise_xor.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_xor_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::bitwise_xor.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_xor(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::bitwise_xor.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_xor(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::__xor__.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __xor__(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::__xor__.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __xor__(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::__lshift__.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __lshift__(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::__lshift__.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __lshift__(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::bitwise_left_shift.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_left_shift(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::bitwise_left_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_left_shift_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::bitwise_left_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_left_shift_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::bitwise_left_shift.Tensor_Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_left_shift(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::bitwise_left_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_left_shift_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::bitwise_left_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_left_shift_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::bitwise_left_shift.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_left_shift(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::__rshift__.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __rshift__(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::__rshift__.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __rshift__(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::bitwise_right_shift.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_right_shift(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::bitwise_right_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_right_shift_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::bitwise_right_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_right_shift_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::bitwise_right_shift.Tensor_Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_right_shift(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::bitwise_right_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_right_shift_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::bitwise_right_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_right_shift_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::bitwise_right_shift.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_right_shift(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::addbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addbmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor addbmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2);

// aten::addbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addbmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @Const @ByRef Scalar beta, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor addbmm(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor addbmm(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2);

// aten::diag.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diag_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByRef Tensor diag_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::diag.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diag_outf(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal, @ByRef Tensor out);

// aten::diag(Tensor self, int diagonal=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor diag(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByVal Tensor diag(@Const @ByRef Tensor self);

// aten::diag_backward(Tensor grad, int[] input_sizes, int diagonal) -> Tensor
@Namespace("at") public static native @ByVal Tensor diag_backward(@Const @ByRef Tensor grad, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_sizes, @Cast("int64_t") long diagonal);
@Namespace("at") public static native @ByVal Tensor diag_backward(@Const @ByRef Tensor grad, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_sizes, @Cast("int64_t") long diagonal);

// aten::cross.out(Tensor self, Tensor other, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cross_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByRef Tensor cross_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::cross.out(Tensor self, Tensor other, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cross_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal LongOptional dim, @ByRef Tensor out);

// aten::cross(Tensor self, Tensor other, int? dim=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor cross(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByVal Tensor cross(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::triu.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor triu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByRef Tensor triu_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::triu.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor triu_outf(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal, @ByRef Tensor out);

// aten::triu(Tensor self, int diagonal=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor triu(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByVal Tensor triu(@Const @ByRef Tensor self);

// aten::tril.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tril_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByRef Tensor tril_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::tril.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tril_outf(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal, @ByRef Tensor out);

// aten::tril(Tensor self, int diagonal=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor tril(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByVal Tensor tril(@Const @ByRef Tensor self);

// aten::tril_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor tril_indices(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset/*=0*/, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor tril_indices(@Cast("int64_t") long row, @Cast("int64_t") long col);

// aten::tril_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor tril_indices(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::triu_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor triu_indices(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset/*=0*/, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor triu_indices(@Cast("int64_t") long row, @Cast("int64_t") long col);

// aten::triu_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor triu_indices(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::trace(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor trace(@Const @ByRef Tensor self);

// aten::trace_backward(Tensor grad, int[] sizes) -> Tensor
@Namespace("at") public static native @ByVal Tensor trace_backward(@Const @ByRef Tensor grad, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("at") public static native @ByVal Tensor trace_backward(@Const @ByRef Tensor grad, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sizes);

// aten::ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ne_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ne_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::ne.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor ne(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ne_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ne_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::ne.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor ne(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::not_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor not_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::not_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor not_equal_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::not_equal.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor not_equal(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::not_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor not_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::not_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor not_equal_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::not_equal.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor not_equal(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::eq.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eq_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::eq.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eq_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::eq.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor eq(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::eq.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eq_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::eq.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eq_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::eq.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor eq(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ge_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ge_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::ge.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor ge(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ge_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ge_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::ge.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor ge(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::greater_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::greater_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_equal_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::greater_equal.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor greater_equal(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::greater_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::greater_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_equal_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::greater_equal.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor greater_equal(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::le.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor le_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::le.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor le_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::le.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor le(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::le.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor le_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::le.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor le_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::le.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor le(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::less_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::less_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_equal_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::less_equal.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor less_equal(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::less_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::less_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_equal_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::less_equal.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor less_equal(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::gt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gt_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::gt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gt_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::gt.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor gt(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::gt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gt_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::gt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gt_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::gt.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor gt(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::greater.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::greater.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::greater.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor greater(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::greater.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::greater.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::greater.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor greater(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::lt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lt_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::lt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lt_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::lt.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor lt(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::lt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lt_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::lt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lt_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::lt.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor lt(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::less.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::less.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::less.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor less(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::less.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::less.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::less.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor less(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::take.out(Tensor self, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor take_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor index);

// aten::take.out(Tensor self, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor take_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor index, @ByRef Tensor out);

// aten::take(Tensor self, Tensor index) -> Tensor
@Namespace("at") public static native @ByVal Tensor take(@Const @ByRef Tensor self, @Const @ByRef Tensor index);

// aten::take_along_dim.out(Tensor self, Tensor indices, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor take_along_dim_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByRef Tensor take_along_dim_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices);

// aten::take_along_dim.out(Tensor self, Tensor indices, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor take_along_dim_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal LongOptional dim, @ByRef Tensor out);

// aten::take_along_dim(Tensor self, Tensor indices, int? dim=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor take_along_dim(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByVal Tensor take_along_dim(@Const @ByRef Tensor self, @Const @ByRef Tensor indices);

// aten::index_select.out(Tensor self, int dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_select_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index);

// aten::index_select.out(Tensor self, int dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_select_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @ByRef Tensor out);

// aten::index_select(Tensor self, int dim, Tensor index) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_select(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index);

// aten::index_select.dimname_out(Tensor self, Dimname dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_select_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index);

// aten::index_select.dimname_out(Tensor self, Dimname dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_select_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @ByRef Tensor out);

// aten::index_select.dimname(Tensor self, Dimname dim, Tensor index) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_select(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index);

// aten::index_select_backward(Tensor grad, int[] self_sizes, int dim, Tensor index) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_select_backward(@Const @ByRef Tensor grad, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef self_sizes, @Cast("int64_t") long dim, @Const @ByRef Tensor index);
@Namespace("at") public static native @ByVal Tensor index_select_backward(@Const @ByRef Tensor grad, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] self_sizes, @Cast("int64_t") long dim, @Const @ByRef Tensor index);

// aten::masked_select.out(Tensor self, Tensor mask, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor masked_select_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mask);

// aten::masked_select.out(Tensor self, Tensor mask, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor masked_select_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @ByRef Tensor out);

// aten::masked_select(Tensor self, Tensor mask) -> Tensor
@Namespace("at") public static native @ByVal Tensor masked_select(@Const @ByRef Tensor self, @Const @ByRef Tensor mask);

// aten::masked_select_backward(Tensor grad, Tensor input, Tensor mask) -> Tensor
@Namespace("at") public static native @ByVal Tensor masked_select_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor input, @Const @ByRef Tensor mask);

// aten::nonzero.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nonzero_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::nonzero.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nonzero_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::nonzero(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor nonzero(@Const @ByRef Tensor self);

// aten::nonzero_numpy(Tensor self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector nonzero_numpy(@Const @ByRef Tensor self);

// aten::gather.out(Tensor self, int dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gather_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad/*=false*/);
@Namespace("at") public static native @ByRef Tensor gather_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index);

// aten::gather.out(Tensor self, int dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gather_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad, @ByRef Tensor out);

// aten::gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor gather(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad/*=false*/);
@Namespace("at") public static native @ByVal Tensor gather(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index);

// aten::gather_backward(Tensor grad, Tensor self, int dim, Tensor index, bool sparse_grad) -> Tensor
@Namespace("at") public static native @ByVal Tensor gather_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad);

// aten::gather.dimname_out(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gather_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad/*=false*/);
@Namespace("at") public static native @ByRef Tensor gather_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index);

// aten::gather.dimname_out(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gather_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad, @ByRef Tensor out);

// aten::gather.dimname(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor gather(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad/*=false*/);
@Namespace("at") public static native @ByVal Tensor gather(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index);

// aten::_gather_sparse_backward(Tensor self, int dim, Tensor index, Tensor grad) -> Tensor
@Namespace("at") public static native @ByVal Tensor _gather_sparse_backward(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor grad);

// aten::addcmul.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addcmul_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar value);
@Namespace("at") public static native @ByRef Tensor addcmul_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2);

// aten::addcmul.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addcmul_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @Const @ByRef Scalar value, @ByRef Tensor out);

// aten::addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor addcmul(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar value);
@Namespace("at") public static native @ByVal Tensor addcmul(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2);

// aten::addcdiv.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addcdiv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar value);
@Namespace("at") public static native @ByRef Tensor addcdiv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2);

// aten::addcdiv.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addcdiv_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @Const @ByRef Scalar value, @ByRef Tensor out);

// aten::addcdiv(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor addcdiv(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar value);
@Namespace("at") public static native @ByVal Tensor addcdiv(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2);

// aten::cross_entropy_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100, float label_smoothing=0.0) -> Tensor
@Namespace("at") public static native @ByVal Tensor cross_entropy_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("int64_t") long ignore_index/*=-100*/, double label_smoothing/*=0.0*/);
@Namespace("at") public static native @ByVal Tensor cross_entropy_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::lstsq.X(Tensor self, Tensor A, *, Tensor(a!) X, Tensor(b!) qr) -> (Tensor(a!) solution, Tensor(b!) QR)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> lstsq_out(@ByRef Tensor X, @ByRef Tensor qr, @Const @ByRef Tensor self, @Const @ByRef Tensor A);

// aten::lstsq.X(Tensor self, Tensor A, *, Tensor(a!) X, Tensor(b!) qr) -> (Tensor(a!) solution, Tensor(b!) QR)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> lstsq_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor A, @ByRef Tensor X, @ByRef Tensor qr);

// aten::lstsq(Tensor self, Tensor A) -> (Tensor solution, Tensor QR)
@Namespace("at") public static native @ByVal TensorTensorTuple lstsq(@Const @ByRef Tensor self, @Const @ByRef Tensor A);

// aten::triangular_solve.X(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False, *, Tensor(a!) X, Tensor(b!) M) -> (Tensor(a!) solution, Tensor(b!) cloned_coefficient)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> triangular_solve_out(@ByRef Tensor X, @ByRef Tensor M, @Const @ByRef Tensor self, @Const @ByRef Tensor A, @Cast("bool") boolean upper/*=true*/, @Cast("bool") boolean transpose/*=false*/, @Cast("bool") boolean unitriangular/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> triangular_solve_out(@ByRef Tensor X, @ByRef Tensor M, @Const @ByRef Tensor self, @Const @ByRef Tensor A);

// aten::triangular_solve.X(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False, *, Tensor(a!) X, Tensor(b!) M) -> (Tensor(a!) solution, Tensor(b!) cloned_coefficient)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> triangular_solve_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor A, @Cast("bool") boolean upper, @Cast("bool") boolean transpose, @Cast("bool") boolean unitriangular, @ByRef Tensor X, @ByRef Tensor M);

// aten::triangular_solve(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False) -> (Tensor solution, Tensor cloned_coefficient)
@Namespace("at") public static native @ByVal TensorTensorTuple triangular_solve(@Const @ByRef Tensor self, @Const @ByRef Tensor A, @Cast("bool") boolean upper/*=true*/, @Cast("bool") boolean transpose/*=false*/, @Cast("bool") boolean unitriangular/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple triangular_solve(@Const @ByRef Tensor self, @Const @ByRef Tensor A);

// aten::symeig.e(Tensor self, bool eigenvectors=False, bool upper=True, *, Tensor(a!) e, Tensor(b!) V) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> symeig_out(@ByRef Tensor e, @ByRef Tensor V, @Const @ByRef Tensor self, @Cast("bool") boolean eigenvectors/*=false*/, @Cast("bool") boolean upper/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> symeig_out(@ByRef Tensor e, @ByRef Tensor V, @Const @ByRef Tensor self);

// aten::symeig.e(Tensor self, bool eigenvectors=False, bool upper=True, *, Tensor(a!) e, Tensor(b!) V) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> symeig_outf(@Const @ByRef Tensor self, @Cast("bool") boolean eigenvectors, @Cast("bool") boolean upper, @ByRef Tensor e, @ByRef Tensor V);

// aten::symeig(Tensor self, bool eigenvectors=False, bool upper=True) -> (Tensor eigenvalues, Tensor eigenvectors)
@Namespace("at") public static native @ByVal TensorTensorTuple symeig(@Const @ByRef Tensor self, @Cast("bool") boolean eigenvectors/*=false*/, @Cast("bool") boolean upper/*=true*/);
@Namespace("at") public static native @ByVal TensorTensorTuple symeig(@Const @ByRef Tensor self);

// aten::_symeig_helper(Tensor self, bool eigenvectors, bool upper) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _symeig_helper(@Const @ByRef Tensor self, @Cast("bool") boolean eigenvectors, @Cast("bool") boolean upper);

// aten::eig.e(Tensor self, bool eigenvectors=False, *, Tensor(a!) e, Tensor(b!) v) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> eig_out(@ByRef Tensor e, @ByRef Tensor v, @Const @ByRef Tensor self, @Cast("bool") boolean eigenvectors/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> eig_out(@ByRef Tensor e, @ByRef Tensor v, @Const @ByRef Tensor self);

// aten::eig.e(Tensor self, bool eigenvectors=False, *, Tensor(a!) e, Tensor(b!) v) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> eig_outf(@Const @ByRef Tensor self, @Cast("bool") boolean eigenvectors, @ByRef Tensor e, @ByRef Tensor v);

// aten::eig(Tensor self, bool eigenvectors=False) -> (Tensor eigenvalues, Tensor eigenvectors)
@Namespace("at") public static native @ByVal TensorTensorTuple eig(@Const @ByRef Tensor self, @Cast("bool") boolean eigenvectors/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple eig(@Const @ByRef Tensor self);

// aten::svd.U(Tensor self, bool some=True, bool compute_uv=True, *, Tensor(a!) U, Tensor(b!) S, Tensor(c!) V) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) V)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> svd_out(@ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor V, @Const @ByRef Tensor self, @Cast("bool") boolean some/*=true*/, @Cast("bool") boolean compute_uv/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> svd_out(@ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor V, @Const @ByRef Tensor self);

// aten::svd.U(Tensor self, bool some=True, bool compute_uv=True, *, Tensor(a!) U, Tensor(b!) S, Tensor(c!) V) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) V)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> svd_outf(@Const @ByRef Tensor self, @Cast("bool") boolean some, @Cast("bool") boolean compute_uv, @ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor V);

// aten::svd(Tensor self, bool some=True, bool compute_uv=True) -> (Tensor U, Tensor S, Tensor V)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple svd(@Const @ByRef Tensor self, @Cast("bool") boolean some/*=true*/, @Cast("bool") boolean compute_uv/*=true*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple svd(@Const @ByRef Tensor self);

// aten::_svd_helper(Tensor self, bool some, bool compute_uv) -> (Tensor U, Tensor S, Tensor V)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _svd_helper(@Const @ByRef Tensor self, @Cast("bool") boolean some, @Cast("bool") boolean compute_uv);

// aten::swapaxes(Tensor(a) self, int axis0, int axis1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor swapaxes(@Const @ByRef Tensor self, @Cast("int64_t") long axis0, @Cast("int64_t") long axis1);

// aten::swapdims(Tensor(a) self, int dim0, int dim1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor swapdims(@Const @ByRef Tensor self, @Cast("int64_t") long dim0, @Cast("int64_t") long dim1);

// aten::cholesky.out(Tensor self, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cholesky_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByRef Tensor cholesky_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::cholesky.out(Tensor self, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cholesky_outf(@Const @ByRef Tensor self, @Cast("bool") boolean upper, @ByRef Tensor out);

// aten::cholesky(Tensor self, bool upper=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor cholesky(@Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByVal Tensor cholesky(@Const @ByRef Tensor self);

// aten::cholesky_solve.out(Tensor self, Tensor input2, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cholesky_solve_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByRef Tensor cholesky_solve_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor input2);

// aten::cholesky_solve.out(Tensor self, Tensor input2, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cholesky_solve_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Cast("bool") boolean upper, @ByRef Tensor out);

// aten::cholesky_solve(Tensor self, Tensor input2, bool upper=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor cholesky_solve(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByVal Tensor cholesky_solve(@Const @ByRef Tensor self, @Const @ByRef Tensor input2);

// aten::_cholesky_solve_helper(Tensor self, Tensor A, bool upper) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cholesky_solve_helper(@Const @ByRef Tensor self, @Const @ByRef Tensor A, @Cast("bool") boolean upper);

// aten::solve(Tensor self, Tensor A) -> (Tensor solution, Tensor LU)
@Namespace("at") public static native @ByVal TensorTensorTuple solve(@Const @ByRef Tensor self, @Const @ByRef Tensor A);

// aten::solve.solution(Tensor self, Tensor A, *, Tensor(a!) solution, Tensor(b!) lu) -> (Tensor(a!) solution, Tensor(b!) LU)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> solve_out(@ByRef Tensor solution, @ByRef Tensor lu, @Const @ByRef Tensor self, @Const @ByRef Tensor A);

// aten::solve.solution(Tensor self, Tensor A, *, Tensor(a!) solution, Tensor(b!) lu) -> (Tensor(a!) solution, Tensor(b!) LU)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> solve_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor A, @ByRef Tensor solution, @ByRef Tensor lu);

// aten::_solve_helper(Tensor self, Tensor A) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _solve_helper(@Const @ByRef Tensor self, @Const @ByRef Tensor A);

// aten::cholesky_inverse(Tensor self, bool upper=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor cholesky_inverse(@Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByVal Tensor cholesky_inverse(@Const @ByRef Tensor self);

// aten::cholesky_inverse.out(Tensor self, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cholesky_inverse_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByRef Tensor cholesky_inverse_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::cholesky_inverse.out(Tensor self, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cholesky_inverse_outf(@Const @ByRef Tensor self, @Cast("bool") boolean upper, @ByRef Tensor out);

// aten::qr.Q(Tensor self, bool some=True, *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> qr_out(@ByRef Tensor Q, @ByRef Tensor R, @Const @ByRef Tensor self, @Cast("bool") boolean some/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> qr_out(@ByRef Tensor Q, @ByRef Tensor R, @Const @ByRef Tensor self);

// aten::qr.Q(Tensor self, bool some=True, *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> qr_outf(@Const @ByRef Tensor self, @Cast("bool") boolean some, @ByRef Tensor Q, @ByRef Tensor R);

// aten::qr(Tensor self, bool some=True) -> (Tensor Q, Tensor R)
@Namespace("at") public static native @ByVal TensorTensorTuple qr(@Const @ByRef Tensor self, @Cast("bool") boolean some/*=true*/);
@Namespace("at") public static native @ByVal TensorTensorTuple qr(@Const @ByRef Tensor self);

// aten::geqrf.a(Tensor self, *, Tensor(a!) a, Tensor(b!) tau) -> (Tensor(a!) a, Tensor(b!) tau)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> geqrf_out(@ByRef Tensor a, @ByRef Tensor tau, @Const @ByRef Tensor self);

// aten::geqrf.a(Tensor self, *, Tensor(a!) a, Tensor(b!) tau) -> (Tensor(a!) a, Tensor(b!) tau)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> geqrf_outf(@Const @ByRef Tensor self, @ByRef Tensor a, @ByRef Tensor tau);

// aten::geqrf(Tensor self) -> (Tensor a, Tensor tau)
@Namespace("at") public static native @ByVal TensorTensorTuple geqrf(@Const @ByRef Tensor self);

// aten::orgqr(Tensor self, Tensor input2) -> Tensor
@Namespace("at") public static native @ByVal Tensor orgqr(@Const @ByRef Tensor self, @Const @ByRef Tensor input2);

// aten::orgqr.out(Tensor self, Tensor input2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor orgqr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor input2);

// aten::orgqr.out(Tensor self, Tensor input2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor orgqr_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @ByRef Tensor out);

// aten::ormqr.out(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ormqr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Const @ByRef Tensor input3, @Cast("bool") boolean left/*=true*/, @Cast("bool") boolean transpose/*=false*/);
@Namespace("at") public static native @ByRef Tensor ormqr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Const @ByRef Tensor input3);

// aten::ormqr.out(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ormqr_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Const @ByRef Tensor input3, @Cast("bool") boolean left, @Cast("bool") boolean transpose, @ByRef Tensor out);

// aten::ormqr(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor ormqr(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Const @ByRef Tensor input3, @Cast("bool") boolean left/*=true*/, @Cast("bool") boolean transpose/*=false*/);
@Namespace("at") public static native @ByVal Tensor ormqr(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Const @ByRef Tensor input3);

// aten::_lu_with_info(Tensor self, bool pivot=True, bool check_errors=True) -> (Tensor LU, Tensor pivots, Tensor info)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _lu_with_info(@Const @ByRef Tensor self, @Cast("bool") boolean pivot/*=true*/, @Cast("bool") boolean check_errors/*=true*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _lu_with_info(@Const @ByRef Tensor self);

// aten::lu_solve.out(Tensor self, Tensor LU_data, Tensor LU_pivots, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lu_solve_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots);

// aten::lu_solve.out(Tensor self, Tensor LU_data, Tensor LU_pivots, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lu_solve_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots, @ByRef Tensor out);

// aten::lu_solve(Tensor self, Tensor LU_data, Tensor LU_pivots) -> Tensor
@Namespace("at") public static native @ByVal Tensor lu_solve(@Const @ByRef Tensor self, @Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots);

// aten::lu_unpack(Tensor LU_data, Tensor LU_pivots, bool unpack_data=True, bool unpack_pivots=True) -> (Tensor P, Tensor L, Tensor U)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple lu_unpack(@Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots, @Cast("bool") boolean unpack_data/*=true*/, @Cast("bool") boolean unpack_pivots/*=true*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple lu_unpack(@Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots);

// aten::lu_unpack.out(Tensor LU_data, Tensor LU_pivots, bool unpack_data=True, bool unpack_pivots=True, *, Tensor(a!) P, Tensor(b!) L, Tensor(c!) U) -> (Tensor(a!) P, Tensor(b!) L, Tensor(c!) U)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> lu_unpack_out(@ByRef Tensor P, @ByRef Tensor L, @ByRef Tensor U, @Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots, @Cast("bool") boolean unpack_data/*=true*/, @Cast("bool") boolean unpack_pivots/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> lu_unpack_out(@ByRef Tensor P, @ByRef Tensor L, @ByRef Tensor U, @Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots);

// aten::lu_unpack.out(Tensor LU_data, Tensor LU_pivots, bool unpack_data=True, bool unpack_pivots=True, *, Tensor(a!) P, Tensor(b!) L, Tensor(c!) U) -> (Tensor(a!) P, Tensor(b!) L, Tensor(c!) U)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> lu_unpack_outf(@Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots, @Cast("bool") boolean unpack_data, @Cast("bool") boolean unpack_pivots, @ByRef Tensor P, @ByRef Tensor L, @ByRef Tensor U);

// aten::multinomial.out(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multinomial_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long num_samples, @Cast("bool") boolean replacement/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor multinomial_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long num_samples);

// aten::multinomial.out(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multinomial_outf(@Const @ByRef Tensor self, @Cast("int64_t") long num_samples, @Cast("bool") boolean replacement, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::multinomial(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor multinomial(@Const @ByRef Tensor self, @Cast("int64_t") long num_samples, @Cast("bool") boolean replacement/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor multinomial(@Const @ByRef Tensor self, @Cast("int64_t") long num_samples);

// aten::lgamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lgamma_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::lgamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lgamma_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::lgamma(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor lgamma(@Const @ByRef Tensor self);

// aten::digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor digamma_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor digamma_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::digamma(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor digamma(@Const @ByRef Tensor self);

// aten::polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor polygamma_out(@ByRef Tensor out, @Cast("int64_t") long n, @Const @ByRef Tensor self);

// aten::polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor polygamma_outf(@Cast("int64_t") long n, @Const @ByRef Tensor self, @ByRef Tensor out);

// aten::polygamma(int n, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor polygamma(@Cast("int64_t") long n, @Const @ByRef Tensor self);

// aten::erfinv(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor erfinv(@Const @ByRef Tensor self);

// aten::erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erfinv_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erfinv_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::i0(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor i0(@Const @ByRef Tensor self);

// aten::i0_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor i0_(@ByRef Tensor self);

// aten::i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor i0_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor i0_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::sign(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor sign(@Const @ByRef Tensor self);

// aten::sign.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sign_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::sign.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sign_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::signbit(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor signbit(@Const @ByRef Tensor self);

// aten::signbit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor signbit_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::signbit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor signbit_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::dist(Tensor self, Tensor other, Scalar p=2) -> Tensor
@Namespace("at") public static native @ByVal Tensor dist(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar p);
@Namespace("at") public static native @ByVal Tensor dist(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::atan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atan2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::atan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atan2_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::atan2(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor atan2(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::lerp.Scalar_out(Tensor self, Tensor end, Scalar weight, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lerp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor end, @Const @ByRef Scalar weight);

// aten::lerp.Scalar_out(Tensor self, Tensor end, Scalar weight, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lerp_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor end, @Const @ByRef Scalar weight, @ByRef Tensor out);

// aten::lerp.Tensor_out(Tensor self, Tensor end, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lerp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor end, @Const @ByRef Tensor weight);

// aten::lerp.Tensor_out(Tensor self, Tensor end, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lerp_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor end, @Const @ByRef Tensor weight, @ByRef Tensor out);

// aten::lerp.Scalar(Tensor self, Tensor end, Scalar weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor lerp(@Const @ByRef Tensor self, @Const @ByRef Tensor end, @Const @ByRef Scalar weight);

// aten::lerp.Tensor(Tensor self, Tensor end, Tensor weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor lerp(@Const @ByRef Tensor self, @Const @ByRef Tensor end, @Const @ByRef Tensor weight);

// aten::histc.out(Tensor self, int bins=100, Scalar min=0, Scalar max=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor histc_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long bins/*=100*/, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar min, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar max);
@Namespace("at") public static native @ByRef Tensor histc_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::histc.out(Tensor self, int bins=100, Scalar min=0, Scalar max=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor histc_outf(@Const @ByRef Tensor self, @Cast("int64_t") long bins, @Const @ByRef Scalar min, @Const @ByRef Scalar max, @ByRef Tensor out);

// aten::histc(Tensor self, int bins=100, Scalar min=0, Scalar max=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor histc(@Const @ByRef Tensor self, @Cast("int64_t") long bins/*=100*/, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar min, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar max);
@Namespace("at") public static native @ByVal Tensor histc(@Const @ByRef Tensor self);

// aten::histogram.bins_tensor_out(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -> (Tensor(a!) hist, Tensor(b!) bin_edges)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> histogram_out(@ByRef Tensor hist, @ByRef Tensor bin_edges, @Const @ByRef Tensor self, @Const @ByRef Tensor bins, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> histogram_out(@ByRef Tensor hist, @ByRef Tensor bin_edges, @Const @ByRef Tensor self, @Const @ByRef Tensor bins);

// aten::histogram.bins_tensor_out(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -> (Tensor(a!) hist, Tensor(b!) bin_edges)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> histogram_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor bins, @Const @ByRef TensorOptional weight, @Cast("bool") boolean density, @ByRef Tensor hist, @ByRef Tensor bin_edges);

// aten::histogram.bins_tensor(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor bin_edges)
@Namespace("at") public static native @ByVal TensorTensorTuple histogram(@Const @ByRef Tensor self, @Const @ByRef Tensor bins, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple histogram(@Const @ByRef Tensor self, @Const @ByRef Tensor bins);

// aten::histogram.bin_ct_out(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -> (Tensor(a!) hist, Tensor(b!) bin_edges)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> histogram_out(@ByRef Tensor hist, @ByRef Tensor bin_edges, @Const @ByRef Tensor self, @Cast("int64_t") long bins/*=100*/, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") DoubleArrayRefOptional range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> histogram_out(@ByRef Tensor hist, @ByRef Tensor bin_edges, @Const @ByRef Tensor self);

// aten::histogram.bin_ct_out(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -> (Tensor(a!) hist, Tensor(b!) bin_edges)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> histogram_outf(@Const @ByRef Tensor self, @Cast("int64_t") long bins, @ByVal DoubleArrayRefOptional range, @Const @ByRef TensorOptional weight, @Cast("bool") boolean density, @ByRef Tensor hist, @ByRef Tensor bin_edges);

// aten::histogram.bin_ct(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor bin_edges)
@Namespace("at") public static native @ByVal TensorTensorTuple histogram(@Const @ByRef Tensor self, @Cast("int64_t") long bins/*=100*/, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") DoubleArrayRefOptional range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple histogram(@Const @ByRef Tensor self);

// aten::fmod.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::fmod.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmod_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::fmod.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor fmod(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::fmod.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::fmod.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmod_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::fmod.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor fmod(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::hypot.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hypot_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::hypot.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hypot_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::hypot(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor hypot(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::igamma.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor igamma_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::igamma.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor igamma_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::igamma(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor igamma(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::igammac.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor igammac_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::igammac.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor igammac_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::igammac(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor igammac(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::nextafter.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nextafter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::nextafter.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nextafter_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::nextafter(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor nextafter(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::remainder.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor remainder_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::remainder.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor remainder_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::remainder.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor remainder(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::remainder.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor remainder_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::remainder.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor remainder_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::remainder.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor remainder(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::remainder.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor remainder(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::min(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor min(@Const @ByRef Tensor self);

// aten::fmin(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor fmin(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::fmin.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmin_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::fmin.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmin_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::max(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor max(@Const @ByRef Tensor self);

// aten::fmax(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor fmax(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::fmax.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::fmax.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmax_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::maximum(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor maximum(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::maximum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor maximum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::maximum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor maximum_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::max.other(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor max(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::max.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::max.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::minimum(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor minimum(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::minimum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor minimum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::minimum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor minimum_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::min.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor min_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::min.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor min_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::min.other(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor min(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::quantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, double q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor quantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, double q);

// aten::quantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantile_outf(@Const @ByRef Tensor self, double q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::quantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantile(@Const @ByRef Tensor self, double q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor quantile(@Const @ByRef Tensor self, double q);

// aten::quantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor quantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor q);

// aten::quantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantile_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::quantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantile(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor quantile(@Const @ByRef Tensor self, @Const @ByRef Tensor q);

// aten::nanquantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanquantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, double q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor nanquantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, double q);

// aten::nanquantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanquantile_outf(@Const @ByRef Tensor self, double q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::nanquantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor nanquantile(@Const @ByRef Tensor self, double q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor nanquantile(@Const @ByRef Tensor self, double q);

// aten::nanquantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanquantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor nanquantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor q);

// aten::nanquantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanquantile_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::nanquantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor nanquantile(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor nanquantile(@Const @ByRef Tensor self, @Const @ByRef Tensor q);

// aten::quantile.new_scalar_out(Tensor self, float q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, double q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByVal @Cast("c10::string_view*") Pointer interpolation);

// aten::quantile.new_scalar_out(Tensor self, float q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantile_outf(@Const @ByRef Tensor self, double q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByVal @Cast("c10::string_view*") Pointer interpolation, @ByRef Tensor out);

// aten::quantile.new_scalar(Tensor self, float q, int? dim, bool keepdim, *, str interpolation) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantile(@Const @ByRef Tensor self, double q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByVal @Cast("c10::string_view*") Pointer interpolation);

// aten::quantile.new_out(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByVal @Cast("c10::string_view*") Pointer interpolation);

// aten::quantile.new_out(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantile_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByVal @Cast("c10::string_view*") Pointer interpolation, @ByRef Tensor out);

// aten::quantile.new(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantile(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByVal @Cast("c10::string_view*") Pointer interpolation);

// aten::nanquantile.new_scalar_out(Tensor self, float q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanquantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, double q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByVal @Cast("c10::string_view*") Pointer interpolation);

// aten::nanquantile.new_scalar_out(Tensor self, float q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanquantile_outf(@Const @ByRef Tensor self, double q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByVal @Cast("c10::string_view*") Pointer interpolation, @ByRef Tensor out);

// aten::nanquantile.new_scalar(Tensor self, float q, int? dim, bool keepdim, *, str interpolation) -> Tensor
@Namespace("at") public static native @ByVal Tensor nanquantile(@Const @ByRef Tensor self, double q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByVal @Cast("c10::string_view*") Pointer interpolation);

// aten::nanquantile.new_out(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanquantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByVal @Cast("c10::string_view*") Pointer interpolation);

// aten::nanquantile.new_out(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanquantile_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByVal @Cast("c10::string_view*") Pointer interpolation, @ByRef Tensor out);

// aten::nanquantile.new(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation) -> Tensor
@Namespace("at") public static native @ByVal Tensor nanquantile(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByVal @Cast("c10::string_view*") Pointer interpolation);

// aten::sort.values(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self);

// aten::sort.values(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean descending, @ByRef Tensor values, @ByRef Tensor indices);

// aten::sort.values_stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal BoolOptional stable, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal BoolOptional stable);

// aten::sort.values_stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_outf(@Const @ByRef Tensor self, @ByVal BoolOptional stable, @Cast("int64_t") long dim, @Cast("bool") boolean descending, @ByRef Tensor values, @ByRef Tensor indices);

// aten::sort(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple sort(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple sort(@Const @ByRef Tensor self);

// aten::sort.stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple sort(@Const @ByRef Tensor self, @ByVal BoolOptional stable, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple sort(@Const @ByRef Tensor self, @ByVal BoolOptional stable);

// aten::sort.dimname_values(Tensor self, Dimname dim, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::sort.dimname_values(Tensor self, Dimname dim, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean descending, @ByRef Tensor values, @ByRef Tensor indices);

// aten::sort.dimname_values_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal BoolOptional stable, @ByVal Dimname dim, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal BoolOptional stable, @ByVal Dimname dim);

// aten::sort.dimname_values_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_outf(@Const @ByRef Tensor self, @ByVal BoolOptional stable, @ByVal Dimname dim, @Cast("bool") boolean descending, @ByRef Tensor values, @ByRef Tensor indices);

// aten::sort.dimname(Tensor self, Dimname dim, bool descending=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple sort(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple sort(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::sort.dimname_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple sort(@Const @ByRef Tensor self, @ByVal BoolOptional stable, @ByVal Dimname dim, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple sort(@Const @ByRef Tensor self, @ByVal BoolOptional stable, @ByVal Dimname dim);

// aten::msort.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor msort_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::msort.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor msort_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::msort(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor msort(@Const @ByRef Tensor self);

// aten::argsort(Tensor self, int dim=-1, bool descending=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor argsort(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal Tensor argsort(@Const @ByRef Tensor self);

// aten::argsort.dimname(Tensor self, Dimname dim, bool descending=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor argsort(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal Tensor argsort(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::topk.values(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> topk_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean largest/*=true*/, @Cast("bool") boolean sorted/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> topk_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k);

// aten::topk.values(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> topk_outf(@Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim, @Cast("bool") boolean largest, @Cast("bool") boolean sorted, @ByRef Tensor values, @ByRef Tensor indices);

// aten::topk(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple topk(@Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean largest/*=true*/, @Cast("bool") boolean sorted/*=true*/);
@Namespace("at") public static native @ByVal TensorTensorTuple topk(@Const @ByRef Tensor self, @Cast("int64_t") long k);

// aten::all(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor all(@Const @ByRef Tensor self);

// aten::all.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor all_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::all.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor all_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::any(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor any(@Const @ByRef Tensor self);

// aten::any.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor any_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::any.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor any_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::renorm.out(Tensor self, Scalar p, int dim, Scalar maxnorm, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor renorm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar p, @Cast("int64_t") long dim, @Const @ByRef Scalar maxnorm);

// aten::renorm.out(Tensor self, Scalar p, int dim, Scalar maxnorm, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor renorm_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar p, @Cast("int64_t") long dim, @Const @ByRef Scalar maxnorm, @ByRef Tensor out);

// aten::renorm(Tensor self, Scalar p, int dim, Scalar maxnorm) -> Tensor
@Namespace("at") public static native @ByVal Tensor renorm(@Const @ByRef Tensor self, @Const @ByRef Scalar p, @Cast("int64_t") long dim, @Const @ByRef Scalar maxnorm);

// aten::unfold_backward(Tensor grad_in, int[] input_sizes, int dim, int size, int step) -> Tensor
@Namespace("at") public static native @ByVal Tensor unfold_backward(@Const @ByRef Tensor grad_in, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long size, @Cast("int64_t") long step);
@Namespace("at") public static native @ByVal Tensor unfold_backward(@Const @ByRef Tensor grad_in, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long size, @Cast("int64_t") long step);

// aten::equal(Tensor self, Tensor other) -> bool
@Namespace("at") public static native @Cast("bool") boolean equal(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::pow.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pow_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor exponent);

// aten::pow.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pow_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor exponent, @ByRef Tensor out);

// aten::pow.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor
@Namespace("at") public static native @ByVal Tensor pow(@Const @ByRef Tensor self, @Const @ByRef Tensor exponent);

// aten::pow.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pow_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor exponent);

// aten::pow.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pow_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor exponent, @ByRef Tensor out);

// aten::pow.Scalar(Scalar self, Tensor exponent) -> Tensor
@Namespace("at") public static native @ByVal Tensor pow(@Const @ByRef Scalar self, @Const @ByRef Tensor exponent);

// aten::pow.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pow_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar exponent);

// aten::pow.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pow_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar exponent, @ByRef Tensor out);

// aten::pow.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor
@Namespace("at") public static native @ByVal Tensor pow(@Const @ByRef Tensor self, @Const @ByRef Scalar exponent);

// aten::float_power.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor float_power_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor exponent);

// aten::float_power.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor float_power_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor exponent, @ByRef Tensor out);

// aten::float_power.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor
@Namespace("at") public static native @ByVal Tensor float_power(@Const @ByRef Tensor self, @Const @ByRef Tensor exponent);

// aten::float_power.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor float_power_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor exponent);

// aten::float_power.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor float_power_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor exponent, @ByRef Tensor out);

// aten::float_power.Scalar(Scalar self, Tensor exponent) -> Tensor
@Namespace("at") public static native @ByVal Tensor float_power(@Const @ByRef Scalar self, @Const @ByRef Tensor exponent);

// aten::float_power.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor float_power_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar exponent);

// aten::float_power.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor float_power_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar exponent, @ByRef Tensor out);

// aten::float_power.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor
@Namespace("at") public static native @ByVal Tensor float_power(@Const @ByRef Tensor self, @Const @ByRef Scalar exponent);

// aten::normal.Tensor_float_out(Tensor mean, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, @Const @ByRef Tensor mean, double std/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, @Const @ByRef Tensor mean);

// aten::normal.Tensor_float_out(Tensor mean, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_outf(@Const @ByRef Tensor mean, double std, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::normal.Tensor_float(Tensor mean, float std=1, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor normal(@Const @ByRef Tensor mean, double std/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor normal(@Const @ByRef Tensor mean);

// aten::normal.float_Tensor_out(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, double mean, @Const @ByRef Tensor std, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, double mean, @Const @ByRef Tensor std);

// aten::normal.float_Tensor_out(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_outf(double mean, @Const @ByRef Tensor std, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::normal.float_Tensor(float mean, Tensor std, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor normal(double mean, @Const @ByRef Tensor std, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor normal(double mean, @Const @ByRef Tensor std);

// aten::normal.Tensor_Tensor_out(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, @Const @ByRef Tensor mean, @Const @ByRef Tensor std, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, @Const @ByRef Tensor mean, @Const @ByRef Tensor std);

// aten::normal.Tensor_Tensor_out(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_outf(@Const @ByRef Tensor mean, @Const @ByRef Tensor std, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::normal.Tensor_Tensor(Tensor mean, Tensor std, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor normal(@Const @ByRef Tensor mean, @Const @ByRef Tensor std, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor normal(@Const @ByRef Tensor mean, @Const @ByRef Tensor std);

// aten::normal.float_float(float mean, float std, int[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::normal.float_float(float mean, float std, int[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::normal.float_float_out(float mean, float std, int[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, double mean, double std, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, double mean, double std, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, double mean, double std, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, double mean, double std, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::normal.float_float_out(float mean, float std, int[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_outf(double mean, double std, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor normal_outf(double mean, double std, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::alias(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor alias(@Const @ByRef Tensor self);

// aten::_index_copy_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _index_copy_(@ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source);

// aten::_amp_foreach_non_finite_check_and_unscale_(Tensor(a!)[] self, Tensor(b!) found_inf, Tensor inv_scale) -> ()
@Namespace("at") public static native void _amp_foreach_non_finite_check_and_unscale_(@ByVal TensorArrayRef self, @ByRef Tensor found_inf, @Const @ByRef Tensor inv_scale);

// aten::_amp_update_scale_(Tensor(a!) self, Tensor(b!) growth_tracker, Tensor found_inf, float scale_growth_factor, float scale_backoff_factor, int growth_interval) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _amp_update_scale_(@ByRef Tensor self, @ByRef Tensor growth_tracker, @Const @ByRef Tensor found_inf, double scale_growth_factor, double scale_backoff_factor, @Cast("int64_t") long growth_interval);

// aten::_cat(Tensor[] tensors, int dim=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cat(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor _cat(@ByVal TensorArrayRef tensors);

// aten::_cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _cat_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByRef Tensor _cat_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);

// aten::_cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _cat_outf(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim, @ByRef Tensor out);

// aten::_foreach_add.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_add(@ByVal TensorArrayRef tensors, @Const @ByRef Scalar scalar);

// aten::_foreach_add_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
@Namespace("at") public static native void _foreach_add_(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);

// aten::_foreach_sub.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_sub(@ByVal TensorArrayRef tensors, @Const @ByRef Scalar scalar);

// aten::_foreach_sub_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
@Namespace("at") public static native void _foreach_sub_(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);

// aten::_foreach_mul.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_mul(@ByVal TensorArrayRef tensors, @Const @ByRef Scalar scalar);

// aten::_foreach_mul_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
@Namespace("at") public static native void _foreach_mul_(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);

// aten::_foreach_div.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_div(@ByVal TensorArrayRef tensors, @Const @ByRef Scalar scalar);

// aten::_foreach_div_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
@Namespace("at") public static native void _foreach_div_(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);

// aten::_foreach_add.List(Tensor[] tensors1, Tensor[] tensors2, *, Scalar alpha=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_add(@ByVal TensorArrayRef tensors1, @ByVal TensorArrayRef tensors2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_add(@ByVal TensorArrayRef tensors1, @ByVal TensorArrayRef tensors2);

// aten::_foreach_add_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()
@Namespace("at") public static native void _foreach_add_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native void _foreach_add_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);

// aten::_foreach_sub.List(Tensor[] tensors1, Tensor[] tensors2, *, Scalar alpha=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_sub(@ByVal TensorArrayRef tensors1, @ByVal TensorArrayRef tensors2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_sub(@ByVal TensorArrayRef tensors1, @ByVal TensorArrayRef tensors2);

// aten::_foreach_sub_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()
@Namespace("at") public static native void _foreach_sub_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native void _foreach_sub_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);

// aten::_foreach_mul.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_mul(@ByVal TensorArrayRef tensors1, @ByVal TensorArrayRef tensors2);

// aten::_foreach_mul_.List(Tensor(a!)[] self, Tensor[] other) -> ()
@Namespace("at") public static native void _foreach_mul_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);

// aten::_foreach_div.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_div(@ByVal TensorArrayRef tensors1, @ByVal TensorArrayRef tensors2);

// aten::_foreach_div_.List(Tensor(a!)[] self, Tensor[] other) -> ()
@Namespace("at") public static native void _foreach_div_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);

// aten::_foreach_add.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_add(@ByVal TensorArrayRef tensors, @ByVal ScalarArrayRef scalars);

// aten::_foreach_add_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
@Namespace("at") public static native void _foreach_add_(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);

// aten::_foreach_sub.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_sub(@ByVal TensorArrayRef tensors, @ByVal ScalarArrayRef scalars);

// aten::_foreach_sub_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
@Namespace("at") public static native void _foreach_sub_(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);

// aten::_foreach_div.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_div(@ByVal TensorArrayRef tensors, @ByVal ScalarArrayRef scalars);

// aten::_foreach_div_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
@Namespace("at") public static native void _foreach_div_(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);

// aten::_foreach_mul.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_mul(@ByVal TensorArrayRef tensors, @ByVal ScalarArrayRef scalars);

// aten::_foreach_mul_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
@Namespace("at") public static native void _foreach_mul_(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);

// aten::_foreach_exp(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_exp(@ByVal TensorArrayRef tensors);

// aten::_foreach_zero_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_zero_(@ByVal TensorArrayRef self);

// aten::_foreach_exp_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_exp_(@ByVal TensorArrayRef self);

// aten::_foreach_sqrt(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_sqrt(@ByVal TensorArrayRef tensors);

// aten::_foreach_sqrt_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_sqrt_(@ByVal TensorArrayRef self);

// aten::_foreach_abs(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_abs(@ByVal TensorArrayRef tensors);

// aten::_foreach_abs_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_abs_(@ByVal TensorArrayRef self);

// aten::_foreach_acos(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_acos(@ByVal TensorArrayRef tensors);

// aten::_foreach_acos_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_acos_(@ByVal TensorArrayRef self);

// aten::_foreach_asin(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_asin(@ByVal TensorArrayRef tensors);

// aten::_foreach_asin_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_asin_(@ByVal TensorArrayRef self);

// aten::_foreach_atan(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_atan(@ByVal TensorArrayRef tensors);

// aten::_foreach_atan_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_atan_(@ByVal TensorArrayRef self);

// aten::_foreach_ceil(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_ceil(@ByVal TensorArrayRef tensors);

// aten::_foreach_ceil_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_ceil_(@ByVal TensorArrayRef self);

// aten::_foreach_cos(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_cos(@ByVal TensorArrayRef tensors);

// aten::_foreach_cos_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_cos_(@ByVal TensorArrayRef self);

// aten::_foreach_cosh(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_cosh(@ByVal TensorArrayRef tensors);

// aten::_foreach_cosh_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_cosh_(@ByVal TensorArrayRef self);

// aten::_foreach_erf(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_erf(@ByVal TensorArrayRef tensors);

// aten::_foreach_erf_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_erf_(@ByVal TensorArrayRef self);

// aten::_foreach_erfc(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_erfc(@ByVal TensorArrayRef tensors);

// aten::_foreach_erfc_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_erfc_(@ByVal TensorArrayRef self);

// aten::_foreach_expm1(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_expm1(@ByVal TensorArrayRef tensors);

// aten::_foreach_expm1_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_expm1_(@ByVal TensorArrayRef self);

// aten::_foreach_floor(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_floor(@ByVal TensorArrayRef tensors);

// aten::_foreach_floor_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_floor_(@ByVal TensorArrayRef self);

// aten::_foreach_log(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_log(@ByVal TensorArrayRef tensors);

// aten::_foreach_log_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_log_(@ByVal TensorArrayRef self);

// aten::_foreach_log10(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_log10(@ByVal TensorArrayRef tensors);

// aten::_foreach_log10_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_log10_(@ByVal TensorArrayRef self);

// aten::_foreach_log1p(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_log1p(@ByVal TensorArrayRef tensors);

// aten::_foreach_log1p_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_log1p_(@ByVal TensorArrayRef self);

// aten::_foreach_log2(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_log2(@ByVal TensorArrayRef tensors);

// aten::_foreach_log2_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_log2_(@ByVal TensorArrayRef self);

// aten::_foreach_neg(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_neg(@ByVal TensorArrayRef tensors);

// aten::_foreach_neg_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_neg_(@ByVal TensorArrayRef self);

// aten::_foreach_tan(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_tan(@ByVal TensorArrayRef tensors);

// aten::_foreach_tan_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_tan_(@ByVal TensorArrayRef self);

// aten::_foreach_tanh(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_tanh(@ByVal TensorArrayRef tensors);

// aten::_foreach_tanh_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_tanh_(@ByVal TensorArrayRef self);

// aten::_foreach_sin(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_sin(@ByVal TensorArrayRef tensors);

// aten::_foreach_sin_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_sin_(@ByVal TensorArrayRef self);

// aten::_foreach_sinh(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_sinh(@ByVal TensorArrayRef tensors);

// aten::_foreach_sinh_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_sinh_(@ByVal TensorArrayRef self);

// aten::_foreach_round(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_round(@ByVal TensorArrayRef tensors);

// aten::_foreach_round_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_round_(@ByVal TensorArrayRef self);

// aten::_foreach_lgamma(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_lgamma(@ByVal TensorArrayRef tensors);

// aten::_foreach_lgamma_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_lgamma_(@ByVal TensorArrayRef self);

// aten::_foreach_frac(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_frac(@ByVal TensorArrayRef tensors);

// aten::_foreach_frac_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_frac_(@ByVal TensorArrayRef self);

// aten::_foreach_reciprocal(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_reciprocal(@ByVal TensorArrayRef tensors);

// aten::_foreach_reciprocal_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_reciprocal_(@ByVal TensorArrayRef self);

// aten::_foreach_sigmoid(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_sigmoid(@ByVal TensorArrayRef tensors);

// aten::_foreach_sigmoid_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_sigmoid_(@ByVal TensorArrayRef self);

// aten::_foreach_trunc(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_trunc(@ByVal TensorArrayRef tensors);

// aten::_foreach_trunc_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_trunc_(@ByVal TensorArrayRef self);

// aten::_foreach_addcdiv_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()
@Namespace("at") public static native void _foreach_addcdiv_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar value);
@Namespace("at") public static native void _foreach_addcdiv_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2);

// aten::_foreach_addcmul_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()
@Namespace("at") public static native void _foreach_addcmul_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar value);
@Namespace("at") public static native void _foreach_addcmul_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2);

// aten::_foreach_addcdiv_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()
@Namespace("at") public static native void _foreach_addcdiv_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @ByVal ScalarArrayRef scalars);

// aten::_foreach_addcmul_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()
@Namespace("at") public static native void _foreach_addcmul_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @ByVal ScalarArrayRef scalars);

// aten::_foreach_addcdiv.Scalar(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_addcdiv(@ByVal TensorArrayRef input, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar value);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_addcdiv(@ByVal TensorArrayRef input, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2);

// aten::_foreach_addcmul.Scalar(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_addcmul(@ByVal TensorArrayRef input, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar value);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_addcmul(@ByVal TensorArrayRef input, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2);

// aten::_foreach_addcdiv.ScalarList(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_addcdiv(@ByVal TensorArrayRef input, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @ByVal ScalarArrayRef scalars);

// aten::_foreach_addcmul.ScalarList(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_addcmul(@ByVal TensorArrayRef input, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @ByVal ScalarArrayRef scalars);

// aten::_foreach_maximum.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_maximum(@ByVal TensorArrayRef tensors1, @ByVal TensorArrayRef tensors2);

// aten::_foreach_minimum.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_minimum(@ByVal TensorArrayRef tensors1, @ByVal TensorArrayRef tensors2);

// aten::bucketize.Tensor(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor bucketize(@Const @ByRef Tensor self, @Const @ByRef Tensor boundaries, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/);
@Namespace("at") public static native @ByVal Tensor bucketize(@Const @ByRef Tensor self, @Const @ByRef Tensor boundaries);

// aten::bucketize.Tensor_out(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bucketize_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor boundaries, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/);
@Namespace("at") public static native @ByRef Tensor bucketize_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor boundaries);

// aten::bucketize.Tensor_out(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bucketize_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor boundaries, @Cast("bool") boolean out_int32, @Cast("bool") boolean right, @ByRef Tensor out);

// aten::bucketize.Scalar(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor bucketize(@Const @ByRef Scalar self, @Const @ByRef Tensor boundaries, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/);
@Namespace("at") public static native @ByVal Tensor bucketize(@Const @ByRef Scalar self, @Const @ByRef Tensor boundaries);

// aten::searchsorted.Tensor(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor searchsorted(@Const @ByRef Tensor sorted_sequence, @Const @ByRef Tensor self, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/);
@Namespace("at") public static native @ByVal Tensor searchsorted(@Const @ByRef Tensor sorted_sequence, @Const @ByRef Tensor self);

// aten::searchsorted.Tensor_out(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor searchsorted_out(@ByRef Tensor out, @Const @ByRef Tensor sorted_sequence, @Const @ByRef Tensor self, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/);
@Namespace("at") public static native @ByRef Tensor searchsorted_out(@ByRef Tensor out, @Const @ByRef Tensor sorted_sequence, @Const @ByRef Tensor self);

// aten::searchsorted.Tensor_out(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor searchsorted_outf(@Const @ByRef Tensor sorted_sequence, @Const @ByRef Tensor self, @Cast("bool") boolean out_int32, @Cast("bool") boolean right, @ByRef Tensor out);

// aten::searchsorted.Scalar(Tensor sorted_sequence, Scalar self, *, bool out_int32=False, bool right=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor searchsorted(@Const @ByRef Tensor sorted_sequence, @Const @ByRef Scalar self, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/);
@Namespace("at") public static native @ByVal Tensor searchsorted(@Const @ByRef Tensor sorted_sequence, @Const @ByRef Scalar self);

// aten::_convert_indices_from_coo_to_csr(Tensor self, int size, *, bool out_int32=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _convert_indices_from_coo_to_csr(@Const @ByRef Tensor self, @Cast("int64_t") long size, @Cast("bool") boolean out_int32/*=false*/);
@Namespace("at") public static native @ByVal Tensor _convert_indices_from_coo_to_csr(@Const @ByRef Tensor self, @Cast("int64_t") long size);

// aten::_convert_indices_from_coo_to_csr.out(Tensor self, int size, *, bool out_int32=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _convert_indices_from_coo_to_csr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long size, @Cast("bool") boolean out_int32/*=false*/);
@Namespace("at") public static native @ByRef Tensor _convert_indices_from_coo_to_csr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long size);

// aten::_convert_indices_from_coo_to_csr.out(Tensor self, int size, *, bool out_int32=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _convert_indices_from_coo_to_csr_outf(@Const @ByRef Tensor self, @Cast("int64_t") long size, @Cast("bool") boolean out_int32, @ByRef Tensor out);

// aten::mse_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mse_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor mse_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::mse_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mse_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor out);

// aten::mse_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor mse_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor mse_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::mse_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mse_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);

// aten::mse_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mse_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor grad_input);

// aten::mse_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor
@Namespace("at") public static native @ByVal Tensor mse_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);

// aten::l1_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor l1_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor l1_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::l1_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor l1_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor out);

// aten::l1_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor l1_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor l1_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor l1_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);

// aten::l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor l1_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor grad_input);

// aten::l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor
@Namespace("at") public static native @ByVal Tensor l1_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);

// aten::multi_margin_loss.out(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar p, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar margin, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::multi_margin_loss.out(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef Scalar p, @Const @ByRef Scalar margin, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByRef Tensor out);

// aten::multi_margin_loss(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor multi_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar p, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar margin, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor multi_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::multi_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Scalar p, Scalar margin, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef Scalar p, @Const @ByRef Scalar margin, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef Scalar p, @Const @ByRef Scalar margin);

// aten::multi_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Scalar p, Scalar margin, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef Scalar p, @Const @ByRef Scalar margin, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByRef Tensor grad_input);

// aten::multi_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, Scalar p, Scalar margin, Tensor? weight=None, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor multi_margin_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef Scalar p, @Const @ByRef Scalar margin, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor multi_margin_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef Scalar p, @Const @ByRef Scalar margin);

// aten::multilabel_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multilabel_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor multilabel_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::multilabel_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multilabel_margin_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor out);

// aten::multilabel_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor multilabel_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor multilabel_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::multilabel_margin_loss_forward.output(Tensor self, Tensor target, int reduction, *, Tensor(a!) output, Tensor(b!) is_target) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> multilabel_margin_loss_forward_out(@ByRef Tensor output, @ByRef Tensor is_target, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);

// aten::multilabel_margin_loss_forward.output(Tensor self, Tensor target, int reduction, *, Tensor(a!) output, Tensor(b!) is_target) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> multilabel_margin_loss_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor output, @ByRef Tensor is_target);

// aten::multilabel_margin_loss_forward(Tensor self, Tensor target, int reduction) -> (Tensor output, Tensor is_target)
@Namespace("at") public static native @ByVal TensorTensorTuple multilabel_margin_loss_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);

// aten::multilabel_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, Tensor is_target, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multilabel_margin_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @Const @ByRef Tensor is_target);

// aten::multilabel_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, Tensor is_target, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multilabel_margin_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @Const @ByRef Tensor is_target, @ByRef Tensor grad_input);

// aten::multilabel_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, Tensor is_target) -> Tensor
@Namespace("at") public static native @ByVal Tensor multilabel_margin_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @Const @ByRef Tensor is_target);

// aten::nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("int64_t") long ignore_index/*=-100*/);
@Namespace("at") public static native @ByRef Tensor nll_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @ByRef Tensor out);

// aten::nll_loss_nd(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss_nd(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("int64_t") long ignore_index/*=-100*/);
@Namespace("at") public static native @ByVal Tensor nll_loss_nd(@Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::nll_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("int64_t") long ignore_index/*=-100*/);
@Namespace("at") public static native @ByVal Tensor nll_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nll_loss_forward_out(@ByRef Tensor output, @ByRef Tensor total_weight, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index);

// aten::nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nll_loss_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @ByRef Tensor output, @ByRef Tensor total_weight);

// aten::nll_loss_forward(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index) -> (Tensor output, Tensor total_weight)
@Namespace("at") public static native @ByVal TensorTensorTuple nll_loss_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index);

// aten::nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight);

// aten::nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight, @ByRef Tensor grad_input);

// aten::nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight);

// aten::nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("int64_t") long ignore_index/*=-100*/);
@Namespace("at") public static native @ByRef Tensor nll_loss2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @ByRef Tensor out);

// aten::nll_loss2d(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss2d(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("int64_t") long ignore_index/*=-100*/);
@Namespace("at") public static native @ByVal Tensor nll_loss2d(@Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nll_loss2d_forward_out(@ByRef Tensor output, @ByRef Tensor total_weight, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index);

// aten::nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nll_loss2d_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @ByRef Tensor output, @ByRef Tensor total_weight);

// aten::nll_loss2d_forward(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index) -> (Tensor output, Tensor total_weight)
@Namespace("at") public static native @ByVal TensorTensorTuple nll_loss2d_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index);

// aten::nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight);

// aten::nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight, @ByRef Tensor grad_input);

// aten::nll_loss2d_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight);

// aten::smooth_l1_loss.out(Tensor self, Tensor target, int reduction=Mean, float beta=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor smooth_l1_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, double beta/*=1.0*/);
@Namespace("at") public static native @ByRef Tensor smooth_l1_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::smooth_l1_loss.out(Tensor self, Tensor target, int reduction=Mean, float beta=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor smooth_l1_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double beta, @ByRef Tensor out);

// aten::smooth_l1_loss(Tensor self, Tensor target, int reduction=Mean, float beta=1.0) -> Tensor
@Namespace("at") public static native @ByVal Tensor smooth_l1_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, double beta/*=1.0*/);
@Namespace("at") public static native @ByVal Tensor smooth_l1_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::smooth_l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor smooth_l1_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double beta);

// aten::smooth_l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor smooth_l1_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double beta, @ByRef Tensor grad_input);

// aten::smooth_l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta) -> Tensor
@Namespace("at") public static native @ByVal Tensor smooth_l1_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double beta);

// aten::huber_loss.out(Tensor self, Tensor target, int reduction=Mean, float delta=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor huber_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, double delta/*=1.0*/);
@Namespace("at") public static native @ByRef Tensor huber_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::huber_loss.out(Tensor self, Tensor target, int reduction=Mean, float delta=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor huber_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double delta, @ByRef Tensor out);

// aten::huber_loss(Tensor self, Tensor target, int reduction=Mean, float delta=1.0) -> Tensor
@Namespace("at") public static native @ByVal Tensor huber_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, double delta/*=1.0*/);
@Namespace("at") public static native @ByVal Tensor huber_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::huber_loss_backward.out(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor huber_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double delta);

// aten::huber_loss_backward.out(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor huber_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double delta, @ByRef Tensor grad_input);

// aten::huber_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta) -> Tensor
@Namespace("at") public static native @ByVal Tensor huber_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double delta);

// aten::soft_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor soft_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor soft_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::soft_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor soft_margin_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor out);

// aten::soft_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor soft_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor soft_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::soft_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor soft_margin_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);

// aten::soft_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor soft_margin_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor grad_input);

// aten::soft_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor
@Namespace("at") public static native @ByVal Tensor soft_margin_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);

// aten::elu.out(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor elu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar scale, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar input_scale);
@Namespace("at") public static native @ByRef Tensor elu_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::elu.out(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor elu_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar alpha, @Const @ByRef Scalar scale, @Const @ByRef Scalar input_scale, @ByRef Tensor out);

// aten::elu(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor elu(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar scale, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar input_scale);
@Namespace("at") public static native @ByVal Tensor elu(@Const @ByRef Tensor self);

// aten::elu_backward.grad_input(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor elu_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Scalar alpha, @Const @ByRef Scalar scale, @Const @ByRef Scalar input_scale, @Cast("bool") boolean is_result, @Const @ByRef Tensor self_or_result);

// aten::elu_backward.grad_input(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor elu_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Scalar alpha, @Const @ByRef Scalar scale, @Const @ByRef Scalar input_scale, @Cast("bool") boolean is_result, @Const @ByRef Tensor self_or_result, @ByRef Tensor grad_input);

// aten::elu_backward(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result) -> Tensor
@Namespace("at") public static native @ByVal Tensor elu_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Scalar alpha, @Const @ByRef Scalar scale, @Const @ByRef Scalar input_scale, @Cast("bool") boolean is_result, @Const @ByRef Tensor self_or_result);

// aten::elu_(Tensor(a!) self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor elu_(@ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar scale, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar input_scale);
@Namespace("at") public static native @ByRef Tensor elu_(@ByRef Tensor self);

// aten::glu.out(Tensor self, int dim=-1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor glu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByRef Tensor glu_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::glu.out(Tensor self, int dim=-1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor glu_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor out);

// aten::glu(Tensor self, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor glu(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor glu(@Const @ByRef Tensor self);

// aten::glu_backward.grad_input(Tensor grad_output, Tensor self, int dim, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor glu_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::glu_backward.grad_input(Tensor grad_output, Tensor self, int dim, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor glu_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor grad_input);

// aten::glu_backward(Tensor grad_output, Tensor self, int dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor glu_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::hardsigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardsigmoid_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::hardsigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardsigmoid_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::hardsigmoid(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardsigmoid(@Const @ByRef Tensor self);

// aten::hardsigmoid_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardsigmoid_(@ByRef Tensor self);

// aten::hardsigmoid_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardsigmoid_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);

// aten::hardsigmoid_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardsigmoid_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByRef Tensor grad_input);

// aten::hardsigmoid_backward(Tensor grad_output, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardsigmoid_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);

// aten::hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardtanh_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(-1)") Scalar min_val, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar max_val);
@Namespace("at") public static native @ByRef Tensor hardtanh_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardtanh_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar min_val, @Const @ByRef Scalar max_val, @ByRef Tensor out);

// aten::hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardtanh(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(-1)") Scalar min_val, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar max_val);
@Namespace("at") public static native @ByVal Tensor hardtanh(@Const @ByRef Tensor self);

// aten::hardtanh_backward.grad_input(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardtanh_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar min_val, @Const @ByRef Scalar max_val);

// aten::hardtanh_backward.grad_input(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardtanh_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar min_val, @Const @ByRef Scalar max_val, @ByRef Tensor grad_input);

// aten::hardtanh_backward(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardtanh_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar min_val, @Const @ByRef Scalar max_val);

// aten::hardtanh_(Tensor(a!) self, Scalar min_val=-1, Scalar max_val=1) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardtanh_(@ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(-1)") Scalar min_val, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar max_val);
@Namespace("at") public static native @ByRef Tensor hardtanh_(@ByRef Tensor self);

// aten::hardswish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardswish_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::hardswish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardswish_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::hardswish(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardswish(@Const @ByRef Tensor self);

// aten::hardswish_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardswish_(@ByRef Tensor self);

// aten::hardswish_backward(Tensor grad_output, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardswish_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);

// aten::leaky_relu.out(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor leaky_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.01)") Scalar negative_slope);
@Namespace("at") public static native @ByRef Tensor leaky_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::leaky_relu.out(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor leaky_relu_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar negative_slope, @ByRef Tensor out);

// aten::leaky_relu(Tensor self, Scalar negative_slope=0.01) -> Tensor
@Namespace("at") public static native @ByVal Tensor leaky_relu(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.01)") Scalar negative_slope);
@Namespace("at") public static native @ByVal Tensor leaky_relu(@Const @ByRef Tensor self);

// aten::leaky_relu_backward.grad_input(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor leaky_relu_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar negative_slope, @Cast("bool") boolean self_is_result);

// aten::leaky_relu_backward.grad_input(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor leaky_relu_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar negative_slope, @Cast("bool") boolean self_is_result, @ByRef Tensor grad_input);

// aten::leaky_relu_backward(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result) -> Tensor
@Namespace("at") public static native @ByVal Tensor leaky_relu_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar negative_slope, @Cast("bool") boolean self_is_result);

// aten::leaky_relu_(Tensor(a!) self, Scalar negative_slope=0.01) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor leaky_relu_(@ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.01)") Scalar negative_slope);
@Namespace("at") public static native @ByRef Tensor leaky_relu_(@ByRef Tensor self);

// aten::log_sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_sigmoid_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::log_sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_sigmoid_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::log_sigmoid(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor log_sigmoid(@Const @ByRef Tensor self);

// aten::log_sigmoid_forward.output(Tensor self, *, Tensor(a!) output, Tensor(b!) buffer) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> log_sigmoid_forward_out(@ByRef Tensor output, @ByRef Tensor buffer, @Const @ByRef Tensor self);

// aten::log_sigmoid_forward.output(Tensor self, *, Tensor(a!) output, Tensor(b!) buffer) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> log_sigmoid_forward_outf(@Const @ByRef Tensor self, @ByRef Tensor output, @ByRef Tensor buffer);

// aten::log_sigmoid_forward(Tensor self) -> (Tensor output, Tensor buffer)
@Namespace("at") public static native @ByVal TensorTensorTuple log_sigmoid_forward(@Const @ByRef Tensor self);

// aten::log_sigmoid_backward.grad_input(Tensor grad_output, Tensor self, Tensor buffer, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_sigmoid_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor buffer);

// aten::log_sigmoid_backward.grad_input(Tensor grad_output, Tensor self, Tensor buffer, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_sigmoid_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor buffer, @ByRef Tensor grad_input);

// aten::log_sigmoid_backward(Tensor grad_output, Tensor self, Tensor buffer) -> Tensor
@Namespace("at") public static native @ByVal Tensor log_sigmoid_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor buffer);

// aten::rrelu_with_noise.out(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor noise, @Const @ByRef(nullValue = "at::Scalar(0.125)") Scalar lower, @Const @ByRef(nullValue = "at::Scalar(0.3333333333333333)") Scalar upper, @Cast("bool") boolean training/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor noise);

// aten::rrelu_with_noise.out(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor noise, @Const @ByRef Scalar lower, @Const @ByRef Scalar upper, @Cast("bool") boolean training, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::rrelu_with_noise(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rrelu_with_noise(@Const @ByRef Tensor self, @Const @ByRef Tensor noise, @Const @ByRef(nullValue = "at::Scalar(0.125)") Scalar lower, @Const @ByRef(nullValue = "at::Scalar(0.3333333333333333)") Scalar upper, @Cast("bool") boolean training/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor rrelu_with_noise(@Const @ByRef Tensor self, @Const @ByRef Tensor noise);

// aten::rrelu_with_noise_backward(Tensor grad_output, Tensor self, Tensor noise, Scalar lower, Scalar upper, bool training, bool self_is_result) -> Tensor
@Namespace("at") public static native @ByVal Tensor rrelu_with_noise_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor noise, @Const @ByRef Scalar lower, @Const @ByRef Scalar upper, @Cast("bool") boolean training, @Cast("bool") boolean self_is_result);

// aten::rrelu_with_noise_(Tensor(a!) self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_(@ByRef Tensor self, @Const @ByRef Tensor noise, @Const @ByRef(nullValue = "at::Scalar(0.125)") Scalar lower, @Const @ByRef(nullValue = "at::Scalar(0.3333333333333333)") Scalar upper, @Cast("bool") boolean training/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_(@ByRef Tensor self, @Const @ByRef Tensor noise);

// aten::softplus.out(Tensor self, Scalar beta=1, Scalar threshold=20, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softplus_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(20)") Scalar threshold);
@Namespace("at") public static native @ByRef Tensor softplus_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::softplus.out(Tensor self, Scalar beta=1, Scalar threshold=20, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softplus_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar beta, @Const @ByRef Scalar threshold, @ByRef Tensor out);

// aten::softplus(Tensor self, Scalar beta=1, Scalar threshold=20) -> Tensor
@Namespace("at") public static native @ByVal Tensor softplus(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(20)") Scalar threshold);
@Namespace("at") public static native @ByVal Tensor softplus(@Const @ByRef Tensor self);

// aten::softplus_backward.grad_input(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softplus_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar beta, @Const @ByRef Scalar threshold, @Const @ByRef Tensor output);

// aten::softplus_backward.grad_input(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softplus_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar beta, @Const @ByRef Scalar threshold, @Const @ByRef Tensor output, @ByRef Tensor grad_input);

// aten::softplus_backward(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold, Tensor output) -> Tensor
@Namespace("at") public static native @ByVal Tensor softplus_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar beta, @Const @ByRef Scalar threshold, @Const @ByRef Tensor output);

// aten::softshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softshrink_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.5)") Scalar lambd);
@Namespace("at") public static native @ByRef Tensor softshrink_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::softshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softshrink_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar lambd, @ByRef Tensor out);

// aten::softshrink(Tensor self, Scalar lambd=0.5) -> Tensor
@Namespace("at") public static native @ByVal Tensor softshrink(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.5)") Scalar lambd);
@Namespace("at") public static native @ByVal Tensor softshrink(@Const @ByRef Tensor self);

// aten::softshrink_backward.grad_input(Tensor grad_output, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softshrink_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar lambd);

// aten::softshrink_backward.grad_input(Tensor grad_output, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softshrink_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar lambd, @ByRef Tensor grad_input);

// aten::softshrink_backward(Tensor grad_output, Tensor self, Scalar lambd) -> Tensor
@Namespace("at") public static native @ByVal Tensor softshrink_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar lambd);

// aten::adaptive_avg_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::adaptive_avg_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByRef Tensor out);

// aten::adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::mkldnn_adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_adaptive_avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor mkldnn_adaptive_avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::mkldnn_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_adaptive_avg_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);

// aten::_adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor _adaptive_avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor _adaptive_avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _adaptive_avg_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);

// aten::adaptive_avg_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::adaptive_avg_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByRef Tensor out);

// aten::adaptive_avg_pool3d(Tensor self, int[3] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::_adaptive_avg_pool3d(Tensor self, int[3] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor _adaptive_avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor _adaptive_avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::adaptive_avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);

// aten::adaptive_avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByRef Tensor grad_input);

// aten::_adaptive_avg_pool3d_backward(Tensor grad_output, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _adaptive_avg_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);

// aten::adaptive_max_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool2d_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool2d_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::adaptive_max_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByRef Tensor out, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByRef Tensor out, @ByRef Tensor indices);

// aten::adaptive_max_pool2d(Tensor self, int[2] output_size) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple adaptive_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal TensorTensorTuple adaptive_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::adaptive_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_max_pool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices);

// aten::adaptive_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_max_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);

// aten::adaptive_max_pool2d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor
@Namespace("at") public static native @ByVal Tensor adaptive_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices);

// aten::adaptive_max_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool3d_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool3d_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::adaptive_max_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByRef Tensor out, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByRef Tensor out, @ByRef Tensor indices);

// aten::adaptive_max_pool3d(Tensor self, int[3] output_size) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple adaptive_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal TensorTensorTuple adaptive_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::adaptive_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_max_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices);

// aten::adaptive_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_max_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);

// aten::adaptive_max_pool3d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor
@Namespace("at") public static native @ByVal Tensor adaptive_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices);

// aten::avg_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::avg_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor out);

// aten::avg_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::avg_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);

// aten::avg_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor grad_input);

// aten::avg_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor
@Namespace("at") public static native @ByVal Tensor avg_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);

// aten::avg_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::avg_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor out);

// aten::avg_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);

// aten::avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor grad_input);

// aten::avg_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor
@Namespace("at") public static native @ByVal Tensor avg_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);

// aten::fractional_max_pool2d.output(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool2d_out(@ByRef Tensor output, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor random_samples);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool2d_out(@ByRef Tensor output, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor random_samples);

// aten::fractional_max_pool2d.output(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor random_samples, @ByRef Tensor output, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor random_samples, @ByRef Tensor output, @ByRef Tensor indices);

// aten::fractional_max_pool2d(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple fractional_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor random_samples);
@Namespace("at") public static native @ByVal TensorTensorTuple fractional_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor random_samples);

// aten::fractional_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] output_size, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fractional_max_pool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor fractional_max_pool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor indices);

// aten::fractional_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] output_size, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fractional_max_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor fractional_max_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);

// aten::fractional_max_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] output_size, Tensor indices) -> Tensor
@Namespace("at") public static native @ByVal Tensor fractional_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByVal Tensor fractional_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor indices);

// aten::fractional_max_pool3d.output(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool3d_out(@ByRef Tensor output, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor random_samples);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool3d_out(@ByRef Tensor output, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor random_samples);

// aten::fractional_max_pool3d.output(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor random_samples, @ByRef Tensor output, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor random_samples, @ByRef Tensor output, @ByRef Tensor indices);

// aten::fractional_max_pool3d(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple fractional_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor random_samples);
@Namespace("at") public static native @ByVal TensorTensorTuple fractional_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor random_samples);

// aten::fractional_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] output_size, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fractional_max_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor fractional_max_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor indices);

// aten::fractional_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] output_size, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fractional_max_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor fractional_max_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);

// aten::fractional_max_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] output_size, Tensor indices) -> Tensor
@Namespace("at") public static native @ByVal Tensor fractional_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByVal Tensor fractional_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor indices);

// aten::max_pool2d_with_indices.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool2d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool2d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool2d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool2d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::max_pool2d_with_indices.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool2d_with_indices_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool2d_with_indices_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out, @ByRef Tensor indices);

// aten::max_pool2d_with_indices(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool2d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool2d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool2d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool2d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::max_pool2d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_pool2d_with_indices_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor max_pool2d_with_indices_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);

// aten::max_pool2d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_pool2d_with_indices_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor max_pool2d_with_indices_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);

// aten::max_pool2d_with_indices_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_pool2d_with_indices_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByVal Tensor max_pool2d_with_indices_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);

// aten::max_pool3d_with_indices.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool3d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool3d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool3d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool3d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::max_pool3d_with_indices.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool3d_with_indices_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool3d_with_indices_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out, @ByRef Tensor indices);

// aten::max_pool3d_with_indices(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool3d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool3d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool3d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool3d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::max_pool3d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_pool3d_with_indices_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor max_pool3d_with_indices_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);

// aten::max_pool3d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_pool3d_with_indices_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor max_pool3d_with_indices_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);

// aten::max_pool3d_with_indices_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_pool3d_with_indices_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByVal Tensor max_pool3d_with_indices_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);

// aten::max_unpool2d.out(Tensor self, Tensor indices, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_unpool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor max_unpool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::max_unpool2d.out(Tensor self, Tensor indices, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_unpool2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor max_unpool2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByRef Tensor out);

// aten::max_unpool2d(Tensor self, Tensor indices, int[2] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_unpool2d(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor max_unpool2d(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::max_unpool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, int[2] output_size, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_unpool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor max_unpool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::max_unpool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, int[2] output_size, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_unpool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor max_unpool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByRef Tensor grad_input);

// aten::max_unpool2d_backward(Tensor grad_output, Tensor self, Tensor indices, int[2] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_unpool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor max_unpool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::max_unpool3d.out(Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_unpool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor max_unpool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::max_unpool3d.out(Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_unpool3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor max_unpool3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);

// aten::max_unpool3d(Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_unpool3d(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor max_unpool3d(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::max_unpool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_unpool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor max_unpool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::max_unpool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_unpool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor max_unpool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor grad_input);

// aten::max_unpool3d_backward(Tensor grad_output, Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_unpool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor max_unpool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::reflection_pad1d.out(Tensor self, int[2] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::reflection_pad1d.out(Tensor self, int[2] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);

// aten::reflection_pad1d(Tensor self, int[2] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor reflection_pad1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::reflection_pad1d_backward.grad_input(Tensor grad_output, Tensor self, int[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::reflection_pad1d_backward.grad_input(Tensor grad_output, Tensor self, int[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor grad_input);

// aten::reflection_pad1d_backward(Tensor grad_output, Tensor self, int[2] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad1d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor reflection_pad1d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::reflection_pad2d.out(Tensor self, int[4] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::reflection_pad2d.out(Tensor self, int[4] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);

// aten::reflection_pad2d(Tensor self, int[4] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor reflection_pad2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::reflection_pad2d_backward.grad_input(Tensor grad_output, Tensor self, int[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::reflection_pad2d_backward.grad_input(Tensor grad_output, Tensor self, int[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor grad_input);

// aten::reflection_pad2d_backward(Tensor grad_output, Tensor self, int[4] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor reflection_pad2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::reflection_pad3d.out(Tensor self, int[6] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::reflection_pad3d.out(Tensor self, int[6] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);

// aten::reflection_pad3d(Tensor self, int[6] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor reflection_pad3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::reflection_pad3d_backward.grad_input(Tensor grad_output, Tensor self, int[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::reflection_pad3d_backward.grad_input(Tensor grad_output, Tensor self, int[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor grad_input);

// aten::reflection_pad3d_backward(Tensor grad_output, Tensor self, int[6] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor reflection_pad3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::replication_pad1d.out(Tensor self, int[2] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::replication_pad1d.out(Tensor self, int[2] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad1d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor replication_pad1d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);

// aten::replication_pad1d(Tensor self, int[2] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::replication_pad1d_backward.grad_input(Tensor grad_output, Tensor self, int[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::replication_pad1d_backward.grad_input(Tensor grad_output, Tensor self, int[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad1d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor replication_pad1d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor grad_input);

// aten::replication_pad1d_backward(Tensor grad_output, Tensor self, int[2] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad1d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad1d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::replication_pad2d.out(Tensor self, int[4] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::replication_pad2d.out(Tensor self, int[4] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor replication_pad2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);

// aten::replication_pad2d(Tensor self, int[4] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::replication_pad2d_backward.grad_input(Tensor grad_output, Tensor self, int[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::replication_pad2d_backward.grad_input(Tensor grad_output, Tensor self, int[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor replication_pad2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor grad_input);

// aten::replication_pad2d_backward(Tensor grad_output, Tensor self, int[4] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::replication_pad3d.out(Tensor self, int[6] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::replication_pad3d.out(Tensor self, int[6] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor replication_pad3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);

// aten::replication_pad3d(Tensor self, int[6] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::replication_pad3d_backward.grad_input(Tensor grad_output, Tensor self, int[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::replication_pad3d_backward.grad_input(Tensor grad_output, Tensor self, int[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor replication_pad3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor grad_input);

// aten::replication_pad3d_backward(Tensor grad_output, Tensor self, int[6] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::upsample_linear1d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_linear1d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);

// aten::upsample_linear1d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);

// aten::upsample_bilinear2d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);

// aten::upsample_bilinear2d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);

// aten::upsample_trilinear3d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);

// aten::upsample_trilinear3d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);

// aten::upsample_bicubic2d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);

// aten::upsample_bicubic2d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);

// aten::upsample_nearest1d.vec(Tensor input, int[]? output_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);

// aten::upsample_nearest1d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal DoubleArrayRefOptional scale_factors);

// aten::upsample_nearest2d.vec(Tensor input, int[]? output_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);

// aten::upsample_nearest2d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal DoubleArrayRefOptional scale_factors);

// aten::upsample_nearest3d.vec(Tensor input, int[]? output_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);

// aten::upsample_nearest3d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal DoubleArrayRefOptional scale_factors);

// aten::upsample_linear1d.out(Tensor self, int[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);

// aten::upsample_linear1d.out(Tensor self, int[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales, @ByRef Tensor out);

// aten::upsample_linear1d(Tensor self, int[1] output_size, bool align_corners, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_linear1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);

// aten::upsample_linear1d_backward.grad_input(Tensor grad_output, int[1] output_size, int[3] input_size, bool align_corners, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);

// aten::upsample_linear1d_backward.grad_input(Tensor grad_output, int[1] output_size, int[3] input_size, bool align_corners, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales, @ByRef Tensor grad_input);

// aten::upsample_linear1d_backward(Tensor grad_output, int[1] output_size, int[3] input_size, bool align_corners, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);

// aten::upsample_bilinear2d.out(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);

// aten::upsample_bilinear2d.out(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);

// aten::upsample_bilinear2d(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);

// aten::upsample_bilinear2d_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);

// aten::upsample_bilinear2d_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);

// aten::upsample_bilinear2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);

// aten::upsample_bicubic2d.out(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);

// aten::upsample_bicubic2d.out(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);

// aten::upsample_bicubic2d(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);

// aten::upsample_bicubic2d_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);

// aten::upsample_bicubic2d_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);

// aten::upsample_bicubic2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);

// aten::upsample_trilinear3d.out(Tensor self, int[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);

// aten::upsample_trilinear3d.out(Tensor self, int[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);

// aten::upsample_trilinear3d(Tensor self, int[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);

// aten::upsample_trilinear3d_backward.grad_input(Tensor grad_output, int[3] output_size, int[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);

// aten::upsample_trilinear3d_backward.grad_input(Tensor grad_output, int[3] output_size, int[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);

// aten::upsample_trilinear3d_backward(Tensor grad_output, int[3] output_size, int[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);

// aten::upsample_nearest1d.out(Tensor self, int[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::upsample_nearest1d.out(Tensor self, int[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal DoubleOptional scales, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal DoubleOptional scales, @ByRef Tensor out);

// aten::upsample_nearest1d(Tensor self, int[1] output_size, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::upsample_nearest1d_backward.grad_input(Tensor grad_output, int[1] output_size, int[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... input_size);

// aten::upsample_nearest1d_backward.grad_input(Tensor grad_output, int[1] output_size, int[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal DoubleOptional scales, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal DoubleOptional scales, @ByRef Tensor grad_input);

// aten::upsample_nearest1d_backward(Tensor grad_output, int[1] output_size, int[3] input_size, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... input_size);

// aten::upsample_nearest2d.out(Tensor self, int[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::upsample_nearest2d.out(Tensor self, int[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);

// aten::upsample_nearest2d(Tensor self, int[2] output_size, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::upsample_nearest2d_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... input_size);

// aten::upsample_nearest2d_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);

// aten::upsample_nearest2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... input_size);

// aten::upsample_nearest3d.out(Tensor self, int[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::upsample_nearest3d.out(Tensor self, int[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);

// aten::upsample_nearest3d(Tensor self, int[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::upsample_nearest3d_backward.grad_input(Tensor grad_output, int[3] output_size, int[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... input_size);

// aten::upsample_nearest3d_backward.grad_input(Tensor grad_output, int[3] output_size, int[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);

// aten::upsample_nearest3d_backward(Tensor grad_output, int[3] output_size, int[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... input_size);

// aten::sigmoid_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sigmoid_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output);

// aten::sigmoid_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sigmoid_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @ByRef Tensor grad_input);

// aten::sigmoid_backward(Tensor grad_output, Tensor output) -> Tensor
@Namespace("at") public static native @ByVal Tensor sigmoid_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output);

// aten::logit_backward.grad_input(Tensor grad_output, Tensor self, float? eps=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logit_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByRef Tensor logit_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);

// aten::logit_backward.grad_input(Tensor grad_output, Tensor self, float? eps=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logit_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal DoubleOptional eps, @ByRef Tensor grad_input);

// aten::logit_backward(Tensor grad_output, Tensor self, float? eps=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor logit_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByVal Tensor logit_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);

// aten::tanh_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tanh_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output);

// aten::tanh_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tanh_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @ByRef Tensor grad_input);

// aten::tanh_backward(Tensor grad_output, Tensor output) -> Tensor
@Namespace("at") public static native @ByVal Tensor tanh_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output);

// aten::slow_conv_transpose2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::slow_conv_transpose2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByRef Tensor out);

// aten::slow_conv_transpose2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int[2] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::slow_conv_transpose2d_backward.grad_output(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation, Tensor columns, Tensor ones, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv_transpose2d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Const @ByRef Tensor columns, @Const @ByRef Tensor ones);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv_transpose2d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Const @ByRef Tensor columns, @Const @ByRef Tensor ones);

// aten::slow_conv_transpose2d_backward.grad_output(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation, Tensor columns, Tensor ones, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv_transpose2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Const @ByRef Tensor columns, @Const @ByRef Tensor ones, @ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv_transpose2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Const @ByRef Tensor columns, @Const @ByRef Tensor ones, @ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias);

// aten::slow_conv_transpose2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation, Tensor columns, Tensor ones, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple slow_conv_transpose2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Const @ByRef Tensor columns, @Const @ByRef Tensor ones, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple slow_conv_transpose2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Const @ByRef Tensor columns, @Const @ByRef Tensor ones, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::slow_conv_transpose3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] output_padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::slow_conv_transpose3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] output_padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByRef Tensor out);

// aten::slow_conv_transpose3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] output_padding=0, int[3] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::slow_conv_transpose3d_backward.grad_output(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] output_padding, int[3] dilation, Tensor finput, Tensor fgrad_input, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv_transpose3d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv_transpose3d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input);

// aten::slow_conv_transpose3d_backward.grad_output(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] output_padding, int[3] dilation, Tensor finput, Tensor fgrad_input, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv_transpose3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input, @ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv_transpose3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input, @ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias);

// aten::slow_conv_transpose3d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] output_padding, int[3] dilation, Tensor finput, Tensor fgrad_input, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple slow_conv_transpose3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple slow_conv_transpose3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::thnn_conv2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::thnn_conv2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);

// aten::thnn_conv2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor thnn_conv2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor thnn_conv2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor thnn_conv2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByVal Tensor thnn_conv2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::_slow_conv2d_forward.output(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, *, Tensor(a!) output, Tensor(b!) finput) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _slow_conv2d_forward_out(@ByRef Tensor output, @ByRef Tensor finput, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _slow_conv2d_forward_out(@ByRef Tensor output, @ByRef Tensor finput, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::_slow_conv2d_forward.output(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, *, Tensor(a!) output, Tensor(b!) finput) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _slow_conv2d_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor output, @ByRef Tensor finput);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _slow_conv2d_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor output, @ByRef Tensor finput);

// aten::_slow_conv2d_forward(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding) -> (Tensor output, Tensor finput)
@Namespace("at") public static native @ByVal TensorTensorTuple _slow_conv2d_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal TensorTensorTuple _slow_conv2d_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::_slow_conv2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, Tensor finput, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _slow_conv2d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Const @ByRef Tensor finput);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _slow_conv2d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Const @ByRef Tensor finput);

// aten::_slow_conv2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, Tensor finput, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _slow_conv2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Const @ByRef Tensor finput, @ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _slow_conv2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Const @ByRef Tensor finput, @ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias);

// aten::_slow_conv2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, Tensor finput, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _slow_conv2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Const @ByRef Tensor finput, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _slow_conv2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Const @ByRef Tensor finput, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::_conv_depthwise2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, int[2] dilation, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor _conv_depthwise2d_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @Const @ByRef Tensor _conv_depthwise2d_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);

// aten::_conv_depthwise2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, int[2] dilation, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor _conv_depthwise2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Const @ByRef Tensor out);
@Namespace("at") public static native @Const @ByRef Tensor _conv_depthwise2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Const @ByRef Tensor out);

// aten::_conv_depthwise2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, int[2] dilation) -> Tensor
@Namespace("at") public static native @ByVal Tensor _conv_depthwise2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor _conv_depthwise2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);

// aten::_conv_depthwise2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, *, Tensor(a!) grad_input, Tensor(b!) grad_weight) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _conv_depthwise2d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _conv_depthwise2d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);

// aten::_conv_depthwise2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, *, Tensor(a!) grad_input, Tensor(b!) grad_weight) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _conv_depthwise2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByRef Tensor grad_input, @ByRef Tensor grad_weight);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _conv_depthwise2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByRef Tensor grad_input, @ByRef Tensor grad_weight);

// aten::_conv_depthwise2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[2] output_mask) -> (Tensor grad_input, Tensor grad_weight)
@Namespace("at") public static native @ByVal TensorTensorTuple _conv_depthwise2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTuple _conv_depthwise2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);

// aten::conv_depthwise3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding, int[3] dilation) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv_depthwise3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor conv_depthwise3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);

// aten::conv_depthwise3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> conv_depthwise3d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> conv_depthwise3d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);

// aten::conv_depthwise3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> conv_depthwise3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> conv_depthwise3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias);

// aten::conv_depthwise3d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple conv_depthwise3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple conv_depthwise3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::slow_conv3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::slow_conv3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);

// aten::slow_conv3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor slow_conv3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByVal Tensor slow_conv3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::slow_conv3d_forward.output(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding, *, Tensor(a!) output, Tensor(b!) finput, Tensor(c!) fgrad_input) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv3d_forward_out(@ByRef Tensor output, @ByRef Tensor finput, @ByRef Tensor fgrad_input, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv3d_forward_out(@ByRef Tensor output, @ByRef Tensor finput, @ByRef Tensor fgrad_input, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::slow_conv3d_forward.output(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding, *, Tensor(a!) output, Tensor(b!) finput, Tensor(c!) fgrad_input) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv3d_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor output, @ByRef Tensor finput, @ByRef Tensor fgrad_input);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv3d_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor output, @ByRef Tensor finput, @ByRef Tensor fgrad_input);

// aten::slow_conv3d_forward(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding) -> (Tensor output, Tensor finput, Tensor fgrad_input)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple slow_conv3d_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple slow_conv3d_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// aten::slow_conv3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, Tensor finput, Tensor fgrad_input, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv3d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv3d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input);

// aten::slow_conv3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, Tensor finput, Tensor fgrad_input, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input, @ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input, @ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias);

// aten::slow_conv3d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, Tensor finput, Tensor fgrad_input, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple slow_conv3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple slow_conv3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::slow_conv_dilated2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::slow_conv_dilated2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple slow_conv_dilated2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple slow_conv_dilated2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::slow_conv_dilated3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::slow_conv_dilated3d_backward(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple slow_conv_dilated3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple slow_conv_dilated3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::col2im.out(Tensor self, int[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor col2im_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor col2im_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);

// aten::col2im.out(Tensor self, int[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor col2im_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor col2im_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByRef Tensor out);

// aten::col2im(Tensor self, int[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
@Namespace("at") public static native @ByVal Tensor col2im(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor col2im(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);

// aten::col2im_backward.grad_input(Tensor grad_output, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor col2im_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor col2im_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);

// aten::col2im_backward.grad_input(Tensor grad_output, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor col2im_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor col2im_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByRef Tensor grad_input);

// aten::col2im_backward(Tensor grad_output, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
@Namespace("at") public static native @ByVal Tensor col2im_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor col2im_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);

// aten::column_stack(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor column_stack(@ByVal TensorArrayRef tensors);

// aten::column_stack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor column_stack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);

// aten::column_stack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor column_stack_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);

// aten::im2col.out(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor im2col_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor im2col_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);

// aten::im2col.out(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor im2col_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor im2col_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByRef Tensor out);

// aten::im2col(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
@Namespace("at") public static native @ByVal Tensor im2col(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor im2col(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);

// aten::im2col_backward.grad_input(Tensor grad_output, int[2] input_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor im2col_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor im2col_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);

// aten::im2col_backward.grad_input(Tensor grad_output, int[2] input_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor im2col_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor im2col_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByRef Tensor grad_input);

// aten::im2col_backward(Tensor grad_output, int[2] input_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
@Namespace("at") public static native @ByVal Tensor im2col_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor im2col_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);

// aten::isfinite(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor isfinite(@Const @ByRef Tensor self);

// aten::isinf(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor isinf(@Const @ByRef Tensor self);

// aten::isposinf(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor isposinf(@Const @ByRef Tensor self);

// aten::isposinf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isposinf_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::isposinf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isposinf_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::isneginf(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor isneginf(@Const @ByRef Tensor self);

// aten::isneginf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isneginf_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::isneginf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isneginf_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::_add_batch_dim(Tensor self, int batch_dim, int level) -> Tensor
@Namespace("at") public static native @ByVal Tensor _add_batch_dim(@Const @ByRef Tensor self, @Cast("int64_t") long batch_dim, @Cast("int64_t") long level);

// aten::_remove_batch_dim(Tensor self, int level, int batch_size, int out_dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor _remove_batch_dim(@Const @ByRef Tensor self, @Cast("int64_t") long level, @Cast("int64_t") long batch_size, @Cast("int64_t") long out_dim);

// aten::special_entr(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_entr(@Const @ByRef Tensor self);

// aten::special_entr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_entr_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::special_entr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_entr_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::special_ndtri(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_ndtri(@Const @ByRef Tensor self);

// aten::special_ndtri.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_ndtri_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::special_ndtri.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_ndtri_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::special_expm1(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_expm1(@Const @ByRef Tensor self);

// aten::special_expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_expm1_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::special_expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_expm1_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::special_exp2(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_exp2(@Const @ByRef Tensor self);

// aten::special_exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_exp2_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::special_exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_exp2_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::special_psi(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_psi(@Const @ByRef Tensor self);

// aten::special_psi.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_psi_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::special_psi.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_psi_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::special_digamma(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_digamma(@Const @ByRef Tensor self);

// aten::special_digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_digamma_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::special_digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_digamma_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::special_gammaln(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_gammaln(@Const @ByRef Tensor self);

// aten::special_gammaln.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_gammaln_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::special_gammaln.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_gammaln_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::special_erf(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_erf(@Const @ByRef Tensor self);

// aten::special_erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erf_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::special_erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erf_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::special_erfc(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_erfc(@Const @ByRef Tensor self);

// aten::special_erfc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erfc_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::special_erfc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erfc_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::special_erfcx(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_erfcx(@Const @ByRef Tensor self);

// aten::special_erfcx.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erfcx_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::special_erfcx.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erfcx_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::special_erfinv(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_erfinv(@Const @ByRef Tensor self);

// aten::special_erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erfinv_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::special_erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erfinv_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::special_ndtr(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_ndtr(@Const @ByRef Tensor self);

// aten::special_ndtr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_ndtr_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::special_ndtr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_ndtr_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::special_xlog1py(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_xlog1py(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::special_xlog1py.self_scalar(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_xlog1py(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::special_xlog1py.other_scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_xlog1py(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::special_xlog1py.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlog1py_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::special_xlog1py.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlog1py_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_xlog1py.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlog1py_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::special_xlog1py.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlog1py_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_xlog1py.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlog1py_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::special_xlog1py.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlog1py_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::special_xlogy(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_xlogy(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::special_xlogy.self_scalar(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_xlogy(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::special_xlogy.other_scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_xlogy(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::special_xlogy.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlogy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::special_xlogy.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlogy_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_xlogy.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlogy_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::special_xlogy.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlogy_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_xlogy.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlogy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::special_xlogy.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlogy_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::special_zeta(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_zeta(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::special_zeta.self_scalar(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_zeta(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::special_zeta.other_scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_zeta(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::special_zeta.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_zeta_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::special_zeta.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_zeta_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_zeta.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_zeta_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::special_zeta.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_zeta_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_zeta.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_zeta_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::special_zeta.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_zeta_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::special_i0(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_i0(@Const @ByRef Tensor self);

// aten::special_i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i0_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::special_i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i0_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::special_i0e(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_i0e(@Const @ByRef Tensor self);

// aten::special_i0e.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i0e_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::special_i0e.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i0e_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::special_i1(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_i1(@Const @ByRef Tensor self);

// aten::special_i1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i1_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::special_i1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i1_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::special_i1e(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_i1e(@Const @ByRef Tensor self);

// aten::special_i1e.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i1e_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::special_i1e.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i1e_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::special_logit(Tensor self, float? eps=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_logit(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByVal Tensor special_logit(@Const @ByRef Tensor self);

// aten::special_logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_logit_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByRef Tensor special_logit_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::special_logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_logit_outf(@Const @ByRef Tensor self, @ByVal DoubleOptional eps, @ByRef Tensor out);

// aten::special_polygamma(int n, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_polygamma(@Cast("int64_t") long n, @Const @ByRef Tensor self);

// aten::special_polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_polygamma_out(@ByRef Tensor out, @Cast("int64_t") long n, @Const @ByRef Tensor self);

// aten::special_polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_polygamma_outf(@Cast("int64_t") long n, @Const @ByRef Tensor self, @ByRef Tensor out);

// aten::special_logsumexp(Tensor self, int[1] dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_logsumexp(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor special_logsumexp(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor special_logsumexp(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor special_logsumexp(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::special_logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor special_logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor special_logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor special_logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::special_logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_logsumexp_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor special_logsumexp_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::special_expit(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_expit(@Const @ByRef Tensor self);

// aten::special_expit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_expit_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::special_expit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_expit_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::special_sinc(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_sinc(@Const @ByRef Tensor self);

// aten::special_sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_sinc_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::special_sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_sinc_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::special_round(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_round(@Const @ByRef Tensor self);

// aten::special_round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_round_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::special_round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_round_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::special_log1p(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_log1p(@Const @ByRef Tensor self);

// aten::special_log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_log1p_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::special_log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_log1p_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::special_log_softmax(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor special_log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::special_gammainc.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_gammainc_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::special_gammainc.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_gammainc_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_gammainc(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_gammainc(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::special_gammaincc.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_gammaincc_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::special_gammaincc.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_gammaincc_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_gammaincc(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_gammaincc(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::special_multigammaln(Tensor self, int p) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_multigammaln(@Const @ByRef Tensor self, @Cast("int64_t") long p);

// aten::special_multigammaln.out(Tensor self, int p, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_multigammaln_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long p);

// aten::special_multigammaln.out(Tensor self, int p, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_multigammaln_outf(@Const @ByRef Tensor self, @Cast("int64_t") long p, @ByRef Tensor out);

// aten::fft_fft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_fft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_fft(@Const @ByRef Tensor self);

// aten::fft_fft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_fft_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::fft_fft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);

// aten::fft_ifft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ifft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_ifft(@Const @ByRef Tensor self);

// aten::fft_ifft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_ifft_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::fft_ifft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);

// aten::fft_rfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_rfft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_rfft(@Const @ByRef Tensor self);

// aten::fft_rfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_rfft_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::fft_rfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);

// aten::fft_irfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_irfft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_irfft(@Const @ByRef Tensor self);

// aten::fft_irfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_irfft_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::fft_irfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);

// aten::fft_hfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_hfft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_hfft(@Const @ByRef Tensor self);

// aten::fft_hfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_hfft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_hfft_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::fft_hfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_hfft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);

// aten::fft_ihfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ihfft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_ihfft(@Const @ByRef Tensor self);

// aten::fft_ihfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ihfft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_ihfft_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::fft_ihfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ihfft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);

// aten::fft_fft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_fft2(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_fft2(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_fft2(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);

// aten::fft_fft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_fft2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_fft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);

// aten::fft_fft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_fft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);

// aten::fft_ifft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ifft2(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_ifft2(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_ifft2(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);

// aten::fft_ifft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_ifft2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_ifft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);

// aten::fft_ifft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_ifft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);

// aten::fft_rfft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_rfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_rfft2(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_rfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);

// aten::fft_rfft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_rfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_rfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);

// aten::fft_rfft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_rfft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);

// aten::fft_irfft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_irfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_irfft2(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_irfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);

// aten::fft_irfft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_irfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_irfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);

// aten::fft_irfft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_irfft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);

// aten::fft_fftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_fftn(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_fftn(@Const @ByRef Tensor self);

// aten::fft_fftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_fftn_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::fft_fftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fftn_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);

// aten::fft_ifftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ifftn(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_ifftn(@Const @ByRef Tensor self);

// aten::fft_ifftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_ifftn_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::fft_ifftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifftn_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);

// aten::fft_rfftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_rfftn(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_rfftn(@Const @ByRef Tensor self);

// aten::fft_rfftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_rfftn_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::fft_rfftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfftn_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);

// aten::fft_irfftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_irfftn(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_irfftn(@Const @ByRef Tensor self);

// aten::fft_irfftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_irfftn_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::fft_irfftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfftn_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);

// aten::fft_fftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_fftfreq(@Cast("int64_t") long n, double d/*=1.0*/, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor fft_fftfreq(@Cast("int64_t") long n);

// aten::fft_fftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_fftfreq(@Cast("int64_t") long n, double d, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::fft_fftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fftfreq_out(@ByRef Tensor out, @Cast("int64_t") long n, double d/*=1.0*/);
@Namespace("at") public static native @ByRef Tensor fft_fftfreq_out(@ByRef Tensor out, @Cast("int64_t") long n);

// aten::fft_fftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fftfreq_outf(@Cast("int64_t") long n, double d, @ByRef Tensor out);

// aten::fft_rfftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_rfftfreq(@Cast("int64_t") long n, double d/*=1.0*/, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor fft_rfftfreq(@Cast("int64_t") long n);

// aten::fft_rfftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_rfftfreq(@Cast("int64_t") long n, double d, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::fft_rfftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfftfreq_out(@ByRef Tensor out, @Cast("int64_t") long n, double d/*=1.0*/);
@Namespace("at") public static native @ByRef Tensor fft_rfftfreq_out(@ByRef Tensor out, @Cast("int64_t") long n);

// aten::fft_rfftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfftfreq_outf(@Cast("int64_t") long n, double d, @ByRef Tensor out);

// aten::fft_fftshift(Tensor self, int[1]? dim=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_fftshift(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim);
@Namespace("at") public static native @ByVal Tensor fft_fftshift(@Const @ByRef Tensor self);

// aten::fft_ifftshift(Tensor self, int[1]? dim=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ifftshift(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim);
@Namespace("at") public static native @ByVal Tensor fft_ifftshift(@Const @ByRef Tensor self);

// aten::linalg_cholesky_ex(Tensor self, *, bool upper=False, bool check_errors=False) -> (Tensor L, Tensor info)
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_cholesky_ex(@Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_cholesky_ex(@Const @ByRef Tensor self);

// aten::linalg_cholesky_ex.L(Tensor self, *, bool upper=False, bool check_errors=False, Tensor(a!) L, Tensor(b!) info) -> (Tensor(a!) L, Tensor(b!) info)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_cholesky_ex_out(@ByRef Tensor L, @ByRef Tensor info, @Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_cholesky_ex_out(@ByRef Tensor L, @ByRef Tensor info, @Const @ByRef Tensor self);

// aten::linalg_cholesky_ex.L(Tensor self, *, bool upper=False, bool check_errors=False, Tensor(a!) L, Tensor(b!) info) -> (Tensor(a!) L, Tensor(b!) info)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_cholesky_ex_outf(@Const @ByRef Tensor self, @Cast("bool") boolean upper, @Cast("bool") boolean check_errors, @ByRef Tensor L, @ByRef Tensor info);

// aten::linalg_cholesky(Tensor self, *, bool upper=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_cholesky(@Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_cholesky(@Const @ByRef Tensor self);

// aten::linalg_cholesky.out(Tensor self, *, bool upper=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_cholesky_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_cholesky_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::linalg_cholesky.out(Tensor self, *, bool upper=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_cholesky_outf(@Const @ByRef Tensor self, @Cast("bool") boolean upper, @ByRef Tensor out);

// aten::linalg_det(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_det(@Const @ByRef Tensor self);

// aten::linalg_det.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_det_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::linalg_det.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_det_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::det(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor det(@Const @ByRef Tensor self);

// aten::_det_lu_based_helper(Tensor self) -> (Tensor det, Tensor lu, Tensor pivs)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _det_lu_based_helper(@Const @ByRef Tensor self);

// aten::_det_lu_based_helper_backward_helper(Tensor det_grad, Tensor det, Tensor self, Tensor lu, Tensor pivs) -> Tensor
@Namespace("at") public static native @ByVal Tensor _det_lu_based_helper_backward_helper(@Const @ByRef Tensor det_grad, @Const @ByRef Tensor det, @Const @ByRef Tensor self, @Const @ByRef Tensor lu, @Const @ByRef Tensor pivs);

// aten::linalg_lstsq(Tensor self, Tensor b, float? rcond=None, *, str? driver=None) -> (Tensor solution, Tensor residuals, Tensor rank, Tensor singular_values)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple linalg_lstsq(@Const @ByRef Tensor self, @Const @ByRef Tensor b, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional rcond, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer driver);
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple linalg_lstsq(@Const @ByRef Tensor self, @Const @ByRef Tensor b);

// aten::linalg_lstsq.out(Tensor self, Tensor b, float? rcond=None, *, str? driver=None, Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values) -> (Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_lstsq_out(@ByRef Tensor solution, @ByRef Tensor residuals, @ByRef Tensor rank, @ByRef Tensor singular_values, @Const @ByRef Tensor self, @Const @ByRef Tensor b, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional rcond, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer driver);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_lstsq_out(@ByRef Tensor solution, @ByRef Tensor residuals, @ByRef Tensor rank, @ByRef Tensor singular_values, @Const @ByRef Tensor self, @Const @ByRef Tensor b);

// aten::linalg_lstsq.out(Tensor self, Tensor b, float? rcond=None, *, str? driver=None, Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values) -> (Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_lstsq_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor b, @ByVal DoubleOptional rcond, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer driver, @ByRef Tensor solution, @ByRef Tensor residuals, @ByRef Tensor rank, @ByRef Tensor singular_values);

// aten::linalg_matmul(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matmul(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::linalg_matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matmul_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::linalg_matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matmul_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::linalg_slogdet(Tensor self) -> (Tensor sign, Tensor logabsdet)
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_slogdet(@Const @ByRef Tensor self);

// aten::linalg_slogdet.out(Tensor self, *, Tensor(a!) sign, Tensor(b!) logabsdet) -> (Tensor(a!) sign, Tensor(b!) logabsdet)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_slogdet_out(@ByRef Tensor sign, @ByRef Tensor logabsdet, @Const @ByRef Tensor self);

// aten::linalg_slogdet.out(Tensor self, *, Tensor(a!) sign, Tensor(b!) logabsdet) -> (Tensor(a!) sign, Tensor(b!) logabsdet)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_slogdet_outf(@Const @ByRef Tensor self, @ByRef Tensor sign, @ByRef Tensor logabsdet);

// aten::linalg_eig(Tensor self) -> (Tensor eigenvalues, Tensor eigenvectors)
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_eig(@Const @ByRef Tensor self);

// aten::linalg_eig.out(Tensor self, *, Tensor(a!) eigenvalues, Tensor(b!) eigenvectors) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_eig_out(@ByRef Tensor eigenvalues, @ByRef Tensor eigenvectors, @Const @ByRef Tensor self);

// aten::linalg_eig.out(Tensor self, *, Tensor(a!) eigenvalues, Tensor(b!) eigenvectors) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_eig_outf(@Const @ByRef Tensor self, @ByRef Tensor eigenvalues, @ByRef Tensor eigenvectors);

// aten::linalg_eigvals(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_eigvals(@Const @ByRef Tensor self);

// aten::linalg_eigvals.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_eigvals_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::linalg_eigvals.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_eigvals_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::linalg_eigh(Tensor self, str UPLO="L") -> (Tensor eigenvalues, Tensor eigenvectors)
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_eigh(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::string_view(\"L\")") @Cast("c10::string_view*") Pointer UPLO);
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_eigh(@Const @ByRef Tensor self);

// aten::linalg_eigh.eigvals(Tensor self, str UPLO="L", *, Tensor(a!) eigvals, Tensor(b!) eigvecs) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_eigh_out(@ByRef Tensor eigvals, @ByRef Tensor eigvecs, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::string_view(\"L\")") @Cast("c10::string_view*") Pointer UPLO);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_eigh_out(@ByRef Tensor eigvals, @ByRef Tensor eigvecs, @Const @ByRef Tensor self);

// aten::linalg_eigh.eigvals(Tensor self, str UPLO="L", *, Tensor(a!) eigvals, Tensor(b!) eigvecs) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_eigh_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer UPLO, @ByRef Tensor eigvals, @ByRef Tensor eigvecs);

// aten::linalg_eigvalsh(Tensor self, str UPLO="L") -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_eigvalsh(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::string_view(\"L\")") @Cast("c10::string_view*") Pointer UPLO);
@Namespace("at") public static native @ByVal Tensor linalg_eigvalsh(@Const @ByRef Tensor self);

// aten::linalg_eigvalsh.out(Tensor self, str UPLO='L', *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_eigvalsh_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::string_view(\"L\")") @Cast("c10::string_view*") Pointer UPLO);
@Namespace("at") public static native @ByRef Tensor linalg_eigvalsh_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::linalg_eigvalsh.out(Tensor self, str UPLO='L', *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_eigvalsh_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer UPLO, @ByRef Tensor out);

// aten::linalg_householder_product(Tensor input, Tensor tau) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_householder_product(@Const @ByRef Tensor input, @Const @ByRef Tensor tau);

// aten::linalg_householder_product.out(Tensor input, Tensor tau, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_householder_product_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor tau);

// aten::linalg_householder_product.out(Tensor input, Tensor tau, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_householder_product_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor tau, @ByRef Tensor out);

// aten::_linalg_inv_out_helper_(Tensor(a!) self, Tensor(b!) infos_lu, Tensor(c!) infos_getri) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _linalg_inv_out_helper_(@ByRef Tensor self, @ByRef Tensor infos_lu, @ByRef Tensor infos_getri);

// aten::linalg_inv_ex(Tensor self, *, bool check_errors=False) -> (Tensor inverse, Tensor info)
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_inv_ex(@Const @ByRef Tensor self, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_inv_ex(@Const @ByRef Tensor self);

// aten::linalg_inv_ex.inverse(Tensor self, *, bool check_errors=False, Tensor(a!) inverse, Tensor(b!) info) -> (Tensor(a!) inverse, Tensor(b!) info)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_inv_ex_out(@ByRef Tensor inverse, @ByRef Tensor info, @Const @ByRef Tensor self, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_inv_ex_out(@ByRef Tensor inverse, @ByRef Tensor info, @Const @ByRef Tensor self);

// aten::linalg_inv_ex.inverse(Tensor self, *, bool check_errors=False, Tensor(a!) inverse, Tensor(b!) info) -> (Tensor(a!) inverse, Tensor(b!) info)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_inv_ex_outf(@Const @ByRef Tensor self, @Cast("bool") boolean check_errors, @ByRef Tensor inverse, @ByRef Tensor info);

// aten::linalg_inv(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_inv(@Const @ByRef Tensor self);

// aten::linalg_inv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_inv_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::linalg_inv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_inv_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::inner(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor inner(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::inner.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor inner_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::inner.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor inner_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::outer(Tensor self, Tensor vec2) -> Tensor
@Namespace("at") public static native @ByVal Tensor outer(@Const @ByRef Tensor self, @Const @ByRef Tensor vec2);

// aten::outer.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor outer_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor vec2);

// aten::outer.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor outer_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor vec2, @ByRef Tensor out);

// aten::ger(Tensor self, Tensor vec2) -> Tensor
@Namespace("at") public static native @ByVal Tensor ger(@Const @ByRef Tensor self, @Const @ByRef Tensor vec2);

// aten::ger.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ger_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor vec2);

// aten::ger.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ger_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor vec2, @ByRef Tensor out);

// aten::linalg_norm(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_norm(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional ord, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor linalg_norm(@Const @ByRef Tensor self);

// aten::linalg_norm.ord_str(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_norm(@Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer ord, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor linalg_norm(@Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer ord);

// aten::linalg_norm.out(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional ord, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor linalg_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::linalg_norm.out(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional ord, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::linalg_norm.ord_str_out(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer ord, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor linalg_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer ord);

// aten::linalg_norm.ord_str_out(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_norm_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer ord, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::linalg_vector_norm(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_vector_norm(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar ord, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor linalg_vector_norm(@Const @ByRef Tensor self);

// aten::linalg_vector_norm.out(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_vector_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar ord, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor linalg_vector_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::linalg_vector_norm.out(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_vector_norm_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar ord, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::linalg_matrix_norm(Tensor self, Scalar ord, int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matrix_norm(@Const @ByRef Tensor self, @Const @ByRef Scalar ord, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_norm(@Const @ByRef Tensor self, @Const @ByRef Scalar ord);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_norm(@Const @ByRef Tensor self, @Const @ByRef Scalar ord, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);

// aten::linalg_matrix_norm.out(Tensor self, Scalar ord, int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar ord, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar ord);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar ord, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);

// aten::linalg_matrix_norm.out(Tensor self, Scalar ord, int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar ord, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar ord, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::linalg_matrix_norm.str_ord(Tensor self, str ord='fro', int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matrix_norm(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::string_view(\"fro\")") @Cast("c10::string_view*") Pointer ord, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_norm(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_norm(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::string_view(\"fro\")") @Cast("c10::string_view*") Pointer ord, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);

// aten::linalg_matrix_norm.str_ord_out(Tensor self, str ord='fro', int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::string_view(\"fro\")") @Cast("c10::string_view*") Pointer ord, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::string_view(\"fro\")") @Cast("c10::string_view*") Pointer ord, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);

// aten::linalg_matrix_norm.str_ord_out(Tensor self, str ord='fro', int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer ord, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer ord, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::linalg_svd.U(Tensor self, bool full_matrices=True, *, Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_svd_out(@ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor Vh, @Const @ByRef Tensor self, @Cast("bool") boolean full_matrices/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_svd_out(@ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor Vh, @Const @ByRef Tensor self);

// aten::linalg_svd.U(Tensor self, bool full_matrices=True, *, Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_svd_outf(@Const @ByRef Tensor self, @Cast("bool") boolean full_matrices, @ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor Vh);

// aten::linalg_svd(Tensor self, bool full_matrices=True) -> (Tensor U, Tensor S, Tensor Vh)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple linalg_svd(@Const @ByRef Tensor self, @Cast("bool") boolean full_matrices/*=true*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple linalg_svd(@Const @ByRef Tensor self);

// aten::linalg_svdvals(Tensor input) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_svdvals(@Const @ByRef Tensor input);

// aten::linalg_svdvals.out(Tensor input, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_svdvals_out(@ByRef Tensor out, @Const @ByRef Tensor input);

// aten::linalg_svdvals.out(Tensor input, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_svdvals_outf(@Const @ByRef Tensor input, @ByRef Tensor out);

// aten::linalg_cond(Tensor self, Scalar? p=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_cond(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional p);
@Namespace("at") public static native @ByVal Tensor linalg_cond(@Const @ByRef Tensor self);

// aten::linalg_cond.out(Tensor self, Scalar? p=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_cond_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional p);
@Namespace("at") public static native @ByRef Tensor linalg_cond_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::linalg_cond.out(Tensor self, Scalar? p=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_cond_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByRef Tensor out);

// aten::linalg_cond.p_str(Tensor self, str p) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_cond(@Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer p);

// aten::linalg_cond.p_str_out(Tensor self, str p, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_cond_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer p);

// aten::linalg_cond.p_str_out(Tensor self, str p, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_cond_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer p, @ByRef Tensor out);

// aten::linalg_pinv(Tensor self, float rcond=1e-15, bool hermitian=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self, double rcond/*=1e-15*/, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self);

// aten::linalg_pinv.rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self, @Const @ByRef Tensor rcond, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self, @Const @ByRef Tensor rcond);

// aten::linalg_pinv.out(Tensor self, float rcond=1e-15, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, double rcond/*=1e-15*/, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::linalg_pinv.out(Tensor self, float rcond=1e-15, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_pinv_outf(@Const @ByRef Tensor self, double rcond, @Cast("bool") boolean hermitian, @ByRef Tensor out);

// aten::linalg_pinv.out_rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor rcond, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor rcond);

// aten::linalg_pinv.out_rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_pinv_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor rcond, @Cast("bool") boolean hermitian, @ByRef Tensor out);

// aten::linalg_solve(Tensor input, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_solve(@Const @ByRef Tensor input, @Const @ByRef Tensor other);

// aten::linalg_solve.out(Tensor input, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_solve_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor other);

// aten::linalg_solve.out(Tensor input, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_solve_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::linalg_tensorinv(Tensor self, int ind=2) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_tensorinv(@Const @ByRef Tensor self, @Cast("int64_t") long ind/*=2*/);
@Namespace("at") public static native @ByVal Tensor linalg_tensorinv(@Const @ByRef Tensor self);

// aten::linalg_tensorinv.out(Tensor self, int ind=2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_tensorinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long ind/*=2*/);
@Namespace("at") public static native @ByRef Tensor linalg_tensorinv_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::linalg_tensorinv.out(Tensor self, int ind=2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_tensorinv_outf(@Const @ByRef Tensor self, @Cast("int64_t") long ind, @ByRef Tensor out);

// aten::linalg_tensorsolve(Tensor self, Tensor other, int[]? dims=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_tensorsolve(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dims);
@Namespace("at") public static native @ByVal Tensor linalg_tensorsolve(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::linalg_tensorsolve.out(Tensor self, Tensor other, int[]? dims=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_tensorsolve_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::optional<at::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dims);
@Namespace("at") public static native @ByRef Tensor linalg_tensorsolve_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::linalg_tensorsolve.out(Tensor self, Tensor other, int[]? dims=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_tensorsolve_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal LongArrayRefOptional dims, @ByRef Tensor out);

// aten::linalg_qr(Tensor self, str mode='reduced') -> (Tensor Q, Tensor R)
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_qr(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::string_view(\"reduced\")") @Cast("c10::string_view*") Pointer mode);
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_qr(@Const @ByRef Tensor self);

// aten::linalg_qr.out(Tensor self, str mode='reduced', *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_qr_out(@ByRef Tensor Q, @ByRef Tensor R, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::string_view(\"reduced\")") @Cast("c10::string_view*") Pointer mode);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_qr_out(@ByRef Tensor Q, @ByRef Tensor R, @Const @ByRef Tensor self);

// aten::linalg_qr.out(Tensor self, str mode='reduced', *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_qr_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer mode, @ByRef Tensor Q, @ByRef Tensor R);

// aten::_linalg_qr_helper(Tensor self, str mode) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _linalg_qr_helper(@Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer mode);

// aten::linalg_matrix_power(Tensor self, int n) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matrix_power(@Const @ByRef Tensor self, @Cast("int64_t") long n);

// aten::linalg_matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_power_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long n);

// aten::linalg_matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_power_outf(@Const @ByRef Tensor self, @Cast("int64_t") long n, @ByRef Tensor out);

// aten::linalg_matrix_rank(Tensor self, float? tol=None, bool hermitian=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matrix_rank(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional tol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_rank(@Const @ByRef Tensor self);

// aten::linalg_matrix_rank.out(Tensor self, float? tol=None, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional tol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_out(@ByRef Tensor out, @Const @ByRef Tensor self);

// aten::linalg_matrix_rank.out(Tensor self, float? tol=None, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_outf(@Const @ByRef Tensor self, @ByVal DoubleOptional tol, @Cast("bool") boolean hermitian, @ByRef Tensor out);

// aten::linalg_matrix_rank.tol_tensor(Tensor input, Tensor tol, bool hermitian=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matrix_rank(@Const @ByRef Tensor input, @Const @ByRef Tensor tol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_rank(@Const @ByRef Tensor input, @Const @ByRef Tensor tol);

// aten::linalg_matrix_rank.out_tol_tensor(Tensor input, Tensor tol, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor tol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor tol);

// aten::linalg_matrix_rank.out_tol_tensor(Tensor input, Tensor tol, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor tol, @Cast("bool") boolean hermitian, @ByRef Tensor out);

// aten::linalg_multi_dot(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_multi_dot(@ByVal TensorArrayRef tensors);

// aten::linalg_multi_dot.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_multi_dot_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);

// aten::linalg_multi_dot.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_multi_dot_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);

// aten::_test_serialization_subcmul(Tensor self, Tensor other, Scalar alpha=1) -> Tensor


// aten::_test_optional_intlist(Tensor values, int[]? addends) -> Tensor


// aten::_test_optional_filled_intlist(Tensor values, int[2]? addends) -> Tensor


// aten::_test_optional_floatlist(Tensor values, float[]? addends) -> Tensor


// aten::_test_string_default(Tensor dummy, str a="\"'\\", str b='"\'\\') -> Tensor


// aten::_test_ambiguous_defaults.a(Tensor dummy, int a=1, int b=1) -> Tensor


// aten::_test_ambiguous_defaults.b(Tensor dummy, int a=2, str b="2") -> Tensor


// aten::segment_reduce(Tensor data, str reduce, *, Tensor? lengths=None, Tensor? indices=None, int axis=0, bool unsafe=False, Scalar? initial=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor segment_reduce(@Const @ByRef Tensor data, @ByVal @Cast("c10::string_view*") Pointer reduce, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional lengths, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional indices, @Cast("int64_t") long axis/*=0*/, @Cast("bool") boolean unsafe/*=false*/, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional initial);
@Namespace("at") public static native @ByVal Tensor segment_reduce(@Const @ByRef Tensor data, @ByVal @Cast("c10::string_view*") Pointer reduce);

// aten::_segment_reduce_backward(Tensor grad, Tensor output, Tensor data, str reduce, *, Tensor? lengths=None, int axis=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor _segment_reduce_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor output, @Const @ByRef Tensor data, @ByVal @Cast("c10::string_view*") Pointer reduce, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional lengths, @Cast("int64_t") long axis/*=0*/);
@Namespace("at") public static native @ByVal Tensor _segment_reduce_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor output, @Const @ByRef Tensor data, @ByVal @Cast("c10::string_view*") Pointer reduce);

// aten::pad_sequence(Tensor[] sequences, bool batch_first=False, float padding_value=0.0) -> Tensor
@Namespace("at") public static native @ByVal Tensor pad_sequence(@ByVal TensorArrayRef sequences, @Cast("bool") boolean batch_first/*=false*/, double padding_value/*=0.0*/);
@Namespace("at") public static native @ByVal Tensor pad_sequence(@ByVal TensorArrayRef sequences);

// aten::flatten_dense_tensors(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor flatten_dense_tensors(@ByVal TensorArrayRef tensors);

// aten::unflatten_dense_tensors(Tensor flat, Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unflatten_dense_tensors(@Const @ByRef Tensor flat, @ByVal TensorArrayRef tensors);

// Special C++ only overloads for std()-like functions (See gh-40287)
// These are needed because int -> bool conversion takes precedence over int -> IntArrayRef
// So, for example std(0) would select the std(unbiased=False) overload
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, int dim);
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, int dim);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, int dim);
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, int dim);

@Namespace("at::detail") public static native void noopDelete(Pointer arg0);


// Targeting ../TensorMaker.java



@Namespace("at") public static native @ByVal @NoException(true) TensorMaker for_blob(Pointer data, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("at") public static native @ByVal @NoException(true) TensorMaker for_blob(Pointer data, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sizes);

@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @Const @ByRef Deleter deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options,
    @Const @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional target_device);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @Const @ByRef Deleter deleter);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @Const @ByRef Deleter deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options,
    @Const @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional target_device);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @Const @ByRef Deleter deleter);

@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @Const @ByRef Deleter deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @Const @ByRef Deleter deleter);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @Const @ByRef Deleter deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @Const @ByRef Deleter deleter);

@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... strides);

@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sizes);

@Namespace("at") public static native @Cast("int64_t") long numel(@Const @ByRef Tensor tensor);

@Namespace("at") public static native @Cast("int64_t") long size(@Const @ByRef Tensor tensor, @Cast("int64_t") long dim);

@Namespace("at") public static native @Cast("int64_t") long stride(@Const @ByRef Tensor tensor, @Cast("int64_t") long dim);

@Namespace("at") public static native @Cast("bool") boolean is_complex(@Const @ByRef Tensor tensor);

@Namespace("at") public static native @Cast("bool") boolean is_floating_point(@Const @ByRef Tensor tensor);

@Namespace("at") public static native @Cast("bool") boolean is_signed(@Const @ByRef Tensor tensor);

@Namespace("at") public static native @Cast("bool") boolean is_inference(@Const @ByRef Tensor tensor);

@Namespace("at") public static native @Cast("bool") boolean is_conj(@Const @ByRef Tensor tensor);

@Namespace("at") public static native @ByVal Tensor conj(@Const @ByRef Tensor tensor);

@Namespace("at") public static native @Cast("bool") boolean is_neg(@Const @ByRef Tensor tensor);




// Parsed from ATen/NamedTensor.h

// #include <ATen/core/NamedTensor.h>


// Parsed from ATen/NamedTensorUtils.h

// #pragma once
// #include <ATen/NamedTensor.h>
// #include <ATen/TensorNames.h>

// #include <ATen/core/Tensor.h>
// #include <ATen/core/DimVector.h>
// #include <functional>

@Namespace("at") public static native @Cast("bool") boolean has_names(@ByVal TensorArrayRef tensors);

// Converts dim to an positional index. Errors if `dim` cannot be used to
// refer to any dimension of tensor.
@Namespace("at") public static native @Cast("int64_t") long dimname_to_position(@Const @ByRef Tensor tensor, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector dimnames_to_positions(@Const @ByRef Tensor tensor, @ByVal DimnameArrayRef dims);

// Unifies two DimnameList to produce a third. This is useful for implementing
// the named inference rule for binary broadcasting operations like add.
//
// There are three main constraints:
// 1) Check matching: Names must match positionally from the right.
// 2) Check misaligned: If a name `n` is in `names`, then it must appear at
//    the same index from the right in other.
// 3) The output names are obtained by unifying the names individually from the right.
@Namespace("at") public static native @StdMove DimnameVector unify_from_right(@ByVal DimnameArrayRef names, @ByVal DimnameArrayRef other, @Cast("const char*") BytePointer action/*="broadcast"*/);
@Namespace("at") public static native @StdMove DimnameVector unify_from_right(@ByVal DimnameArrayRef names, @ByVal DimnameArrayRef other);
@Namespace("at") public static native @StdMove DimnameVector unify_from_right(@ByVal DimnameArrayRef names, @ByVal DimnameArrayRef other, String action/*="broadcast"*/);

@Namespace("at") public static native void reportNYIDimnameOverload(@Cast("const char*") BytePointer op_name);
@Namespace("at") public static native void reportNYIDimnameOverload(String op_name);

// [NOTE] Writing name inference rules
//
// Operators that support named tensors are either composed of operations that
// support named tensors or implement some name inference rule. An op that
// implements its own name inference rule generally looks like the following:
//
// Tensor op(...) {
//   perform_shape_checks(...);
//   # (1)
//   auto maybe_outnames = compute_outnames(...);
//   auto result = [&]() {
//     NoNamesGuard guard;
//     return op_impl(...);
//   }();
//   # (2)
//   propagate_names_if_nonempty(result, maybe_outnames);
//
// Each op has (1) a compute outnames step and (2) a propagate names step.
//
// compute_outnames is responsible for checking that input names match and
// determining what the output names should be. It returns either:
// - {} (if the inputs tensors are all unnamed)
// - non-empty outnames.
//
// propagate_names_if_nonempty propagates the outnames if they exist to the result
// tensors.
//
// The {} case is an optimization; if the user does not use named tensors they
// pay no perf cost for it.

// Propagates `names` to `result` if `names` is not empty.
// `names` can be empty; see [NOTE] Writing name inference rules
// If `names` is not empty, `names.size()` should equal `result.dim()`.
// When in doubt, use this overload instead of the others.
@Namespace("at::namedinference") public static native @Const @ByRef Tensor propagate_names_if_nonempty(
    @Const @ByRef Tensor result,
    @ByVal DimnameArrayRef maybe_names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native @Const @ByRef Tensor propagate_names_if_nonempty(
    @Const @ByRef Tensor result,
    @ByVal DimnameArrayRef maybe_names);

// Propagates `names` to `result`. Only use this if we are certain that there are
// names to propagate (that names is not empty).
@Namespace("at::namedinference") public static native @Const @ByRef Tensor propagate_names(
    @Const @ByRef Tensor result,
    @ByVal DimnameArrayRef names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native @Const @ByRef Tensor propagate_names(
    @Const @ByRef Tensor result,
    @ByVal DimnameArrayRef names);

// Propagates all names from src to result.
@Namespace("at::namedinference") public static native void propagate_names(@Const @ByRef Tensor result, @Const @ByRef Tensor src);

// Propagates all names except for those at the excluded_idxs.
@Namespace("at::namedinference") public static native void propagate_names_except(@Const @ByRef Tensor result, @Const @ByRef Tensor src, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef excluded_idxs);
@Namespace("at::namedinference") public static native void propagate_names_except(@Const @ByRef Tensor result, @Const @ByRef Tensor src, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... excluded_idxs);

// Used for reduction ops that have a `keepdim` arg.
@Namespace("at::namedinference") public static native void propagate_names_for_reduction(@Const @ByRef Tensor result, @Const @ByRef Tensor src, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef excluded_idxs, @Cast("bool") boolean keepdim);
@Namespace("at::namedinference") public static native void propagate_names_for_reduction(@Const @ByRef Tensor result, @Const @ByRef Tensor src, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] excluded_idxs, @Cast("bool") boolean keepdim);

@Namespace("at::namedinference") public static native void propagate_names_for_expand(@Const @ByRef Tensor result, @Const @ByRef Tensor self);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_cat_outnames(@ByVal TensorArrayRef tensors);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_broadcast_outnames(
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other);

@Namespace("at::namedinference") public static native @StdMove DimnameVector broadcast_to_outnames(
    @Const @ByRef Tensor tensor,
    @Const @ByRef Tensor reference_tensor,
    @Cast("const char*") BytePointer op_name);
@Namespace("at::namedinference") public static native @StdMove DimnameVector broadcast_to_outnames(
    @Const @ByRef Tensor tensor,
    @Const @ByRef Tensor reference_tensor,
    String op_name);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_matmul_outnames(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_cdist_outnames(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_bmm_outnames(
    @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_squeeze_outnames(@Const @ByRef Tensor tensor);



// TensorImpl* overloads for Legacy TH/THC code. Use these sparingly.

@Namespace("at::namedinference") public static native TensorImpl propagate_names_if_nonempty(
    TensorImpl result,
    @ByVal DimnameArrayRef maybe_names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native TensorImpl propagate_names_if_nonempty(
    TensorImpl result,
    @ByVal DimnameArrayRef maybe_names);

@Namespace("at::namedinference") public static native TensorImpl propagate_names(
    TensorImpl result,
    @ByVal DimnameArrayRef names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native TensorImpl propagate_names(
    TensorImpl result,
    @ByVal DimnameArrayRef names);

@Namespace("at::namedinference") public static native void propagate_names(TensorImpl result,TensorImpl src);

// result = m1 @ m2 + bias
@Namespace("at::namedinference") public static native @StdMove DimnameVector propagate_names_for_addmm(
    @Const @ByRef Tensor m1,
    @Const @ByRef Tensor m2,
    @Const @ByRef Tensor bias);

@Namespace("at::namedinference") public static native @StdMove DimnameVector propagate_names_for_addmv(
    @Const @ByRef Tensor mat,
    @Const @ByRef Tensor vec,
    @Const @ByRef Tensor bias);

@Namespace("at::namedinference") public static native void check_names_for_dot(TensorImpl vec1, TensorImpl vec2);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_baddbmm_outnames(
    @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other,
    @Const @ByRef Tensor bias);

@Namespace("at::namedinference") public static native @Cast("bool") boolean are_names_equal(TensorImpl self, TensorImpl other);

 // namespace namedinference

 // namespace at


// Parsed from ATen/ScalarOps.h

// #pragma once

// #include <c10/core/Scalar.h>
// #include <ATen/Tensor.h>
// #include <ATen/Functions.h>
// When filling a number to 1-element CPU tensor, we want to skip
// everything but manipulate data ptr directly.
// Ideally this fast pass should be implemented in TensorIterator,
// but we also want to skip compute_types which in not avoidable
// in TensorIterator for now.

@Namespace("at::detail") public static native @ByVal Tensor scalar_tensor_static(@Const @ByRef Scalar s, @ByVal ScalarTypeOptional dtype_opt, @ByVal DeviceOptional device_opt);
 // namespace detail
 // namespace at

// This is in the c10 namespace because we use ADL to find the functions in it.

// FIXME: this should be (and was) Scalar::toTensor, but there is currently no way
// to implement this without going through Derived Types (which are not part of core).
@Namespace("c10") public static native @ByVal Tensor scalar_to_tensor(@Const @ByRef Scalar s, @Const @ByVal(nullValue = "c10::Device(at::kCPU)") Device device);
@Namespace("c10") public static native @ByVal Tensor scalar_to_tensor(@Const @ByRef Scalar s);

 // namespace c10

@Namespace("at::native") public static native @ByVal Tensor wrapped_scalar_tensor(@Const @ByRef Scalar scalar, @Const @ByVal(nullValue = "c10::Device(at::kCPU)") Device device);
@Namespace("at::native") public static native @ByVal Tensor wrapped_scalar_tensor(@Const @ByRef Scalar scalar);

 // namespace native
 // namsepace at


// Parsed from ATen/SequenceNumber.h

// #pragma once

// #include <cstdint>
// #include <torch/csrc/WindowsTorchApiMacro.h>

// A simple thread local enumeration, used to link forward and backward pass
// ops and is used by autograd and observers framework

@Namespace("at::sequence_number") public static native @Cast("uint64_t") long peek();
@Namespace("at::sequence_number") public static native @Cast("uint64_t") long get_and_increment();

 // namespace sequence_number
 // namespace at


// Parsed from ATen/TensorIndexing.h

// #pragma once

// #include <c10/util/Optional.h>
// #include <ATen/core/TensorBody.h>
// #include <ATen/ExpandUtils.h>
// #include <ATen/Functions.h>
// #include <ATen/ScalarOps.h>

// TODO: try to remove this
// There is some back story, see https://github.com/pytorch/pytorch/issues/48684
// #include <ATen/NativeFunctions.h>

// #include <ATen/core/List.h>

@Namespace("at::indexing") @MemberGetter public static native @Cast("const int64_t") long INDEX_MAX();
@Namespace("at::indexing") @MemberGetter public static native @Cast("const int64_t") long INDEX_MIN();

@Namespace("at::indexing") public enum TensorIndexType { None(0), Ellipsis(1), Integer(2), Boolean(3), Slice(4), Tensor(5);

    public final int value;
    private TensorIndexType(int v) { this.value = v; }
    private TensorIndexType(TensorIndexType e) { this.value = e.value; }
    public TensorIndexType intern() { for (TensorIndexType e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("at::indexing") @MemberGetter public static native @ByRef @Cast("const c10::nullopt_t*") Pointer None();
// Targeting ../EllipsisIndexType.java


@Namespace("at::indexing") @MemberGetter public static native @Const @ByRef EllipsisIndexType Ellipsis();
// Targeting ../Slice.java



@Namespace("at::indexing") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer stream, @Const @ByRef Slice slice);
// Targeting ../TensorIndex.java



@Namespace("at::indexing") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer stream, @Const @ByRef TensorIndex tensor_index);
@Namespace("at::indexing") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer stream, @Const @ByRef TensorIndexVector tensor_indices);
@Namespace("at::indexing::impl") public static native @ByVal Tensor applySlice(
    @Const @ByRef Tensor self,
    @Cast("int64_t") long dim,
    @Cast("int64_t") long start,
    @Cast("int64_t") long stop,
    @Cast("int64_t") long step,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device self_device,
    @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef self_sizes);
@Namespace("at::indexing::impl") public static native @ByVal Tensor applySlice(
    @Const @ByRef Tensor self,
    @Cast("int64_t") long dim,
    @Cast("int64_t") long start,
    @Cast("int64_t") long stop,
    @Cast("int64_t") long step,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device self_device,
    @ByRef @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... self_sizes);

@Namespace("at::indexing::impl") public static native @ByVal Tensor applySelect(
    @Const @ByRef Tensor self,
    @Cast("int64_t") long dim,
    @Cast("int64_t") long index,
    @Cast("int64_t") long real_dim,
    @Const @ByRef Device self_device,
    @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef self_sizes);
@Namespace("at::indexing::impl") public static native @ByVal Tensor applySelect(
    @Const @ByRef Tensor self,
    @Cast("int64_t") long dim,
    @Cast("int64_t") long index,
    @Cast("int64_t") long real_dim,
    @Const @ByRef Device self_device,
    @ByRef @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... self_sizes);

@Namespace("at::indexing::impl") public static native @ByVal Tensor boolToIndexingTensorCPUOrCUDA(@Const @ByRef Tensor self, @Cast("bool") boolean value);

@Namespace("at::indexing::impl") public static native @ByVal Tensor boolToIndexingTensorNonNativeDeviceType(@Const @ByRef Tensor self, @Cast("bool") boolean value);

@Namespace("at::indexing::impl") public static native @ByVal Tensor boolToIndexingTensor(@Const @ByRef Tensor self, @Cast("bool") boolean value, @Const @ByRef Device self_device);

@Namespace("at::indexing::impl") public static native @ByVal Tensor scalarToTensorNonNativeDeviceType(@Const @ByRef Scalar v, @Const @ByRef TensorOptions options);

@Namespace("at::indexing::impl") public static native void recordTensorIndex(@Const @ByRef Tensor tensor, @ByRef TensorVector outIndices, @Cast("int64_t*") LongPointer dim_ptr);
@Namespace("at::indexing::impl") public static native void recordTensorIndex(@Const @ByRef Tensor tensor, @ByRef TensorVector outIndices, @Cast("int64_t*") LongBuffer dim_ptr);
@Namespace("at::indexing::impl") public static native void recordTensorIndex(@Const @ByRef Tensor tensor, @ByRef TensorVector outIndices, @Cast("int64_t*") long[] dim_ptr);

// NOTE: Why do we mirror instead of replace the `count_specified_dimensions` function
// in torch/csrc/autograd/python_variable_indexing.cpp? It's because
// `count_specified_dimensions` is on the hot path of Python tensor multi-dim indexing
// (i.e. it's called by `applySlicing` which is called by `THPVariable_getitem` /
// `THPVariable_setitem` when handling indexing of more than one dimension). If we were
// to merge the Python/C++ `count_specified_dimensions` function, on the Python side
// we would have to construct a `std::vector` container to be consumed by the C++
// `count_specified_dimensions` function, which adds 100s of nanoseconds overhead and
// is undesirable.
@Namespace("at::indexing::impl") public static native @Cast("int64_t") long count_specified_dimensions(@Const @ByRef TensorIndexArrayRef indices);
 // namespace impl

// NOTE: Many functions below are only for consumption from Python indexing
// implementation, they include:
//
// - `Tensor scalarToTensor(...)`
// - `IntArrayRef slicePrefix1sSize(...)`
// - `void copy_to(...)`
// - `Tensor handleDimInMultiDimIndexing(...)`
// - `Tensor dispatch_index(...)`
// - `Tensor dispatch_index_put_(...)`
// - `Tensor get_item(...)`
// - `void set_item(...)`
//
// The rest of the functions are in `at::indexing::impl` namespace, signifying
// that they shouldn't be used from Python indexing implementation.
@Namespace("at::indexing") public static native @ByVal Tensor scalarToTensor(@Const @ByRef Scalar v, @Const @ByRef TensorOptions options, @Const @ByRef Device self_device);

// To match numpy semantics:
// As a special case for backwards compatibility,
// strip away unit dimensions from the left of 'src'
@Namespace("at::indexing") public static native @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef slicePrefix1sSize(@ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);

@Namespace("at::indexing") public static native void copy_to(@Const @ByRef Tensor dst, @Const @ByRef Tensor src);

// See NOTE [ Setting `disable_slice_optimization` when calling C++ tensor indexing functions from Python ]
@Namespace("at::indexing") public static native @ByVal Tensor handleDimInMultiDimIndexing(
    @Const @ByRef Tensor prev_dim_result,
    @Const @ByRef Tensor original_tensor,
    @Const @ByRef TensorIndex index,
    @Cast("int64_t*") LongPointer dim_ptr,
    @Cast("int64_t*") LongPointer specified_dims_ptr,
    @Cast("int64_t") long real_dim,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device original_tensor_device,
    @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef prev_dim_result_sizes);
@Namespace("at::indexing") public static native @ByVal Tensor handleDimInMultiDimIndexing(
    @Const @ByRef Tensor prev_dim_result,
    @Const @ByRef Tensor original_tensor,
    @Const @ByRef TensorIndex index,
    @Cast("int64_t*") LongBuffer dim_ptr,
    @Cast("int64_t*") LongBuffer specified_dims_ptr,
    @Cast("int64_t") long real_dim,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device original_tensor_device,
    @ByRef @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... prev_dim_result_sizes);
@Namespace("at::indexing") public static native @ByVal Tensor handleDimInMultiDimIndexing(
    @Const @ByRef Tensor prev_dim_result,
    @Const @ByRef Tensor original_tensor,
    @Const @ByRef TensorIndex index,
    @Cast("int64_t*") long[] dim_ptr,
    @Cast("int64_t*") long[] specified_dims_ptr,
    @Cast("int64_t") long real_dim,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device original_tensor_device,
    @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef prev_dim_result_sizes);
@Namespace("at::indexing") public static native @ByVal Tensor handleDimInMultiDimIndexing(
    @Const @ByRef Tensor prev_dim_result,
    @Const @ByRef Tensor original_tensor,
    @Const @ByRef TensorIndex index,
    @Cast("int64_t*") LongPointer dim_ptr,
    @Cast("int64_t*") LongPointer specified_dims_ptr,
    @Cast("int64_t") long real_dim,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device original_tensor_device,
    @ByRef @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... prev_dim_result_sizes);
@Namespace("at::indexing") public static native @ByVal Tensor handleDimInMultiDimIndexing(
    @Const @ByRef Tensor prev_dim_result,
    @Const @ByRef Tensor original_tensor,
    @Const @ByRef TensorIndex index,
    @Cast("int64_t*") LongBuffer dim_ptr,
    @Cast("int64_t*") LongBuffer specified_dims_ptr,
    @Cast("int64_t") long real_dim,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device original_tensor_device,
    @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef prev_dim_result_sizes);
@Namespace("at::indexing") public static native @ByVal Tensor handleDimInMultiDimIndexing(
    @Const @ByRef Tensor prev_dim_result,
    @Const @ByRef Tensor original_tensor,
    @Const @ByRef TensorIndex index,
    @Cast("int64_t*") long[] dim_ptr,
    @Cast("int64_t*") long[] specified_dims_ptr,
    @Cast("int64_t") long real_dim,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device original_tensor_device,
    @ByRef @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... prev_dim_result_sizes);
// This mirrors `applySlicing` in torch/csrc/autograd/python_variable_indexing.cpp
@Namespace("at::indexing::impl") public static native @ByVal Tensor applySlicing(
    @Const @ByRef Tensor self,
    @Const @ByRef TensorIndexArrayRef indices,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device self_device,
    @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef self_sizes);
@Namespace("at::indexing::impl") public static native @ByVal Tensor applySlicing(
    @Const @ByRef Tensor self,
    @Const @ByRef TensorIndexArrayRef indices,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device self_device,
    @ByRef @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... self_sizes);
 // namespace impl

@Namespace("at::indexing") public static native @ByVal Tensor dispatch_index(@Const @ByRef Tensor self, @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector indices);

@Namespace("at::indexing") public static native @ByVal Tensor dispatch_index_put_(@ByRef Tensor self, @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector indices, @Const @ByRef Tensor value);

// NOTE [ Setting `disable_slice_optimization` when calling C++ tensor indexing functions from Python ]
//
// Question: When should we set `disable_slice_optimization` to `true` when calling C++ tensor indexing
// functions from Python indexing code?
//
// Answer: What "slice optimization" means: when we have a slicing expression like `x[0:5, 0]`, where the sliced tensor
// was of size 5 in dimension 0, we would skip dispatching the actual slice call as an optimization. However, here are
// the cases where we DON'T want this optimization:
//
// 1. When we are doing 1-D slicing (e.g. `tensor[:]`).
//    Reason: we always return a shallow copy for expressions such as `tensor[:]` / `tensor[...]` / `tensor[:, :]`.
//    (Note that for `tensor[:, :]`, we return an alias of `tensor` by doing the following:
//    ```
//    Tensor sliced = impl::applySlicing(self, indices, tensorIndices, disable_slice_optimization, self_device, self_sizes);
//    if (tensorIndices.empty()) {
//      if (sliced.is_same(self)) {
//        // ensure we return a shallow copy for things like x[...]
//        sliced = at::alias(sliced);
//      }
//      return sliced;
//    }
//    ```)
// 2. When we are doing JIT tracing.
//    Reason: JIT tracing needs the `self.slice(...)` call to properly trace the slice operation.

// This mirrors `THPVariable_getitem` in torch/csrc/autograd/python_variable_indexing.cpp
// See NOTE [ Setting `disable_slice_optimization` when calling C++ tensor indexing functions from Python ]
@Namespace("at::indexing") public static native @ByVal Tensor get_item(@Const @ByRef Tensor self, @Const @ByRef TensorIndexArrayRef indices, @Cast("bool") boolean disable_slice_optimization/*=false*/);
@Namespace("at::indexing") public static native @ByVal Tensor get_item(@Const @ByRef Tensor self, @Const @ByRef TensorIndexArrayRef indices);

// This mirrors `THPVariable_setitem` in torch/csrc/autograd/python_variable_indexing.cpp
// for "the assigned value is a Tensor" case
// See NOTE [ Setting `disable_slice_optimization` when calling C++ tensor indexing functions from Python ]
@Namespace("at::indexing") public static native void set_item(@Const @ByRef Tensor self, @Const @ByRef TensorIndexArrayRef indices, @Const @ByRef Tensor value, @Cast("bool") boolean disable_slice_optimization/*=false*/);
@Namespace("at::indexing") public static native void set_item(@Const @ByRef Tensor self, @Const @ByRef TensorIndexArrayRef indices, @Const @ByRef Tensor value);

 // namespace indexing
 // namespace at


// Parsed from ATen/TensorOperators.h

// #pragma once

// #include <c10/core/Scalar.h>
// #include <ATen/Tensor.h>

// #include <string>
// #include <stdexcept>





















// #define AT_FORALL_BINARY_OPS(_)
// _(+,x.add(y), y.add(x))
// _(*,x.mul(y), y.mul(x))
// _(-,x.sub(y), ::at::empty_like(y, at::MemoryFormat::Preserve).fi_(x).sub_(y))
// _(/,x.div(y), ::at::empty_like(y, at::MemoryFormat::Preserve).fi_(x).div_(y))
// _(%,x.remainder(y), ::at::empty_like(y, at::MemoryFormat::Preserve).fi_(x).remainder_(y))
// _(&,x.bitwise_and(y), y.bitwise_and(x))
// _(|,x.bitwise_or(y), y.bitwise_or(x))
// _(^,x.bitwise_xor(y), y.bitwise_xor(x))
// _(<,x.t(y), y.gt(x))
// _(<=,x.e(y), y.ge(x))
// _(>,x.gt(y),y.t(x))
// _(>=,x.ge(y), y.e(x))
// _(==,x.eq(y), y.eq(x))
// _(!=,x.ne(y), y.ne(x))

// #define DEFINE_OPERATOR(op,body,reverse_scalar_body)
// static inline Tensor operator op(const Tensor & x, const Tensor & y) {
//   return body;
// }
// static inline Tensor operator op(const Tensor & x, const Scalar& y) {
//   return body;
// }
// static inline Tensor operator op(const Scalar& x, const Tensor & y) {
//   return reverse_scalar_body;
// }
@Namespace("at") public static native @ByVal @Name("operator +") Tensor add(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator *") Tensor multiply(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator -") Tensor subtract(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator /") Tensor divide(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator %") Tensor mod(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator %") Tensor mod(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
@Namespace("at") public static native @ByVal @Name("operator %") Tensor mod(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator &") Tensor and(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator &") Tensor and(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
@Namespace("at") public static native @ByVal @Name("operator &") Tensor and(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator |") Tensor or(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator |") Tensor or(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
@Namespace("at") public static native @ByVal @Name("operator |") Tensor or(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator ^") Tensor xor(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator ^") Tensor xor(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
@Namespace("at") public static native @ByVal @Name("operator ^") Tensor xor(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator <") Tensor lessThan(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator <") Tensor lessThan(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
@Namespace("at") public static native @ByVal @Name("operator <") Tensor lessThan(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator <=") Tensor lessThanEquals(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator <=") Tensor lessThanEquals(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
@Namespace("at") public static native @ByVal @Name("operator <=") Tensor lessThanEquals(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator >") Tensor greaterThan(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator >") Tensor greaterThan(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
@Namespace("at") public static native @ByVal @Name("operator >") Tensor greaterThan(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator >=") Tensor greaterThanEquals(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator >=") Tensor greaterThanEquals(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
@Namespace("at") public static native @ByVal @Name("operator >=") Tensor greaterThanEquals(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator ==") Tensor equals(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator ==") Tensor equals(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
@Namespace("at") public static native @ByVal @Name("operator ==") Tensor equals(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator !=") Tensor notEquals(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator !=") Tensor notEquals(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
@Namespace("at") public static native @ByVal @Name("operator !=") Tensor notEquals(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
// #undef DEFINE_OPERATOR
// #undef AT_FORALL_BINARY_OPS




// Parsed from ATen/Version.h

// #include <ATen/Context.h>

/** Returns a detailed string describing the configuration PyTorch. */
@Namespace("at") public static native @StdString BytePointer show_config();

@Namespace("at") public static native @StdString BytePointer get_mkl_version();

@Namespace("at") public static native @StdString BytePointer get_mkldnn_version();

@Namespace("at") public static native @StdString BytePointer get_openmp_version();

@Namespace("at") public static native @StdString BytePointer get_cxx_flags();

  // namespace at


// Parsed from torch/autograd.h

// #pragma once

// #include <torch/csrc/autograd/autograd.h>
// #include <torch/csrc/autograd/custom_function.h>
// #include <torch/csrc/autograd/autograd_not_implemented_fallback.h>


// Parsed from torch/script.h

// #pragma once

// #include <torch/csrc/api/include/torch/types.h>
// #include <torch/csrc/autograd/generated/variable_factories.h>
// #include <torch/csrc/autograd/grad_mode.h>
// #include <torch/csrc/autograd/InferenceMode.h>
// #include <torch/csrc/jit/runtime/custom_operator.h>
// #include <torch/csrc/jit/serialization/import.h>
// #include <torch/csrc/jit/serialization/pickle.h>
// #include <torch/csrc/autograd/custom_function.h>
// #include <torch/custom_class.h>

// #include <ATen/ATen.h>


// Parsed from torch/csrc/onnx/onnx.h

// #pragma once

@Namespace("torch::onnx") public enum OperatorExportTypes {
  ONNX(0), // Strict ONNX export
  ONNX_ATEN(1), // ONNX With ATen op everywhere
  ONNX_ATEN_FALLBACK(2), // ONNX export with ATen fallback
  ONNX_FALLTHROUGH(3);// Export supported ONNX ops. Pass through unsupported ops.

    public final int value;
    private OperatorExportTypes(int v) { this.value = v; }
    private OperatorExportTypes(OperatorExportTypes e) { this.value = e.value; }
    public OperatorExportTypes intern() { for (OperatorExportTypes e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("torch::onnx") public enum TrainingMode {
  EVAL(0), // Inference mode
  PRESERVE(1), // Preserve model state (eval/training)
  TRAINING(2);// Training mode

    public final int value;
    private TrainingMode(int v) { this.value = v; }
    private TrainingMode(TrainingMode e) { this.value = e.value; }
    public TrainingMode intern() { for (TrainingMode e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

// We pin IR version instead of using onnx::IR_VERSION so that the
// test_operators.py will be more stable. Only bump it when
// necessary.


 // namespace onnx
 // namespace torch


// Parsed from torch/csrc/WindowsTorchApiMacro.h

// #pragma once

// #include <c10/macros/Export.h>

// #ifdef _WIN32
// #else
// #define TORCH_PYTHON_API TORCH_API
// #endif


// Parsed from torch/csrc/api/include/torch/imethod.h

// #pragma once
// #include <ATen/core/ivalue.h>
// #include <vector>
// Targeting ../IMethod.java



 // namespace torch


// Parsed from torch/csrc/api/include/torch/types.h

// #pragma once

// #include <ATen/ATen.h>

// #include <c10/util/Optional.h>

// #include <torch/csrc/autograd/generated/variable_factories.h>
// #include <torch/csrc/autograd/variable.h>

// NOTE [ Exposing declarations in `at::` to `torch::` ]
//
// The following line `using namespace at;` is responsible for exposing all declarations in
// `at::` namespace to `torch::` namespace.
//
// According to the rules laid out in
// https://en.cppreference.com/w/cpp/language/qualified_lookup, section "Namespace members":
// ```
// Qualified lookup within the scope of a namespace N first considers all declarations that are
// located in N and all declarations that are located in the inline namespace members of N
// (and, transitively, in their inline namespace members). If there are no declarations in that set
// then it considers declarations in all namespaces named by using-directives found in N and
// in all transitive inline namespace members of N.
// ```
//
// This means that if both `at::` and `torch::` namespaces have a function with the same signature
// (e.g. both `at::func()` and `torch::func()` exist), after `namespace torch { using namespace at; }`,
// when we call `torch::func()`, the `func()` function defined in `torch::` namespace will always
// be called, and the `func()` function defined in `at::` namespace is always hidden. // NOLINT

/** Fixed width dtypes. */

/** Rust-style short dtypes. */
 // namespace torch


// Parsed from torch/csrc/api/include/torch/cuda.h

// #pragma once

// #include <torch/csrc/WindowsTorchApiMacro.h>

// #include <cstdint>
// #include <cstddef>

/** Returns the number of CUDA devices available. */
@Namespace("torch::cuda") public static native @Cast("size_t") @Name("device_count") long cuda_device_count();

/** Returns true if at least one CUDA device is available. */
@Namespace("torch::cuda") public static native @Cast("bool") @Name("is_available") boolean cuda_is_available();

/** Returns true if CUDA is available, and CuDNN is available. */
@Namespace("torch::cuda") public static native @Cast("bool") boolean cudnn_is_available();

/** Sets the seed for the current GPU. */
@Namespace("torch::cuda") public static native @Name("manual_seed") void cuda_manual_seed(@Cast("uint64_t") long seed);

/** Sets the seed for all available GPUs. */
@Namespace("torch::cuda") public static native @Name("manual_seed_all") void cuda_manual_seed_all(@Cast("uint64_t") long seed);

/** Waits for all kernels in all streams on a CUDA device to complete. */
@Namespace("torch::cuda") public static native @Name("synchronize") void cuda_synchronize(@Cast("int64_t") long device_index/*=-1*/);
@Namespace("torch::cuda") public static native @Name("synchronize") void cuda_synchronize();

 // namespace cuda
 // namespace torch


// Parsed from torch/csrc/utils/disallow_copy.h

// #pragma once

// #include <ATen/Utils.h>

// #define TH_DISALLOW_COPY_AND_ASSIGN AT_DISALLOW_COPY_AND_ASSIGN


// Parsed from torch/csrc/utils/memory.h

// #pragma once

// #include <memory>

// Reference:
// https://github.com/llvm-mirror/libcxx/blob/master/include/memory#L3091


 // namespace torch


// Parsed from torch/csrc/utils/python_stub.h

// #pragma once
// Targeting ../_object.java




// Parsed from torch/csrc/utils/variadic.h

// #pragma once

// #include <ATen/ATen.h>
// #include <torch/csrc/autograd/variable.h>
// #include <ATen/core/Variadic.h>

// #include <cstdint>
// #include <tuple>
// #include <type_traits>
// #include <utility>
// Targeting ../Indices.java



// Decrements the index N, adds N-1 to the list of indices and forwards
// whatever we already have.
// Targeting ../MakeIndices.java



//===----------------------------------------------------------------------===//
//                                 Utilities
//===----------------------------------------------------------------------===//
// Targeting ../pack.java



// Targeting ../all_of.java


// Targeting ../any_of.java



 // namespace torch


// Parsed from torch/csrc/autograd/anomaly_mode.h

// #pragma once

// #include <string>
// #include <memory>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// Targeting ../AnomalyMode.java


// Targeting ../DetectAnomalyGuard.java


// Targeting ../AnomalyMetadata.java






// Parsed from torch/csrc/autograd/edge.h

// #pragma once

// #include <cstdint>
// #include <functional>
// #include <memory>

// #include <c10/util/hash.h>
// Targeting ../Edge.java


 // namespace torch::autograd

// The idiomatic way of enabling use of a custom type as the key of hash
// containers in C++11. This method removes the requirement of having to pass
// a custom hasher to std::unordered_{map, set}.
// See http://en.cppreference.com/w/cpp/utility/hash for more information.
 // namespace std


// Parsed from torch/csrc/autograd/grad_mode.h

// #pragma once

// #include <ATen/core/grad_mode.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>




// Parsed from torch/csrc/autograd/InferenceMode.h

// #pragma once

// #include <c10/core/InferenceMode.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>




// Parsed from torch/csrc/autograd/input_metadata.h

// #pragma once

// #include <ATen/ATen.h>
// #include <c10/core/Device.h>
// #include <c10/core/DeviceType.h>
// #include <c10/core/Stream.h>
// #include <c10/core/impl/DeviceGuardImplInterface.h>

// #include <cstdint>
// Targeting ../InputMetadata.java



 // torch::autograd


// Parsed from torch/csrc/autograd/function_hook.h

// #pragma once

// #include <vector>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <ATen/Tensor.h>

// A hook that's called on gradients
// Targeting ../FunctionPreHook.java


// Targeting ../FunctionPostHook.java



 // namespace torch::autograd


// Parsed from torch/csrc/autograd/cpp_hook.h

// #pragma once
// #include <torch/csrc/autograd/function_hook.h>
// #include <functional>
// #include <memory>
// Targeting ../CppFunctionPreHook.java


 // namespace torch::autograd


// Parsed from torch/csrc/autograd/profiler.h

// #pragma once

// #include <torch/csrc/autograd/profiler_legacy.h>
// #include <torch/csrc/autograd/profiler_kineto.h>


// Parsed from torch/csrc/autograd/saved_variable_hooks.h

// #pragma once

// #include <ATen/ATen.h>
// Targeting ../SavedVariableHooks.java






// Parsed from torch/csrc/autograd/saved_variable.h

// #pragma once

// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/csrc/autograd/forward_grad.h>
// #include <torch/csrc/autograd/saved_variable_hooks.h>

// #include <ATen/ATen.h>

// #include <cstdint>
// #include <memory>

@Namespace("torch::autograd") public static native @Cast("const char*") BytePointer ERR_BACKWARD_TWICE(); public static native void ERR_BACKWARD_TWICE(BytePointer setter);
// Targeting ../SavedVariable.java


 // namespace torch::autograd


// Parsed from torch/csrc/autograd/forward_grad.h

// #pragma once

// #include <ATen/ATen.h>

// [ Using ForwardGrad ]
// ForwardGrad needs to be a shared_ptr to satisfy constraints of its inner design. But
// this shared_ptr must be uniquely associated with the object that stores it (as of
// writing, either AutogradMeta or SavedVariable). This object is called the "owning object"
// in the discussions below. This owning object must call `ForwardGrad::clear()` when it
// is destroyed to ensure that the ForwardGrad is properly de-allocated.

// This file contains two classes that are used to store forward AD gradients and
// ensure that they are scoped properly.
// Because forward AD runs concurrently with the evaluation of the function, we need
// a mechanism to separate different forward AD invocations and be able to compute the
// right gradients. We model such invocations as levels here.
// The particular scoping issue mentioned above has two main drivers:
//   - Ensure that we can conveniently use forward AD within a high level API without
//     leaking the forward AD states outside.
//   - Ensure that we can keep the level that we expose to the user API simple (an integer
//     that represents the nesting depth) while avoiding confusions when the level index
//     is re-used.

// The important external APIs from this file are:
//   - ForwardADLevel::get_next_idx() that can be used to enter a new level and get its index
//   - ForwardADLevel::release_idx() that can be used to exit a given level.
//   - ForwardGrad() can be used to store a given forward gradient that will handle the level
//     tracking automatically.

// The basic implementation strategy is as follows:
// Every tensor has a ForwardGrad, maintaining a map from levels to tangents.
// ForwardGrad is responsible for registering itself to the appropriate ForwardADLevel when a new
// tangent is added to it via ForwardGrad::set_value and to un-register itself from this same level
// if that tangent is removed via ForwardGrad::reset.
// The ForwardADLevel is created when a new level is entered via ForwardADLevel::get_next_idx.
// A reference to the new ForwardADLevel is stored into a global (for the whole process) vector that
// ensure it can be accessed via ForwardADLevel::get_by_idx. This reference is deleted when the index is
// released by the user when calling ForwardADLevel::release_idx.
// When it is destructed, the ForwardADLevel is responsible for clearing all the tangents for its
// level stored in all the ForwardGrad that registered with it.
//
// This process-wide level design, compared to a thread local one, allows us to use very simple user facing
// handle for the level (an int) while enabling cross-thread forward AD.
// The only required synchronization for the user is when entering and exiting the levels.
// Some discussion on alternative design is in https://github.com/pytorch/pytorch/pull/49097#discussion_r543716453
// and can be refined in the future.

// Correctness of concurrency:
// Each class uses its own lock when reading or modifying internal storages. This allows in particular
// to safely remove tangents from ForwardGrad when the ForwardADLevel is being exited.
// We ensure no deadlock by ensuring that a methods never calls into another class's method while
// the local class's lock is held except in one single case: calling from ForwardADLevel's destructor
// into ForwardGrad::reset with update_level=false.

// The lifetime of these objects is as follows:
// The ForwardADLevel can be in three states:
//      - Initialized: where one of its reference is held by the global vector and there may be more
//        references held by temporary variables in ForwardGrad's methods.
//      - About to be destructed: where "release_idx" has been called and the only reason for the
//        ForwardADLevel not to be destructed right away is that some methods in ForwardGrad have
//        owning reference to it. This is done so that a ForwardADLevel can never be destructed when
//        a ForwardGrad is registered with it and in the process of adding something to its internal state.
//      - Being destructed: Here the ForwardADLevel is not referenced anymore and can be safely reset
//        all of the ForwardGrad. Note that we can have more than one reset being called here (which is ok)
//        but we are guaranteed that there is at least one.
// The ForwardGrad is simpler as there is no intermediary state and no special destructor for. The logic to
// unregister it from the different ForwardADLevel is done when the owning object (AutogradMeta or
// SavedVariable) is being destroyed.

// Other considered design:
// To avoid having the ForwardGrad::clear, we considered storing weak_ptr inside the ForwardADLevel. While this
// would work, it would mean that the set inside the ForwardADLevel would only grow unless we do an
// expensive linear scan to remove all the dangling weak pointers. Hence this approach was not used.

// Data structures in this file are optimized for this maximum number of levels.
// The number of levels corresponds to the degree of the gradient being
// computed using forward AD and we don't expect more than second order gradients
// to be common.
public static final int EXPECTED_MAX_LEVEL = 2;
// Targeting ../ForwardADLevel.java


// Targeting ../ForwardGrad.java



 // namespace torch::autograd


// Parsed from torch/csrc/autograd/variable.h

// #pragma once

// #include <torch/csrc/utils/python_stub.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/csrc/autograd/edge.h>
// #include <torch/csrc/autograd/function_hook.h>
// #include <torch/csrc/autograd/cpp_hook.h>
// #include <torch/csrc/autograd/forward_grad.h>

// #include <ATen/ATen.h>
// #include <ATen/NamedTensorUtils.h>
// #include <c10/util/Exception.h>

// #include <memory>
// #include <mutex>
// #include <stdexcept>
// #include <string>
// #include <utility>
// #include <vector>
// #include <cstdint>

/** {@code Variable} is exactly the same as {@code Tensor} (i.e. we have {@code using Variable = at::Tensor}).
 *  This means you can perform all the usual mathematical and other
 *  operations you can perform on {@code Tensor}s also on {@code Variable}s.
 * 
 *  The only reason we are keeping the {@code Variable} class is backward compatibility
 *  with external user's legacy C++ frontend code. Our intention is to eliminate
 *  the {@code Variable} class in the near future. */

 // namespace autograd
 // namespace torch

// The following are all internal APIs and should not be shown in libtorch docs.
// Therefore, we wrap the following code with `#ifndef DOXYGEN_SHOULD_SKIP_THIS ... #endif`

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

/** Check if this type is supported by the autograd engine.
 *  If you change this, update the doc at the top of the torch/autograd/__init__.py file
 *  and "test_set_requires_grad_only_for_continuous_types" in test/test_autograd.py */
@Namespace("torch::autograd") public static native @Cast("bool") boolean isDifferentiableType(ScalarType t);

/**~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *                                 Variable
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  A {@code Variable} augments a {@code Tensor} with the ability to interact in our
 *  autograd machinery. Conceptually, {@code Variable}s travel along {@code Edge}s between
 *  {@code Node}s in the autograd graph. A {@code Variable} can either be a leaf, like a
 *  weight in a neural network, or an interior variable, when it is the result
 *  of an operation between variables. Every {@code Variable} also stores another
 *  {@code Variable} called its {@code grad} (gradient). If the variable is a leaf, its
 *  gradient will be accumulated into this variable.
 * 
 *  Every Tensor is a Variable, but sometimes we colloquially refer to Variables
 *  that don't require gradients as Tensors (since none of the autograd
 *  machinery for Variables applies).  Historically, Variables and Tensors
 *  were separate concepts, but now they are exactly the same (i.e. we have
 *  {@code using Variable = at::Tensor}).
 * 
 *                               Gradient Edges
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  Furthermore, {@code Variable}s have the notion of a {@code gradient_edge}, which is the
 *  edge in the autograd graph that connects the variable to a particular input
 *  of the gradient function that will be invoked with the variable during the
 *  backward pass. More precisely, this gradient function can be one of two
 *  things:
 *  1. A {@code grad_fn}, if the variable is in the interior of the graph. This is the
 *     gradient of the function that produced the variable.
 *  2. A {@code grad_accumulator}, if the variable is a leaf, which accumulates a
 *     scalar gradient value into its {@code grad} variable.
 * 
 *                                Versioning
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  Another major feature of {@code Variable}s are *versions*. Versions are
 *  incremented when an in-place mutation of a variable occurs. Versions are
 *  useful when constructing {@code SavedVariable}s, which take a snapshot of a
 *  {@code Variable} at a certain version. You can retrieve a {@code Variable}'s version
 *  through its {@code current_version()} method.
 * 
 *                                  Views
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  It is possible for a  {@code Variable} to be a *view* of another {@code Variable}, in
 *  which case it tracks that {@code Variable}'s data and autograd history. Beyond
 *  construction, the interface of a view is identical to that of a regular
 *  {@code Variable}. You can determine whether {@code Variable} is in fact a view by
 *  probing its {@code is_view()} method. Note that the *view* semantics are only
 *  meaningful for {@code Variable} relations that are relevant to autograd.
 *  See NOTE [ Autograd View Variables ] for more details.
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ */

// Private-ish functions for manipulating variables; we don't want to put them
// on Tensor proper

  // WARNING: This may return a nullptr.  If you require AutogradMeta to return
  // a materialized structure, use materialize_autograd_meta instead.
  @Namespace("torch::autograd::impl") public static native AutogradMeta get_autograd_meta(@Const @ByRef TensorBase arg0);

  // WARNING: This will return a nullptr if the Tensor is not a view.
  @Namespace("torch::autograd::impl") public static native DifferentiableViewMeta get_view_autograd_meta(@Const @ByRef TensorBase arg0);

  // Returns the current autograd meta, materializing it if it was previously
  // none.  This counts as a *mutating* operation, so do not call it on
  // "read-only" operators; in particular, this is NOT thread safe
  @Namespace("torch::autograd::impl") public static native AutogradMeta materialize_autograd_meta(@Const @ByRef TensorBase arg0);

  /** Set the gradient accumulator of the {@code Variable}. This is only applicable to
   *  leaf variables. Interior variables should call {@code set_gradient_edge()}. */

  /** Attempts to get a pointer to the gradient accumulator of the {@code Variable},
   *  if it still exists. If the gradient accumulator function has been
   *  destroyed, returns a {@code nullptr}. */
  @Namespace("torch::autograd::impl") public static native @SharedPtr Node try_get_grad_accumulator(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

  /** Gets the gradient accumulator of the {@code Variable} if it has one, or else
   *  create one on the fly and return it. */
  @Namespace("torch::autograd::impl") public static native @SharedPtr Node grad_accumulator(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

  /** Returns the "canonical" gradient edge of this {@code Variable}, i.e. either the
   *  gradient function if this is an interior {@code Variable}, or the gradient
   *  accumulator otherwise. If the {@code Variable} is interior, the returned {@code Edge}
   *  will store the input index of the {@code Node} to which this variable is
   *  connected in its {@code input_nr} field. For leaves, the {@code input_nr} is always
   *  zero. Note that {@code set_gradient_edge} and {@code gradient_edge} are not
   *  symmetric. You must use {@code set_gradient_edge} to set the {@code grad_fn} and
   *  {@code set_grad_accumulator} to set the accumulator. */
  @Namespace("torch::autograd::impl") public static native @ByVal Edge gradient_edge(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

  /** Set the gradient edge -- i.e. {@code grad_fn} and {@code input_nr} -- of the
   *  {@code Variable}.
   *  NOTE: This will always set the {@code grad_fn}, even if this is a leaf variable,
   *  and never the {@code grad_accumulator}. For the latter, use
   *  {@code set_grad_accumulator}. This allows late construction of an interior
   *  {@code Variable}. */
  
  ///
  @Namespace("torch::autograd::impl") public static native void set_gradient_edge(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0, @ByVal Edge edge);

  // Autograd Graph Interaction
  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  /** Update the {@code grad_fn} of an existing Variable. Called after in-place
   *  modifications.
   * 
   *  For View Variables:
   *  Called after in-place modifications. Modifies the grad_fn of the base
   *  Variable. */
  @Namespace("torch::autograd::impl") public static native void rebase_history(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0, @ByVal Edge gradient_edge);

  /** Gets the raw gradient function pointer, whatever it currently is. */
  @Namespace("torch::autograd::impl") public static native Node grad_fn_unsafe(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

  /** Increments the version count of this {@code Variable}. */
  @Namespace("torch::autograd::impl") public static native void bump_version(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);
  @Namespace("torch::autograd::impl") public static native void set_version_counter(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0, @Const @ByRef VariableVersion version_counter);

  /** Retrieves this {@code Variable}s version counter. */
  @Namespace("torch::autograd::impl") public static native @Const @ByRef VariableVersion version_counter(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

  @Namespace("torch::autograd::impl") public static native void set_name(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0, @StdString BytePointer name);
  @Namespace("torch::autograd::impl") public static native void set_name(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0, @StdString String name);

  @Namespace("torch::autograd::impl") public static native void add_hook(@Const @ByRef TensorBase arg0, @SharedPtr FunctionPreHook hook);
  @Namespace("torch::autograd::impl") public static native @Const @ByRef FunctionPreVector hooks(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);
  @Namespace("torch::autograd::impl") public static native void clear_hooks(@Const @ByRef TensorBase arg0);

  @Namespace("torch::autograd::impl") public static native void create_cpp_hook(@Const @ByRef TensorBase arg0);

// Targeting ../AutogradMeta.java



//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
//                     DifferentiableViewMeta
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

/** NOTE [ Autograd View Variables ]
 * 
 *  Many operations return Variable that shares storage with an input Variable.
 *  The returned Variable is called a **view** Variable on the input **base**
 *  Variable.
 * 
 *  In PyTorch, we have two types of views: differentiable views, and
 *  non-differentiable views. In either type, to support proper version
 *  checking, the base and view Variables must always share the same
 *  version_counter.
 * 
 * 
 *  Differentiable Views
 *  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  This class allows to track both forward and backward AD differentiable views.
 *  These views can have different base as non-differentiable view for forward
 *  and backward mode AD are not the same.
 * 
 *  Most function are either both forward and backward differentiable views (for
 *  example: view, select, narrow, transpose, etc) or both not forward and not
 *  backward differentiable views (for example: indices, values, eq, lt, etc).
 *  But there are also functions that are forward but not backward differentiable
 *  views (only detach for now) or functions that are backward but not forward
 *  differentiable view (only make_dual and unpack dual for now).
 * 
 *  A concrete example of two views with different bases is as follow:
 * 
 *      # Have:
 *      #   dual is a dual Tensor that is neither a forward or backward view
 *      detached_dual = dual.detach()
 *      view = detached_dual.view_as(dual)
 *      # The forward base of view is dual
 *      # The backward base of view is detached_dual
 * 
 *  - Backward Mode View
 *  Differentiable views are the view variables where you want gradients to flow
 *  back to the base variables. Out-of-place operations on views are quite
 *  straightforward, but in-place ones are very tricky. Even if the base
 *  variable may not require grad when we create the view, we still need to
 *  track the view relation because future in-place ops may require back-proping
 *  through it. For example, we need to support
 * 
 *    (1) in-place operation on view, e.g.,
 * 
 *      # Have:
 *      #   base.requires_grad = False
 *      #   var.requires_grad = True
 *      base[1] = var  # i.e., base[1].copy_(var)
 *      torch.autograd.grad(base.sum(), var)  <- should return an all ones tensor
 * 
 *    (2) in-place operation on base after view is created, e.g.,
 * 
 *      # Have:
 *      #   base.requires_grad = False
 *      #   var.requires_grad = True
 *      view = base[1]
 *      base.copy_(var)
 *      torch.autograd.grad(view.sum(), var)  <- should return a tensor with
 *                                               var[1] filled with all ones and
 *                                               zeros everywhere else
 * 
 *  - Forward Mode View
 *  Forward differentiable views follow the same semantic as backward ones but
 *  show up differently as they are computed along with the forward evaluation.
 *  The hard examples above are thus very similar
 * 
 *    (1) in-place operation on view, e.g.,
 * 
 *      # Have:
 *      #   base is a regular Tensor
 *      #   var is a dual Tensor whose tangent is all ones
 *      base[1] = var  # i.e., base[1].copy_(var)
 *      # Now, base is a dual Tensor
 *      _, fw_grad = fwAD.unpack_dual(base) <- fw_grad should be a tensor with
 *                                               fw_grad[1] filled with all ones and
 *                                               zeros everywhere else
 * 
 *    (2) in-place operation on base after view is created, e.g.,
 * 
 *      # Have:
 *      #   base is a regular Tensor
 *      #   var is a dual Tensor whose tangent is all ones
 *      view = base[1]
 *      base.copy_(var)
 *      _, fw_grad = fwAD.unpack_dual(view) <- fw_grad should be an all ones tensor
 * 
 *  See Note [Forward Grad View/inplace] for more details on how we handle these hard cases.
 * 
 * 
 *  DifferentiableViewMeta is created to support gradient tracking of
 *  such **in-place** operations. In particular,
 *    + if an in-place op is done on base, the grad_fn field of the view may
 *      become stale. So accesses should always go through grad_fn(), which
 *      reconstructs an updated grad_fn if the version_counter has incremented.
 *      All other fields are always valid.
 *    + if an in-place op is done on view, in rebase_history() of view, which is
 *      called after every in-place op in VariableType.cpp, the grad_fn of base
 *      is updated.
 *    + if a single autograd Node returns multiple differentiable views, if any
 *      output is modified by an inplace operation, the autograd engine will make
 *      an equivalent graph (corresponding to the view operations) without using
 *      equivalent graph, where each output is treated as if it were produced by a
 *      distinct view operation. This discards the original (e.g., user provided)
 *      grad_fn. If the provided grad_fn does more than the backward of the view,
 *      then the DifferentiableViewMeta must be created with creation_meta=
 *      CreationMeta::MULTI_OUTPUT_NODE to prevent the engine from ignoring the
 *      provided grad_fn.
 * 
 *  Interaction with GradMode:
 *  The particular case that we consider here is:
 * 
 *      # Have:
 *      #   base.requires_grad = True or False
 *      with torch.no_grad():
 *          view = base[1]
 *      base.requires_grad_()
 *      view.copy_(var)
 *      torch.autograd.grad(base.sum(), var)  <- what should it return?
 * 
 *  Given that this particular code example is ambiguous and can easily be replace by
 *  either moving both inside the no_grad block or both outside, we explicitly forbid
 *  it. For now, it is deprecated by a warning. This is achieved by setting
 *  creation_meta=CreationMeta::NO_GRAD_MODE for all differentiable views created
 *  in no_grad mode.
 * 
 *  See Note [View + Inplace update for base tensor]
 *  and Note [View + Inplace update for view tensor] for the details how autograd
 *  handles inplace update with view ops.
 * 
 *  Non-Differentiable Views
 *  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  In certain cases, although function outputs share storage with inputs, they
 *  will **never** require gradient history tracking. Instead of registering the
 *  view relation via DifferentiableViewMeta in autograd, the views will be
 *  using usual AutogradMeta and just share the version counters with the base
 *  Variables.
 *  Such views include:
 *    1. Views created from .detach()
 *    2. Views that are non-differentiable by its nature.
 *       E.g., {@code sparse_tensor.indices()} is a integral view on a (possibly)
 *       floating point tensor.
 *       See top of {@code derivatives.yaml} on how to specify that outputs of a
 *       function are non-differentiable.
 *  These are called non-differentiable views as the gradients do not flow
 *  through the view relation.
 * 
 *  Relevant logic for both differentiable and non-differentiable views is implemented in
 *  make_variable_(non_)differentiable_view below, and wrap_output of gen_variable_type.py.
 <p>
 <p>
 *  NOTE [ View + Inplace detection ]
 * 
 *  We want to detect views followed by inplace as they are often forbidden to ensure
 *  correctness of the computed gradients. But since we want to only notify the user
 *  when both happen, we tag the DifferentiableViewMeta when the view is created
 *  via the {@code make_variable_*_view()} functions. This tag is then checked by the
 *  {@code check_inplace()} function from {@code VariableTypeUtils.h} that should be called before
 *  every inplace operation and to detect cases where other views are modified and this
 *  one is rebased by side effect, we also check in the {@code VariableHooks::grad_fn()}.
 <p>
 *  Flag that gives more information about when this view was created:
 *  - IN_CUSTOM_FUNCTION should be set when the view is created inside a custom
 *    autograd Function is returned.
 *  - NO_GRAD_MODE should be set when a view in created when GradMode is disabled
 *  - MULTI_OUTPUT_NODE should be set when a Node created by codegen code returns
 *    multiple differentiable views
 *  - Inference_MODE should be set when a view of normal tensor is created in InferenceMode.
 *  - DEFAULT is for all other cases */
@Namespace("torch::autograd") public enum CreationMeta { DEFAULT((byte)(0)), IN_CUSTOM_FUNCTION((byte)(1)), MULTI_OUTPUT_NODE((byte)(2)),
                                   NO_GRAD_MODE((byte)(3)), INFERENCE_MODE((byte)(4));

    public final byte value;
    private CreationMeta(byte v) { this.value = v; }
    private CreationMeta(CreationMeta e) { this.value = e.value; }
    public CreationMeta intern() { for (CreationMeta e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

/** Handles correctly propagating CreationMeta when a new view is created from a previous view.
 *  In general, we don't want the new view to be _less_ restrictive than the previous view
 *  (it's okay to be _more_ restrictive).
 *  A CreationMeta value of DEFAULT is currently the least restrictive, as the behavior for
 *  all other CreationMeta values is to error out for in-place ops.
 *  A CreationMeta value of INFERENCE_MODE is currently the most restrictive, so it takes
 *  precedence in propagation.
 *  If this changes, the logic here will need to be updated to properly handle the new semantics. */
@Namespace("torch::autograd") public static native CreationMeta propagate_creation_meta(CreationMeta prev_view_creation_meta, CreationMeta new_view_creation_meta);
@Namespace("torch::autograd") public static native @Cast("torch::autograd::CreationMeta") byte propagate_creation_meta(@Cast("torch::autograd::CreationMeta") byte prev_view_creation_meta, @Cast("torch::autograd::CreationMeta") byte new_view_creation_meta);

/** Unified function to handle error checking when rebase happens
 *  indirect=true means that the caller is not doing the inplace, but the inplace happened
 *  somewhere else. */
@Namespace("torch::autograd") public static native void handle_view_on_rebase(DifferentiableViewMeta diff_view_meta, @Cast("bool") boolean indirect/*=false*/);
@Namespace("torch::autograd") public static native void handle_view_on_rebase(DifferentiableViewMeta diff_view_meta);
// Targeting ../DifferentiableViewMeta.java



//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
//                        Variable Implementation
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

// Factory Functions
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

/** Creates a {@code Variable} that is a *view* of another (*base*) variable.
 *  The {@code gradient_edge} is an optional (gradient_function, input_number) pair.
 *  {@code is_differentiable} is a bool that specifies whether this view is
 *  differentiable, i.e., whether the relation should be tracked by autograd.
 *  See NOTE [ Autograd View Variables ] for details.
 <p>
 *  NOTE: {@code allow_tensor_metadata_change} is set to true by default, because there
 *  are a lot of call sites to these factory functions that need to change the
 *  variable's size or storage afterwards, and they don't expect the original
 *  tensor (where the variable is created from) to be updated. Setting
 *  {@code allow_tensor_metadata_change_} to false by default would unnecessarily
 *  prevent those changes from happening and is undesirable. */

// See NOTE [ Autograd View Variables ] for details.
// Differentiable view. Track history with DifferentiableViewMeta.
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_differentiable_view(
    @Const @ByRef Tensor data,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer backward_info,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer forward_info,
    @Cast("bool") boolean shared_view_info,
    CreationMeta creation_meta,
    @Cast("bool") boolean allow_tensor_metadata_change/*=true*/);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_differentiable_view(
    @Const @ByRef Tensor data,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer backward_info,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer forward_info,
    @Cast("bool") boolean shared_view_info,
    CreationMeta creation_meta);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_differentiable_view(
    @Const @ByRef Tensor data,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer backward_info,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer forward_info,
    @Cast("bool") boolean shared_view_info,
    @Cast("torch::autograd::CreationMeta") byte creation_meta,
    @Cast("bool") boolean allow_tensor_metadata_change/*=true*/);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_differentiable_view(
    @Const @ByRef Tensor data,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer backward_info,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer forward_info,
    @Cast("bool") boolean shared_view_info,
    @Cast("torch::autograd::CreationMeta") byte creation_meta);

// See NOTE [ Autograd View Variables ] for details.
// Non-differentiable view. Just share version counter.

///
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_non_differentiable_view(
    @ByVal @Cast("torch::autograd::Variable*") Tensor base,
    @Const @ByRef Tensor data,
    @Cast("bool") boolean allow_tensor_metadata_change/*=true*/);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_non_differentiable_view(
    @ByVal @Cast("torch::autograd::Variable*") Tensor base,
    @Const @ByRef Tensor data);

/** Creates a {@code Variable} from the given {@code Tensor}, copying its underlying {@code TensorImpl}.
 *  {@code requires_grad} should be
 *  set only for leaves, and determines whether the {@code Variable} will accumulate
 *  gradients. NOTE: {@code data} must *not* be a {@code Variable} already. Its dynamic
 *  type *must* be {@code Tensor}.
 * 
 *  TODO: Eliminate this function as much as possible, as it can be expressed
 *  more clearly as detach() or a no-op in most call sites (especially when
 *  there is only one use of the variable). */
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable(
    @ByVal Tensor data,
    @Cast("bool") boolean requires_grad/*=false*/,
    @Cast("bool") boolean allow_tensor_metadata_change/*=true*/);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable(
    @ByVal Tensor data);

/** Creates a {@code Variable} from the given {@code Tensor}, copying its underlying {@code TensorImpl}.
 *  {@code gradient_edge} should be a (function, input_nr) pair specifying the function
 *  in the autograd graph, and what particular input of that function, this
 *  variable is connected to. */
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable(
    @ByVal Tensor data,
    @ByVal Edge gradient_edge,
    @Cast("bool") boolean allow_tensor_metadata_change/*=true*/);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable(
    @ByVal Tensor data,
    @ByVal Edge gradient_edge);


 // namespace torch::autograd

// #endif /* DOXYGEN_SHOULD_SKIP_THIS */


// Parsed from torch/csrc/autograd/function.h

// #pragma once

// #include <torch/csrc/autograd/edge.h>
// #include <torch/csrc/autograd/grad_mode.h>
// #include <torch/csrc/autograd/anomaly_mode.h>
// #include <torch/csrc/autograd/profiler.h>
// #include <torch/csrc/autograd/saved_variable.h>
// #include <torch/csrc/autograd/input_metadata.h>
// #include <torch/csrc/autograd/variable.h>
// #include <torch/csrc/utils/python_stub.h>
// #include <torch/csrc/utils/variadic.h>

// #include <ATen/ATen.h>
// #include <ATen/SequenceNumber.h>
// #include <c10/util/Exception.h>

// #include <algorithm>
// #include <cstdint>
// #include <initializer_list>
// #include <memory>
// #include <string>
// #include <utility>
// #include <vector>

// Custom deleter to prevent stack overflows.
@Namespace("torch::autograd") public static native void deleteNode(Node function);
// Targeting ../NodeGuard.java


// Targeting ../Node.java


// Targeting ../TraceableFunction.java



//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
//                       Associated Free Nodes
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Implementation of `collect_next_edges` (see below).
// NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)
 // namespace detail

/** Create an {@code Edge} between the given {@code variable} and the {@code function}, which is
 *  assumed to be the gradient function of this variable (i.e. the function
 *  through which this variable is backpropagated during the backward pass).
 *  This sets the {@code grad_fn} property of the {@code variable}. This function assumes
 *  that the {@code Variable} is a new input to the gradient function and its
 *  {@code input_nr} thus equal to {@code function->num_inputs()}. Additionally, it
 *  increments the {@code Node}'s number of inputs by one. Approximately
 *  equivalent to {@code variable.set_gradient_edge(function,
 *  function->add_input_metadata(variable.dispatch_type(), variable.sizes()))}.
 *  If you don't want the {@code Node}'s {@code num_inputs} to be incremented, use
 *  {@code set_gradient_edge} directly. */
@Namespace("torch::autograd") public static native void create_gradient_edge(
    @Cast("torch::autograd::Variable*") @ByRef Tensor variable,
    @SharedPtr Node function);

/** Return true if any of the variables in the list require a gradient. */
@Namespace("torch::autograd") public static native @Cast("bool") boolean any_variable_requires_grad(@Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector variables);

/** Return the next edges of all the given variables, or tuples of variables. */
 // namespace torch::autograd


// Parsed from torch/csrc/autograd/custom_function.h

// #pragma once

// #include <torch/csrc/autograd/function.h>
// #include <torch/csrc/autograd/variable.h>
// #include <ATen/core/ivalue.h>
// #include <c10/util/flat_hash_map.h>
// #include <c10/util/irange.h>
// #include <vector>

@Namespace("torch::autograd") public static native @ByVal TensorOptionalVector _wrap_outputs(
  @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector input_vars,
  @Const @ByRef TensorImplSet non_differentiable,
  @Const @ByRef TensorImplSet dirty_inputs,
  @Const @ByVal TensorOptionalArrayRef raw_outputs,
  @SharedPtr Node cdata,
  @ByVal @Cast("torch::autograd::_jvp_fn_t*") Pointer jvp_user_function);

@Namespace("torch::autograd") public static native void check_variable_result(@Const @ByRef TensorBase original,
    @Const @ByRef TensorBase result, @StdString BytePointer hook_name);
@Namespace("torch::autograd") public static native void check_variable_result(@Const @ByRef TensorBase original,
    @Const @ByRef TensorBase result, @StdString String hook_name);

// Get the return type of the forward function of the custom Function class X

///
///
///
///
///

/** To use custom autograd operations, implement a Function subclass with
 *  static forward and backward functions:
 * 
 *  {@code forward} can take as many arguments as you want and should return either a
 *  variable list or a Variable. Use of any direct Variable arguments will be
 *  registered in the graph but no vectors/sets or any other data structures
 *  will be traversed. You can use c10::optional<Tensor> as one of the arguments
 *  and it will be registered as a variable in the graph if the argument has a
 *  value. It should take a pointer to {@code torch::autograd::AutogradContext} as the
 *  first argument. Variables can be saved in the {@code ctx} using
 *  {@code ctx->save_for_backward}
 *  (see {@code torch::autograd::AutogradContext::save_for_backward}) and other data
 *  can be saved in the {@code ctx->saved_data} map
 *  (see {@code torch::autograd::AutogradContext::saved_data})
 *  in the form of {@code <std::string, at::IValue>} pairs.
 * 
 *  {@code backward} should take a pointer to {@code torch::autograd::AutogradContext}
 *  and a variable list containing as many Variables as there were outputs from
 *  {@code forward} as arguments. It should return as many Variables as there were
 *  inputs with each of them containing the gradient w.r.t. its corresponding
 *  input. Variables saved in {@code forward} can be accessed with
 *  {@code ctx->get_saved_variables} (see
 *  {@code torch::autograd::AutogradContext::get_saved_variables}) and other saved
 *  data can be accessed from {@code ctx->saved_data}.
 * 
 *  For example:
 *  <pre>{@code
 *  class MyFunction : public Function<MyFunction> {
 *    public:
 *    static variable_list forward(AutogradContext *ctx, int n, Variable var) {
 *       // Save data for backward in context
 *       ctx->saved_data["n"] = n;
 *       var.mul_(2);
 *       // Mark var as modified by inplace operation
 *       ctx->mark_dirty({var});
 *       return {var};
 *    }
 * 
 *    static variable_list backward(AutogradContext *ctx, variable_list
 *    grad_output) {
 *       // Use data saved in forward
 *       auto n = ctx->saved_data["n"].toInt();
 *       return {grad_output[0]*n};
 *    }
 *  };
 *  }</pre>
 * 
 *  To use {@code MyFunction}:
 *  <pre>{@code
 *  Variable x;
 *  auto y = MyFunction::apply(6, x);
 *  // Example backward call
 *  y[0].sum().backward();
 *  }</pre> */
// Targeting ../AutogradContext.java


// Targeting ../VariableInfo.java



// CppNode<T> is the Node in the autograd graph that represents the user defined
// backward function for Function<T>. Calls to CppNode::apply are forward to
// T::backward().

@Namespace("torch::autograd") public static native @ByVal TensorOptionalVector to_optional(@Cast("torch::autograd::Variable*") @ByRef Tensor output);

@Namespace("torch::autograd") public static native @ByVal TensorOptionalVector to_optional(@ByRef TensorVector output);



// The logic here is the same as PyNode::apply, so changes to it should be done
// in both the places








 // namespace torch::autograd


// Parsed from torch/csrc/autograd/autograd.h

// #pragma once

// #include <torch/csrc/autograd/variable.h>

// #include <ATen/ATen.h>

/** Computes the sum of gradients of given tensors with respect to graph leaves.
 * 
 *  The graph is differentiated using the chain rule. If any of {@code }tensors{@code }
 *  are non-scalar (i.e. their data has more than one element) and require gradient,
 *  then the Jacobian-vector product would be computed, in this case the function
 *  additionally requires specifying {@code grad_tensors}. It should be a sequence of
 *  matching length, that contains the "vector" in the Jacobian-vector product,
 *  usually the gradient of the differentiated function w.r.t. corresponding tensors
 *  ({@code torch::Tensor()} is an acceptable value for all tensors that don't need
 *  gradient tensors).
 * 
 *  This function accumulates gradients in the leaves - you might need to zero them
 *  before calling it.
 * 
 *  @param tensors Tensors of which the derivative will be computed.
 *  @param grad_tensors The "vector" in the Jacobian-vector product, usually gradients
 *      w.r.t. each element of corresponding tensors. {@code torch::Tensor()} values can be
 *      specified for scalar Tensors or ones that don't require grad. If a {@code torch::Tensor()} value
 *      would be acceptable for all grad_tensors, then this argument is optional.
 *  @param retain_graph If {@code false}, the graph used to compute the grad will be freed.
 *      Note that in nearly all cases setting this option to {@code true} is not needed
 *      and often can be worked around in a much more efficient way. Defaults to the
 *      value of {@code create_graph}.
 *  @param create_graph If {@code true}, graph of the derivative will be constructed, allowing
 *      to compute higher order derivative products. Defaults to {@code false}.
 *  @param inputs Inputs w.r.t. which the gradient will be accumulated into
 *      {@code at::Tensor::grad}. All other Tensors will be ignored. If not provided, the gradient
 *      is accumulated into all the leaf Tensors that were used to compute param {@code tensors}. */
//      When inputs are provided and a given input is not a leaf,
//      the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).
//      It is an implementation detail on which the user should not rely.
//      See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.

///
///
@Namespace("torch::autograd") public static native void backward(
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensors,
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector grad_tensors/*={}*/,
    @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional retain_graph,
    @Cast("bool") boolean create_graph/*=false*/,
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector inputs/*={}*/);
@Namespace("torch::autograd") public static native void backward(
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensors);

/** Computes and returns the sum of gradients of outputs with respect to the inputs.
 * 
 *  {@code }grad_outputs{@code } should be a sequence of length matching {@code }output{@code }
 *  containing the "vector" in Jacobian-vector product, usually the pre-computed
 *  gradients w.r.t. each of the outputs. If an output doesn't require_grad,
 *  then the gradient can be {@code }torch::Tensor(){@code }).
 * 
 *  @param outputs outputs of the differentiated function.
 *  @param inputs Inputs w.r.t. which the gradient will be
 *      returned (and not accumulated into {@code }at::Tensor::grad{@code }).
 *  @param grad_outputs The "vector" in the Jacobian-vector product.
 *      Usually gradients w.r.t. each output. {@code torch::Tensor()} values can be specified for scalar
 *      Tensors or ones that don't require grad. If a {@code torch::Tensor()} value would be acceptable
 *      for all grad_tensors, then this argument is optional. Default: {@code {}}.
 *  @param retain_graph If {@code }false{@code }, the graph used to compute the grad
 *      will be freed. Note that in nearly all cases setting this option to {@code }true{@code }
 *      is not needed and often can be worked around in a much more efficient
 *      way. Defaults to the value of {@code }create_graph{@code }.
 *  @param create_graph If {@code }true{@code }, graph of the derivative will
 *      be constructed, allowing to compute higher order derivative products.
 *      Default: {@code }false{@code }.
 *  @param allow_unused If {@code }false{@code }, specifying inputs that were not
 *      used when computing outputs (and therefore their grad is always zero)
 *      is an error. Defaults to {@code }false{@code }. */
@Namespace("torch::autograd") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector grad(
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector outputs,
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector inputs,
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector grad_outputs/*={}*/,
    @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional retain_graph,
    @Cast("bool") boolean create_graph/*=false*/,
    @Cast("bool") boolean allow_unused/*=false*/);
@Namespace("torch::autograd") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector grad(
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector outputs,
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector inputs);

/** Creates a new dual level and returns its index. This level index should then be used to call
 *  into the other functions below.
 *  This API supports entering a new level before the previous one is exited. We call them nested
 *  forward AD levels. These can be used to compute higher order derivatives. */
@Namespace("torch::autograd::forward_ad") public static native @Cast("uint64_t") long enter_dual_level();

/** Exits the given level. This will clear up all the gradients from this level and all dual Tensors
 *  that had gradients for this level will become regular Tensors again.
 *  This function can only be used to exit the innermost nesting level and so exiting must happen in
 *  reverse order compared to the entering that was done with the function above. */
@Namespace("torch::autograd::forward_ad") public static native void exit_dual_level(@Cast("uint64_t") long level);

 // namespace forward_ad
 // namespace autograd
 // namespace torch


// Parsed from torch/csrc/autograd/generated/VariableType.h

// #pragma once

// @generated from tools/autograd/templates/VariableType.h

// #include <ATen/ATen.h>

// #include <c10/util/intrusive_ptr.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/csrc/autograd/autograd_not_implemented_fallback.h>

// #include <cstdint> // for size_t
// #include <functional> // for function
// #include <memory> // for unique_ptr
// #include <string>
// #include <vector>
// Targeting ../Quantizer.java



// This is temporary typedef to enable Quantizer in aten native function API
// we'll remove them when we are actually exposing Quantizer class
// to frontend
  @Namespace("torch::autograd::VariableType") public static native @Cast("at::DeprecatedTypeProperties**") @StdVector PointerPointer allCUDATypes();
  @Namespace("torch::autograd::VariableType") public static native @Cast("at::DeprecatedTypeProperties**") @StdVector PointerPointer allCPUTypes();

  
  
  
  


 // namespace torch::autograd


// Parsed from torch/csrc/autograd/generated/variable_factories.h

// #pragma once

// @generated from tools/autograd/templates/variable_factories.h

// #include <ATen/ATen.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/grad_mode.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/core/MemoryFormat.h>
// #include <torch/csrc/api/include/torch/detail/TensorDataContainer.h>
// #include <torch/csrc/autograd/variable.h>
// #include <torch/csrc/jit/frontend/tracer.h>
// #include <torch/csrc/jit/ir/ir.h>

// #include <functional>
// #include <initializer_list>
// #include <utility>

/** NOTE: Currently {@code torch::tensor(...)} doesn't support mixed data types
 *  (i.e. {@code torch::tensor({{bool, 2.0}})} doesn't work). We might be able to
 *  support it in the future by iterating over all sub-lists to find
 *  the largest data type that can represent all of the elements, or by using
 *  variadic templates.
 * 
 *  NOTE: C++ {@code torch::tensor} with a floating-point type or an {@code at::ArrayRef} / {@code std::vector} /
 *  (nested) braced-init-list of floating-point types always produces a tensor of dtype
 *  {@code torch::get_default_dtype()}, matching Python {@code torch.tensor} behavior.
 * 
 *  NOTE: C++ {@code torch::tensor} with an integer type or an {@code at::ArrayRef} / {@code std::vector} /
 *  (nested) braced-init-list of integer types always produces a tensor of dtype {@code at::kLong}
 *  (aka. int64_t), matching Python {@code torch.tensor} behavior.
 * 
 *  NOTE: The following dtypes are not supported by {@code torch::tensor} currently:
 *  - {@code unsigned int}
 *  - {@code unsigned long int}
 *  - {@code unsigned long long int}
 *  - {@code long long int} */
@Namespace("torch") public static native @ByVal Tensor tensor(@ByVal @Cast("torch::detail::TensorDataContainer*") Pointer tensor_data_container, @Const @ByRef(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal Tensor tensor(@ByVal @Cast("torch::detail::TensorDataContainer*") Pointer tensor_data_container);

/** A generic deleter function. */

/** Exposes the given {@code data} as a {@code Tensor} without taking ownership of the
 *  original data. {@code sizes} should specify the shape of the tensor, {@code strides} the
 *  stride in each dimension. The {@code deleter} function (a
 *  {@code std::function<void(void*)>}) will be called on the {@code data} when the Tensor
 *  data would normally be deallocated. The {@code TensorOptions} specify additional
 *  configuration options for the returned tensor, such as what type to
 *  interpret the {@code data} as. */
@Namespace("torch") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @Const @ByRef Deleter deleter,
    @Const @ByRef(nullValue = "at::TensorOptions()") TensorOptions options);
@Namespace("torch") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @Const @ByRef Deleter deleter,
    @Const @ByRef(nullValue = "at::TensorOptions()") TensorOptions options);

/** Exposes the given {@code data} as a {@code Tensor} without taking ownership of the
 *  original data. {@code sizes} should specify the shape of the tensor, {@code strides} the
 *  stride in each dimension. The {@code TensorOptions}
 *  specify additional configuration options for the returned tensor, such as
 *  what type to interpret the {@code data} as. */

/** Exposes the given {@code data} as a {@code Tensor} without taking ownership of the
 *  original data. {@code sizes} should specify the shape of the tensor. The {@code deleter}
 *  (a {@code std::function<void(void*)>}) function will be called on the {@code data} when
 *  the Tensor data would normally be deallocated. The {@code TensorOptions} specify
 *  additional configuration options for the returned tensor, such as what type
 *  to interpret the {@code data} as. */

/** Exposes the given {@code data} as a {@code Tensor} without taking ownership of the
 *  original data. {@code sizes} should specify the shape of the tensor. The
 *  {@code TensorOptions} specify additional configuration options for the returned
 *  tensor, such as what type to interpret the {@code data} as. */



 // namespace torch


// Parsed from torch/csrc/jit/frontend/function_schema_parser.h

// #pragma once

// #include <ATen/core/Macros.h>
// #include <ATen/core/function_schema.h>
// #include <c10/util/either.h>
// #include <string>


@Namespace("torch::jit") public static native @ByVal FunctionSchema parseSchema(@StdString BytePointer schema);
@Namespace("torch::jit") public static native @ByVal FunctionSchema parseSchema(@StdString String schema);
@Namespace("torch::jit") public static native @ByVal OperatorName parseName(@StdString BytePointer name);
@Namespace("torch::jit") public static native @ByVal OperatorName parseName(@StdString String name);

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/name_mangler.h

// #pragma once

// #include <ATen/core/qualified_name.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// Targeting ../NameMangler.java



 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/parser_constants.h

// #pragma once
@Namespace("torch::jit") public static native @Cast("const char*") BytePointer valid_single_char_tokens(); public static native void valid_single_char_tokens(BytePointer setter);
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/source_range.h

// #pragma once
// #include <c10/util/Exception.h>
// #include <c10/util/Optional.h>

// #include <algorithm>
// #include <iostream>
// #include <memory>
// #include <unordered_map>
// Targeting ../Source.java


// Targeting ../SourceRange.java


// Targeting ../SourceRangeHasher.java


// Targeting ../StackEntry.java



@Namespace("torch::jit") public static native void format_stack_trace(
    @Cast("std::ostream*") @ByRef Pointer out,
    @Const @ByRef StackEntryVector entries);

@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef SourceRange range);
// Targeting ../TaggedRange.java



 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/sugared_value.h

// #pragma once
// #include <functional>
// #include <memory>
// #include <string>
// #include <utility>

// #include <ATen/core/interned_strings.h>
// #include <torch/csrc/jit/api/module.h>
// #include <torch/csrc/jit/frontend/error_report.h>
// #include <torch/csrc/jit/frontend/schema_matching.h>
// #include <torch/csrc/jit/frontend/versioned_symbols.h>
// #include <torch/csrc/jit/ir/ir.h>
// Targeting ../SugaredValue.java


// Targeting ../SimpleValue.java


// Targeting ../BuiltinFunction.java


// Targeting ../SugaredTupleValue.java


// Targeting ../BuiltinModule.java


// Targeting ../ClassValue.java


// Targeting ../NamedTupleConstructor.java


// Targeting ../FunctionValue.java


// Targeting ../ClosureValue.java


// Targeting ../MethodValue.java


// Targeting ../PrintValue.java


// Targeting ../CastValue.java


// Targeting ../TensorCastValue.java


// Targeting ../MagicMethod.java


// Targeting ../SpecialFormValue.java


// Targeting ../RangeValue.java


// Targeting ../IterableTree.java



@Namespace("torch::jit") public static native @ByVal ValueVector toValues(
    @ByRef Graph g,
    @ByVal NamedValueArrayRef nvs);
// Targeting ../SimpleSelf.java


// Targeting ../ExceptionMessageValue.java


// Targeting ../ExceptionValue.java


// Targeting ../SugaredEnumClass.java


// Targeting ../SliceValue.java



 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/resolver.h

// #pragma once

// #include <ATen/core/jit_type.h>
// #include <ATen/core/qualified_name.h>
// #include <torch/csrc/jit/frontend/sugared_value.h>
// Targeting ../Resolver.java


// Targeting ../NativeResolver.java



@Namespace("torch::jit") public static native @SharedPtr NativeResolver nativeResolver();
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/tracer.h

// #pragma once

// #include <ATen/core/Dimname.h>
// #include <ATen/core/jit_type.h>
// #include <ATen/core/stack.h>
// #include <c10/util/Exception.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>

// #include <torch/csrc/jit/api/object.h>
// #include <torch/csrc/jit/frontend/source_range.h>
// #include <torch/csrc/utils/variadic.h>

// #include <cstdint>
// #include <iostream>
// #include <memory>
// #include <mutex>
// #include <unordered_map>
// #include <vector>
// Targeting ../TracingState.java



// This is meant to be used as a thread local place, where we can store extra
// info that gets lost when we call into ATen from Python bindings. One example
// for when this happens is when we get an IntArrayRef argument with e.g. sizes
// for view. When tracing, those might be tensors, which let us encode extra
// data dependencies, but once they get to the ATen call where we actually have
// the tracing logic, they get converted into a raw IntArrayRef, and we loose
// all information. To prevent this, we temporarily stash it in here.
// NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)

// Retrieve or set the current tracing state. Returns a nullptr if tracing is
// disabled.
@Namespace("torch::jit::tracer") public static native @SharedPtr TracingState getTracingState();
@Namespace("torch::jit::tracer") public static native void setTracingState(@SharedPtr TracingState state);

@Namespace("torch::jit::tracer") public static native @Cast("bool") boolean isTracing();
// Targeting ../warn_fn_type.java


@Namespace("torch::jit::tracer") public static native @Cast("const char*") BytePointer WARN_PYTHON_DATAFLOW(); public static native void WARN_PYTHON_DATAFLOW(BytePointer setter);
@Namespace("torch::jit::tracer") public static native @Cast("const char*") BytePointer WARN_CONSTRUCTOR(); public static native void WARN_CONSTRUCTOR(BytePointer setter);
@Namespace("torch::jit::tracer") public static native @Cast("const char*") BytePointer WARN_RESIZE(); public static native void WARN_RESIZE(BytePointer setter);
@Namespace("torch::jit::tracer") public static native @Cast("const char*") BytePointer STRICT_TRACER_MSG(); public static native void STRICT_TRACER_MSG(BytePointer setter);
@Namespace("torch::jit::tracer") public static native void _do_warn(@Cast("const char*") BytePointer _reason, @Cast("const char*") BytePointer _kind);
@Namespace("torch::jit::tracer") public static native void _do_warn(String _reason, String _kind);
@Namespace("torch::jit::tracer") public static native void warn(@Cast("const char*") BytePointer _reason, @Cast("const char*") BytePointer _kind/*=nullptr*/);
@Namespace("torch::jit::tracer") public static native void warn(@Cast("const char*") BytePointer _reason);
@Namespace("torch::jit::tracer") public static native void warn(String _reason, String _kind/*=nullptr*/);
@Namespace("torch::jit::tracer") public static native void warn(String _reason);
@Namespace("torch::jit::tracer") public static native void setWarn(@ByVal @Cast("torch::jit::tracer::warn_fn_type*") warn_fn_type fn);
// Targeting ../NoWarn.java


// Targeting ../WithNestedTracingFrame.java


@Namespace("torch::jit::tracer") public static native void recordSourceLocation(JitNode n);
// Targeting ../V_JitNode.java


@Namespace("torch::jit::tracer") public static native void setRecordSourceLocation(V_JitNode v);

@Namespace("torch::jit::tracer") public static native @ByVal StackEntryVector pythonCallstack();
// Targeting ../StackEntryVector_V.java


@Namespace("torch::jit::tracer") public static native void setPythonCallstack(StackEntryVector_V v);

// Having finished adding a new 'node' to the graph IR 'setValueTrace'
// associates this node with an output variable, so that further operations
// involving this variable know which node in the IR to reference.
@Namespace("torch::jit::tracer") public static native void setValueTrace(@Const @ByRef IValue v, Value value);

@Namespace("torch::jit::tracer") public static native void delValueTrace(@Const @ByRef IValue var);

@Namespace("torch::jit::tracer") public static native @ByVal @Cast("std::function<void()>*") Pointer pauseTracing();

@Namespace("torch::jit::tracer") public static native Value getValueTrace(@Const @ByRef IValue var);



@Namespace("torch::jit::tracer") public static native void abandon();

// NB: those serve both as an intermediate steps in addInputs below,
// as well as the overloads that terminate template recursion
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, @Cast("int64_t") long value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, @Cast("int64_t") long value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @ByVal LongOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @ByVal LongOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, @Cast("bool") boolean value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, @Cast("bool") boolean value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef BoolOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef BoolOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, double value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, double value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef DoubleOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef DoubleOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, @Const @ByRef Scalar value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, @Const @ByRef Scalar value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef ScalarOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef ScalarOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, @Const @ByRef Tensor value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, @Const @ByRef Tensor value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef TensorOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef TensorOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef LongArrayRefOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef LongArrayRefOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @ByVal TensorArrayRef value,
    @Cast("bool") boolean allow_undefined/*=false*/);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @ByVal TensorArrayRef value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @ByVal TensorArrayRef value,
    @Cast("bool") boolean allow_undefined/*=false*/);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @ByVal TensorArrayRef value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @ByVal @Cast("c10::ArrayRef<c10::intrusive_ptr<c10::ivalue::Object> >*") Pointer value,
    @Const @SharedPtr @ByRef ClassType class_type);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @ByVal @Cast("c10::ArrayRef<c10::intrusive_ptr<c10::ivalue::Object> >*") Pointer value,
    @Const @SharedPtr @ByRef ClassType class_type);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, @ByVal DoubleArrayRef value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, @ByVal DoubleArrayRef value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef DoubleArrayRefOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef DoubleArrayRefOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @ByVal @Cast("const c10::string_view*") Pointer value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @ByVal @Cast("const c10::string_view*") Pointer value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, @ByVal Device value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, @ByVal Device value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, @ByVal Stream stream);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, @ByVal Stream stream);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, @ByVal Layout value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, @ByVal Layout value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, ScalarType value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, ScalarType value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef ScalarTypeOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef ScalarTypeOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef DeviceOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef DeviceOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef LayoutOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef LayoutOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, @ByVal MemoryFormat value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, @ByVal MemoryFormat value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @ByVal DimnameListOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @ByVal DimnameListOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef MemoryFormatOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef MemoryFormatOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef GeneratorOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef GeneratorOptional value);

@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef BoolVector value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef BoolVector value);

@Namespace("torch::jit::tracer") public static native void ensureUniqueIfOutOfPlaced(
    @Cast("const char*") BytePointer name,
    @Const @ByRef Tensor tensor);
@Namespace("torch::jit::tracer") public static native void ensureUniqueIfOutOfPlaced(
    String name,
    @Const @ByRef Tensor tensor);
@Namespace("torch::jit::tracer") public static native void ensureUniqueIfOutOfPlaced(
    @Cast("const char*") BytePointer name,
    @Const @ByRef TensorOptional tensor);
@Namespace("torch::jit::tracer") public static native void ensureUniqueIfOutOfPlaced(
    String name,
    @Const @ByRef TensorOptional tensor);
@Namespace("torch::jit::tracer") public static native void addOutput(JitNode node, @Const @ByRef Tensor tensor);
@Namespace("torch::jit::tracer") public static native void setOutput(Value value, @Const @ByRef Tensor output);
@Namespace("torch::jit::tracer") public static native void addOutput(JitNode node, @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector list);
@Namespace("torch::jit::tracer") public static native void addOutput(
    JitNode node,
    @Cast("const c10::intrusive_ptr<c10::ivalue::Object>*") @ByRef Pointer output);

@Namespace("torch::jit::tracer") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor getSizeOf(
    @Cast("const torch::autograd::Variable*") @ByRef Tensor var,
    @Cast("int64_t") long dim);

 // namespace tracer
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/lexer.h

// #pragma once
// #include <ATen/core/Macros.h>
// #include <c10/util/C++17.h>
// #include <c10/util/Exception.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/csrc/jit/frontend/parser_constants.h>
// #include <torch/csrc/jit/frontend/source_range.h>
// #include <torch/csrc/jit/frontend/strtod.h>
// #include <algorithm>
// #include <clocale>
// #include <cstdlib>
// #include <memory>
// #include <sstream>
// #include <string>
// #include <vector>

// single character tokens are just the character itself '+'
// multi-character tokens need an entry here
// if the third entry is not the empty string, it is used
// in the lexer to match this token.

// These kinds are also used in Tree.h as the kind of the AST node.
// Some kinds TK_APPLY, TK_LIST are only used in the AST and are not seen in the
// lexer.

// #define TC_FORALL_TOKEN_KINDS(_)
//   _(TK_EOF, "eof", "")
//   _(TK_WHITESPACE, "whitespace", "")
//   _(TK_WHITESPACE_EOF, "whitespace_eof", "")
//   _(TK_NUMBER, "number", "")
//   _(TK_NEWLINE, "newline", "")
//   _(TK_INDENT, "indent", "")
//   _(TK_DEDENT, "dedent", "")
//   _(TK_DEF, "def", "def")
//   _(TK_EQUIVALENT, "equivalent", "<=>")
//   _(TK_IDENT, "ident", "")
//   _(TK_STRING, "string", "")
//   _(TK_STRINGLITERAL, "string_literal", "")
//   _(TK_CONST, "const", "")
//   _(TK_LIST, "list", "")
//   _(TK_DICT, "dict", "")
//   _(TK_OPTION, "option", "")
//   _(TK_APPLY, "apply", "")
//   _(TK_COMPREHENSION, "comprehension", "")
//   _(TK_RANGE_CONSTRAINT, "range_constraint", "")
//   _(TK_PARAM, "param", "")
//   _(TK_INFERRED, "inferred", "")
//   _(TK_ACCESS, "access", "")
//   _(TK_ASSIGN, "assign", "")
//   _(TK_AUG_ASSIGN, "aug_assign", "")
//   _(TK_ATTRIBUTE, "attribute", "")
//   _(TK_IF, "if", "if")
//   _(TK_ELSE, "else", "else")
//   _(TK_ELIF, "elif", "elif")
//   _(TK_WHILE, "while", "while")
//   _(TK_EXPR_STMT, "expression statement", "")
//   _(TK_RETURN, "return", "return")
//   _(TK_IS, "is", "is")
//   _(TK_ISNOT, "is not", "is not")
//   _(TK_NE, "ne", "!=")
//   _(TK_EQ, "eq", "==")
//   _(TK_LE, "le", "<=")
//   _(TK_GE, "ge", ">=")
//   _(TK_FLOOR_DIV, "floordiv", "//")
//   _(TK_IF_EXPR, "if", "")
//   _(TK_TRUE, "True", "True")
//   _(TK_FALSE, "False", "False")
//   _(TK_NONE, "None", "None")
//   _(TK_AND, "and", "and")
//   _(TK_OR, "or", "or")
//   _(TK_NOT, "not", "not")
//   _(TK_LSHIFT, "<<", "<<")
//   _(TK_RSHIFT, ">>", ">>")
//   _(TK_CAST, "cast", "")
//   _(TK_PLUS_EQ, "+=", "+=")
//   _(TK_MINUS_EQ, "-=", "-=")
//   _(TK_TIMES_EQ, "*=", "*=")
//   _(TK_DIV_EQ, "/=", "/=")
//   _(TK_MOD_EQ, "%=", "%=")
//   _(TK_BIT_OR_EQ, "|=", "|=")
//   _(TK_BIT_AND_EQ, "&=", "&=")
//   _(TK_BIT_XOR_EQ, "^=", "^=")
//   _(TK_LSHIFT_EQ, "<<=", "<<=")
//   _(TK_RSHIFT_EQ, ">>=", ">>=")
//   _(TK_POW_EQ, "**=", "**=")
//   _(TK_GLOBAL, "global", "global")
//   _(TK_BUILT_IN, "built-in", "")
//   _(TK_SUBSCRIPT, "subscript", "")
//   _(TK_VAR, "variable", "")
//   _(TK_NOTHING, "nothing", "")
//   _(TK_DICT_LITERAL, "dict-literal", "")
//   _(TK_LIST_LITERAL, "list-literal", "")
//   _(TK_TUPLE_LITERAL, "tuple-literal", "")
//   _(TK_FOR, "for", "for")
//   _(TK_IN, "in", "in")
//   _(TK_NOTIN, "not in", "not in")
//   _(TK_STARRED, "starred", "")
//   _(TK_UNARY_MINUS, "unary minus", "")
//   _(TK_POW, "pow operator", "**")
//   _(TK_ARROW, "arrow", "->")
//   _(TK_DECL, "decl", "")
//   _(TK_SLICE_EXPR, "slice expr", "")
//   _(TK_TYPE_COMMENT, "type comment", "# type:")
//   _(TK_RAISE, "raise", "raise")
//   _(TK_ASSERT, "assert", "assert")
//   _(TK_DOTS, "dots", "...")
//   _(TK_LIST_COMP, "list comprehension", "")
//   _(TK_DICT_COMP, "dict comprehension", "")
//   _(TK_BREAK, "break", "break")
//   _(TK_CONTINUE, "continue", "continue")
//   _(TK_DELETE, "del", "del")
//   _(TK_PASS, "pass", "pass")
//   _(TK_CLASS_DEF, "class", "class")
//   _(TK_IMPORT, "import", "import")
//   _(TK_WITH, "with", "with")
//   _(TK_WITH_ITEM, "withitem", "")
//   _(TK_AS, "as", "as")
//   _(TK_PROP, "property", "")
//   _(TK_ELLIPSIS, "Ellipsis", "Ellipsis")
//   _(TK_NONE_TYPE, "NoneType", "NoneType")

@Namespace("torch::jit") public enum TokenKind {
  // we use characters to represent themselves so skip all valid characters
  // before
  // assigning enum values to multi-char tokens.
  TK_DUMMY_START(256),
  TK_EOF(257),
  TK_WHITESPACE(258),
  TK_WHITESPACE_EOF(259),
  TK_NUMBER(260),
  TK_NEWLINE(261),
  TK_INDENT(262),
  TK_DEDENT(263),
  TK_DEF(264),
  TK_EQUIVALENT(265),
  TK_IDENT(266),
  TK_STRING(267),
  TK_STRINGLITERAL(268),
  TK_CONST(269),
  TK_LIST(270),
  TK_DICT(271),
  TK_OPTION(272),
  TK_APPLY(273),
  TK_COMPREHENSION(274),
  TK_RANGE_CONSTRAINT(275),
  TK_PARAM(276),
  TK_INFERRED(277),
  TK_ACCESS(278),
  TK_ASSIGN(279),
  TK_AUG_ASSIGN(280),
  TK_ATTRIBUTE(281),
  TK_IF(282),
  TK_ELSE(283),
  TK_ELIF(284),
  TK_WHILE(285),
  TK_EXPR_STMT(286),
  TK_RETURN(287),
  TK_IS(288),
  TK_ISNOT(289),
  TK_NE(290),
  TK_EQ(291),
  TK_LE(292),
  TK_GE(293),
  TK_FLOOR_DIV(294),
  TK_IF_EXPR(295),
  TK_TRUE(296),
  TK_FALSE(297),
  TK_NONE(298),
  TK_AND(299),
  TK_OR(300),
  TK_NOT(301),
  TK_LSHIFT(302),
  TK_RSHIFT(303),
  TK_CAST(304),
  TK_PLUS_EQ(305),
  TK_MINUS_EQ(306),
  TK_TIMES_EQ(307),
  TK_DIV_EQ(308),
  TK_MOD_EQ(309),
  TK_BIT_OR_EQ(310),
  TK_BIT_AND_EQ(311),
  TK_BIT_XOR_EQ(312),
  TK_LSHIFT_EQ(313),
  TK_RSHIFT_EQ(314),
  TK_POW_EQ(315),
  TK_GLOBAL(316),
  TK_BUILT_IN(317),
  TK_SUBSCRIPT(318),
  TK_VAR(319),
  TK_NOTHING(320),
  TK_DICT_LITERAL(321),
  TK_LIST_LITERAL(322),
  TK_TUPLE_LITERAL(323),
  TK_FOR(324),
  TK_IN(325),
  TK_NOTIN(326),
  TK_STARRED(327),
  TK_UNARY_MINUS(328),
  TK_POW(329),
  TK_ARROW(330),
  TK_DECL(331),
  TK_SLICE_EXPR(332),
  TK_TYPE_COMMENT(333),
  TK_RAISE(334),
  TK_ASSERT(335),
  TK_DOTS(336),
  TK_LIST_COMP(337),
  TK_DICT_COMP(338),
  TK_BREAK(339),
  TK_CONTINUE(340),
  TK_DELETE(341),
  TK_PASS(342),
  TK_CLASS_DEF(343),
  TK_IMPORT(344),
  TK_WITH(345),
  TK_WITH_ITEM(346),
  TK_AS(347),
  TK_PROP(348),
  TK_ELLIPSIS(349),
  TK_NONE_TYPE(350);

    public final int value;
    private TokenKind(int v) { this.value = v; }
    private TokenKind(TokenKind e) { this.value = e.value; }
    public TokenKind intern() { for (TokenKind e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("torch::jit") public static native @StdString BytePointer kindToString(int kind);
@Namespace("torch::jit") public static native int stringToKind(@StdString BytePointer str);
@Namespace("torch::jit") public static native int stringToKind(@StdString String str);

// nested hash tables that indicate char-by-char what is a valid token.
// Targeting ../TokenTrie.java


// Targeting ../SharedParserData.java



@Namespace("torch::jit") public static native @ByRef SharedParserData sharedParserData();
// Targeting ../Token.java


// Targeting ../Lexer.java


 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/strtod.h

// #pragma once

// #include <ATen/core/Macros.h>

@Namespace("torch::jit") public static native double strtod_c(@Cast("const char*") BytePointer nptr, @Cast("char**") PointerPointer endptr);
@Namespace("torch::jit") public static native double strtod_c(@Cast("const char*") BytePointer nptr, @Cast("char**") @ByPtrPtr BytePointer endptr);
@Namespace("torch::jit") public static native double strtod_c(String nptr, @Cast("char**") @ByPtrPtr ByteBuffer endptr);
@Namespace("torch::jit") public static native double strtod_c(@Cast("const char*") BytePointer nptr, @Cast("char**") @ByPtrPtr byte[] endptr);
@Namespace("torch::jit") public static native double strtod_c(String nptr, @Cast("char**") @ByPtrPtr BytePointer endptr);
@Namespace("torch::jit") public static native double strtod_c(@Cast("const char*") BytePointer nptr, @Cast("char**") @ByPtrPtr ByteBuffer endptr);
@Namespace("torch::jit") public static native double strtod_c(String nptr, @Cast("char**") @ByPtrPtr byte[] endptr);
@Namespace("torch::jit") public static native float strtof_c(@Cast("const char*") BytePointer nptr, @Cast("char**") PointerPointer endptr);
@Namespace("torch::jit") public static native float strtof_c(@Cast("const char*") BytePointer nptr, @Cast("char**") @ByPtrPtr BytePointer endptr);
@Namespace("torch::jit") public static native float strtof_c(String nptr, @Cast("char**") @ByPtrPtr ByteBuffer endptr);
@Namespace("torch::jit") public static native float strtof_c(@Cast("const char*") BytePointer nptr, @Cast("char**") @ByPtrPtr byte[] endptr);
@Namespace("torch::jit") public static native float strtof_c(String nptr, @Cast("char**") @ByPtrPtr BytePointer endptr);
@Namespace("torch::jit") public static native float strtof_c(@Cast("const char*") BytePointer nptr, @Cast("char**") @ByPtrPtr ByteBuffer endptr);
@Namespace("torch::jit") public static native float strtof_c(String nptr, @Cast("char**") @ByPtrPtr byte[] endptr);

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/tree.h

// #pragma once

// #include <functional>
// #include <memory>
// #include <unordered_map>
// #include <vector>

// #include <c10/util/SmallVector.h>
// #include <c10/util/intrusive_ptr.h>
// #include <torch/csrc/jit/frontend/lexer.h>

// Trees are used to represent all forms of TC IR, pre- and post-typechecking.
// Rather than have a full class hierarchy for all TC statements, trees are a
// slight variation of Lisp s-expressions. For instance, the expression a*b+1
// is represented as:
// (+ (* (ident a) (ident b)) (const 1))
// Atoms like 'a', 'b', and '1' are represented by subclasses of Tree which
// define stringValue(). Everything else is a Compound object, which has a
// 'kind' that is a token from lexer.h's TokenKind enum. Single-character
// operators like '+' are represented using the character itself (so, add.kind()
// would be '+'). Each Compound object also contains a list of subtrees and is
// associated with a SourceRange for error reporting.
// Memory management of trees is done using intrusive_ptr.

@Namespace("torch::jit") @MemberGetter public static native @ByRef @Cast("const torch::jit::TreeList*") Pointer empty_trees();
// Targeting ../Tree.java


// Targeting ../JitString.java



@Namespace("torch::jit") public static native @ByVal SourceRange mergeRanges(@ByVal SourceRange c, @Cast("const torch::jit::TreeList*") @ByRef Pointer others);
// Targeting ../Compound.java


// Targeting ../pretty_tree.java



@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @ByVal pretty_tree t_);

@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Cast("const torch::jit::TreeRef*") @ByRef Pointer t);

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/error_report.h

// #pragma once

// #include <c10/util/Optional.h>
// #include <torch/csrc/jit/frontend/tree.h>
// Targeting ../Call.java


// Targeting ../ErrorReport.java



 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/tree_views.h

// #pragma once
// #include <c10/util/string_utils.h>
// #include <torch/csrc/jit/frontend/error_report.h>
// #include <torch/csrc/jit/frontend/strtod.h>
// #include <torch/csrc/jit/frontend/tree.h>

// #include <c10/util/complex.h>
// #include <functional>
// #include <iostream>
// #include <string>
// Targeting ../TreeView.java


// Targeting ../DefMaybe.java


// Targeting ../ExprMaybe.java


// Targeting ../VarMaybe.java


// Targeting ../Ident.java


// Targeting ../Stmt.java


// Targeting ../Expr.java


// Targeting ../Attribute.java


// Targeting ../Param.java


// Targeting ../Decl.java


// Targeting ../Def.java


// Targeting ../Property.java


// Targeting ../ClassDef.java




// Targeting ../If.java


// Targeting ../While.java


// Targeting ../For.java


// Targeting ../ListComp.java


// Targeting ../DictComp.java


// Targeting ../Global.java


// Targeting ../AugAssignKind.java


// Targeting ../AugAssign.java


// Targeting ../Assign.java


// Targeting ../Return.java


// Targeting ../Raise.java


// Targeting ../Assert.java


// Targeting ../Pass.java


// Targeting ../Dots.java


// Targeting ../Break.java


// Targeting ../Continue.java


// Targeting ../ExprStmt.java


// Targeting ../BinOp.java


// Targeting ../UnaryOp.java


// Targeting ../ConstExpr.java


// Targeting ../StringLiteral.java


// Targeting ../Apply.java


// Targeting ../Select.java


// Targeting ../SliceExpr.java


// Targeting ../Subscript.java


// Targeting ../Var.java


// Targeting ../WithItem.java


// Targeting ../With.java


// Targeting ../TernaryIf.java


// Targeting ../ListLiteral.java


// Targeting ../TupleLiteral.java


// Targeting ../DictLiteral.java


// Targeting ../Starred.java


// Targeting ../Delete.java



 // namespace jit
 // namespace torch

 // namespace std


// Parsed from torch/csrc/jit/ir/attributes.h

// #pragma once
// #include <ATen/ATen.h>
// #include <string>
// #include <vector>

// #include <ATen/core/interned_strings.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>
 // namespace c10

@Namespace("torch::jit") @MemberGetter public static native int max_tensor_display_size();

@Name("torch::jit::AttributeKind") public enum JitAttributeKind {
  f(0),
  fs(1),
  c(2),
  cs(3),
  i(4),
  is(5),
  s(6),
  ss(7),
  t(8),
  ts(9),
  g(10),
  gs(11),
  ty(12),
  tys(13),
  ival(14);

    public final int value;
    private JitAttributeKind(int v) { this.value = v; }
    private JitAttributeKind(JitAttributeKind e) { this.value = e.value; }
    public JitAttributeKind intern() { for (JitAttributeKind e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
@Namespace("torch::jit") public static native @Cast("const char*") BytePointer toString(JitAttributeKind kind);
// Targeting ../AttributeValue.java


// Targeting ../GraphAttr.java


// Targeting ../GraphsAttr.java


// Targeting ../IRAttributeError.java


 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/ir/constants.h

// #pragma once
// #include <ATen/core/ivalue.h>
// #include <ATen/core/jit_type.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/csrc/jit/frontend/source_range.h>
// #include <torch/csrc/jit/ir/scope.h>

// helpers for handling constants in the IR
// - create constant nodes from ints, floats, complex, intlist, Tensors, and
// other types
// - implement primitive constant ops.

// thrown when insertConstant cannot encode the IValue into a graph

@Namespace("torch::jit") public static native Value insertConstant(
    @ByRef Graph g,
    @Const @ByRef IValue val,
    @ByVal(nullValue = "c10::optional<torch::jit::SourceRange>(c10::nullopt)") SourceRangeOptional loc,
    @ByVal(nullValue = "c10::optional<torch::jit::ScopePtr>(c10::nullopt)") @Cast("c10::optional<torch::jit::ScopePtr>*") ScopeOptional scope);
@Namespace("torch::jit") public static native Value insertConstant(
    @ByRef Graph g,
    @Const @ByRef IValue val);

// note: prefer g.insertConsant(val, loc) which does exactly the same thing
// this function is only declared/defined here because its implementation is
// closely related to the implementation of prim::Constant that is also in
// constants.cpp.
//
// returns a c10::nullopt if the IValue kind cannot be inserted as a constant
@Namespace("torch::jit") public static native @ByVal ValueOptional tryInsertConstant(
    @ByRef Graph g,
    @Const @ByRef IValue val,
    @ByVal(nullValue = "c10::optional<torch::jit::SourceRange>(c10::nullopt)") SourceRangeOptional loc,
    @ByVal(nullValue = "c10::optional<torch::jit::ScopePtr>(c10::nullopt)") @Cast("c10::optional<torch::jit::ScopePtr>*") ScopeOptional scope);
@Namespace("torch::jit") public static native @ByVal ValueOptional tryInsertConstant(
    @ByRef Graph g,
    @Const @ByRef IValue val);

////////////////////////////////////////////////////////////////////////////////
// Helper for retrieving constants
////////////////////////////////////////////////////////////////////////////////

// attempt to convert a (possibly constant) Value* into an interpreter value
// (IValue). returns c10::nullopt if the Value* was not constant
@Namespace("torch::jit") public static native @ByVal IValueOptional toIValue(@Const Value v);

// if a value is a constant then try to turn into type T using the
// same rules as the interpreter
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/ir/graph_node_list.h

// #pragma once

// #include <c10/util/Exception.h>

// Intrusive doubly linked lists with sane reverse iterators.
// The header file is named generic_graph_node_list.h because it is ONLY
// used for Graph's Node lists, and if you want to use it for other
// things, you will have to do some refactoring.
//
// At the moment, the templated type T must support a few operations:
//
//  - It must have a field: T* next_in_graph[2] = { nullptr, nullptr };
//    which are used for the intrusive linked list pointers.
//
//  - It must have a method 'destroy()', which removes T from the
//    list and frees a T.
//
// In practice, we are only using it with Node and const Node.  'destroy()'
// needs to be renegotiated if you want to use this somewhere else.
//
// Regardless of the iteration direction, iterators always physically point
// to the element they logically point to, rather than
// the off-by-one behavior for all standard library reverse iterators like
// std::list.

// The list is includes two sentinel nodes, one at the beginning and one at the
// end with a circular link between them. It is an error to insert nodes after
// the end sentinel node but before the beginning node:

// Visualization showing only the next() links:
//  HEAD -> first -> second  -> ... -> last -> TAIL
//   ^------------------------------------------

// Visualization showing only the prev() links:
//  HEAD <- first <- second  <- ... <- last <- TAIL
//   ------------------------------------------^

@Namespace("torch::jit") @MemberGetter public static native int kNextDirection();
public static final int kNextDirection = kNextDirection();
@Namespace("torch::jit") @MemberGetter public static native int kPrevDirection();
public static final int kPrevDirection = kPrevDirection();
// Targeting ../graph_node_list_iterator.java


// Targeting ../graph_node_list.java



 // namespace jit
 // namespace torch

 // namespace std


// Parsed from torch/csrc/jit/ir/named_value.h

// #pragma once
// #include <ATen/ATen.h>
// #include <ATen/core/ivalue.h>
// #include <torch/csrc/jit/frontend/source_range.h>
// #include <torch/csrc/jit/ir/constants.h>
// #include <torch/csrc/utils/variadic.h>
// Targeting ../NamedValue.java



 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/ir/scope.h

// #pragma once
// #include <ATen/core/interned_strings.h>
// #include <ATen/core/jit_type.h>
// #include <c10/util/Optional.h>
// #include <c10/util/intrusive_ptr.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/csrc/jit/frontend/source_range.h>
// #include <unordered_map>
@Namespace("torch::jit") @MemberGetter public static native @Cast("const size_t") long kModuleInstanceInfo();

 // namespace utils

// Scope is a node of a trie that represents the tree of nested scopes.
// Individual scopes are pushed and popped from Graph, which holds a
// pointer to the current scope. Each Node in Graph holds a pointer
// to the scope that was current when the node was created.
// The trie never needs to shrink, it only grows until it is disposed
// of when Graph is deallocated. Hence, pointers to scopes held by nodes
// will always be valid as long as Graph is alive.
// Targeting ../Scope.java


// Targeting ../ModuleInstanceInfo.java



/**
 * InlinedCallStack is an element in a list representing callstack of functions
 * that have been inlined.
 *
 * Each such element holds info about the current callsite (Function and
 * SourceRange) and a pointer to the next element in the list. The last element
 * in the list represents the innermost function that was inlined.
 *
 * For instance, if a node has a callstack
 *    [foo, source_range1] -> [bar, source_range2]
 * it means that this node was originally from function 'bar' that was called
 * at 'source_range2' in function 'foo' that was called in the current function
 * at 'source_range1'.
 *
 * If a node did not come from any inlined function, its callstack will be
 * empty.
 *
 * The callstack lists only grow, we never remove elements from them, which
 * allows us to reuse same elements in different lists. For instance, if we
 * inline function 'bar' to 'foo' and then inline 'foo' to two functions 'ham'
 * and 'baz', the callstacks would look like:
 *
 *  [baz, source_range3]  --
 *                           \
 *                             --> [foo, source_range1] -> [bar, source_range2]
 *                           /
 *  [ham, source_range4]  --
 */
// Targeting ../InlinedCallStack.java



// {source range, node name, InlinedCallStack}
// We store node name because same debug infor will be used for
// profiling as well, so we need to know op names as well.
@Namespace("torch::jit") @MemberGetter public static native @Cast("const size_t") long kDebugInfoTupleSourceRangeIndex();
@Namespace("torch::jit") @MemberGetter public static native @Cast("const size_t") long kDebugInfoTupleNodeNameIndex();
@Namespace("torch::jit") @MemberGetter public static native @Cast("const size_t") long kDebugInfoTupleInlinedCSIndex();
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/ir/ir.h

// #pragma once

// #include <torch/csrc/jit/ir/attributes.h>
// #include <torch/csrc/jit/ir/graph_node_list.h>
// #include <torch/csrc/jit/ir/named_value.h>
// #include <torch/csrc/jit/ir/scope.h>
// #include <torch/csrc/jit/runtime/operator.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/csrc/utils/disallow_copy.h>
// #include <torch/csrc/utils/python_stub.h>

// #include <ATen/ATen.h>
// #include <ATen/core/function_schema.h>
// #include <ATen/core/functional.h>
// #include <ATen/core/interned_strings.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/jit_type.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>

// #include <functional>
// #include <iostream>
// #include <unordered_set>
// #include <vector>

// Forward declare, the real meat is in python_ir.cpp
@Namespace("torch::jit::utils") public static native @StdString BytePointer getNodesModuleHierarchy(@Const @ByRef JitNode n);

// Targeting ../AliasDb.java



// #define C10_USING(T) using ::c10::T;
// #undef C10_USING

// #define C10_USING(T) using ::c10::T##Ptr;
// #undef C10_USING



// #ifndef __HIP_PLATFORM_HCC__
// #endif

// Targeting ../MatchedSchema.java



// A Graph represents one "function" of computation.
// It uses a simple ownership model where the graph owns all the nodes inside
// it. All references inside the graph are raw pointers. Destroying the Graph
// will invalidate any pointers to nodes in the graph.

// Node is the base class of the IR graph. It represents one computation
// and dependencies on a list of Values. The "prim-ops", so to speak.

// A Value represents an input or output to node that is either a
// Tensor or an opaque Handle object, as determined by type().

@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Graph g);
@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef JitNode n);

// A list of nodes, with inputs and outputs
// Targeting ../Use.java



// Note [User node does not uniquely identify use]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// A while back, we wrote some code manipulating uses that looked like this:
//
//    for (auto& use : used_val->uses_) {
//      if (use.user == this_node) {
//        use.offset += 1;
//        break;
//      }
//    }
//
// This code is trying to find a particular use (our node's use) to update it.
// However, it's wrong: there may be *multiple* uses of a value %x in a node,
// as might be the case in this IR:
//
//    %y = Add %x %x
//
// In this case, there are two uses of %x whose user is the node 'Add %x %x'.
// So, "use induced by this node" is not a well-formed concept.
//
// If you are looking for "use induced by an input", it's best to use
// findUseForInput() to get it.

// the list types are intentionally simple, but we type-def
// them here so if we need to change them, refactoring will be easier
// Targeting ../BlockWrap.java


// Targeting ../JitNodeWrap.java


// Targeting ../ValueWrap.java


// Targeting ../Value.java


// Targeting ../JitNode.java


// Targeting ../Block.java


// Targeting ../Graph.java


// Targeting ../WithInsertPoint.java


// Targeting ../WithCurrentScope.java



// NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)








/************* All nodes not required to be defined before Graph **************/
// Targeting ../ProfileIValueOp.java


// Targeting ../PythonOp.java



@Namespace("torch::jit") public static native void LintGraph(@Const @SharedPtr @ByRef Graph graph);

@Namespace("torch::jit") public static native @ByVal ValueArrayRef createTupleUnpack(Value v);

/** Insert graph \p CALLEE into graph \p G using \p INPUTS as input values.
 * The insertion happens at the current insertion point.
 * Optionally, one can also pass \p VALUE_MAP to get a map between \p CALLEE
 * values and their cloned copies in \p G.
 */
@Namespace("torch::jit") public static native @ByVal ValueVector insertGraph(
    @ByRef Graph g,
    @ByRef Graph callee,
    @ByVal ValueArrayRef inputs);
@Namespace("torch::jit") public static native @ByVal ValueVector insertGraph(
    @ByRef Graph g,
    @ByRef Graph callee,
    @ByVal ValueArrayRef inputs,
    @ByRef ValueValueMap value_map);

/** Insert function \p CALLEE after node \p TO_REPLACE, remove the node and
 * replace all its uses with corresponding outputs of the inserted function.
 * This asserts that the number of outputs of the original node and the
 * graph are the same.
 */
@Namespace("torch::jit") public static native @ByVal ValueVector inlineCallTo(
    JitNode to_replace,
    Function callee,
    @Cast("bool") boolean use_graph/*=true*/);
@Namespace("torch::jit") public static native @ByVal ValueVector inlineCallTo(
    JitNode to_replace,
    Function callee);

/** If there is only one value in \p OUTPUTS and its kind is Tuple, insert a
 * tuple unpack node and return the resulting values.
 */
@Namespace("torch::jit") public static native @ByVal ValueVector unpackOutputs(@Const @ByRef ValueVector outputs);
// Targeting ../OperatorSet.java



 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/ir/type_hashing.h

// #pragma once

// #include <ATen/core/jit_type.h>
// #include <torch/csrc/jit/ir/ir.h>
// Targeting ../HashType.java


// Targeting ../EqualType.java



 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/passes/shape_analysis.h

// #pragma once

// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/csrc/jit/ir/ir.h>
// #include <memory>

@Namespace("torch::jit") public static native void EraseShapeInformation(@Const @SharedPtr @ByRef Graph graph);
@Namespace("torch::jit") public static native void PropagateInputShapes(@Const @SharedPtr @ByRef Graph graph);

@Namespace("torch::jit") public static native @Cast("bool") boolean mergeTypes(
    @ByVal ValueArrayRef lhs,
    @ByVal ValueArrayRef rhs,
    @ByVal ValueArrayRef outputs);

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/python/update_graph_executor_opt.h

// #pragma once
// #include <torch/csrc/WindowsTorchApiMacro.h>
@Namespace("torch::jit") public static native void setGraphExecutorOptimize(@Cast("bool") boolean o);
@Namespace("torch::jit") public static native @Cast("bool") boolean getGraphExecutorOptimize();
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/runtime/argument_spec.h

// #pragma once

// #include <ATen/core/jit_type.h>
// #include <ATen/core/stack.h>
// #include <c10/util/hash.h>
// #include <c10/util/irange.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/csrc/autograd/variable.h>
// #include <torch/csrc/jit/ir/ir.h>
// #include <iostream>
// #include <vector>
// Targeting ../ArgumentInfo.java


// Targeting ../ArgumentSpec.java


@Namespace("torch::jit") @MemberGetter public static native @Cast("const size_t") long ARG_SPEC_DEPTH_LIMIT();
public static final long ARG_SPEC_DEPTH_LIMIT = ARG_SPEC_DEPTH_LIMIT();

// Targeting ../ArgumentSpecCreator.java


// Targeting ../CompleteArgumentInfoPOD.java


// Targeting ../CompleteArgumentSpec.java


// Targeting ../CompleteArgumentInfo.java



@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef ArgumentInfo info);

@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef ArgumentSpec spec);

@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(
    @Cast("std::ostream*") @ByRef Pointer out,
    @Const @ByRef CompleteArgumentInfo info);

@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(
    @Cast("std::ostream*") @ByRef Pointer out,
    @Const @ByRef CompleteArgumentSpec spec);



@Namespace("torch::jit") public static native @ByVal ByteOptional convertOptional(
    @Const @ByRef ScalarTypeOptional from);

 // namespace jit
 // namespace torch
 // namespace std


// Parsed from torch/csrc/jit/runtime/instruction.h

// #pragma once

// #include <cstdint>
// #include <typeinfo>
// #include <unordered_set>
// instruction look like:
// op_code X, N
// meaning of X, N depend on the op:
// O - index into operator table
// R - index into register table
// I - literal integer
// C - index into constant table
// P - jump offset relative to beginning of current instruction
// F - index into function table
// T - index into the type table, used for guard instructions
// S - index into object slots
// C - index into code table

// #define FORALL_OPCODES(_)
//   _(OP, "O") /* invoke operator X */
//   _(OPN, "OI") /* invoke vararg operator X with N arguments */
//   _(LOAD, "R") /* push a value from a register X */
//   _(MOVE, "R") /* push a value from register X, clearing the register */
//   _(STOREN, "RI") /* store N values to registers [X, X+N) */
//   _(STORE, "R") /* store 1 value to registers X */
//   _(DROP, "") /* drop 1 value from the top of the stack */
//   _(DROPR, "R") /* clear register X */
//   _(LOADC, "C") /* push the constant X */
//   _(JF, "P") /* pop the top of the stack, if false, branch to P */
//   _(JMP, "P") /* unconditional branch to X */
//   _(LOOP, "PI") /* perform a loop, X is where to branch if cond is false */
//   _(RET, "") /* exit execution */
//   _(WAIT, "") /* wait for a future to be complete */
//   _(CALL, "F") /* call function X */
//   _(GUARD, "T") /* check a guard against type_table, true if passes */
//   _(TYPECHECK, "TN") /* check each type of input[i] against type_table[X+N] */
//   _(FAIL_GUARD, "T") /* fail a guard, patch back to GUARD */
//   _(PROFILE_OP, "F") /* get a callback from profile_function_table at X */
//   _(TAIL_CALL, "F") /* replace current frame with function F */
//   _(INTERFACE_CALL, "CI") /* call method X on the first argument (of N) */
//   _(GET_ATTR, "S") /* get attribute from slot X in an Object */
//   _(SET_ATTR, "S") /* set attribute to slot X in an Object */
//   _(LIST_UNPACK, "I") /* unpack list expecting length I */
//   _(TUPLE_CONSTRUCT, "I") /* construct a tuple using X inputs */
//   _(NAMED_TUPLE_CONSTRUCT,
//     "TI") /* construct a tuple of type X, using N inputs */
//   _(LIST_CONSTRUCT, "TI") /* construct a list of type X, using N inputs */
//   _(DICT_CONSTRUCT, "TI") /* construct a dict of type X, using N inputs */
//   _(CREATE_OBJECT, "T") /* create an object of type X */
//   _(ISINSTANCE, "TI") /* check object is one of  types[X:X+N]  */
//   _(TUPLE_SLICE, "II") /* slice tup[X:(X+N)] */
//   _(FORK, "CN") /* launch a thread to run code entry x with N inputs  */
//   _(WARN, "I") /* emit a warning with line information */
//   _(ENTER, "EN") /* enter scope of a contextmanager */
//   _(EXIT, "EX") /* exit the last entered contextmanager */

@Namespace("torch::jit") public enum OpCode {
  OP((byte)(0)), /* invoke operator X */
  OPN((byte)(1)), /* invoke vararg operator X with N arguments */
  LOAD((byte)(2)), /* push a value from a register X */
  MOVE((byte)(3)), /* push a value from register X, clearing the register */
  STOREN((byte)(4)), /* store N values to registers [X, X+N) */
  STORE((byte)(5)), /* store 1 value to registers X */
  DROP((byte)(6)), /* drop 1 value from the top of the stack */
  DROPR((byte)(7)), /* clear register X */
  LOADC((byte)(8)), /* push the constant X */
  JF((byte)(9)), /* pop the top of the stack, if false, branch to P */
  JMP((byte)(10)), /* unconditional branch to X */
  LOOP((byte)(11)), /* perform a loop, X is where to branch if cond is false */
  RET((byte)(12)), /* exit execution */
  WAIT((byte)(13)), /* wait for a future to be complete */
  CALL((byte)(14)), /* call function X */
  GUARD((byte)(15)), /* check a guard against type_table, true if passes */
  TYPECHECK((byte)(16)), /* check each type of input[i] against type_table[X+N] */
  FAIL_GUARD((byte)(17)), /* fail a guard, patch back to GUARD */
  PROFILE_OP((byte)(18)), /* get a callback from profile_function_table at X */
  TAIL_CALL((byte)(19)), /* replace current frame with function F */
  INTERFACE_CALL((byte)(20)), /* call method X on the first argument (of N) */
  GET_ATTR((byte)(21)), /* get attribute from slot X in an Object */
  SET_ATTR((byte)(22)), /* set attribute to slot X in an Object */
  LIST_UNPACK((byte)(23)), /* unpack list expecting length I */
  TUPLE_CONSTRUCT((byte)(24)), /* construct a tuple using X inputs */
  NAMED_TUPLE_CONSTRUCT((byte)(25)), /* construct a tuple of type X, using N inputs */
  LIST_CONSTRUCT((byte)(26)), /* construct a list of type X, using N inputs */
  DICT_CONSTRUCT((byte)(27)), /* construct a dict of type X, using N inputs */
  CREATE_OBJECT((byte)(28)), /* create an object of type X */
  ISINSTANCE((byte)(29)), /* check object is one of  types[X:X+N]  */
  TUPLE_SLICE((byte)(30)), /* slice tup[X:(X+N)] */
  FORK((byte)(31)), /* launch a thread to run code entry x with N inputs  */
  WARN((byte)(32)), /* emit a warning with line information */
  ENTER((byte)(33)), /* enter scope of a contextmanager */
  EXIT((byte)(34));

    public final byte value;
    private OpCode(byte v) { this.value = v; }
    private OpCode(OpCode e) { this.value = e.value; }
    public OpCode intern() { for (OpCode e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../Instruction.java





 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/runtime/interpreter.h

// #pragma once
// #include <c10/util/Optional.h>
// #include <memory>
// #include <vector>

// #include <ATen/ThreadLocalState.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/jit_type.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/csrc/jit/frontend/source_range.h>



 // namespace at
 // namespace c10
// Targeting ../CodeImpl.java



// Targeting ../InterpreterStateImpl.java


// Targeting ../Code.java


// Targeting ../MobileCode.java


// Targeting ../InterpreterState.java


// Targeting ../Suspend.java


// Targeting ../InterpreterContinuation.java



// what is the tensors type, including state from the current execution context
// that modifies how the tensor behaves. For instance if no_grad is enabled
// this will cause the TensorType to have requires_grad=False.
@Namespace("torch::jit") public static native @SharedPtr @ByVal TensorType tensorTypeInCurrentExecutionContext(
    @Const @ByRef Tensor t);

// current (TLS) TorchScript interpreter callstack
@Namespace("torch::jit") public static native @ByVal StackEntryVector currentCallstack();
@Namespace("torch::jit") public static native @ByVal StringVector currentModuleHierarchy();

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/runtime/graph_executor.h

// #pragma once

// #include <atomic>
// #include <memory>

// #include <torch/csrc/jit/ir/ir.h>
// #include <torch/csrc/jit/python/update_graph_executor_opt.h>
// #include <torch/csrc/jit/runtime/argument_spec.h>
// #include <torch/csrc/jit/runtime/interpreter.h>
// #include <torch/csrc/jit/runtime/variable_tensor_list.h>


// Targeting ../ExecutionPlan.java


// Targeting ../GraphExecutorState.java


// Targeting ../EnableProfilingGuard.java


// Targeting ../GraphExecutorImplBase.java


// Targeting ../GraphExecutor.java



@Namespace("torch::jit") public static native JitNode replaceBlockWithFallbackGraph(
    Block b,
    @ByVal ValueArrayRef inputs);

// These passes need to run before it is valid to pass to the interpreter
// regardless of whether sizes have been specialized or not.
@Namespace("torch::jit") public static native void runRequiredPasses(@Const @SharedPtr @ByRef Graph g);

@Namespace("torch::jit") public static native void debugSetFusionGroupInlining(@Cast("bool") boolean state);
@Namespace("torch::jit") public static native @Cast("bool") boolean getFusionGroupInlining();

@Namespace("torch::jit") public static native void debugSetAutodiffSubgraphInlining(@Cast("bool") boolean state);
@Namespace("torch::jit") public static native @SharedPtr @ByVal Graph lastExecutedOptimizedGraph();
@Namespace("torch::jit") public static native @Cast("bool") boolean IsNewExecutorEnabled();
// Targeting ../GraphOptimizerEnabledGuard.java







// for debugging information we expose a way to get the last actually
// run graph. Previous approaches allowed querying the GraphExecutor
// for what graph it would run in certain circumstances (graphFor), but
// this is fragile because we sometimes change how these decisions are made.
// This interface still allows our tests to look at optimized graphs, but
// with less plumbing.
 // namespace detail

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/runtime/operator_options.h

// #pragma once

// #include <ATen/core/dispatch/OperatorOptions.h>

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/runtime/operator.h

// in memory description of all ATen Ops similar to Caffe2 schema
// once C10 exists this can be removed, or stubbed out, but we need
// it now to implement correct semantic checking for script
// #pragma once

// #include <ATen/core/dispatch/Dispatcher.h>
// #include <ATen/core/dispatch/OperatorOptions.h>
// #include <ATen/core/op_registration/op_allowlist.h>
// #include <ATen/core/stack.h>
// #include <c10/util/Exception.h>
// #include <torch/csrc/jit/frontend/function_schema_parser.h>
// #include <torch/csrc/jit/runtime/operator_options.h>
// #include <torch/library.h>

// #include <ATen/ATen.h>
// #include <ATen/core/function_schema.h>
// #include <ATen/core/interned_strings.h>

// #include <functional>
// #include <initializer_list>
// #include <memory>
// #include <string>
// #include <unordered_map>
// #include <utility>
// #include <vector>
// Targeting ../OperationCreator.java


// Targeting ../Operator.java



@Namespace("torch::jit") public static native @StdString BytePointer canonicalSchemaString(@Const @ByRef FunctionSchema schema);

@Namespace("torch::jit") public static native @Const @ByVal OperatorVector getAllOperators();
@Namespace("torch::jit") public static native @Const @ByRef OperatorVector getAllOperatorsFor(
    @ByVal Symbol name);

// given a operator with an overload name, find the specific operator related to
// it, may return nullptr if no operator exists.
@Namespace("torch::jit") public static native @SharedPtr @ByVal Operator findOperatorFor(
    @Const @ByRef OperatorName full_name);

@Namespace("torch::jit") public static native @ByVal SymbolVector findSimilarOperators(@ByVal Symbol input_op);

@Namespace("torch::jit") public static native void registerOperator(@ByRef(true) Operator op);
@Namespace("torch::jit") public static native void deregisterOperator(@Const @ByRef FunctionSchema schema);

// XXX: this function is meant to be used with string literals only!
@Namespace("torch::jit") public static native @SharedPtr @ByVal Operator getOperatorForLiteral(
    @Cast("const char*") BytePointer signature);
@Namespace("torch::jit") public static native @SharedPtr @ByVal Operator getOperatorForLiteral(
    String signature);

// Ensure the thing that registers c10 ops is defined.
// Otherwise, our registry will not have c10 ops. You can run into this
// scenario if you're querying registered ops during static init.
//
// This fn is defined in register_c10_ops.cpp
@Namespace("torch::jit") public static native void ensure_c10_registerer_defined();

// Used to assert that unschematized operators have an analysis method written
@Namespace("torch::jit") public static native @Cast("bool") boolean aliasAnalysisHasSpecialCaseFor(@ByVal Symbol sym);

// A factory function to generate an optional operator. It has two
// instantiations depending on the template bool arg value. The arg can be a
// compile-time function for the selective op registration based on schema
// string.

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/runtime/custom_operator.h

// #pragma once

// #include <ATen/core/op_registration/op_registration.h>
// #include <ATen/core/stack.h>
// #include <torch/csrc/jit/runtime/operator.h>
// Targeting ../RegisterOperators.java



 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/api/compilation_unit.h

// #pragma once
// #include <ATen/core/function.h>
// #include <c10/util/Exception.h>
// #include <torch/csrc/jit/api/function_impl.h>
// #include <torch/csrc/jit/frontend/name_mangler.h>
// #include <torch/csrc/jit/frontend/source_range.h>
// #include <torch/csrc/jit/ir/ir.h>
// #include <torch/csrc/jit/runtime/graph_executor.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/csrc/utils/memory.h>

// #include <ATen/core/function_schema.h>
// #include <ATen/core/qualified_name.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Optional.h>

// #include <functional>
// #include <memory>
// #include <mutex>
// #include <ostream>
// #include <string>
// #include <unordered_map>
// #include <vector>
// Targeting ../Self.java


// Targeting ../CompilationUnit.java


// Targeting ../StrongFunctionPtr.java


// We once had a `script::` namespace that was deleted. This is for backcompat
// of the public API; new code should not use this type alias.
 // namespace script
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/api/function_impl.h

// #pragma once

// #include <ATen/core/function.h>
// #include <torch/csrc/jit/ir/ir.h>
// #include <torch/csrc/jit/runtime/graph_executor.h>
// #include <torch/csrc/utils/memory.h>
// Targeting ../GraphFunction.java


 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/api/method.h

// #pragma once

// #include <ATen/core/function.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/stack.h>
// #include <torch/csrc/api/include/torch/imethod.h>
// #include <torch/csrc/jit/api/function_impl.h>
// Targeting ../Method.java


// We once had a `script::` namespace that was deleted. This is for backcompat
// of the public API; new code should not use this type alias.
 // namespace script

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/api/object.h

// #pragma once

// #include <ATen/core/functional.h>
// #include <ATen/core/ivalue.h>
// #include <c10/util/Optional.h>
// #include <torch/csrc/jit/api/method.h>

// Throw this in C++ land if `attr` fails. This will be converted to a Python
// AttributeError by the Python binding code
// Targeting ../JitObject.java


// We once had a `script::` namespace that was deleted. This is for backcompat
// of the public API; new code should not use this type alias.
 // namespace script
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/api/module.h

// #pragma once
// #include <c10/util/Exception.h>
// #include <torch/csrc/autograd/variable.h>
// #include <torch/csrc/jit/api/object.h>
// #include <torch/csrc/jit/frontend/source_range.h>
// #include <torch/csrc/jit/ir/ir.h>
// #include <torch/csrc/jit/ir/named_value.h>
// #include <torch/csrc/jit/passes/shape_analysis.h>
// #include <torch/csrc/jit/runtime/argument_spec.h>
// #include <torch/csrc/jit/runtime/graph_executor.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/csrc/api/include/torch/ordered_dict.h>
// #include <torch/csrc/jit/api/compilation_unit.h>
// #include <torch/csrc/utils/memory.h>

// #include <ATen/core/function_schema.h>
// #include <ATen/core/qualified_name.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Optional.h>
// #include <c10/util/irange.h>

// #include <functional>
// #include <memory>
// #include <mutex>
// #include <ostream>
// #include <string>
// #include <unordered_map>
// #include <unordered_set>
// #include <utility>
// #include <vector>

// This file contains classes which assist in desugaring Python style
// modules and their methods into flattened graphs which don't have any
// function calls.
// Map which stores filename to content.
// Targeting ../NamedJitModule.java


// Targeting ../NamedTensor.java


// Targeting ../NamedIValue.java


 // namespace detail
// Targeting ../JitModule.java



// C++ equivalent api of `torch.jit.freeze`. See documentation there for
// details.
@Namespace("torch::jit") public static native @ByVal JitModule freeze(
    @Const @ByRef JitModule module,
    @ByVal(nullValue = "c10::optional<std::vector<std::string> >(c10::nullopt)") StringVectorOptional preserved_attrs,
    @Cast("bool") boolean optimize_numerics/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule freeze(
    @Const @ByRef JitModule module);

// C++ equivalent api of `torch.jit.optimize_for_inference`. See documentation
// there for details.
@Namespace("torch::jit") public static native @ByVal JitModule optimize_for_inference(@ByRef JitModule module);
// Targeting ../SlotCursor.java




// Targeting ../module_iterator.java


// Targeting ../named_module_iterator.java


// Targeting ../parameter_iterator.java


// Targeting ../named_parameter_iterator.java


// Targeting ../attribute_iterator.java


// Targeting ../named_attribute_iterator.java


// Targeting ../buffer_iterator.java


// Targeting ../named_buffer_iterator.java


// Targeting ../module_list.java


// Targeting ../named_module_list.java


// Targeting ../parameter_list.java


// Targeting ../named_parameter_list.java


// Targeting ../attribute_list.java


// Targeting ../named_attribute_list.java


// Targeting ../buffer_list.java


// Targeting ../named_buffer_list.java


// Targeting ../ModulePolicy.java


// Targeting ../ParameterPolicy.java


// Targeting ../BufferPolicy.java


// Targeting ../AttributePolicy.java


// Targeting ../NamedModulePolicy.java


// Targeting ../NamedParameterPolicy.java


// Targeting ../NamedAttributePolicy.java


// Targeting ../NamedBufferPolicy.java



 // namespace detail

@Namespace("torch::jit") public static native @Cast("bool*") @ByRef BoolPointer getInlineEverythingMode();
// We once had a `script::` namespace that was deleted. This is for backcompat
// of the public API; new code should not use this type alias.
 // namespace script

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/serialization/source_range_serialization.h

// #pragma once

// #include <c10/core/Allocator.h>
// #include <torch/csrc/jit/frontend/source_range.h>

// #include <ATen/core/ivalue.h>

// #include <unordered_map>
// #include <vector>

// Targeting ../SourceRangeSerializer.java


@Namespace("torch::jit") @MemberGetter public static native @Cast("const size_t") long kByteOffsetIndex();
public static final long kByteOffsetIndex = kByteOffsetIndex();
@Namespace("torch::jit") @MemberGetter public static native @Cast("const size_t") long kSourceRangeIndex();
public static final long kSourceRangeIndex = kSourceRangeIndex();
@Namespace("torch::jit") @MemberGetter public static native @Cast("const size_t") long kSourceRangeTagIndex();
public static final long kSourceRangeTagIndex = kSourceRangeTagIndex();
// Targeting ../SourceRangePickler.java


// Targeting ../SourceRangeDeserializer.java


// Targeting ../SourceRangeUnpickler.java



 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/serialization/pickler.h

// #pragma once

// #include <ATen/core/qualified_name.h>
// #include <string>
// #include <utility>
// #include <vector>

// #include <ATen/core/ivalue.h>
// #include <ATen/core/jit_type.h>
// #include <c10/util/ArrayRef.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/csrc/utils/disallow_copy.h>

// See Python's pickletools.py for a detailed description of each of these codes
@Namespace("torch::jit") public enum PickleOpCode {
  MARK((byte)('(')),
  STOP((byte)('.')),
  POP((byte)('0')),
  POP_MARK((byte)('1')),
  DUP((byte)('2')),
  FLOAT((byte)('F')),
  INT((byte)('I')),
  BININT((byte)('J')),
  BININT1((byte)('K')),
  LONG((byte)('L')),
  BININT2((byte)('M')),
  NONE((byte)('N')),
  PERSID((byte)('P')),
  BINPERSID((byte)('Q')),
  REDUCE((byte)('R')),
  STRING((byte)('S')),
  BINSTRING((byte)('T')),
  SHORT_BINSTRING((byte)('U')),
  // NB: Avoid using UNICODE as it is a macro in the Windows API
  UNICODE_((byte)('V')),
  BINUNICODE((byte)('X')),
  APPEND((byte)('a')),
  BUILD((byte)('b')),
  GLOBAL((byte)('c')),
  DICT((byte)('d')),
  EMPTY_DICT((byte)('}')),
  APPENDS((byte)('e')),
  GET((byte)('g')),
  BINGET((byte)('h')),
  INST((byte)('i')),
  LONG_BINGET((byte)('j')),
  LIST((byte)('l')),
  EMPTY_LIST((byte)(']')),
  OBJ((byte)('o')),
  PUT((byte)('p')),
  BINPUT((byte)('q')),
  LONG_BINPUT((byte)('r')),
  SETITEM((byte)('s')),
  TUPLE((byte)('t')),
  EMPTY_TUPLE((byte)(')')),
  SETITEMS((byte)('u')),
  BINFLOAT((byte)('G')),

  // Protocol 2
  PROTO((byte)(0x80)),
  NEWOBJ((byte)(0x81)),
  EXT1((byte)(0x82)),
  EXT2((byte)(0x83)),
  EXT4((byte)(0x84)),
  TUPLE1((byte)(0x85)),
  TUPLE2((byte)(0x86)),
  TUPLE3((byte)(0x87)),
  NEWTRUE((byte)(0x88)),
  NEWFALSE((byte)(0x89)),
  LONG1((byte)(0x8a)),
  LONG4((byte)(0x8b)),

  // Protocol 3 (Python 3.x)
  BINBYTES((byte)('B')),
  SHORT_BINBYTES((byte)('C')),

  // Protocol 4
  SHORT_BINUNICODE((byte)(0x8c)),
  BINUNICODE8((byte)(0x8d)),
  BINBYTES8((byte)(0x8e)),
  EMPTY_SET((byte)(0x8f)),
  ADDITEMS((byte)(0x90)),
  FROZENSET((byte)(0x91)),
  NEWOBJ_EX((byte)(0x92)),
  STACK_GLOBAL((byte)(0x93)),
  MEMOIZE((byte)(0x94)),
  FRAME((byte)(0x95));

    public final byte value;
    private PickleOpCode(byte v) { this.value = v; }
    private PickleOpCode(PickleOpCode e) { this.value = e.value; }
    public PickleOpCode intern() { for (PickleOpCode e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../WriteableTensorData.java





// Targeting ../Pickler.java



// returns a (tensor, record_size) for a tensor, converting it to a CPU tensor
// if it was CUDA and to_cpu is True.
@Namespace("torch::jit") public static native @ByVal WriteableTensorData getWriteableTensorData(@Const @ByRef Tensor tensor, @Cast("bool") boolean to_cpu/*=true*/);
@Namespace("torch::jit") public static native @ByVal WriteableTensorData getWriteableTensorData(@Const @ByRef Tensor tensor);

// return the value of the tensor's storage pointer


// if the cls has __getstate__/__setstate__
// assert they have the right schema and return true,
// otherwise return false


 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/serialization/unpickler.h

// #pragma once

// #include <ATen/core/ivalue.h>
// #include <c10/util/ArrayRef.h>
// #include <caffe2/serialize/inline_container.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/csrc/jit/serialization/pickler.h>
// Targeting ../Unpickler.java





 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/serialization/import.h

// #pragma once

// #include <ATen/core/ivalue.h>
// #include <caffe2/serialize/inline_container.h>
// #include <torch/csrc/jit/api/module.h>
// #include <torch/csrc/jit/ir/ir.h>
// #include <torch/csrc/jit/serialization/unpickler.h>

// #include <istream>
// Targeting ../ReadAdapterInterface.java


 // namespace serialize
 // namespace caffe2

@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @StdString BytePointer filename,
    @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @StdString BytePointer filename);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @StdString String filename,
    @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @StdString String filename);

@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @Cast("std::istream*") @ByRef Pointer in,
    @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @Cast("std::istream*") @ByRef Pointer in);

@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @UniquePtr ReadAdapterInterface rai,
    @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @UniquePtr ReadAdapterInterface rai);

@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @StdString BytePointer filename,
    @ByVal DeviceOptional device,
    @Cast("torch::jit::ExtraFilesMap*") @ByRef IValueIValueMap extra_files);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @StdString String filename,
    @ByVal DeviceOptional device,
    @Cast("torch::jit::ExtraFilesMap*") @ByRef IValueIValueMap extra_files);

// For reading unified serialization format from torch.Package
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @ByVal @Cast("std::shared_ptr<caffe2::serialize::PyTorchStreamReader>*") Pointer reader,
    @SharedPtr DeserializationStorageContext storage_context,
    @ByVal DeviceOptional device,
    @StdString BytePointer ts_id);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @ByVal @Cast("std::shared_ptr<caffe2::serialize::PyTorchStreamReader>*") Pointer reader,
    @SharedPtr DeserializationStorageContext storage_context,
    @ByVal DeviceOptional device,
    @StdString String ts_id);

@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @Cast("std::istream*") @ByRef Pointer in,
    @ByVal DeviceOptional device,
    @Cast("torch::jit::ExtraFilesMap*") @ByRef IValueIValueMap extra_files);


///
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @UniquePtr ReadAdapterInterface rai,
    @ByVal DeviceOptional device,
    @Cast("torch::jit::ExtraFilesMap*") @ByRef IValueIValueMap extra_files);

/** Loads a serialized {@code Module} from the given {@code istream}.
 * 
 *  The istream must contain a serialized {@code Module}, exported via
 *  {@code torch::jit::ExportModule} in C++. */
@Namespace("torch::jit") public static native @ByVal JitModule load(@Cast("std::istream*") @ByRef Pointer in, @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("torch::jit") public static native @ByVal JitModule load(@Cast("std::istream*") @ByRef Pointer in);


///
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @Cast("std::istream*") @ByRef Pointer in,
    @ByVal DeviceOptional device,
    @Cast("torch::jit::ExtraFilesMap*") @ByRef IValueIValueMap extra_files);

/** Loads a serialized {@code Module} from the given {@code filename}.
 * 
 *  The file stored at the location given in {@code filename} must contain a
 *  serialized {@code Module}, exported either via {@code ScriptModule.save()} in
 *  Python or {@code torch::jit::ExportModule} in C++. */
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @StdString BytePointer filename,
    @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @StdString BytePointer filename);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @StdString String filename,
    @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @StdString String filename);


///
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @StdString BytePointer filename,
    @ByVal DeviceOptional device,
    @Cast("torch::jit::ExtraFilesMap*") @ByRef IValueIValueMap extra_files);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @StdString String filename,
    @ByVal DeviceOptional device,
    @Cast("torch::jit::ExtraFilesMap*") @ByRef IValueIValueMap extra_files);

/** Loads a serialized {@code Module} from the given shared_ptr {@code rai}.
 * 
 *  The reader adapter, which is for customized input stream, must contain a
 *  serialized {@code Module}, exported either via {@code ScriptModule.save()} in
 *  Python or {@code torch::jit::ExportModule} in C++. */
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @SharedPtr ReadAdapterInterface rai,
    @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @SharedPtr ReadAdapterInterface rai);

@Namespace("torch::jit") public static native @ByVal JitModule load(
    @SharedPtr ReadAdapterInterface rai,
    @ByVal DeviceOptional device,
    @Cast("torch::jit::ExtraFilesMap*") @ByRef IValueIValueMap extra_files);

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/serialization/pickle.h

// #pragma once

// #include <ATen/core/ivalue.h>
// #include <c10/util/ArrayRef.h>
// #include <caffe2/serialize/inline_container.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/csrc/jit/serialization/pickler.h>
// #include <torch/csrc/jit/serialization/unpickler.h>

/** Pickle an IValue by calling a function to handle writing the data.
 * 
 *  {@code writer} is a function that takes in a pointer to a chunk of memory and its
 *  size and consumes it.
 * 
 *  See {@code jit::pickle} for more details. */

///
///
///
///
///
///
///
///
@Namespace("torch::jit") public static native void pickle(
    @ByVal Writer writer,
    @Const @ByRef IValue ivalue,
    TensorVector tensor_table/*=nullptr*/);
@Namespace("torch::jit") public static native void pickle(
    @ByVal Writer writer,
    @Const @ByRef IValue ivalue);

/** Save a {@code torch::IValue} in a format compatible with Python's {@code pickle} module
 * 
 *  If present, {@code tensor_table} is a pointer to a table in which tensors that
 *  are contained within {@code ivalue} are stored, and the bytes returned by the
 *  pickler will only include references to these tensors in the table. This can
 *  be used to keep the binary blob size small.
 *  If not provided, tensors are stored in the same byte stream as the pickle
 *  data, similar to {@code torch.save()} in eager Python.
 * 
 *  Pickled values can be loaded in Python and C++:
 *  \rst
 *  .. code-block:: cpp
 * 
 *   torch::IValue float_value(2.3);
 * 
 *   // TODO: when tensors are stored in the pickle, delete this
 *   std::vector<at::Tensor> tensor_table;
 *   auto data = torch::jit::pickle(float_value, &tensor_table);
 * 
 *   std::vector<torch::IValue> ivalues =
 *       torch::jit::unpickle(data.data(), data.size());
 * 
 *  .. code-block:: python
 * 
 *    values = torch.load('data.pkl')
 *    print(values)
 * 
 *  \endrst */
@Namespace("torch::jit") public static native @Cast("char*") @StdVector BytePointer pickle(
    @Const @ByRef IValue ivalue,
    TensorVector tensor_table/*=nullptr*/);
@Namespace("torch::jit") public static native @Cast("char*") @StdVector BytePointer pickle(
    @Const @ByRef IValue ivalue);

/** Save a {@code torch::IValue} in a format that can be loaded by both
 *  {@code torch::pickle_load} in C++ and {@code torch.load} in Python. */
@Namespace("torch::jit") public static native @Cast("char*") @StdVector BytePointer pickle_save(@Const @ByRef IValue ivalue);

/** Deserialize a {@code torch::IValue} from bytes produced by either
 *  {@code torch::pickle_save} in C++ or {@code torch.save} in Python */
@Namespace("torch::jit") public static native @ByVal IValue pickle_load(@Cast("char*") @StdVector BytePointer data);
@Namespace("torch::jit") public static native @ByVal IValue pickle_load(@Cast("char*") @StdVector ByteBuffer data);
@Namespace("torch::jit") public static native @ByVal IValue pickle_load(@Cast("char*") @StdVector byte[] data);

/** {@code reader} is a function that takes in a size to read from some pickled
 *  binary. {@code reader} should remember where it last read, and return
 *  the number of bytes read.
 *  See {@code torch::pickle} for details.
 *  type_resolver is used to resolve any JIT type based on type str */

///
///
@Namespace("torch::jit") public static native @ByVal IValue unpickle(
    @ByVal Reader reader,
    @ByVal @Cast("torch::jit::TypeResolver*") Pointer type_resolver,
    @ByVal TensorArrayRef tensor_table);

/** Decode a chunk of memory containing pickled data into its {@code torch::IValue}s.
 * 
 *  If any {@code torch::IValue}s in the pickled data are {@code Object}s, then a
 *  {@code class_resolver} function must be provided.
 * 
 *  See {@code torch::pickle} for details. */
@Namespace("torch::jit") public static native @ByVal IValue unpickle(
    @Cast("const char*") BytePointer data,
    @Cast("size_t") long size,
    @ByVal(nullValue = "torch::jit::TypeResolver(nullptr)") @Cast("torch::jit::TypeResolver*") Pointer type_resolver,
    @ByVal(nullValue = "c10::ArrayRef<at::Tensor>{}") TensorArrayRef tensor_table);
@Namespace("torch::jit") public static native @ByVal IValue unpickle(
    @Cast("const char*") BytePointer data,
    @Cast("size_t") long size);
@Namespace("torch::jit") public static native @ByVal IValue unpickle(
    String data,
    @Cast("size_t") long size,
    @ByVal(nullValue = "torch::jit::TypeResolver(nullptr)") @Cast("torch::jit::TypeResolver*") Pointer type_resolver,
    @ByVal(nullValue = "c10::ArrayRef<at::Tensor>{}") TensorArrayRef tensor_table);
@Namespace("torch::jit") public static native @ByVal IValue unpickle(
    String data,
    @Cast("size_t") long size);

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/serialization/python_print.h

// #pragma once
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/csrc/jit/ir/ir.h>
// #include <iostream>
// #include <vector>
// Targeting ../PythonPrintImpl.java


// Targeting ../PrintDepsTable.java


// Targeting ../PythonPrint.java




 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/serialization/type_name_uniquer.h

// #pragma once

// #include <torch/csrc/jit/frontend/name_mangler.h>
// #include <torch/csrc/jit/ir/type_hashing.h>
// Targeting ../TypeNameUniquer.java


 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/serialization/storage_context.h

// #pragma once

// #include <ATen/core/ivalue.h>
// Targeting ../SerializationStorageContext.java


// Targeting ../DeserializationStorageContext.java



 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/serialization/export.h

// #pragma once

// #include <caffe2/serialize/inline_container.h>
// #include <torch/csrc/jit/api/module.h>
// #include <torch/csrc/jit/ir/ir.h>
// #include <torch/csrc/jit/serialization/pickler.h>
// #include <torch/csrc/jit/serialization/python_print.h>
// #include <torch/csrc/jit/serialization/storage_context.h>
// #include <torch/csrc/jit/serialization/type_name_uniquer.h>
// #include <torch/csrc/onnx/onnx.h>

// #include <ostream>


// This map is used to keep track of parameters that should be exported
// externally. When `defer_weight_export` is true, the returned map contains
// kv pairs that map {external reference name} -> {at::Tensor to be exported}.
// It is the responsibility of the caller to export these appropriately.
//
// For example, when exporting to a zip archive, the caller may write out files
// for each entry in the export map, with the filename being the key and the
// file contents being the raw tensor data.





@Namespace("torch::jit") public static native void check_onnx_proto(@StdString BytePointer proto_string);
@Namespace("torch::jit") public static native void check_onnx_proto(@StdString String proto_string);
// Targeting ../ScriptModuleSerializer.java



// For testing purposes
@Namespace("torch::jit") public static native @StdString BytePointer pretty_print_onnx(
    @Const @SharedPtr @ByRef Graph graph,
    @Const @ByRef StringTensorMap initializers,
    @Cast("int64_t") long onnx_opset_version,
    @Cast("bool") boolean defer_weight_export,
    OperatorExportTypes operator_export_type/*=torch::onnx::OperatorExportTypes::ONNX*/,
    @Cast("bool") boolean google_printer/*=false*/,
    @Cast("bool") boolean keep_initializers_as_inputs/*=true*/,
    @Const @ByRef(nullValue = "std::map<std::string,int>{}") StringIntMap custom_opsets,
    @Cast("bool") boolean add_node_names/*=true*/);
@Namespace("torch::jit") public static native @StdString BytePointer pretty_print_onnx(
    @Const @SharedPtr @ByRef Graph graph,
    @Const @ByRef StringTensorMap initializers,
    @Cast("int64_t") long onnx_opset_version,
    @Cast("bool") boolean defer_weight_export);
@Namespace("torch::jit") public static native @StdString String pretty_print_onnx(
    @Const @SharedPtr @ByRef Graph graph,
    @Const @ByRef StringTensorMap initializers,
    @Cast("int64_t") long onnx_opset_version,
    @Cast("bool") boolean defer_weight_export,
    @Cast("torch::onnx::OperatorExportTypes") int operator_export_type/*=torch::onnx::OperatorExportTypes::ONNX*/,
    @Cast("bool") boolean google_printer/*=false*/,
    @Cast("bool") boolean keep_initializers_as_inputs/*=true*/,
    @Const @ByRef(nullValue = "std::map<std::string,int>{}") StringIntMap custom_opsets,
    @Cast("bool") boolean add_node_names/*=true*/);

@Namespace("torch::jit") public static native void ExportModule(
    @Const @ByRef JitModule module,
    @Cast("std::ostream*") @ByRef Pointer out,
    @Cast("const torch::jit::ExtraFilesMap*") @ByRef(nullValue = "torch::jit::ExtraFilesMap()") IValueIValueMap metadata,
    @Cast("bool") boolean bytecode_format/*=false*/,
    @Cast("bool") boolean save_mobile_debug_info/*=false*/);
@Namespace("torch::jit") public static native void ExportModule(
    @Const @ByRef JitModule module,
    @Cast("std::ostream*") @ByRef Pointer out);

@Namespace("torch::jit") public static native void ExportModule(
    @Const @ByRef JitModule module,
    @StdString BytePointer filename,
    @Cast("const torch::jit::ExtraFilesMap*") @ByRef(nullValue = "torch::jit::ExtraFilesMap()") IValueIValueMap metadata,
    @Cast("bool") boolean bytecode_format/*=false*/,
    @Cast("bool") boolean save_mobile_debug_info/*=false*/);
@Namespace("torch::jit") public static native void ExportModule(
    @Const @ByRef JitModule module,
    @StdString BytePointer filename);
@Namespace("torch::jit") public static native void ExportModule(
    @Const @ByRef JitModule module,
    @StdString String filename,
    @Cast("const torch::jit::ExtraFilesMap*") @ByRef(nullValue = "torch::jit::ExtraFilesMap()") IValueIValueMap metadata,
    @Cast("bool") boolean bytecode_format/*=false*/,
    @Cast("bool") boolean save_mobile_debug_info/*=false*/);
@Namespace("torch::jit") public static native void ExportModule(
    @Const @ByRef JitModule module,
    @StdString String filename);

@Namespace("torch::jit") public static native void ExportModule(
    @Const @ByRef JitModule module,
    @Const @ByRef WriteFunction writer_func,
    @Cast("const torch::jit::ExtraFilesMap*") @ByRef(nullValue = "torch::jit::ExtraFilesMap()") IValueIValueMap metadata,
    @Cast("bool") boolean bytecode_format/*=false*/,
    @Cast("bool") boolean save_mobile_debug_info/*=false*/);
@Namespace("torch::jit") public static native void ExportModule(
    @Const @ByRef JitModule module,
    @Const @ByRef WriteFunction writer_func);

// Write the bytes of a pickle archive and the tensors referenced inside that
// archive
@Namespace("torch::jit") public static native void writeArchiveAndTensors(
    @StdString BytePointer archive_name,
    @Cast("const char*") BytePointer pickle_bytes,
    @Cast("size_t") long size,
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensors,
    @Cast("caffe2::serialize::PyTorchStreamWriter*") @ByRef Pointer out);
@Namespace("torch::jit") public static native void writeArchiveAndTensors(
    @StdString String archive_name,
    String pickle_bytes,
    @Cast("size_t") long size,
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensors,
    @Cast("caffe2::serialize::PyTorchStreamWriter*") @ByRef Pointer out);

// Surrounding system can install an additional hook to produce extra files
// with metadata based on environment every time a module is serialized.
@Namespace("torch::jit") public static native void SetExportModuleExtraFilesHook(@ByVal @Cast("torch::jit::ExportModuleExtraFilesHook*") Pointer hook);

/**
 * Generates new bytecode for a Script module and returns what the op list
 * would be for a LiteScriptModule based off the current code base. If you
 * have a LiteScriptModule and want to get the currently present
 * list of ops call _export_operator_list instead.
 */
@Namespace("torch::jit") public static native @ByVal StringVector export_opnames(@Const @ByRef JitModule m);
// Targeting ../BytecodeEmitMode.java


// Targeting ../BytecodeEmitModeGuard.java



@Namespace("torch::jit") public static native @ByVal IValue to_tuple(@ByVal IValueVector ivalues);
@Namespace("torch::jit") public static native @ByVal IValue Table(@StdVector EnumNameValue entries);
 // namespace jit
 // namespace torch


// Parsed from torch/arg.h

// #pragma once

// #include <utility>

// #define TORCH_ARG(T, name)
//  public:
//   inline auto name(const T& new_##name)->decltype(*this) { /* NOLINT */
//     this->name##_ = new_##name;
//     return *this;
//   }
//   inline auto name(T&& new_##name)->decltype(*this) { /* NOLINT */
//     this->name##_ = std::move(new_##name);
//     return *this;
//   }
//   inline const T& name() const noexcept { /* NOLINT */
//     return this->name##_;
//   }
//   inline T& name() noexcept { /* NOLINT */
//     return this->name##_;
//   }
//  private:
//   T name##_ /* NOLINT */


// Parsed from torch/enum.h

// #pragma once

// #include <string>

// #include <ATen/core/Reduction.h>
// #include <c10/util/Exception.h>
// #include <c10/util/variant.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>

// #define TORCH_ENUM_DECLARE(name)
// namespace torch {
// namespace enumtype {
//   /*
//    NOTE: We need to provide the default constructor for each struct,
//    otherwise Clang 3.8 would complain:
//    ```
//    error: default initialization of an object of const type 'const enumtype::Enum1'
//    without a user-provided default constructor
//    ```
//  */
//   struct k##name { k##name() {} };
// }
// TORCH_API extern const enumtype::k##name k##name;
// }

// #define TORCH_ENUM_DEFINE(name)
// namespace torch {
// const enumtype::k##name k##name;
// }

// #define TORCH_ENUM_PRETTY_PRINT(name)
// std::string operator()(const enumtype::k##name& v) const {
//   std::string k("k");
//   return k + #name;
// }

// NOTE: Backstory on why we need the following two macros:
//
// Consider the following options class:
//
// ```
// struct TORCH_API SomeOptions {
//   typedef c10::variant<enumtype::kNone, enumtype::kMean, enumtype::kSum> reduction_t;
//   SomeOptions(reduction_t reduction = torch::kMean) : reduction_(reduction) {}
//
//   TORCH_ARG(reduction_t, reduction);
// };
// ```
//
// and the functional that uses it:
//
// ```
// Tensor some_functional(
//     const Tensor& input,
//     SomeOptions options = {}) {
//   ...
// }
// ```
//
// Normally, we would expect this to work:
//
// `F::some_functional(input, torch::kNone)`
//
// However, it throws the following error instead:
//
// ```
// error: could not convert `torch::kNone` from `const torch::enumtype::kNone` to `torch::nn::SomeOptions`
// ```
//
// To get around this problem, we explicitly provide the following constructors for `SomeOptions`:
//
// ```
// SomeOptions(torch::enumtype::kNone reduction) : reduction_(torch::kNone) {}
// SomeOptions(torch::enumtype::kMean reduction) : reduction_(torch::kMean) {}
// SomeOptions(torch::enumtype::kSum reduction) : reduction_(torch::kSum) {}
// ```
//
// so that the conversion from `torch::kNone` to `SomeOptions` would work.
//
// Note that we also provide the default constructor `SomeOptions() {}`, so that
// `SomeOptions options = {}` can work.
// #define TORCH_OPTIONS_CTOR_VARIANT_ARG3(OPTIONS_NAME, ARG_NAME, TYPE1, TYPE2, TYPE3)
// OPTIONS_NAME() {}
// OPTIONS_NAME(torch::enumtype::TYPE1 ARG_NAME) : ARG_NAME##_(torch::TYPE1) {}
// OPTIONS_NAME(torch::enumtype::TYPE2 ARG_NAME) : ARG_NAME##_(torch::TYPE2) {}
// OPTIONS_NAME(torch::enumtype::TYPE3 ARG_NAME) : ARG_NAME##_(torch::TYPE3) {}

// #define TORCH_OPTIONS_CTOR_VARIANT_ARG4(OPTIONS_NAME, ARG_NAME, TYPE1, TYPE2, TYPE3, TYPE4)
// OPTIONS_NAME() {}
// OPTIONS_NAME(torch::enumtype::TYPE1 ARG_NAME) : ARG_NAME##_(torch::TYPE1) {}
// OPTIONS_NAME(torch::enumtype::TYPE2 ARG_NAME) : ARG_NAME##_(torch::TYPE2) {}
// OPTIONS_NAME(torch::enumtype::TYPE3 ARG_NAME) : ARG_NAME##_(torch::TYPE3) {}
// OPTIONS_NAME(torch::enumtype::TYPE4 ARG_NAME) : ARG_NAME##_(torch::TYPE4) {}
// Targeting ../kLinear.java

  
// Targeting ../kConv1D.java

  
// Targeting ../kConv2D.java

  
// Targeting ../kConv3D.java

  
// Targeting ../kConvTranspose1D.java

  
// Targeting ../kConvTranspose2D.java

  
// Targeting ../kConvTranspose3D.java

  
// Targeting ../kSigmoid.java

  
// Targeting ../kTanh.java

  
// Targeting ../kReLU.java

  
// Targeting ../kGELU.java

  
// Targeting ../kSiLU.java

  
// Targeting ../kMish.java

  
// Targeting ../kLeakyReLU.java

  
// Targeting ../kFanIn.java

  
// Targeting ../kFanOut.java

  
// Targeting ../kConstant.java

  
// Targeting ../kReflect.java

  
// Targeting ../kReplicate.java

  
// Targeting ../kCircular.java

  
// Targeting ../kNearest.java

  
// Targeting ../kBilinear.java

  
// Targeting ../kBicubic.java

  
// Targeting ../kTrilinear.java

  
// Targeting ../kArea.java

  
// Targeting ../kSum.java

  
// Targeting ../kMean.java

  
// Targeting ../kMax.java

  
// Targeting ../kNone.java

  
// Targeting ../kBatchMean.java

  
// Targeting ../kZeros.java

  
// Targeting ../kBorder.java

  
// Targeting ../kReflection.java

  
// Targeting ../kRNN_TANH.java

  
// Targeting ../kRNN_RELU.java

  
// Targeting ../kLSTM.java

  
// Targeting ../kGRU.java

  
// Targeting ../kValid.java

  
// Targeting ../kSame.java

  
// Targeting ../_compute_enum_name.java



 // namespace enumtype
 // namespace torch


// Parsed from torch/types.h

// #pragma once

// #include <ATen/ATen.h>

// #include <c10/util/Optional.h>

// #include <torch/csrc/autograd/generated/variable_factories.h>
// #include <torch/csrc/autograd/variable.h>

// NOTE [ Exposing declarations in `at::` to `torch::` ]
//
// The following line `using namespace at;` is responsible for exposing all declarations in
// `at::` namespace to `torch::` namespace.
//
// According to the rules laid out in
// https://en.cppreference.com/w/cpp/language/qualified_lookup, section "Namespace members":
// ```
// Qualified lookup within the scope of a namespace N first considers all declarations that are
// located in N and all declarations that are located in the inline namespace members of N
// (and, transitively, in their inline namespace members). If there are no declarations in that set
// then it considers declarations in all namespaces named by using-directives found in N and
// in all transitive inline namespace members of N.
// ```
//
// This means that if both `at::` and `torch::` namespaces have a function with the same signature
// (e.g. both `at::func()` and `torch::func()` exist), after `namespace torch { using namespace at; }`,
// when we call `torch::func()`, the `func()` function defined in `torch::` namespace will always
// be called, and the `func()` function defined in `at::` namespace is always hidden. // NOLINT

/** Fixed width dtypes. */

/** Rust-style short dtypes. */
 // namespace torch


// Parsed from torch/utils.h

// #pragma once

// #include <ATen/Parallel.h>
// #include <ATen/record_function.h>
// #include <torch/csrc/autograd/grad_mode.h>
// #include <torch/csrc/api/include/torch/types.h>
// #include <torch/csrc/utils/crash_handler.h>
// #include <cstdint>

/** A RAII, thread-local guard that disabled gradient calculation.
 * 
 *  Disabling gradient calculation is useful for inference, when you are sure
 *  that you will not call {@code at::Tensor::backward}. It will reduce memory
 *  consumption for computations that would otherwise have {@code requires_grad() == true}.
 * 
 *  In this mode, the result of every computation will have
 *  {@code requires_grad() == false}, even when the inputs have {@code requires_grad() == true}.
 * 
 *  This context manager is thread-local; it will not affect computation
 *  in other threads.
 * 
 *  Example:
 *  <pre>{@code
 *  auto x = torch::tensor({1.}, torch::requires_grad());
 *  {
 *    torch::NoGradGuard no_grad;
 *    auto y = x * 2;
 *    std::cout << y.requires_grad() << std::endl; // prints `false`
 *  }
 *  {
 *    auto doubler = [](torch::Tensor x) {
 *      torch::NoGradGuard no_grad;
 *      return x * 2;
 *    };
 *    auto z = doubler(x);
 *    std::cout << z.requires_grad() << std::endl; // prints `false`
 *  }
 *  }</pre> */

///
///
///
///

/** A RAII, thread-local guard that sets gradient calculation to on or off.
 * 
 *  {@code }AutoGradMode{@code } will enable or disable grads based on its argument {@code enabled}.
 * 
 *  This context manager is thread-local; it will not affect computation
 *  in other threads.
 * 
 *  @param enabled: Flag whether to enable grad ({@code }true{@code }), or disable
 *               ({@code }false{@code }). This can be used to conditionally enable
 *               gradients.
 * 
 *  Example:
 *  <pre>{@code
 *  auto x = torch::tensor({1.}, torch::requires_grad());
 *  {
 *    torch::AutoGradMode enable_grad(true);
 *    auto y = x * 2;
 *    std::cout << y.requires_grad() << std::endl; // prints `true`
 *  }
 *  {
 *    torch::AutoGradMode enable_grad(false);
 *    auto y = x * 2;
 *    std::cout << y.requires_grad() << std::endl; // prints `false`
 *  }
 *  }</pre> */

/** Sets the global random seed for all newly created CPU and CUDA tensors. */

// Called during new thread initialization

// Returns the number of threads used in parallel region.

// Sets the number of threads to be used in parallel region.

// Returns the number of threads used for inter-op parallelism.

// Sets the number of threads to be used for inter-op parallelism.

// Returns true if both t1, t2 are undefined or both are defined and equal
@Namespace("torch") public static native @Cast("bool") boolean equal_if_defined(@ByVal Tensor t1, @ByVal Tensor t2);

// RecordFunction API

 // namespace torch


// Parsed from torch/data.h

// #pragma once

// #include <torch/data/dataloader.h>
// #include <torch/data/datasets.h>
// #include <torch/data/samplers.h>
// #include <torch/data/transforms.h>

// Some "exports".
 // namespace data
 // namespace torch


// Parsed from torch/data/example.h

// #pragma once

// #include <torch/types.h>
// Targeting ../Example.java


// Targeting ../NoTarget.java


 //  namespace example

/** A specialization for {@code Example} that does not have have a target.
 * 
 *  This class exists so that code can be written for a templated {@code Example}
 *  type, and work both for labeled and unlabeled datasets. */
 // namespace data
 // namespace torch


// Parsed from torch/data/iterator.h

// #pragma once

// #include <torch/csrc/utils/variadic.h>
// #include <torch/types.h>

// #include <c10/util/Exception.h>

// #include <functional>
// #include <iterator>
// #include <memory>
// #include <type_traits>
// #include <utility>
// For increased safety and more separated logic, this implementation of
// `Iterator` consists of a `ValidIterator` and a `SentinelIterator`. A
// `ValidIterator` yields new batches until the `DataLoader` is exhausted. While
// the `DataLoader` is not exhausted, `ValidIterator`s compare equal if they are
// the same object. When the `ValidIterator` becomes exhausted, it compares equal
// to the `SentinelIterator`, but not before. Half the code here is to implement
// double dispatch for the comparison. Got damnit, C++.

/** Base class for the {@code ValidIterator} and {@code SentinelIterator} */

// Targeting ../ExampleIterator.java


// Targeting ../ExampleVectorIterator.java


 // namespace data
 // namespace torch


// Parsed from torch/data/worker_exception.h

// #pragma once

// #include <exception>
// #include <string>
// #include <utility>
// Targeting ../WorkerException.java



 // namespace data
 // namespace torch


// Parsed from torch/data/dataloader.h

// #pragma once

// #include <torch/data/dataloader/stateful.h>
// #include <torch/data/dataloader/stateless.h>

// #include <torch/csrc/utils/memory.h>
// #include <torch/csrc/utils/variadic.h>

// #include <c10/util/Exception.h>

// #include <cstddef>
// #include <memory>
// #include <type_traits>
// #include <utility>

/** Creates a {@code DataLoader} instance for a stateless {@code dataset}, a {@code sampler} and
 *  some {@code options}. */

/** Creates a {@code DataLoader} instance for a stateless {@code dataset} and some
 *  {@code options}. A sampler (by default a {@code RandomSampler}) will be constructed from
 *  the size of the dataset. */

/** Creates a {@code DataLoader} for a stateful {@code dataset} and some {@code options}. */
 // namespace data
 // namespace torch


// Parsed from torch/data/dataloader/base.h

// #pragma once

// #include <torch/data/dataloader_options.h>
// #include <torch/data/detail/data_shuttle.h>
// #include <torch/data/detail/sequencers.h>
// #include <torch/data/iterator.h>
// #include <torch/data/samplers/random.h>
// #include <torch/data/worker_exception.h>
// #include <torch/types.h>

// #include <torch/csrc/utils/memory.h>
// #include <torch/csrc/utils/variadic.h>

// #include <c10/util/Exception.h>
// #include <c10/util/irange.h>

// #include <cstddef>
// #include <exception>
// #include <memory>
// #include <thread>
// #include <type_traits>
// #include <utility>
// #include <vector>
// Targeting ../MNISTRandomDataLoaderBase.java


 // namespace data
 // namespace torch


// Parsed from torch/data/dataloader_options.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/types.h>

// #include <chrono>
// #include <cstddef>
// Targeting ../DataLoaderOptions.java


// Targeting ../FullDataLoaderOptions.java


 // namespace data
 // namespace torch


// Parsed from torch/data/dataloader/stateful.h

// #pragma once

// #include <torch/data/dataloader/base.h>

// #include <cstddef>
// #include <thread>
// #include <utility>

/** A dataloader for stateful datasets.
 * 
 *  A dataloader for stateful datatasets differs from one for stateless
 *  datasets one in that the dataset is shared among worker threads, and that
 *  this dataset is itself responsible for producing batches rather than
 *  depending on a sampler. The statefulness here actually refers to the
 *  dataset. The StatefulDataLoader simply alters the data loading algorithm to
 *  accommodate the stateful, shared nature of the dataset. Note that the dataset
 *  must be thread safe if more than one worker thread is used.
 * 
 *  A stateful dataloader is created by calling {@code make_data_loader} with a
 *  stateful dataset. */
 // namespace data
 // namespace torch


// Parsed from torch/data/dataloader/stateless.h

// #pragma once

// #include <torch/data/dataloader/base.h>
// #include <torch/data/worker_exception.h>

// #include <torch/csrc/utils/memory.h>

// #include <c10/util/Exception.h>

// #include <cstddef>
// #include <thread>
// #include <utility>
// Targeting ../MNISTRandomDataLoader.java


 // namespace data
 // namespace torch


// Parsed from torch/data/datasets.h

// #pragma once

// #include <torch/data/datasets/base.h>
// #include <torch/data/datasets/chunk.h>
// #include <torch/data/datasets/map.h>
// #include <torch/data/datasets/mnist.h>
// #include <torch/data/datasets/shared.h>
// #include <torch/data/datasets/stateful.h>
// #include <torch/data/datasets/tensor.h>


// Parsed from torch/data/datasets/base.h

// #pragma once

// #include <torch/data/example.h>
// #include <torch/types.h>

// #include <c10/util/ArrayRef.h>

// #include <cstddef>
// #include <cstdint>
// #include <type_traits>
// #include <utility>
// #include <vector> // NOLINT
 // namespace datasets
 // namespace data
 // namespace torch

// Targeting ../MNISTBatchDataset.java


// Targeting ../MNISTMapBatchDataset.java


// Targeting ../MNISTDataSet.java



/** A {@code StreamDataset} represents a dataset that is a potentially infinite stream.
 *  It takes as batch index only a number, which is the batch size, and yields
 *  that many elements from the stream. */
 // namespace datasets
 // namespace data
 // namespace torch


// Parsed from torch/data/datasets/map.h

// #pragma once

// #include <torch/data/datasets/base.h>
// #include <torch/types.h>

// #include <c10/util/ArrayRef.h>

// #include <cstddef>
// #include <type_traits>
// #include <utility>

// Targeting ../MNISTMapDataset.java



/** Creates a {@code MapDataset} with the given dataset and transform. */

 // namespace datasets
 // namespace data
 // namespace torch


// Parsed from torch/data/datasets/mnist.h

// #pragma once

// #include <torch/data/datasets/base.h>
// #include <torch/data/example.h>
// #include <torch/types.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>

// #include <cstddef>
// #include <string>
// Targeting ../MNIST.java


 // namespace datasets
 // namespace data
 // namespace torch


// Parsed from torch/data/samplers.h

// #pragma once

// #include <torch/data/samplers/base.h>
// #include <torch/data/samplers/custom_batch_request.h>
// #include <torch/data/samplers/distributed.h>
// #include <torch/data/samplers/random.h>
// #include <torch/data/samplers/sequential.h>
// #include <torch/data/samplers/serialize.h>
// #include <torch/data/samplers/stream.h>


// Parsed from torch/data/samplers/base.h

// #pragma once

// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <vector>
// #include <mutex>
 // namespace serialize
 // namespace torch
// Targeting ../Sampler.java



 // namespace samplers
 // namespace data
 // namespace torch


// Parsed from torch/data/samplers/random.h

// #pragma once

// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/data/samplers/base.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <vector>
 // namespace serialize
 // namespace torch
// Targeting ../RandomSampler.java


 // namespace samplers
 // namespace data
 // namespace torch


// Parsed from torch/data/transforms.h

// #pragma once

// #include <torch/data/transforms/base.h>
// #include <torch/data/transforms/collate.h>
// #include <torch/data/transforms/lambda.h>
// #include <torch/data/transforms/stack.h>
// #include <torch/data/transforms/tensor.h>


// Parsed from torch/data/transforms/base.h

// #pragma once

// #include <torch/types.h>

// #include <utility>
// #include <vector>
// Targeting ../ExampleCollation.java



/** A transformation of individual input examples to individual output examples.
 * 
 *  Just like a {@code Dataset} is a {@code BatchDataset}, a {@code Transform} is a
 *  {@code BatchTransform} that can operate on the level of individual examples rather
 *  than entire batches. The batch-level transform is implemented (by default)
 *  in terms of the example-level transform, though this can be customized. */
 // namespace transforms
 // namespace data
 // namespace torch


// Parsed from torch/data/transforms/collate.h

// #pragma once

// #include <torch/data/example.h>
// #include <torch/data/transforms/lambda.h>

// #include <vector>

/** A {@code Collation} is a transform that reduces a batch into a single value.
 *  The result is a {@code BatchDataset} that has the type of the single value as its
 *  {@code BatchType}. */

///
///

/** A {@code Collate} allows passing a custom function to reduce/collate a batch
 *  into a single value. It's effectively the lambda version of {@code Collation},
 *  which you could subclass and override {@code operator()} to achieve the same.
 * 
 *  \rst
 *  .. code-block:: cpp
 *    using namespace torch::data;
 * 
 *    auto dataset = datasets::MNIST("path/to/mnist")
 *      .map(transforms::Collate<Example<>>([](std::vector<Example<>> e) {
 *        return std::move(e.front());
 *      }));
 *  \endrst */
 // namespace transforms
 // namespace data
 // namespace torch


// Parsed from torch/data/transforms/lambda.h

// #pragma once

// #include <torch/data/transforms/base.h>

// #include <functional>
// #include <utility>
// #include <vector>

/** A {@code BatchTransform} that applies a user-provided functor to a batch. */

// A `Transform` that applies a user-provided functor to individual examples.

 // namespace transforms
 // namespace data
 // namespace torch


// Parsed from torch/data/transforms/stack.h

// #pragma once

// #include <torch/data/example.h>
// #include <torch/data/transforms/collate.h>
// #include <torch/types.h>

// #include <utility>
// #include <vector>
// Targeting ../ExampleStack.java



/** A {@code Collation} for {@code Example<Tensor, NoTarget>} types that stacks all data
 *  tensors into one tensor. */
 // namespace transforms
 // namespace data
 // namespace torch


// Parsed from torch/data/transforms/tensor.h

// #pragma once

// #include <torch/data/example.h>
// #include <torch/data/transforms/base.h>
// #include <torch/types.h>

// #include <functional>
// #include <utility>

/** A {@code Transform} that is specialized for the typical {@code Example<Tensor, Tensor>}
 *  combination. It exposes a single {@code operator()} interface hook (for
 *  subclasses), and calls this function on input {@code Example} objects. */

/** A {@code Lambda} specialized for the typical {@code Example<Tensor, Tensor>} input type. */

/** Normalizes input tensors by subtracting the supplied mean and dividing by
 *  the given standard deviation. */
 // namespace transforms
 // namespace data
 // namespace torch


// Parsed from torch/serialize.h

// #pragma once

// #include <c10/util/irange.h>
// #include <torch/serialize/archive.h>
// #include <torch/serialize/tensor.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>

// #include <utility>

/** Serializes the given {@code value}.
 *  There must be an overload of {@code operator<<} between {@code serialize::OutputArchive}
 *  and {@code Value} for this method to be well-formed. Currently, such an overload
 *  is provided for (subclasses of):
 * 
 *  - {@code torch::nn::Module},
 *  - {@code torch::optim::Optimizer}
 *  - {@code torch::Tensor}
 * 
 *  To perform the serialization, a {@code serialize::OutputArchive} is constructed,
 *  and all arguments after the {@code value} are forwarded to its {@code save_to} method.
 *  For example, you can pass a filename, or an {@code ostream}.
 * 
 *  \rst
 *  .. code-block:: cpp
 * 
 *    torch::nn::Linear model(3, 4);
 *    torch::save(model, "model.pt");
 * 
 *    torch::optim::SGD sgd(/*lr=* /0.9);
 *    std::ostringstream stream;
 *    // Note that the same stream cannot be used in multiple torch::save(...)
 *    // invocations, otherwise the header will be corrupted.
 *    torch::save(sgd, stream);
 * 
 *    auto tensor = torch::ones({3, 4});
 *    torch::save(tensor, "my_tensor.pt");
 *  \endrst */

/** Serializes the given {@code tensor_vec} of type {@code std::vector<torch::Tensor>}.
 * 
 *  To perform the serialization, a {@code serialize::OutputArchive} is constructed,
 *  and all arguments after the {@code tensor_vec} are forwarded to its {@code save_to}
 *  method. For example, you can pass a filename, or an {@code ostream}.
 * 
 *  \rst
 *  .. code-block:: cpp
 * 
 *    std::vector<torch::Tensor> tensor_vec = { torch::randn({1, 2}), torch::randn({3, 4}) };
 *    torch::save(tensor_vec, "my_tensor_vec.pt");
 * 
 *    std::vector<torch::Tensor> tensor_vec = { torch::randn({5, 6}), torch::randn({7, 8}) };
 *    std::ostringstream stream;
 *    // Note that the same stream cannot be used in multiple torch::save(...)
 *    // invocations, otherwise the header will be corrupted.
 *    torch::save(tensor_vec, stream);
 *  \endrst */

/** Deserializes the given {@code value}.
 *  There must be an overload of {@code operator>>} between {@code serialize::InputArchive}
 *  and {@code Value} for this method to be well-formed. Currently, such an overload
 *  is provided for (subclasses of):
 * 
 *  - {@code torch::nn::Module},
 *  - {@code torch::optim::Optimizer}
 *  - {@code torch::Tensor}
 * 
 *  To perform the serialization, a {@code serialize::InputArchive} is constructed,
 *  and all arguments after the {@code value} are forwarded to its {@code load_from} method.
 *  For example, you can pass a filename, or an {@code istream}.
 * 
 *  \rst
 *  .. code-block:: cpp
 * 
 *    torch::nn::Linear model(3, 4);
 *    torch::load(model, "model.pt");
 * 
 *    torch::optim::SGD sgd(/*lr=* /0.9);
 *    std::istringstream stream("...");
 *    torch::load(sgd, stream);
 * 
 *    auto tensor = torch::ones({3, 4});
 *    torch::load(tensor, "my_tensor.pt");
 *  \endrst */

/** Deserializes the given {@code tensor_vec} of type {@code std::vector<torch::Tensor>}.
 * 
 *  To perform the serialization, a {@code serialize::InputArchive} is constructed,
 *  and all arguments after the {@code value} are forwarded to its {@code load_from} method.
 *  For example, you can pass a filename, or an {@code istream}.
 * 
 *  \rst
 *  .. code-block:: cpp
 * 
 *    std::vector<torch::Tensor> tensor_vec;
 *    torch::load(tensor_vec, "my_tensor_vec.pt");
 * 
 *    std::vector<torch::Tensor> tensor_vec;
 *    std::istringstream stream("...");
 *    torch::load(tensor_vec, stream);
 *  \endrst */
 // namespace torch


// Parsed from torch/serialize/archive.h

// #pragma once

// #include <torch/serialize/input-archive.h>
// #include <torch/serialize/output-archive.h>


// Parsed from torch/serialize/input-archive.h

// #pragma once

// #include <c10/util/Optional.h>
// #include <c10/core/Device.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// #include <torch/csrc/jit/api/module.h>

// #include <iosfwd>
// #include <memory>
// #include <string>
// #include <utility>
 // namespace at
 // namespace jit
 // namespace torch
// Targeting ../InputArchive.java


 // namespace serialize
 // namespace torch


// Parsed from torch/serialize/output-archive.h

// #pragma once

// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/csrc/jit/api/module.h>

// #include <iosfwd>
// #include <memory>
// #include <string>
// #include <utility>
 // namespace at
 // namespace jit

// Targeting ../OutputArchive.java


 // namespace serialize
 // namespace torch


// Parsed from torch/serialize/tensor.h

// #pragma once

// #include <torch/serialize/archive.h>
// #include <torch/types.h>
@Namespace("torch") public static native @ByRef @Name("operator <<") OutputArchive shiftLeft(
    @ByRef OutputArchive archive,
    @Const @ByRef Tensor tensor);

@Namespace("torch") public static native @ByRef @Name("operator >>") InputArchive shiftRight(
    @ByRef InputArchive archive,
    @ByRef Tensor tensor);
 // namespace torch


// Parsed from torch/nn.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional.h>
// #include <torch/nn/init.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules.h>
// #include <torch/nn/options.h>
// #include <torch/nn/pimpl.h>
// #include <torch/nn/utils.h>


// Parsed from torch/nn/cloneable.h

// #pragma once

// #include <torch/nn/module.h>
// #include <torch/types.h>
// #include <torch/utils.h>

// #include <c10/core/TensorOptions.h>
// #include <c10/util/Exception.h>

// #include <memory>
// #include <utility>
// Targeting ../ModuleDictImplCloneable.java


// Targeting ../ModuleListImplCloneable.java


// Targeting ../SequentialImplCloneable.java


// Targeting ../ParameterDictImplCloneable.java


// Targeting ../ParameterListImplCloneable.java


// Targeting ../AdaptiveLogSoftmaxWithLossImplCloneable.java


// Targeting ../BatchNorm1dImplCloneable.java


// Targeting ../InstanceNorm1dImplCloneable.java


// Targeting ../Conv1dImplCloneable.java


// Targeting ../ConvTranspose1dImplCloneable.java


// Targeting ../DropoutImplCloneable.java


// Targeting ../BatchNorm2dImplCloneable.java


// Targeting ../InstanceNorm2dImplCloneable.java


// Targeting ../Conv2dImplCloneable.java


// Targeting ../ConvTranspose2dImplCloneable.java


// Targeting ../Dropout2dImplCloneable.java


// Targeting ../BatchNorm3dImplCloneable.java


// Targeting ../InstanceNorm3dImplCloneable.java


// Targeting ../Conv3dImplCloneable.java


// Targeting ../ConvTranspose3dImplCloneable.java


// Targeting ../Dropout3dImplCloneable.java


// Targeting ../AlphaDropoutImplCloneable.java


// Targeting ../FeatureAlphaDropoutImplCloneable.java


// Targeting ../CosineSimilarityImplCloneable.java


// Targeting ../PairwiseDistanceImplCloneable.java


// Targeting ../EmbeddingImplCloneable.java


// Targeting ../EmbeddingBagImplCloneable.java


// Targeting ../FoldImplCloneable.java


// Targeting ../UnfoldImplCloneable.java


// Targeting ../IdentityImplCloneable.java


// Targeting ../LinearImplCloneable.java


// Targeting ../BilinearImplCloneable.java


// Targeting ../FlattenImplCloneable.java


// Targeting ../UnflattenImplCloneable.java


// Targeting ../L1LossImplCloneable.java


// Targeting ../KLDivLossImplCloneable.java


// Targeting ../MSELossImplCloneable.java


// Targeting ../BCELossImplCloneable.java


// Targeting ../HingeEmbeddingLossImplCloneable.java


// Targeting ../MultiMarginLossImplCloneable.java


// Targeting ../CosineEmbeddingLossImplCloneable.java


// Targeting ../SmoothL1LossImplCloneable.java


// Targeting ../HuberLossImplCloneable.java


// Targeting ../MultiLabelMarginLossImplCloneable.java


// Targeting ../SoftMarginLossImplCloneable.java


// Targeting ../MultiLabelSoftMarginLossImplCloneable.java


// Targeting ../TripletMarginLossImplCloneable.java


// Targeting ../TripletMarginWithDistanceLossImplCloneable.java


// Targeting ../CTCLossImplCloneable.java


// Targeting ../PoissonNLLLossImplCloneable.java


// Targeting ../MarginRankingLossImplCloneable.java


// Targeting ../NLLLossImplCloneable.java


// Targeting ../CrossEntropyLossImplCloneable.java


// Targeting ../BCEWithLogitsLossImplCloneable.java


// Targeting ../ReflectionPad1dImplCloneable.java


// Targeting ../ReplicationPad1dImplCloneable.java


// Targeting ../ConstantPad1dImplCloneable.java


// Targeting ../AvgPool1dImplCloneable.java


// Targeting ../MaxPool1dImplCloneable.java


// Targeting ../AdaptiveAvgPool1dImplCloneable.java


// Targeting ../AdaptiveMaxPool1dImplCloneable.java


// Targeting ../MaxUnpool1dImplCloneable.java


// Targeting ../LPPool1dImplCloneable.java


// Targeting ../ReflectionPad2dImplCloneable.java


// Targeting ../ReplicationPad2dImplCloneable.java


// Targeting ../ConstantPad2dImplCloneable.java


// Targeting ../ZeroPad2dImplCloneable.java


// Targeting ../AvgPool2dImplCloneable.java


// Targeting ../MaxPool2dImplCloneable.java


// Targeting ../AdaptiveAvgPool2dImplCloneable.java


// Targeting ../AdaptiveMaxPool2dImplCloneable.java


// Targeting ../MaxUnpool2dImplCloneable.java


// Targeting ../FractionalMaxPool2dImplCloneable.java


// Targeting ../LPPool2dImplCloneable.java


// Targeting ../ReflectionPad3dImplCloneable.java


// Targeting ../ReplicationPad3dImplCloneable.java


// Targeting ../ConstantPad3dImplCloneable.java


// Targeting ../AvgPool3dImplCloneable.java


// Targeting ../MaxPool3dImplCloneable.java


// Targeting ../AdaptiveAvgPool3dImplCloneable.java


// Targeting ../AdaptiveMaxPool3dImplCloneable.java


// Targeting ../MaxUnpool3dImplCloneable.java


// Targeting ../FractionalMaxPool3dImplCloneable.java


// Targeting ../RNNImplCloneable.java


// Targeting ../LSTMImplCloneable.java


// Targeting ../GRUImplCloneable.java


// Targeting ../RNNCellImplCloneable.java


// Targeting ../LSTMCellImplCloneable.java


// Targeting ../GRUCellImplCloneable.java


// Targeting ../PixelShuffleImplCloneable.java


// Targeting ../PixelUnshuffleImplCloneable.java


// Targeting ../UpsampleImplCloneable.java


// Targeting ../ELUImplCloneable.java


// Targeting ../SELUImplCloneable.java


// Targeting ../HardshrinkImplCloneable.java


// Targeting ../HardtanhImplCloneable.java


// Targeting ../LeakyReLUImplCloneable.java


// Targeting ../LogSigmoidImplCloneable.java


// Targeting ../SoftmaxImplCloneable.java


// Targeting ../SoftminImplCloneable.java


// Targeting ../LogSoftmaxImplCloneable.java


// Targeting ../Softmax2dImplCloneable.java


// Targeting ../PReLUImplCloneable.java


// Targeting ../ReLUImplCloneable.java


// Targeting ../ReLU6ImplCloneable.java


// Targeting ../RReLUImplCloneable.java


// Targeting ../CELUImplCloneable.java


// Targeting ../GLUImplCloneable.java


// Targeting ../GELUImplCloneable.java


// Targeting ../SiLUImplCloneable.java


// Targeting ../MishImplCloneable.java


// Targeting ../SigmoidImplCloneable.java


// Targeting ../SoftplusImplCloneable.java


// Targeting ../SoftshrinkImplCloneable.java


// Targeting ../SoftsignImplCloneable.java


// Targeting ../TanhImplCloneable.java


// Targeting ../TanhshrinkImplCloneable.java


// Targeting ../ThresholdImplCloneable.java


// Targeting ../MultiheadAttentionImplCloneable.java


// Targeting ../LayerNormImplCloneable.java


// Targeting ../LocalResponseNormImplCloneable.java


// Targeting ../CrossMapLRN2dImplCloneable.java


// Targeting ../GroupNormImplCloneable.java


// Targeting ../TransformerEncoderLayerImplCloneable.java


// Targeting ../TransformerDecoderLayerImplCloneable.java


// Targeting ../TransformerEncoderImplCloneable.java


// Targeting ../TransformerDecoderImplCloneable.java


// Targeting ../TransformerImplCloneable.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/init.h

// #pragma once

// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/enum.h>
// #include <torch/types.h>

 // namespace init
 // nn

/** Return the recommended gain value for the given nonlinearity function. */
@Namespace("torch::nn::init") public static native double calculate_gain(@ByVal NonlinearityType nonlinearity, double param/*=0.01*/);
@Namespace("torch::nn::init") public static native double calculate_gain(@ByVal NonlinearityType nonlinearity);

/** Fills the given {@code tensor} with the provided {@code value} in-place, and returns it.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor constant_(@ByVal Tensor tensor, @ByVal Scalar value);

/** Fills the given {@code tensor} with the Dirac delta function in-place, and returns
 *  it. No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor dirac_(@ByVal Tensor tensor);

/** Fills the given 2-dimensional {@code matrix} with an identity matrix.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor eye_(@ByVal Tensor matrix);

/** Fills the given 2-dimensional {@code matrix} with values drawn from a normal
 *  distribution parameterized by {@code mean} and {@code std}.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor normal_(@ByVal Tensor tensor, double mean/*=0*/, double std/*=1*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor normal_(@ByVal Tensor tensor);

/** Fills the given {@code tensor} with ones.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor ones_(@ByVal Tensor tensor);

/** Fills the input {@code Tensor} with a (semi) orthogonal matrix, as described in
 *  "Exact solutions to the nonlinear dynamics of learning in deep linear neural
 *  networks" - Saxe, A. et al. (2013). The input tensor must have at least 2
 *  dimensions, and for tensors with more than 2 dimensions the trailing
 *  dimensions are flattened.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor orthogonal_(@ByVal Tensor tensor, double gain/*=1.0*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor orthogonal_(@ByVal Tensor tensor);

/** Fills the 2D input {@code Tensor} as a sparse matrix, where the
 *  non-zero elements will be drawn from a centered normal distribution
 *  with the given standard deviation {@code std}, as described in "Deep learning via
 *  Hessian-free optimization" - Martens, J. (2010). The {@code sparsity} is a real
 *  value between 0 and 1 that controls the fraction of elements in each column
 *  to be set to zero.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor sparse_(@ByVal Tensor tensor, double sparsity, double std/*=0.01*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor sparse_(@ByVal Tensor tensor, double sparsity);

/** Fills the given 2-dimensional {@code matrix} with values drawn from a uniform
 *  distribution parameterized by {@code low} and {@code high}.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor uniform_(@ByVal Tensor tensor, double low/*=0*/, double high/*=1*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor uniform_(@ByVal Tensor tensor);

/** Fills the input {@code Tensor} with values according to the method
 *  described in "Delving deep into rectifiers: Surpassing human-level
 *  performance on ImageNet classification" - He, K. et al. (2015), using a
 *  normal distribution. Also known as He initialization.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor kaiming_normal_(
    @ByVal Tensor tensor,
    double a/*=0*/,
    @ByVal(nullValue = "torch::nn::init::FanModeType(torch::kFanIn)") FanModeType mode,
    @ByVal(nullValue = "torch::nn::init::NonlinearityType(torch::kLeakyReLU)") NonlinearityType nonlinearity);
@Namespace("torch::nn::init") public static native @ByVal Tensor kaiming_normal_(
    @ByVal Tensor tensor);

/** Fills the input {@code Tensor} with values according to the method
 *  described in "Delving deep into rectifiers: Surpassing human-level
 *  performance on ImageNet classification" - He, K. et al. (2015), using a
 *  uniform distribution. Also known as He initialization.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor kaiming_uniform_(
    @ByVal Tensor tensor,
    double a/*=0*/,
    @ByVal(nullValue = "torch::nn::init::FanModeType(torch::kFanIn)") FanModeType mode,
    @ByVal(nullValue = "torch::nn::init::NonlinearityType(torch::kLeakyReLU)") NonlinearityType nonlinearity);
@Namespace("torch::nn::init") public static native @ByVal Tensor kaiming_uniform_(
    @ByVal Tensor tensor);

/** Fills the input {@code Tensor} with values according to the method
 *  described in "Understanding the difficulty of training deep feedforward
 *  neural networks" - Glorot, X. & Bengio, Y. (2010). Values are scaled by the
 *  {@code gain} parameter. No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor xavier_normal_(@ByVal Tensor tensor, double gain/*=1.0*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor xavier_normal_(@ByVal Tensor tensor);

/** Fills the input {@code Tensor} with values according to the method
 *  described in "Understanding the difficulty of training deep feedforward
 *  neural networks" - Glorot, X. & Bengio, Y. (2010), using a uniform
 *  distribution. Values are scaled by the {@code gain} parameter
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor xavier_uniform_(@ByVal Tensor tensor, double gain/*=1.0*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor xavier_uniform_(@ByVal Tensor tensor);

/** Fills the given {@code tensor} with zeros.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor zeros_(@ByVal Tensor tensor);

@Namespace("torch::nn::init") public static native @ByVal @Cast("std::tuple<int64_t,int64_t>*") LongPointer _calculate_fan_in_and_fan_out(@Const @ByRef Tensor tensor);

 // namespace init
 // namespace nn
 // namespace torch


// Parsed from torch/nn/pimpl.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/detail/static.h>
// #include <torch/serialize/archive.h>
// #include <torch/types.h>

// #include <torch/csrc/utils/variadic.h>

// #include <memory>
// #include <type_traits>
// #include <utility>
// Dump all the template metaprogramming in this file.
// #include <torch/csrc/api/include/torch/nn/pimpl-inl.h>
 // namespace detail
// Targeting ../ModuleHolder.java


// Targeting ../ModuleDictImplModuleHolder.java


// Targeting ../ModuleListImplModuleHolder.java


// Targeting ../SequentialImplModuleHolder.java


// Targeting ../ParameterDictImplModuleHolder.java


// Targeting ../ParameterListImplModuleHolder.java


// Targeting ../AdaptiveLogSoftmaxWithLossImplModuleHolder.java


// Targeting ../BatchNorm1dImplModuleHolder.java


// Targeting ../InstanceNorm1dImplModuleHolder.java


// Targeting ../Conv1dImplModuleHolder.java


// Targeting ../ConvTranspose1dImplModuleHolder.java


// Targeting ../DropoutImplModuleHolder.java


// Targeting ../BatchNorm2dImplModuleHolder.java


// Targeting ../InstanceNorm2dImplModuleHolder.java


// Targeting ../Conv2dImplModuleHolder.java


// Targeting ../ConvTranspose2dImplModuleHolder.java


// Targeting ../Dropout2dImplModuleHolder.java


// Targeting ../BatchNorm3dImplModuleHolder.java


// Targeting ../InstanceNorm3dImplModuleHolder.java


// Targeting ../Conv3dImplModuleHolder.java


// Targeting ../ConvTranspose3dImplModuleHolder.java


// Targeting ../Dropout3dImplModuleHolder.java


// Targeting ../AlphaDropoutImplModuleHolder.java


// Targeting ../FeatureAlphaDropoutImplModuleHolder.java


// Targeting ../CosineSimilarityImplModuleHolder.java


// Targeting ../PairwiseDistanceImplModuleHolder.java


// Targeting ../EmbeddingImplModuleHolder.java


// Targeting ../EmbeddingBagImplModuleHolder.java


// Targeting ../FoldImplModuleHolder.java


// Targeting ../UnfoldImplModuleHolder.java


// Targeting ../IdentityImplModuleHolder.java


// Targeting ../LinearImplModuleHolder.java


// Targeting ../BilinearImplModuleHolder.java


// Targeting ../FlattenImplModuleHolder.java


// Targeting ../UnflattenImplModuleHolder.java


// Targeting ../L1LossImplModuleHolder.java


// Targeting ../KLDivLossImplModuleHolder.java


// Targeting ../MSELossImplModuleHolder.java


// Targeting ../BCELossImplModuleHolder.java


// Targeting ../HingeEmbeddingLossImplModuleHolder.java


// Targeting ../MultiMarginLossImplModuleHolder.java


// Targeting ../CosineEmbeddingLossImplModuleHolder.java


// Targeting ../SmoothL1LossImplModuleHolder.java


// Targeting ../HuberLossImplModuleHolder.java


// Targeting ../MultiLabelMarginLossImplModuleHolder.java


// Targeting ../SoftMarginLossImplModuleHolder.java


// Targeting ../MultiLabelSoftMarginLossImplModuleHolder.java


// Targeting ../TripletMarginLossImplModuleHolder.java


// Targeting ../TripletMarginWithDistanceLossImplModuleHolder.java


// Targeting ../CTCLossImplModuleHolder.java


// Targeting ../PoissonNLLLossImplModuleHolder.java


// Targeting ../MarginRankingLossImplModuleHolder.java


// Targeting ../NLLLossImplModuleHolder.java


// Targeting ../CrossEntropyLossImplModuleHolder.java


// Targeting ../BCEWithLogitsLossImplModuleHolder.java


// Targeting ../ReflectionPad1dImplModuleHolder.java


// Targeting ../ReplicationPad1dImplModuleHolder.java


// Targeting ../ConstantPad1dImplModuleHolder.java


// Targeting ../AvgPool1dImplModuleHolder.java


// Targeting ../MaxPool1dImplModuleHolder.java


// Targeting ../AdaptiveAvgPool1dImplModuleHolder.java


// Targeting ../AdaptiveMaxPool1dImplModuleHolder.java


// Targeting ../MaxUnpool1dImplModuleHolder.java


// Targeting ../LPPool1dImplModuleHolder.java


// Targeting ../ReflectionPad2dImplModuleHolder.java


// Targeting ../ReplicationPad2dImplModuleHolder.java


// Targeting ../ConstantPad2dImplModuleHolder.java


// Targeting ../ZeroPad2dImplModuleHolder.java


// Targeting ../AvgPool2dImplModuleHolder.java


// Targeting ../MaxPool2dImplModuleHolder.java


// Targeting ../AdaptiveAvgPool2dImplModuleHolder.java


// Targeting ../AdaptiveMaxPool2dImplModuleHolder.java


// Targeting ../MaxUnpool2dImplModuleHolder.java


// Targeting ../FractionalMaxPool2dImplModuleHolder.java


// Targeting ../LPPool2dImplModuleHolder.java


// Targeting ../ReflectionPad3dImplModuleHolder.java


// Targeting ../ReplicationPad3dImplModuleHolder.java


// Targeting ../ConstantPad3dImplModuleHolder.java


// Targeting ../AvgPool3dImplModuleHolder.java


// Targeting ../MaxPool3dImplModuleHolder.java


// Targeting ../AdaptiveAvgPool3dImplModuleHolder.java


// Targeting ../AdaptiveMaxPool3dImplModuleHolder.java


// Targeting ../MaxUnpool3dImplModuleHolder.java


// Targeting ../FractionalMaxPool3dImplModuleHolder.java


// Targeting ../RNNImplModuleHolder.java


// Targeting ../LSTMImplModuleHolder.java


// Targeting ../GRUImplModuleHolder.java


// Targeting ../RNNCellImplModuleHolder.java


// Targeting ../LSTMCellImplModuleHolder.java


// Targeting ../GRUCellImplModuleHolder.java


// Targeting ../PixelShuffleImplModuleHolder.java


// Targeting ../PixelUnshuffleImplModuleHolder.java


// Targeting ../UpsampleImplModuleHolder.java


// Targeting ../ELUImplModuleHolder.java


// Targeting ../SELUImplModuleHolder.java


// Targeting ../HardshrinkImplModuleHolder.java


// Targeting ../HardtanhImplModuleHolder.java


// Targeting ../LeakyReLUImplModuleHolder.java


// Targeting ../LogSigmoidImplModuleHolder.java


// Targeting ../SoftmaxImplModuleHolder.java


// Targeting ../SoftminImplModuleHolder.java


// Targeting ../LogSoftmaxImplModuleHolder.java


// Targeting ../Softmax2dImplModuleHolder.java


// Targeting ../PReLUImplModuleHolder.java


// Targeting ../ReLUImplModuleHolder.java


// Targeting ../ReLU6ImplModuleHolder.java


// Targeting ../RReLUImplModuleHolder.java


// Targeting ../CELUImplModuleHolder.java


// Targeting ../GLUImplModuleHolder.java


// Targeting ../GELUImplModuleHolder.java


// Targeting ../SiLUImplModuleHolder.java


// Targeting ../MishImplModuleHolder.java


// Targeting ../SigmoidImplModuleHolder.java


// Targeting ../SoftplusImplModuleHolder.java


// Targeting ../SoftshrinkImplModuleHolder.java


// Targeting ../SoftsignImplModuleHolder.java


// Targeting ../TanhImplModuleHolder.java


// Targeting ../TanhshrinkImplModuleHolder.java


// Targeting ../ThresholdImplModuleHolder.java


// Targeting ../MultiheadAttentionImplModuleHolder.java


// Targeting ../LayerNormImplModuleHolder.java


// Targeting ../LocalResponseNormImplModuleHolder.java


// Targeting ../CrossMapLRN2dImplModuleHolder.java


// Targeting ../GroupNormImplModuleHolder.java


// Targeting ../TransformerEncoderLayerImplModuleHolder.java


// Targeting ../TransformerDecoderLayerImplModuleHolder.java


// Targeting ../TransformerEncoderImplModuleHolder.java


// Targeting ../TransformerDecoderImplModuleHolder.java


// Targeting ../TransformerImplModuleHolder.java



/** Pretty prints the given {@code Module} into the {@code ostream}. */

/** Serializes a {@code ModuleHolder} into an {@code OutputArchive}. */

/** Deserializes a {@code ModuleHolder} from an {@code InputArchive}. */

 // namespace nn
 // namespace torch

/** Defines a class {@code Name} which inherits from {@code nn::ModuleHolder} to provide a
 *  wrapper over a {@code std::shared_ptr<ImplType>}.
 *  {@code Impl} is a type alias for {@code ImplType} which provides a way to call static
 *  method of {@code ImplType}. */
// #define TORCH_MODULE_IMPL(Name, ImplType)
//   class Name : public torch::nn::ModuleHolder<ImplType> { /* NOLINT */
//    public:
//     using torch::nn::ModuleHolder<ImplType>::ModuleHolder;
//     using Impl = ImplType;
//   }

/** Like {@code TORCH_MODULE_IMPL}, but defaults the {@code ImplType} name to {@code <Name>Impl}. */
// #define TORCH_MODULE(Name) TORCH_MODULE_IMPL(Name, Name##Impl)


// Parsed from torch/nn/utils.h

// #pragma once

// #include <torch/nn/utils/clip_grad.h>
// #include <torch/nn/utils/convert_parameters.h>
// #include <torch/nn/utils/rnn.h>


// Parsed from torch/nn/utils/clip_grad.h

// #pragma once

// #include <torch/csrc/WindowsTorchApiMacro.h>

// Clips gradient norm of a vector of Tensors.
// See
// https://pytorch.org/docs/stable/nn.html?highlight=clip_grad_norm#torch.nn.utils.clip_grad_norm_
// for more details about this module.
//
// Difference with the python version: unlike the python version, even when skipping the finiteness
// checks (error_if_nonfinite = false), this function will introduce a device <=> CPU
// synchronization (for devices where that makes sense!) in order to return a CPU-side `double`.
// This C++ version therefore cannot be run fully asynchronously w.r.t. the device of the
// gradients.
@Namespace("torch::nn::utils") public static native double clip_grad_norm_(
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector parameters,
    double max_norm,
    double norm_type/*=2.0*/,
    @Cast("bool") boolean error_if_nonfinite/*=false*/);
@Namespace("torch::nn::utils") public static native double clip_grad_norm_(
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector parameters,
    double max_norm);

// A wrapper around clip_grad_norm_ that allows us to call the function with a
// braced-init-list of Tensors.

// A wrapper around clip_grad_norm_ that allows us to call the function with a
// single Tensor.
@Namespace("torch::nn::utils") public static native double clip_grad_norm_(
    @ByVal Tensor parameter,
    double max_norm,
    double norm_type/*=2.0*/,
    @Cast("bool") boolean error_if_nonfinite/*=false*/);
@Namespace("torch::nn::utils") public static native double clip_grad_norm_(
    @ByVal Tensor parameter,
    double max_norm);

// Clips gradient of an iterable of parameters at specified value.
// Gradients are modified in-place.
// See https://pytorch.org/docs/stable/nn.html#clip-grad-value
// for more details about this module.
@Namespace("torch::nn::utils") public static native void clip_grad_value_(
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector parameters,
    double clip_value);

// A wrapper around clip_grad_value_ that allows us to call the function with a
// braced-init-list of Tensors.

// A wrapper around clip_grad_value_ that allows us to call the function with a
// single Tensor.
@Namespace("torch::nn::utils") public static native void clip_grad_value_(@ByVal Tensor parameter, double clip_value);

 // namespace utils
 // namespace nn
 // namespace torch


// Parsed from torch/nn/utils/convert_parameters.h

// #pragma once

// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>

// This helper function is to check if the parameters are located
// in the same device. Currently, the conversion between model parameters
// and single vector form is not supported for multiple allocations,
// e.g. parameters in different GPUs, or mixture of CPU/GPU.
@Namespace("torch::nn::utils") public static native @ByVal LongOptional _check_param_device(@Const @ByRef Tensor param, @ByVal LongOptional old_param_device);

// Convert parameters to one vector
@Namespace("torch::nn::utils") public static native @ByVal Tensor parameters_to_vector(@StdVector Tensor parameters);

// Convert one vector to the parameters
@Namespace("torch::nn::utils") public static native void vector_to_parameters(@Const @ByRef Tensor vec, @StdVector Tensor parameters);

 // namespace utils
 // namespace nn
 // namespace torch


// Parsed from torch/nn/utils/rnn.h

// #pragma once

// #include <c10/util/irange.h>
// #include <torch/types.h>


///
///
///
///
///
///
///
@Namespace("torch::nn::utils::rnn") public static native @ByVal Tensor invert_permutation(@Const @ByRef Tensor permutation);
// Targeting ../PackedSequence.java



/** Packs a Tensor containing padded sequences of variable length.
 * 
 *  {@code input} can be of size {@code }T x B x *{@code } where {@code T} is the length of the
 *  longest sequence (equal to {@code }lengths[0]{@code }), {@code }B{@code } is the batch size, and
 *  {@code }*{@code } is any number of dimensions (including 0). If {@code }batch_first{@code } is
 *  {@code }true{@code }, {@code }B x T x *{@code } {@code input} is expected.
 * 
 *  For unsorted sequences, use {@code enforce_sorted = false}. If {@code enforce_sorted} is
 *  {@code }true{@code }, the sequences should be sorted by length in a decreasing order, i.e.
 *  {@code }input[:,0]{@code } should be the longest sequence, and {@code }input[:,B-1]{@code } the shortest
 *  one.
 * 
 *  Note:
 *      This function accepts any input that has at least two dimensions. You
 *      can apply it to pack the labels, and use the output of the RNN with
 *      them to compute the loss directly. A Tensor can be retrieved from
 *      a {@code PackedSequence} object by calling its {@code }.data(){@code } function.
 * 
 *  Arguments:
 *      input (Tensor): padded batch of variable length sequences.
 *      lengths (Tensor): list of sequences lengths of each batch element.
 *      batch_first (bool, optional): if {@code }true{@code }, the input is expected in {@code }B x T x *{@code }
 *          format. Default: {@code }false{@code }.
 *      enforce_sorted (bool, optional): if {@code }true{@code }, the input is expected to
 *          contain sequences sorted by length in a decreasing order. If
 *          {@code }false{@code }, this condition is not checked. Default: {@code }true{@code }.
 * 
 *  Returns:
 *      a {@code PackedSequence} object */

///
///
///
///
///
@Namespace("torch::nn::utils::rnn") public static native @ByVal PackedSequence pack_padded_sequence(
    @ByVal Tensor input,
    @ByVal Tensor lengths,
    @Cast("bool") boolean batch_first/*=false*/,
    @Cast("bool") boolean enforce_sorted/*=true*/);
@Namespace("torch::nn::utils::rnn") public static native @ByVal PackedSequence pack_padded_sequence(
    @ByVal Tensor input,
    @ByVal Tensor lengths);

/** Pads a packed batch of variable length sequences.
 * 
 *  It is an inverse operation to {@code pack_padded_sequence}.
 * 
 *  The returned Tensor's data will be of size {@code }T x B x *{@code }, where {@code T} is the length
 *  of the longest sequence and {@code B} is the batch size. If {@code }batch_first{@code } is true,
 *  the data will be transposed into {@code }B x T x *{@code } format.
 * 
 *  Batch elements will be ordered decreasingly by their length.
 * 
 *  Arguments:
 *      sequence (PackedSequence): batch to pad
 *      batch_first (bool, optional): if {@code }true{@code }, the output will be in {@code }B x T x *{@code }
 *          format.
 *      padding_value (double, optional): values for padded elements.
 *      total_length (int64_t, optional): if specified, the output will be padded to
 *          have length {@code total_length}. This method will throw error
 *          if {@code total_length} is less than the max sequence length in
 *          {@code sequence}.
 * 
 *  Returns:
 *      Tuple of Tensor containing the padded sequence, and a Tensor
 *      containing the list of lengths of each sequence in the batch. */

///
///
///
///
///
@Namespace("torch::nn::utils::rnn") public static native @ByVal TensorTensorTuple pad_packed_sequence(
    @ByVal PackedSequence sequence,
    @Cast("bool") boolean batch_first/*=false*/,
    double padding_value/*=0.0*/,
    @ByVal(nullValue = "c10::optional<int64_t>(torch::nullopt)") LongOptional total_length);
@Namespace("torch::nn::utils::rnn") public static native @ByVal TensorTensorTuple pad_packed_sequence(
    @ByVal PackedSequence sequence);

/** Pad a list of variable length Tensors with {@code }padding_value{@code }
 * 
 *  {@code }pad_sequence{@code } stacks a list of Tensors along a new dimension,
 *  and pads them to equal length. For example, if the input is list of
 *  sequences with size {@code }L x *{@code } and if batch_first is false, and {@code }T x B x *{@code }
 *  otherwise.
 * 
 *  {@code B} is batch size. It is equal to the number of elements in {@code }sequences{@code }.
 *  {@code T} is length of the longest sequence.
 *  {@code L} is length of the sequence.
 *  {@code *} is any number of trailing dimensions, including none.
 * 
 *  Note:
 *      This function returns a Tensor of size {@code }T x B x *{@code } or {@code }B x T x *{@code }
 *      where {@code T} is the length of the longest sequence. This function assumes
 *      trailing dimensions and type of all the Tensors in sequences are same.
 * 
 *  Arguments:
 *      sequences (torch::ArrayRef<Tensor>): list of variable length sequences.
 *      batch_first (bool, optional): output will be in {@code }B x T x *{@code } if true, or in
 *          {@code }T x B x *{@code } otherwise
 *      padding_value (double, optional): value for padded elements. Default: 0.
 * 
 *  Returns:
 *      Tensor of size {@code }T x B x *{@code } if {@code batch_first} is {@code }false{@code }.
 *      Tensor of size {@code }B x T x *{@code } otherwise */

/** Packs a list of variable length Tensors
 * 
 *  {@code }sequences{@code } should be a list of Tensors of size {@code }L x *{@code }, where {@code L} is
 *  the length of a sequence and {@code *} is any number of trailing dimensions,
 *  including zero.
 * 
 *  For unsorted sequences, use {@code enforce_sorted = false}. If {@code }enforce_sorted{@code }
 *  is {@code }true{@code }, the sequences should be sorted in the order of decreasing length.
 * 
 * 
 *  Arguments:
 *      sequences (torch::ArrayRef<Tensor>): A list of sequences of decreasing length.
 *      enforce_sorted (bool, optional): if {@code }true{@code }, checks that the input
 *          contains sequences sorted by length in a decreasing order. If
 *          {@code }false{@code }, this condition is not checked. Default: {@code }true{@code }.
 * 
 *  Returns:
 *      a {@code PackedSequence} object */
@Namespace("torch::nn::utils::rnn") public static native @ByVal PackedSequence pack_sequence(@ByVal TensorArrayRef sequences, @Cast("bool") boolean enforce_sorted/*=true*/);
@Namespace("torch::nn::utils::rnn") public static native @ByVal PackedSequence pack_sequence(@ByVal TensorArrayRef sequences);

 // namespace rnn
 // namespace utils
 // namespace nn
 // namespace torch


// Parsed from torch/nn/options.h

// #pragma once

// #include <torch/nn/options/batchnorm.h>
// #include <torch/nn/options/conv.h>
// #include <torch/nn/options/dropout.h>
// #include <torch/nn/options/fold.h>
// #include <torch/nn/options/linear.h>
// #include <torch/nn/options/loss.h>
// #include <torch/nn/options/normalization.h>
// #include <torch/nn/options/padding.h>
// #include <torch/nn/options/pooling.h>
// #include <torch/nn/options/vision.h>
// #include <torch/nn/options/rnn.h>
// #include <torch/nn/options/pixelshuffle.h>
// #include <torch/nn/options/upsampling.h>
// #include <torch/nn/options/transformerlayer.h>
// #include <torch/nn/options/transformercoder.h>
// #include <torch/nn/options/transformer.h>


// Parsed from torch/nn/options/activation.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// Targeting ../ELUOptions.java


/** Options for {@code torch::nn::functional::elu}.
 * 
 *  See the documentation for {@code torch::nn::ELUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::elu(x, F::ELUFuncOptions().alpha(0.42).inplace(true));
 *  }</pre> */

// Targeting ../SELUOptions.java


/** Options for {@code torch::nn::functional::selu}.
 * 
 *  See the documentation for {@code torch::nn::SELUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::selu(input, F::SELUFuncOptions(false));
 *  }</pre> */

// Targeting ../GLUOptions.java


/** Options for {@code torch::nn::functional::glu}.
 * 
 *  See the documentation for {@code torch::nn::GLUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::glu(input, GLUFuncOptions(1));
 *  }</pre> */

// Targeting ../HardshrinkOptions.java


/** Options for {@code torch::nn::functional::hardshrink}.
 * 
 *  See the documentation for {@code torch::nn::HardshrinkOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::hardshrink(x, F::HardshrinkFuncOptions().lambda(0.42));
 *  }</pre> */

// Targeting ../HardtanhOptions.java


/** Options for {@code torch::nn::functional::hardtanh}.
 * 
 *  See the documentation for {@code torch::nn::HardtanhOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::hardtanh(x, F::HardtanhFuncOptions().min_val(-1.0).max_val(1.0).inplace(true));
 *  }</pre> */

// Targeting ../LeakyReLUOptions.java


/** Options for {@code torch::nn::functional::leaky_relu}.
 * 
 *  See the documentation for {@code torch::nn::LeakyReLUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::leaky_relu(x, F::LeakyReLUFuncOptions().negative_slope(0.42).inplace(true));
 *  }</pre> */

// Targeting ../SoftmaxOptions.java



// ============================================================================
// Targeting ../SoftmaxFuncOptions.java




// Targeting ../SoftminOptions.java



// ============================================================================
// Targeting ../SoftminFuncOptions.java




// Targeting ../LogSoftmaxOptions.java



// ============================================================================
// Targeting ../LogSoftmaxFuncOptions.java




// Targeting ../PReLUOptions.java


// Targeting ../ReLUOptions.java


/** Options for {@code torch::nn::functional::relu}.
 * 
 *  See the documentation for {@code torch::nn::ReLUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::relu(x, F::ReLUFuncOptions().inplace(true));
 *  }</pre> */

// Targeting ../ReLU6Options.java


/** Options for {@code torch::nn::functional::relu6}.
 * 
 *  See the documentation for {@code torch::nn::ReLU6Options} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::relu6(x, F::ReLU6FuncOptions().inplace(true));
 *  }</pre> */

// Targeting ../RReLUOptions.java



// ============================================================================
// Targeting ../RReLUFuncOptions.java




// Targeting ../CELUOptions.java


/** Options for {@code torch::nn::functional::celu}.
 * 
 *  See the documentation for {@code torch::nn::CELUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::celu(x, F::CELUFuncOptions().alpha(0.42).inplace(true));
 *  }</pre> */

// Targeting ../SoftplusOptions.java


/** Options for {@code torch::nn::functional::softplus}.
 * 
 *  See the documentation for {@code torch::nn::SoftplusOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::softplus(x, F::SoftplusFuncOptions().beta(0.5).threshold(3.0));
 *  }</pre> */

// Targeting ../SoftshrinkOptions.java


/** Options for {@code torch::nn::functional::softshrink}.
 * 
 *  See the documentation for {@code torch::nn::SoftshrinkOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::softshrink(x, F::SoftshrinkFuncOptions(0.42));
 *  }</pre> */

// Targeting ../ThresholdOptions.java


/** Options for {@code torch::nn::functional::threshold}.
 * 
 *  See the documentation for {@code torch::nn::ThresholdOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::threshold(x, F::ThresholdFuncOptions(0.5, 0.5).inplace(true));
 *  }</pre> */
 // namespace functional

// ============================================================================
// Targeting ../GumbelSoftmaxFuncOptions.java




// Targeting ../MultiheadAttentionOptions.java



// ============================================================================
// Targeting ../MultiheadAttentionForwardFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/adaptive.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// Targeting ../AdaptiveLogSoftmaxWithLossOptions.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/batchnorm.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// Targeting ../BatchNormOptions.java



/** Options for the {@code BatchNorm1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  BatchNorm1d model(BatchNorm1dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */

///

/** Options for the {@code BatchNorm2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  BatchNorm2d model(BatchNorm2dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */

///

/** Options for the {@code BatchNorm3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  BatchNorm3d model(BatchNorm3dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */

// ============================================================================
// Targeting ../BatchNormFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/conv.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/enum.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/expanding_array.h>
// #include <torch/types.h>
// Targeting ../DetailConv1dOptions.java


// Targeting ../DetailConv2dOptions.java


// Targeting ../DetailConv3dOptions.java




// Targeting ../Conv1dOptions.java


// Targeting ../Conv2dOptions.java


// Targeting ../Conv3dOptions.java



/** {@code ConvOptions} specialized for the {@code Conv1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  Conv1d model(Conv1dOptions(3, 2, 3).stride(1).bias(false));
 *  }</pre> */

///

/** {@code ConvOptions} specialized for the {@code Conv2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  Conv2d model(Conv2dOptions(3, 2, 3).stride(1).bias(false));
 *  }</pre> */

///

/** {@code ConvOptions} specialized for the {@code Conv3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  Conv3d model(Conv3dOptions(3, 2, 3).stride(1).bias(false));
 *  }</pre> */

// ============================================================================
// Targeting ../Conv1dFuncOptions.java


// Targeting ../Conv2dFuncOptions.java


// Targeting ../Conv3dFuncOptions.java



/** {@code ConvFuncOptions} specialized for {@code torch::nn::functional::conv1d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv1d(x, weight, F::Conv1dFuncOptions().stride(1));
 *  }</pre> */

///

/** {@code ConvFuncOptions} specialized for {@code torch::nn::functional::conv2d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv2d(x, weight, F::Conv2dFuncOptions().stride(1));
 *  }</pre> */

///

/** {@code ConvFuncOptions} specialized for {@code torch::nn::functional::conv3d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv3d(x, weight, F::Conv3dFuncOptions().stride(1));
 *  }</pre> */


// Targeting ../ConvTranspose1dOptions.java


// Targeting ../ConvTranspose2dOptions.java


// Targeting ../ConvTranspose3dOptions.java



/** {@code ConvTransposeOptions} specialized for the {@code ConvTranspose1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConvTranspose1d model(ConvTranspose1dOptions(3, 2, 3).stride(1).bias(false));
 *  }</pre> */

///

/** {@code ConvTransposeOptions} specialized for the {@code ConvTranspose2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConvTranspose2d model(ConvTranspose2dOptions(3, 2, 3).stride(1).bias(false));
 *  }</pre> */

///

/** {@code ConvTransposeOptions} specialized for the {@code ConvTranspose3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConvTranspose3d model(ConvTranspose3dOptions(2, 2, 2).stride(1).bias(false));
 *  }</pre> */

// ============================================================================
// Targeting ../ConvTranspose1dFuncOptions.java


// Targeting ../ConvTranspose2dFuncOptions.java


// Targeting ../ConvTranspose3dFuncOptions.java



/** {@code ConvTransposeFuncOptions} specialized for {@code torch::nn::functional::conv_transpose1d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv_transpose1d(x, weight, F::ConvTranspose1dFuncOptions().stride(1));
 *  }</pre> */

///

/** {@code ConvTransposeFuncOptions} specialized for {@code torch::nn::functional::conv_transpose2d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv_transpose2d(x, weight, F::ConvTranspose2dFuncOptions().stride(1));
 *  }</pre> */

///

/** {@code ConvTransposeFuncOptions} specialized for {@code torch::nn::functional::conv_transpose3d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv_transpose3d(x, weight, F::ConvTranspose3dFuncOptions().stride(1));
 *  }</pre> */

 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/distance.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// Targeting ../CosineSimilarityOptions.java


/** Options for {@code torch::nn::functional::cosine_similarity}.
 * 
 *  See the documentation for {@code torch::nn::CosineSimilarityOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::cosine_similarity(input1, input2, F::CosineSimilarityFuncOptions().dim(1));
 *  }</pre> */

// Targeting ../PairwiseDistanceOptions.java


/** Options for {@code torch::nn::functional::pairwise_distance}.
 * 
 *  See the documentation for {@code torch::nn::PairwiseDistanceOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::pairwise_distance(input1, input2, F::PairwiseDistanceFuncOptions().p(1));
 *  }</pre> */
 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/dropout.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// Targeting ../DropoutOptions.java



/** Options for the {@code Dropout2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  Dropout2d model(Dropout2dOptions().p(0.42).inplace(true));
 *  }</pre> */

///

/** Options for the {@code Dropout3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  Dropout3d model(Dropout3dOptions().p(0.42).inplace(true));
 *  }</pre> */

///

/** Options for the {@code AlphaDropout} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AlphaDropout model(AlphaDropoutOptions(0.2).inplace(true));
 *  }</pre> */

///

/** Options for the {@code FeatureAlphaDropout} module.
 * 
 *  Example:
 *  <pre>{@code
 *  FeatureAlphaDropout model(FeatureAlphaDropoutOptions(0.2).inplace(true));
 *  }</pre> */
// Targeting ../DropoutFuncOptions.java



/** Options for {@code torch::nn::functional::dropout2d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::dropout2d(input, F::Dropout2dFuncOptions().p(0.5));
 *  }</pre> */

///

/** Options for {@code torch::nn::functional::dropout3d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::dropout3d(input, F::Dropout3dFuncOptions().p(0.5));
 *  }</pre> */

///
// Targeting ../AlphaDropoutFuncOptions.java


// Targeting ../FeatureAlphaDropoutFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/embedding.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// #include <torch/enum.h>
// Targeting ../EmbeddingOptions.java


// Targeting ../EmbeddingFromPretrainedOptions.java



// ============================================================================
// Targeting ../EmbeddingFuncOptions.java



 // namespace functional

// ============================================================================


///
// Targeting ../EmbeddingBagOptions.java


// Targeting ../EmbeddingBagFromPretrainedOptions.java



// ============================================================================
// Targeting ../EmbeddingBagFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/fold.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/expanding_array.h>
// #include <torch/types.h>
// Targeting ../FoldOptions.java


/** Options for {@code torch::nn::functional::fold}.
 * 
 *  See the documentation for {@code torch::nn::FoldOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::fold(input, F::FoldFuncOptions({3, 2}, {2, 2}));
 *  }</pre> */

// Targeting ../UnfoldOptions.java


/** Options for {@code torch::nn::functional::unfold}.
 * 
 *  See the documentation for {@code torch::nn::UnfoldOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::unfold(input, F::UnfoldFuncOptions({2, 2}).padding(1).stride(2));
 *  }</pre> */
 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/linear.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// #include <c10/util/variant.h>
// Targeting ../LinearOptions.java


// Targeting ../FlattenOptions.java


// Targeting ../UnflattenOptions.java


// Targeting ../BilinearOptions.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/loss.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/enum.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// Targeting ../L1LossOptions.java


/** Options for {@code torch::nn::functional::l1_loss}.
 * 
 *  See the documentation for {@code torch::nn::L1LossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::l1_loss(input, target, F::L1LossFuncOptions(torch::kNone));
 *  }</pre> */

// Targeting ../KLDivLossOptions.java


/** Options for {@code torch::nn::functional::kl_div}.
 * 
 *  See the documentation for {@code torch::nn::KLDivLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::kl_div(input, target, F::KLDivFuncOptions().reduction(torch::kNone).log_target(false));
 *  }</pre> */

// Targeting ../MSELossOptions.java


/** Options for {@code torch::nn::functional::mse_loss}.
 * 
 *  See the documentation for {@code torch::nn::MSELossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::mse_loss(input, target, F::MSELossFuncOptions(torch::kNone));
 *  }</pre> */

// Targeting ../BCELossOptions.java


/** Options for {@code torch::nn::functional::binary_cross_entropy}.
 * 
 *  See the documentation for {@code torch::nn::BCELossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::binary_cross_entropy(input, target, F::BinaryCrossEntropyFuncOptions().weight(weight));
 *  }</pre> */

// Targeting ../HingeEmbeddingLossOptions.java


/** Options for {@code torch::nn::functional::hinge_embedding_loss}.
 * 
 *  See the documentation for {@code torch::nn::HingeEmbeddingLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::hinge_embedding_loss(input, target, F::HingeEmbeddingLossFuncOptions().margin(2));
 *  }</pre> */

// Targeting ../MultiMarginLossOptions.java


/** Options for {@code torch::nn::functional::multi_margin_loss}.
 * 
 *  See the documentation for {@code torch::nn::MultiMarginLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::multi_margin_loss(input, target, F::MultiMarginLossFuncOptions().margin(2).weight(weight));
 *  }</pre> */

// Targeting ../CosineEmbeddingLossOptions.java


/** Options for {@code torch::nn::functional::cosine_embedding_loss}.
 * 
 *  See the documentation for {@code torch::nn::CosineEmbeddingLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::cosine_embedding_loss(input1, input2, target, F::CosineEmbeddingLossFuncOptions().margin(0.5));
 *  }</pre> */

// Targeting ../MultiLabelMarginLossOptions.java


/** Options for {@code torch::nn::functional::multilabel_margin_loss}.
 * 
 *  See the documentation for {@code torch::nn::MultiLabelMarginLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::multilabel_margin_loss(input, target, F::MultilabelMarginLossFuncOptions(torch::kNone));
 *  }</pre> */

// Targeting ../SoftMarginLossOptions.java


/** Options for {@code torch::nn::functional::soft_margin_loss}.
 * 
 *  See the documentation for {@code torch::nn::SoftMarginLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::soft_margin_loss(input, target, F::SoftMarginLossFuncOptions(torch::kNone));
 *  }</pre> */

// Targeting ../MultiLabelSoftMarginLossOptions.java


/** Options for {@code torch::nn::functional::multilabel_soft_margin_loss}.
 * 
 *  See the documentation for {@code torch::nn::MultiLabelSoftMarginLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::multilabel_soft_margin_loss(input, target, F::MultilabelSoftMarginLossFuncOptions().reduction(torch::kNone).weight(weight));
 *  }</pre> */

// Targeting ../TripletMarginLossOptions.java


/** Options for {@code torch::nn::functional::triplet_margin_loss}.
 * 
 *  See the documentation for {@code torch::nn::TripletMarginLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::triplet_margin_loss(anchor, positive, negative, F::TripletMarginLossFuncOptions().margin(1.0));
 *  }</pre> */

// Targeting ../TripletMarginWithDistanceLossOptions.java


/** Options for {@code torch::nn::functional::triplet_margin_with_distance_loss}.
 * 
 *  See the documentation for {@code torch::nn::TripletMarginWithDistanceLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::triplet_margin_with_distance_loss(anchor, positive, negative, F::TripletMarginWithDistanceLossFuncOptions().margin(1.0));
 *  }</pre> */

// Targeting ../CTCLossOptions.java


/** Options for {@code torch::nn::functional::ctc_loss}.
 * 
 *  See the documentation for {@code torch::nn::CTCLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::ctc_loss(log_probs, targets, input_lengths, target_lengths, F::CTCLossFuncOptions().reduction(torch::kNone));
 *  }</pre> */

// Targeting ../SmoothL1LossOptions.java


/** Options for {@code torch::nn::functional::smooth_l1_loss}.
 * 
 *  See the documentation for {@code torch::nn::SmoothL1LossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::smooth_l1_loss(input, target, F::SmoothL1LossFuncOptions(torch::kNone));
 *  }</pre> */

// Targeting ../HuberLossOptions.java


/** Options for {@code torch::nn::functional::huber_loss}.
 * 
 *  See the documentation for {@code torch::nn::HuberLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::huber_loss(input, target, F::HuberLossFuncOptions(torch::kNone));
 *  }</pre> */

// Targeting ../PoissonNLLLossOptions.java


/** Options for {@code torch::nn::functional::poisson_nll_loss}.
 * 
 *  See the documentation for {@code torch::nn::PoissonNLLLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::poisson_nll_loss(input, target, F::PoissonNLLLossFuncOptions().reduction(torch::kNone));
 *  }</pre> */

// Targeting ../MarginRankingLossOptions.java


/** Options for {@code torch::nn::functional::margin_ranking_loss}.
 * 
 *  See the documentation for {@code torch::nn::MarginRankingLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::margin_ranking_loss(input1, input2, target, F::MarginRankingLossFuncOptions().margin(0.5).reduction(torch::kSum));
 *  }</pre> */

// Targeting ../NLLLossOptions.java


/** Options for {@code torch::nn::functional::nll_loss}.
 * 
 *  See the documentation for {@code torch::nn::NLLLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::nll_loss(input, target, F::NLLLossFuncOptions().ignore_index(-100).reduction(torch::kMean));
 *  }</pre> */

// Targeting ../CrossEntropyLossOptions.java


/** Options for {@code torch::nn::functional::cross_entropy}.
 * 
 *  See the documentation for {@code torch::nn::CrossEntropyLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::cross_entropy(input, target, F::CrossEntropyFuncOptions().ignore_index(-100).reduction(torch::kMean));
 *  }</pre> */

// Targeting ../BCEWithLogitsLossOptions.java


/** Options for {@code torch::nn::functional::binary_cross_entropy_with_logits}.
 * 
 *  See the documentation for {@code torch::nn::BCEWithLogitsLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::binary_cross_entropy_with_logits(input, target, F::BinaryCrossEntropyWithLogitsFuncOptions().pos_weight(pos_weight).reduction(torch::kSum));
 *  }</pre> */
 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/normalization.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// #include <vector>
// Targeting ../LayerNormOptions.java



// ============================================================================
// Targeting ../LayerNormFuncOptions.java




// Targeting ../LocalResponseNormOptions.java


/** Options for {@code torch::nn::functional::local_response_norm}.
 * 
 *  See the documentation for {@code torch::nn::LocalResponseNormOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::local_response_norm(x, F::LocalResponseNormFuncOptions(2));
 *  }</pre> */

// Targeting ../CrossMapLRN2dOptions.java



// ============================================================================
// Targeting ../NormalizeFuncOptions.java




// Targeting ../GroupNormOptions.java



// ============================================================================
// Targeting ../GroupNormFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/padding.h

// #pragma once

// #include <c10/util/variant.h>
// #include <torch/arg.h>
// #include <torch/enum.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/expanding_array.h>
// #include <torch/types.h>
// Targeting ../ReflectionPad1dOptions.java


// Targeting ../ReflectionPad2dOptions.java


// Targeting ../ReflectionPad3dOptions.java



/** {@code ReflectionPadOptions} specialized for the {@code ReflectionPad1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ReflectionPad1d model(ReflectionPad1dOptions({3, 1}));
 *  }</pre> */

///

/** {@code ReflectionPadOptions} specialized for the {@code ReflectionPad2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ReflectionPad2d model(ReflectionPad2dOptions({1, 1, 2, 0}));
 *  }</pre> */

///

/** {@code ReflectionPadOptions} specialized for the {@code ReflectionPad3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ReflectionPad3d model(ReflectionPad3dOptions({1, 1, 2, 0, 1, 1}));
 *  }</pre> */
// Targeting ../ReplicationPad1dOptions.java


// Targeting ../ReplicationPad2dOptions.java


// Targeting ../ReplicationPad3dOptions.java



/** {@code ReplicationPadOptions} specialized for the {@code ReplicationPad1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ReplicationPad1d model(ReplicationPad1dOptions({3, 1}));
 *  }</pre> */

///

/** {@code ReplicationPadOptions} specialized for the {@code ReplicationPad2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ReplicationPad2d model(ReplicationPad2dOptions({1, 1, 2, 0}));
 *  }</pre> */

///

/** {@code ReplicationPadOptions} specialized for the {@code ReplicationPad3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ReplicationPad3d model(ReplicationPad3dOptions({1, 2, 1, 2, 1, 2}));
 *  }</pre> */

///
// Targeting ../ZeroPad2dOptions.java


// Targeting ../ConstantPad1dOptions.java


// Targeting ../ConstantPad2dOptions.java


// Targeting ../ConstantPad3dOptions.java



/** {@code ConstantPadOptions} specialized for the {@code ConstantPad1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConstantPad1d model(ConstantPad1dOptions({3, 1}, 3.5));
 *  }</pre> */

///

/** {@code ConstantPadOptions} specialized for the {@code ConstantPad2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConstantPad2d model(ConstantPad2dOptions({3, 0, 2, 1}, 3.5));
 *  }</pre> */

///

/** {@code ConstantPadOptions} specialized for the {@code ConstantPad3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConstantPad3d model(ConstantPad3dOptions({1, 2, 1, 2, 1, 2}, 3.5));
 *  }</pre> */

// ============================================================================
// Targeting ../PadFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/pixelshuffle.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// Targeting ../PixelShuffleOptions.java


// Targeting ../PixelUnshuffleOptions.java


/** Options for {@code torch::nn::functional::pixel_shuffle}.
 * 
 *  See the documentation for {@code torch::nn::PixelShuffleOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::pixel_shuffle(x, F::PixelShuffleFuncOptions(2));
 *  }</pre> */

///
///

/** Options for {@code torch::nn::functional::pixel_unshuffle}.
 * 
 *  See the documentation for {@code torch::nn::PixelUnshuffleOptions} class to learn
 *  what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::pixel_unshuffle(x, F::PixelUnshuffleFuncOptions(2));
 *  }</pre> */
 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/pooling.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/expanding_array.h>
// #include <torch/types.h>
// Targeting ../AvgPool1dOptions.java


// Targeting ../AvgPool2dOptions.java


// Targeting ../AvgPool3dOptions.java



/** {@code AvgPoolOptions} specialized for the {@code AvgPool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AvgPool1d model(AvgPool1dOptions(3).stride(2));
 *  }</pre> */

///

/** {@code AvgPoolOptions} specialized for the {@code AvgPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AvgPool2d model(AvgPool2dOptions({3, 2}).stride({2, 2}));
 *  }</pre> */

///

/** {@code AvgPoolOptions} specialized for the {@code AvgPool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AvgPool3d model(AvgPool3dOptions(5).stride(2));
 *  }</pre> */
/** Options for {@code torch::nn::functional::avg_pool1d}.
 * 
 *  See the documentation for {@code torch::nn::AvgPool1dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::avg_pool1d(x, F::AvgPool1dFuncOptions(3).stride(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::avg_pool2d}.
 * 
 *  See the documentation for {@code torch::nn::AvgPool2dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::avg_pool2d(x, F::AvgPool2dFuncOptions(3).stride(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::avg_pool3d}.
 * 
 *  See the documentation for {@code torch::nn::AvgPool3dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::avg_pool3d(x, F::AvgPool3dFuncOptions(3).stride(2));
 *  }</pre> */

// Targeting ../MaxPool1dOptions.java


// Targeting ../MaxPool2dOptions.java


// Targeting ../MaxPool3dOptions.java



/** {@code MaxPoolOptions} specialized for the {@code MaxPool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxPool1d model(MaxPool1dOptions(3).stride(2));
 *  }</pre> */

///

/** {@code MaxPoolOptions} specialized for the {@code MaxPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxPool2d model(MaxPool2dOptions({3, 2}).stride({2, 2}));
 *  }</pre> */

///

/** {@code MaxPoolOptions} specialized for the {@code MaxPool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxPool3d model(MaxPool3dOptions(3).stride(2));
 *  }</pre> */
/** Options for {@code torch::nn::functional::max_pool1d} and {@code torch::nn::functional::max_pool1d_with_indices}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_pool1d(x, F::MaxPool1dFuncOptions(3).stride(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::max_pool2d} and {@code torch::nn::functional::max_pool2d_with_indices}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_pool2d(x, F::MaxPool2dFuncOptions(3).stride(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::max_pool3d} and {@code torch::nn::functional::max_pool3d_with_indices}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_pool3d(x, F::MaxPool3dFuncOptions(3).stride(2));
 *  }</pre> */

// Targeting ../AdaptiveMaxPool1dOptions.java


// Targeting ../AdaptiveMaxPool2dOptions.java


// Targeting ../AdaptiveMaxPool3dOptions.java



/** {@code AdaptiveMaxPoolOptions} specialized for the {@code AdaptiveMaxPool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveMaxPool1d model(AdaptiveMaxPool1dOptions(3));
 *  }</pre> */

///

/** {@code AdaptiveMaxPoolOptions} specialized for the {@code AdaptiveMaxPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveMaxPool2d model(AdaptiveMaxPool2dOptions({3, 2}));
 *  }</pre> */

///

/** {@code AdaptiveMaxPoolOptions} specialized for the {@code AdaptiveMaxPool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveMaxPool3d model(AdaptiveMaxPool3dOptions(3));
 *  }</pre> */
/** Options for {@code torch::nn::functional::adaptive_max_pool1d} and {@code torch::nn::functional::adaptive_max_pool1d_with_indices}
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_max_pool1d(x, F::AdaptiveMaxPool1dFuncOptions(3));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::adaptive_max_pool2d} and {@code torch::nn::functional::adaptive_max_pool2d_with_indices}
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_max_pool2d(x, F::AdaptiveMaxPool2dFuncOptions(3));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::adaptive_max_pool3d} and {@code torch::nn::functional::adaptive_max_pool3d_with_indices}
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_max_pool3d(x, F::AdaptiveMaxPool3dFuncOptions(3));
 *  }</pre> */

// Targeting ../AdaptiveAvgPool1dOptions.java


// Targeting ../AdaptiveAvgPool2dOptions.java


// Targeting ../AdaptiveAvgPool3dOptions.java



/** {@code AdaptiveAvgPoolOptions} specialized for the {@code AdaptiveAvgPool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveAvgPool1d model(AdaptiveAvgPool1dOptions(5));
 *  }</pre> */

///

/** {@code AdaptiveAvgPoolOptions} specialized for the {@code AdaptiveAvgPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveAvgPool2d model(AdaptiveAvgPool2dOptions({3, 2}));
 *  }</pre> */

///

/** {@code AdaptiveAvgPoolOptions} specialized for the {@code AdaptiveAvgPool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveAvgPool3d model(AdaptiveAvgPool3dOptions(3));
 *  }</pre> */
/** Options for {@code torch::nn::functional::adaptive_avg_pool1d}.
 * 
 *  See the documentation for {@code torch::nn::AdaptiveAvgPool1dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_avg_pool1d(x, F::AdaptiveAvgPool1dFuncOptions(3));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::adaptive_avg_pool2d}.
 * 
 *  See the documentation for {@code torch::nn::AdaptiveAvgPool2dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_avg_pool2d(x, F::AdaptiveAvgPool2dFuncOptions(3));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::adaptive_avg_pool3d}.
 * 
 *  See the documentation for {@code torch::nn::AdaptiveAvgPool3dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_avg_pool3d(x, F::AdaptiveAvgPool3dFuncOptions(3));
 *  }</pre> */

// Targeting ../MaxUnpool1dOptions.java


// Targeting ../MaxUnpool2dOptions.java


// Targeting ../MaxUnpool3dOptions.java



/** {@code MaxUnpoolOptions} specialized for the {@code MaxUnpool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxUnpool1d model(MaxUnpool1dOptions(3).stride(2).padding(1));
 *  }</pre> */

///

/** {@code MaxUnpoolOptions} specialized for the {@code MaxUnpool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxUnpool2d model(MaxUnpool2dOptions(3).stride(2).padding(1));
 *  }</pre> */

///

/** {@code MaxUnpoolOptions} specialized for the {@code MaxUnpool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxUnpool3d model(MaxUnpool3dOptions(3).stride(2).padding(1));
 *  }</pre> */

// ============================================================================
// Targeting ../MaxUnpool1dFuncOptions.java


// Targeting ../MaxUnpool2dFuncOptions.java


// Targeting ../MaxUnpool3dFuncOptions.java



/** {@code MaxUnpoolFuncOptions} specialized for {@code torch::nn::functional::max_unpool1d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_unpool1d(x, indices, F::MaxUnpool1dFuncOptions(3).stride(2).padding(1));
 *  }</pre> */

///

/** {@code MaxUnpoolFuncOptions} specialized for {@code torch::nn::functional::max_unpool2d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_unpool2d(x, indices, F::MaxUnpool2dFuncOptions(3).stride(2).padding(1));
 *  }</pre> */

///

/** {@code MaxUnpoolFuncOptions} specialized for {@code torch::nn::functional::max_unpool3d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_unpool3d(x, indices, F::MaxUnpool3dFuncOptions(3));
 *  }</pre> */


// Targeting ../FractionalMaxPool1dOptions.java


// Targeting ../FractionalMaxPool2dOptions.java


// Targeting ../FractionalMaxPool3dOptions.java



/** {@code FractionalMaxPoolOptions} specialized for the {@code FractionalMaxPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  FractionalMaxPool2d model(FractionalMaxPool2dOptions(5).output_size(1));
 *  }</pre> */

///

/** {@code FractionalMaxPoolOptions} specialized for the {@code FractionalMaxPool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  FractionalMaxPool3d model(FractionalMaxPool3dOptions(5).output_size(1));
 *  }</pre> */
/** Options for {@code torch::nn::functional::fractional_max_pool2d} and {@code torch::nn::functional::fractional_max_pool2d_with_indices}
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::fractional_max_pool2d(x, F::FractionalMaxPool2dFuncOptions(3).output_size(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::fractional_max_pool3d} and {@code torch::nn::functional::fractional_max_pool3d_with_indices}
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::fractional_max_pool3d(x, F::FractionalMaxPool3dFuncOptions(3).output_size(2));
 *  }</pre> */

// Targeting ../LPPool1dOptions.java


// Targeting ../LPPool2dOptions.java


// Targeting ../LPPool3dOptions.java



/** {@code LPPoolOptions} specialized for the {@code LPPool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  LPPool1d model(LPPool1dOptions(1, 2).stride(5).ceil_mode(true));
 *  }</pre> */

///

/** {@code LPPoolOptions} specialized for the {@code LPPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  LPPool2d model(LPPool2dOptions(1, std::vector<int64_t>({3, 4})).stride({5, 6}).ceil_mode(true));
 *  }</pre> */
/** Options for {@code torch::nn::functional::lp_pool1d}.
 * 
 *  See the documentation for {@code torch::nn::LPPool1dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::lp_pool1d(x, F::LPPool1dFuncOptions(2, 3).stride(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::lp_pool2d}.
 * 
 *  See the documentation for {@code torch::nn::LPPool2dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::lp_pool2d(x, F::LPPool2dFuncOptions(2, {2, 3}).stride(2));
 *  }</pre> */
 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/rnn.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/enum.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// Targeting ../RNNOptionsBase.java




// Targeting ../RNNOptions.java


// Targeting ../LSTMOptions.java


// Targeting ../GRUOptions.java


// Targeting ../RNNCellOptionsBase.java




// Targeting ../RNNCellOptions.java


// Targeting ../LSTMCellOptions.java


// Targeting ../GRUCellOptions.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/upsampling.h

// #pragma once

// #include <c10/util/variant.h>
// #include <torch/arg.h>
// #include <torch/enum.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/expanding_array.h>
// #include <torch/types.h>

// #include <vector>
// Targeting ../UpsampleOptions.java


// Targeting ../InterpolateFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/vision.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/enum.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// Targeting ../GridSampleFuncOptions.java



 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/instancenorm.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/nn/options/batchnorm.h>
// #include <torch/types.h>
// Targeting ../InstanceNormOptions.java



/** Options for the {@code InstanceNorm1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  InstanceNorm1d model(InstanceNorm1dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */

///

/** Options for the {@code InstanceNorm2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  InstanceNorm2d model(InstanceNorm2dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */

///

/** Options for the {@code InstanceNorm3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  InstanceNorm3d model(InstanceNorm3dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */
// Targeting ../InstanceNormFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/transformerlayer.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// #include <torch/enum.h>


///
// Targeting ../TransformerEncoderLayerOptions.java


// Targeting ../TransformerDecoderLayerOptions.java




 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/transformercoder.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// #include <torch/enum.h>

// #include <torch/nn/modules/transformerlayer.h>
// #include <torch/nn/modules/container/any.h>
// Targeting ../TransformerEncoderOptions.java


// Targeting ../TransformerDecoderOptions.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/transformer.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// #include <torch/enum.h>

// #include <torch/nn/modules/container/any.h>
// #include <torch/nn/options/transformerlayer.h>
// Targeting ../TransformerOptions.java




 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional.h

// #pragma once

// #include <torch/nn/functional/batchnorm.h>
// #include <torch/nn/functional/conv.h>
// #include <torch/nn/functional/distance.h>
// #include <torch/nn/functional/dropout.h>
// #include <torch/nn/functional/embedding.h>
// #include <torch/nn/functional/fold.h>
// #include <torch/nn/functional/linear.h>
// #include <torch/nn/functional/loss.h>
// #include <torch/nn/functional/normalization.h>
// #include <torch/nn/functional/padding.h>
// #include <torch/nn/functional/pixelshuffle.h>
// #include <torch/nn/functional/pooling.h>
// #include <torch/nn/functional/upsampling.h>
// #include <torch/nn/functional/vision.h>
// #include <torch/nn/functional/instancenorm.h>


// Parsed from torch/nn/functional/activation.h

// #pragma once

// #include <torch/nn/options/activation.h>
// #include <torch/nn/options/linear.h>
// #include <torch/nn/options/dropout.h>
// #include <torch/types.h>
// #include <torch/nn/functional/linear.h>
// #include <torch/nn/functional/dropout.h>
// #include <limits>
// #include <ATen/Dispatch.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor elu(@ByVal Tensor input, double alpha, @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.elu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::ELUFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::elu(x, F::ELUFuncOptions().alpha(0.42).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor elu(@ByVal Tensor input, @Cast("const torch::nn::functional::ELUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::ELUFuncOptions{}") ELUOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor selu(@ByVal Tensor input, @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.selu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SELUFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::selu(input, F::SELUFuncOptions(false));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor selu(@ByVal Tensor input, @Cast("const torch::nn::functional::SELUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::SELUFuncOptions{}") SELUOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor hardshrink(@Const @ByRef Tensor input,
                         double lambda);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.hardshrink
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::HardshrinkFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::hardshrink(x, F::HardshrinkFuncOptions().lambda(0.42));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor hardshrink(@Const @ByRef Tensor input,
                         @Cast("const torch::nn::functional::HardshrinkFuncOptions*") @ByRef(nullValue = "torch::nn::functional::HardshrinkFuncOptions{}") HardshrinkOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor hardtanh(@ByVal Tensor input,
                       double min_val,
                       double max_val,
                       @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.hardtanh
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::HardtanhFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::hardtanh(x, F::HardtanhFuncOptions().min_val(-1.0).max_val(1.0).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor hardtanh(@ByVal Tensor input, @Cast("const torch::nn::functional::HardtanhFuncOptions*") @ByRef(nullValue = "torch::nn::functional::HardtanhFuncOptions{}") HardtanhOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor leaky_relu(@ByVal Tensor input,
                         double negative_slope,
                         @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.leaky_relu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::LeakyReLUFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::leaky_relu(x, F::LeakyReLUFuncOptions().negative_slope(0.42).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor leaky_relu(@ByVal Tensor input, @Cast("const torch::nn::functional::LeakyReLUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::LeakyReLUFuncOptions{}") LeakyReLUOptions options);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal Tensor logsigmoid(@Const @ByRef Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor gumbel_softmax(@Const @ByRef Tensor logits,
                             double tau,
                             @Cast("bool") boolean hard,
                             int dim);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.gumbel_softmax
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::GumbelSoftmaxFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::gumbel_softmax(logits, F::GumbelSoftmaxFuncOptions().hard(true).dim(-1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor gumbel_softmax(@Const @ByRef Tensor logits, @Const @ByRef(nullValue = "torch::nn::functional::GumbelSoftmaxFuncOptions{}") GumbelSoftmaxFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor gumbel_softmax(@Const @ByRef Tensor logits);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.softmax
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SoftmaxFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::softmax(input, F::SoftmaxFuncOptions(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor softmax(@Const @ByRef Tensor input, @Const @ByRef SoftmaxFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor softmin(@Const @ByRef Tensor input, @Cast("int64_t") long dim,
                      @ByVal ScalarTypeOptional dtype);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.softmin
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SoftminFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::softmin(input, F::SoftminFuncOptions(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor softmin(@Const @ByRef Tensor input, @Const @ByRef SoftminFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.log_softmax
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::LogSoftmaxFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::log_softmax(input, LogSoftmaxFuncOptions(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor log_softmax(@Const @ByRef Tensor input, @Const @ByRef LogSoftmaxFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.glu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::GLUFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::glu(input, GLUFuncOptions(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor glu(@Const @ByRef Tensor input, @Cast("const torch::nn::functional::GLUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::GLUFuncOptions{}") GLUOptions options);

// ============================================================================

// ============================================================================

// ============================================================================

// ============================================================================

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor relu(@ByVal Tensor input, @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.relu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::ReLUFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::relu(x, F::ReLUFuncOptions().inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor relu(@ByVal Tensor input, @Cast("const torch::nn::functional::ReLUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::ReLUFuncOptions{}") ReLUOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor relu6(@ByVal Tensor input, @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.relu6
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::ReLU6FuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::relu6(x, F::ReLU6FuncOptions().inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor relu6(@ByVal Tensor input, @Cast("const torch::nn::functional::ReLU6FuncOptions*") @ByRef(nullValue = "torch::nn::functional::ReLU6FuncOptions{}") ReLU6Options options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor rrelu(@ByVal Tensor input,
                    double lower,
                    double upper,
                    @Cast("bool") boolean training,
                    @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.rrelu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::RReLUFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::rrelu(x, F::RReLUFuncOptions().lower(0.1).upper(0.4).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor rrelu(@ByVal Tensor input, @Const @ByRef(nullValue = "torch::nn::functional::RReLUFuncOptions{}") RReLUFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor celu(@ByVal Tensor input,
                   double alpha,
                   @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.celu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::CELUFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::celu(x, F::CELUFuncOptions().alpha(0.42).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor celu(@ByVal Tensor input, @Cast("const torch::nn::functional::CELUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::CELUFuncOptions{}") CELUOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor softplus(@Const @ByRef Tensor input,
                       double beta,
                       double threshold);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.softplus
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SoftplusFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::softplus(x, F::SoftplusFuncOptions().beta(0.5).threshold(3.0));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor softplus(@Const @ByRef Tensor input,
                       @Cast("const torch::nn::functional::SoftplusFuncOptions*") @ByRef(nullValue = "torch::nn::functional::SoftplusFuncOptions{}") SoftplusOptions options);


// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor softshrink(@Const @ByRef Tensor input,
                         double lambda);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.softshrink
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SoftshrinkFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::softshrink(x, F::SoftshrinkFuncOptions(0.42));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor softshrink(@Const @ByRef Tensor input,
                         @Cast("const torch::nn::functional::SoftshrinkFuncOptions*") @ByRef(nullValue = "torch::nn::functional::SoftshrinkFuncOptions{}") SoftshrinkOptions options);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal Tensor softsign(@Const @ByRef Tensor input);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal Tensor tanhshrink(@Const @ByRef Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor threshold(@ByVal Tensor input,
                        double threshold,
                        double value,
                        @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.threshold
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::ThresholdFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::threshold(x, F::ThresholdFuncOptions(0.5, 0.5).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor threshold(@ByVal Tensor input, @Cast("const torch::nn::functional::ThresholdFuncOptions*") @ByRef ThresholdOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal TensorTensorTuple multi_head_attention_forward(
  @Const @ByRef Tensor query,
  @Const @ByRef Tensor key,
  @Const @ByRef Tensor value,
  @Cast("int64_t") long embed_dim_to_check,
  @Cast("int64_t") long num_heads,
  @Const @ByRef Tensor in_proj_weight,
  @Const @ByRef Tensor in_proj_bias,
  @Const @ByRef Tensor bias_k,
  @Const @ByRef Tensor bias_v,
  @Cast("bool") boolean add_zero_attn,
  double dropout_p,
  @Const @ByRef Tensor out_proj_weight,
  @Const @ByRef Tensor out_proj_bias,
  @Cast("bool") boolean training/*=true*/,
  @Const @ByRef(nullValue = "at::Tensor{}") Tensor key_padding_mask,
  @Cast("bool") boolean need_weights/*=true*/,
  @Const @ByRef(nullValue = "at::Tensor{}") Tensor attn_mask,
  @Cast("bool") boolean use_separate_proj_weight/*=false*/,
  @Const @ByRef(nullValue = "at::Tensor{}") Tensor q_proj_weight,
  @Const @ByRef(nullValue = "at::Tensor{}") Tensor k_proj_weight,
  @Const @ByRef(nullValue = "at::Tensor{}") Tensor v_proj_weight,
  @Const @ByRef(nullValue = "at::Tensor{}") Tensor static_k,
  @Const @ByRef(nullValue = "at::Tensor{}") Tensor static_v);
@Namespace("torch::nn::functional::detail") public static native @ByVal TensorTensorTuple multi_head_attention_forward(
  @Const @ByRef Tensor query,
  @Const @ByRef Tensor key,
  @Const @ByRef Tensor value,
  @Cast("int64_t") long embed_dim_to_check,
  @Cast("int64_t") long num_heads,
  @Const @ByRef Tensor in_proj_weight,
  @Const @ByRef Tensor in_proj_bias,
  @Const @ByRef Tensor bias_k,
  @Const @ByRef Tensor bias_v,
  @Cast("bool") boolean add_zero_attn,
  double dropout_p,
  @Const @ByRef Tensor out_proj_weight,
  @Const @ByRef Tensor out_proj_bias);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

@Namespace("torch::nn::functional") public static native @ByVal TensorTensorTuple multi_head_attention_forward(
  @Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value,
  @Const @ByRef MultiheadAttentionForwardFuncOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/batchnorm.h

// #pragma once

// #include <torch/nn/options/batchnorm.h>
// #include <torch/types.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor batch_norm(@Const @ByRef Tensor input,
                         @Const @ByRef Tensor running_mean,
                         @Const @ByRef Tensor running_var,
                         @ByVal Tensor weight,
                         @ByVal Tensor bias,
                         @Cast("bool") boolean training,
                         @ByVal DoubleOptional momentum,
                         double eps);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.batch_norm
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::BatchNormFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::batch_norm(input, mean, variance, F::BatchNormFuncOptions().weight(weight).bias(bias).momentum(0.1).eps(1e-05).training(false));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor batch_norm(@Const @ByRef Tensor input, @Const @ByRef Tensor running_mean,
                         @Const @ByRef Tensor running_var, @Const @ByRef(nullValue = "torch::nn::functional::BatchNormFuncOptions{}") BatchNormFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor batch_norm(@Const @ByRef Tensor input, @Const @ByRef Tensor running_mean,
                         @Const @ByRef Tensor running_var);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/conv.h

// #pragma once

// #include <torch/nn/options/conv.h>
// #include <torch/types.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::nn::functional::detail") public static native @StdString BytePointer padding_unwrap(@ByVal kValid arg0);

@Namespace("torch::nn::functional::detail") public static native @StdString BytePointer padding_unwrap(@ByVal kSame arg0);


@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv1d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
    @Const @ByRef conv_padding_t1 padding,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer dilation,
    @Cast("int64_t") long groups);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::Conv1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv1d(x, weight, F::Conv1dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv1d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::nn::functional::Conv1dFuncOptions{}") Conv1dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv2d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
    @Const @ByRef conv_padding_t2 padding,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer dilation,
    @Cast("int64_t") long groups);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::Conv2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv2d(x, weight, F::Conv2dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv2d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::nn::functional::Conv2dFuncOptions{}") Conv2dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv3d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer stride,
    @Const @ByRef conv_padding_t3 padding,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer dilation,
    @Cast("int64_t") long groups);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::Conv3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv3d(x, weight, F::Conv3dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv3d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::nn::functional::Conv3dFuncOptions{}") Conv3dFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight,
                               @Const @ByRef Tensor bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride,
                               @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding,
                               @Cast("int64_t") long groups, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight,
                               @Const @ByRef Tensor bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride,
                               @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding,
                               @Cast("int64_t") long groups, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv_transpose1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::ConvTranspose1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv_transpose1d(x, weight, F::ConvTranspose1dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv_transpose1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight,
                               @Const @ByRef(nullValue = "torch::nn::functional::ConvTranspose1dFuncOptions{}") ConvTranspose1dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight,
                               @Const @ByRef Tensor bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride,
                               @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding,
                               @Cast("int64_t") long groups, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight,
                               @Const @ByRef Tensor bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride,
                               @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding,
                               @Cast("int64_t") long groups, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv_transpose2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::ConvTranspose2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv_transpose2d(x, weight, F::ConvTranspose2dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv_transpose2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight,
                               @Const @ByRef(nullValue = "torch::nn::functional::ConvTranspose2dFuncOptions{}") ConvTranspose2dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight,
                               @Const @ByRef Tensor bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride,
                               @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding,
                               @Cast("int64_t") long groups, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight,
                               @Const @ByRef Tensor bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride,
                               @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding,
                               @Cast("int64_t") long groups, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv_transpose3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::ConvTranspose3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv_transpose3d(x, weight, F::ConvTranspose3dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv_transpose3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight,
                               @Const @ByRef(nullValue = "torch::nn::functional::ConvTranspose3dFuncOptions{}") ConvTranspose3dFuncOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/distance.h

// #pragma once

// #include <torch/nn/options/distance.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.cosine_similarity
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::CosineSimilarityFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::cosine_similarity(input1, input2, F::CosineSimilarityFuncOptions().dim(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor cosine_similarity(
    @Const @ByRef Tensor x1,
    @Const @ByRef Tensor x2,
    @Cast("const torch::nn::functional::CosineSimilarityFuncOptions*") @ByRef(nullValue = "torch::nn::functional::CosineSimilarityFuncOptions{}") CosineSimilarityOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.pairwise_distance
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::PairwiseDistanceFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::pairwise_distance(input1, input2, F::PairwiseDistanceFuncOptions().p(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor pairwise_distance(
    @Const @ByRef Tensor x1,
    @Const @ByRef Tensor x2,
    @Cast("const torch::nn::functional::PairwiseDistanceFuncOptions*") @ByRef(nullValue = "torch::nn::functional::PairwiseDistanceFuncOptions{}") PairwiseDistanceOptions options);

// ============================================================================

/** Computes the p-norm distance between every pair of row vectors in the input.
 *  This function will be faster if the rows are contiguous. */

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/dropout.h

// #pragma once

// #include <torch/nn/options/dropout.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor dropout(@ByVal Tensor input, double p, @Cast("bool") boolean training, @Cast("bool") boolean inplace);

 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.dropout
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::DropoutFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::dropout(input, F::DropoutFuncOptions().p(0.5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout(@ByVal Tensor input,
    @Const @ByRef(nullValue = "torch::nn::functional::DropoutFuncOptions{}") DropoutFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout(@ByVal Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor dropout2d(@ByVal Tensor input, double p, @Cast("bool") boolean training, @Cast("bool") boolean inplace);

 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.dropout2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::Dropout2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::dropout2d(input, F::Dropout2dFuncOptions().p(0.5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout2d(@ByVal Tensor input,
    @Cast("const torch::nn::functional::Dropout2dFuncOptions*") @ByRef(nullValue = "torch::nn::functional::Dropout2dFuncOptions{}") DropoutFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout2d(@ByVal Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor dropout3d(@ByVal Tensor input, double p, @Cast("bool") boolean training, @Cast("bool") boolean inplace);

 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.dropout3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::Dropout3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::dropout3d(input, F::Dropout3dFuncOptions().p(0.5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout3d(@ByVal Tensor input,
    @Cast("const torch::nn::functional::Dropout3dFuncOptions*") @ByRef(nullValue = "torch::nn::functional::Dropout3dFuncOptions{}") DropoutFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout3d(@ByVal Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor alpha_dropout(@ByVal Tensor input, double p, @Cast("bool") boolean training, @Cast("bool") boolean inplace);

 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.alpha_dropout
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AlphaDropoutFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::alpha_dropout(input, F::AlphaDropoutFuncOptions().p(0.5).training(false));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor alpha_dropout(@ByVal Tensor input, @Const @ByRef(nullValue = "torch::nn::functional::AlphaDropoutFuncOptions{}") AlphaDropoutFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor alpha_dropout(@ByVal Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor feature_alpha_dropout(@ByVal Tensor input, double p, @Cast("bool") boolean training, @Cast("bool") boolean inplace);

 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.feature_alpha_dropout
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::FeatureAlphaDropoutFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::feature_alpha_dropout(input, F::FeatureAlphaDropoutFuncOptions().p(0.5).training(false));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor feature_alpha_dropout(@ByVal Tensor input, @Const @ByRef(nullValue = "torch::nn::functional::FeatureAlphaDropoutFuncOptions{}") FeatureAlphaDropoutFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor feature_alpha_dropout(@ByVal Tensor input);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/embedding.h

// #pragma once

// #include <torch/nn/options/embedding.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native void _no_grad_embedding_renorm_(@ByVal Tensor weight, @Const @ByRef Tensor input, float max_norm, float norm_type);

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor embedding(@Const @ByRef Tensor input,
                        @Const @ByRef Tensor weight,
                        @ByVal LongOptional padding_idx,
                        @ByVal DoubleOptional max_norm,
                        double norm_type,
                        @Cast("bool") boolean scale_grad_by_freq,
                        @Cast("bool") boolean sparse);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.embedding
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::EmbeddingFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::embedding(input, weight, F::EmbeddingFuncOptions().norm_type(2.5).scale_grad_by_freq(true).sparse(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor embedding(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "torch::nn::functional::EmbeddingFuncOptions{}") EmbeddingFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor embedding_bag(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor offsets,
    @ByVal DoubleOptional max_norm,
    double norm_type,
    @Cast("bool") boolean scale_grad_by_freq,
    @ByVal EmbeddingBagMode mode,
    @Cast("bool") boolean sparse,
    @Const @ByRef Tensor per_sample_weights,
    @Cast("bool") boolean include_last_offset,
    @ByVal LongOptional padding_idx);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.embedding_bag
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::EmbeddingBagFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::embedding_bag(input, weight, F::EmbeddingBagFuncOptions().mode(torch::kSum).offsets(offsets));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor embedding_bag(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "torch::nn::functional::EmbeddingBagFuncOptions{}") EmbeddingBagFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor embedding_bag(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/fold.h

// #pragma once

// #include <torch/nn/options/fold.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor fold(@Const @ByRef Tensor input,
                   @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer output_size,
                   @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
                   @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer dilation,
                   @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
                   @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.fold
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::FoldFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::fold(input, F::FoldFuncOptions({3, 2}, {2, 2}));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor fold(@Const @ByRef Tensor input, @Cast("const torch::nn::functional::FoldFuncOptions*") @ByRef FoldOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor unfold(@Const @ByRef Tensor input,
                     @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
                     @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer dilation,
                     @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
                     @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.unfold
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::UnfoldFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::unfold(input, F::UnfoldFuncOptions({2, 2}).padding(1).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor unfold(@Const @ByRef Tensor input, @Cast("const torch::nn::functional::UnfoldFuncOptions*") @ByRef UnfoldOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/linear.h

// #pragma once

// #include <torch/types.h>

@Namespace("torch::nn::functional") public static native @ByVal Tensor bilinear(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "at::Tensor()") Tensor bias);
@Namespace("torch::nn::functional") public static native @ByVal Tensor bilinear(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor weight);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal Tensor linear(@Const @ByRef Tensor input, @Const @ByRef Tensor weight,
                     @Const @ByRef(nullValue = "at::Tensor{}") Tensor bias);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/loss.h

// #pragma once

// #include <ATen/ExpandUtils.h>
// #include <torch/nn/functional/activation.h>
// #include <torch/nn/options/loss.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.l1_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::L1LossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::l1_loss(input, target, F::L1LossFuncOptions(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::L1LossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::L1LossFuncOptions{}") L1LossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor kl_div(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal kldiv_loss_reduction_t reduction,
    @Cast("bool") boolean log_target/*=false*/);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor kl_div(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal kldiv_loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.kl_div
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::KLDivFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::kl_div(input, target, F::KLDivFuncOptions.reduction(torch::kNone).log_target(false));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor kl_div(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::KLDivFuncOptions*") @ByRef(nullValue = "torch::nn::functional::KLDivFuncOptions{}") KLDivLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor mse_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.mse_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MSELossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::mse_loss(input, target, F::MSELossFuncOptions(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor mse_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::MSELossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::MSELossFuncOptions{}") MSELossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor binary_cross_entropy(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Const @ByRef Tensor weight,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.binary_cross_entropy
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::BinaryCrossEntropyFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::binary_cross_entropy(input, target, F::BinaryCrossEntropyFuncOptions().weight(weight));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor binary_cross_entropy(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::BinaryCrossEntropyFuncOptions*") @ByRef(nullValue = "torch::nn::functional::BinaryCrossEntropyFuncOptions{}") BCELossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor hinge_embedding_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    double margin,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.hinge_embedding_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::HingeEmbeddingLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::hinge_embedding_loss(input, target, F::HingeEmbeddingLossFuncOptions().margin(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor hinge_embedding_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::HingeEmbeddingLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::HingeEmbeddingLossFuncOptions{}") HingeEmbeddingLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor multi_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("int64_t") long p,
    double margin,
    @Const @ByRef Tensor weight,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.multi_margin_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MultiMarginLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::multi_margin_loss(input, target, F::MultiMarginLossFuncOptions().margin(2).weight(weight));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor multi_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::MultiMarginLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::MultiMarginLossFuncOptions{}") MultiMarginLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor cosine_embedding_loss(
    @Const @ByRef Tensor input1,
    @Const @ByRef Tensor input2,
    @Const @ByRef Tensor target,
    double margin,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.cosine_embedding_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::CosineEmbeddingLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::cosine_embedding_loss(input1, input2, target, F::CosineEmbeddingLossFuncOptions().margin(0.5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor cosine_embedding_loss(
    @Const @ByRef Tensor input1,
    @Const @ByRef Tensor input2,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::CosineEmbeddingLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::CosineEmbeddingLossFuncOptions{}") CosineEmbeddingLossOptions options);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal Tensor _smooth_l1_loss(@Const @ByRef Tensor input, @Const @ByRef Tensor target, double beta/*=1.*/);
@Namespace("torch::nn::functional") public static native @ByVal Tensor _smooth_l1_loss(@Const @ByRef Tensor input, @Const @ByRef Tensor target);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor smooth_l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal loss_reduction_t reduction,
    double beta/*=1.*/);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor smooth_l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.smooth_l1_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SmoothL1LossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::smooth_l1_loss(input, target, F::SmoothL1LossFuncOptions(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor smooth_l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::SmoothL1LossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::SmoothL1LossFuncOptions{}") SmoothL1LossOptions options,
    double beta/*=1.*/);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor huber_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal loss_reduction_t reduction,
    double delta/*=1.*/);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor huber_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.huber_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::HuberLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::huber_loss(input, target, F::HuberLossFuncOptions().reduction(torch::kNone).delta(0.5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor huber_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::HuberLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::HuberLossFuncOptions{}") HuberLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor multilabel_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.multilabel_margin_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MultilabelMarginLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::multilabel_margin_loss(input, target, F::MultilabelMarginLossFuncOptions(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor multilabel_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::MultilabelMarginLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::MultilabelMarginLossFuncOptions{}") MultiLabelMarginLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor soft_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.soft_margin_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SoftMarginLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::soft_margin_loss(input, target, F::SoftMarginLossFuncOptions(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor soft_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::SoftMarginLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::SoftMarginLossFuncOptions{}") SoftMarginLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor multilabel_soft_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Const @ByRef Tensor weight,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.multilabel_soft_margin_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MultilabelSoftMarginLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::multilabel_soft_margin_loss(input, target, F::MultilabelSoftMarginLossFuncOptions().reduction(torch::kNone).weight(weight));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor multilabel_soft_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::MultilabelSoftMarginLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::MultilabelSoftMarginLossFuncOptions{}") MultiLabelSoftMarginLossOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor multilabel_soft_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor triplet_margin_loss(
    @Const @ByRef Tensor anchor,
    @Const @ByRef Tensor positive,
    @Const @ByRef Tensor negative,
    double margin,
    double p,
    double eps,
    @Cast("bool") boolean swap,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.triplet_margin_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::TripletMarginLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::triplet_margin_loss(anchor, positive, negative, F::TripletMarginLossFuncOptions().margin(1.0));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor triplet_margin_loss(
    @Const @ByRef Tensor anchor,
    @Const @ByRef Tensor positive,
    @Const @ByRef Tensor negative,
    @Cast("const torch::nn::functional::TripletMarginLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::TripletMarginLossFuncOptions{}") TripletMarginLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor triplet_margin_with_distance_loss(
    @Const @ByRef Tensor anchor,
    @Const @ByRef Tensor positive,
    @Const @ByRef Tensor negative,
    @ByVal @Cast("c10::optional<torch::nn::functional::TripletMarginWithDistanceLossFuncOptions::distance_function_t>*") Pointer distance_function,
    double margin,
    @Cast("bool") boolean swap,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.triplet_margin_with_distance_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::TripletMarginWithDistanceLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::triplet_margin_with_distance_loss(anchor, positive, negative, F::TripletMarginWithDistanceLossFuncOptions().margin(1.0));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor triplet_margin_with_distance_loss(
    @Const @ByRef Tensor anchor,
    @Const @ByRef Tensor positive,
    @Const @ByRef Tensor negative,
    @Cast("const torch::nn::functional::TripletMarginWithDistanceLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::TripletMarginWithDistanceLossFuncOptions{}") TripletMarginWithDistanceLossOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor triplet_margin_with_distance_loss(
    @Const @ByRef Tensor anchor,
    @Const @ByRef Tensor positive,
    @Const @ByRef Tensor negative);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs,
                       @Const @ByRef Tensor targets,
                       @Const @ByRef Tensor input_lengths,
                       @Const @ByRef Tensor target_lengths,
                       @Cast("int64_t") long blank,
                       @ByVal loss_reduction_t reduction,
                       @Cast("bool") boolean zero_infinity);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.ctc_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::CTCLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::ctc_loss(log_probs, targets, input_lengths, target_lengths, F::CTCLossFuncOptions().reduction(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs,
                       @Const @ByRef Tensor targets,
                       @Const @ByRef Tensor input_lengths,
                       @Const @ByRef Tensor target_lengths,
                       @Cast("const torch::nn::functional::CTCLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::CTCLossFuncOptions{}") CTCLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor poisson_nll_loss(@Const @ByRef Tensor input,
                               @Const @ByRef Tensor target,
                               @Cast("bool") boolean log_input,
                               @Cast("bool") boolean full,
                               double eps,
                               @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.poisson_nll_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::PoissonNLLLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::poisson_nll_loss(input, target, F::PoissonNLLLossFuncOptions().reduction(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor poisson_nll_loss(@Const @ByRef Tensor input, @Const @ByRef Tensor target,
                               @Cast("const torch::nn::functional::PoissonNLLLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::PoissonNLLLossFuncOptions{}") PoissonNLLLossOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor poisson_nll_loss(@Const @ByRef Tensor input, @Const @ByRef Tensor target);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor margin_ranking_loss(@Const @ByRef Tensor input1,
                                  @Const @ByRef Tensor input2,
                                  @Const @ByRef Tensor target,
                                  double margin,
                                  @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.margin_ranking_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MarginRankingLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::margin_ranking_loss(input1, input2, target, F::MarginRankingLossFuncOptions().margin(0.5).reduction(torch::kSum));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor margin_ranking_loss(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2,
  @Const @ByRef Tensor target, @Cast("const torch::nn::functional::MarginRankingLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::MarginRankingLossFuncOptions{}") MarginRankingLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor nll_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Const @ByRef Tensor weight,
    @Cast("int64_t") long ignore_index,
    @Const @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.nll_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::NLLLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::nll_loss(input, target, F::NLLLossFuncOptions().ignore_index(-100).reduction(torch::kMean));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor nll_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::NLLLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::NLLLossFuncOptions{}") NLLLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor cross_entropy(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Const @ByRef Tensor weight,
    @Cast("int64_t") long ignore_index,
    @ByVal loss_reduction_t reduction,
    double label_smoothing);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.cross_entropy
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::CrossEntropyFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::cross_entropy(input, target, F::CrossEntropyFuncOptions().ignore_index(-100).reduction(torch::kMean));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor cross_entropy(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::CrossEntropyFuncOptions*") @ByRef(nullValue = "torch::nn::functional::CrossEntropyFuncOptions{}") CrossEntropyLossOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor cross_entropy(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor binary_cross_entropy_with_logits(
  @Const @ByRef Tensor input, @Const @ByRef Tensor target, @Const @ByRef Tensor weight,
  @ByVal loss_reduction_t reduction, @Const @ByRef Tensor pos_weight);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.binary_cross_entropy_with_logits
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::BinaryCrossEntropyWithLogitsFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::binary_cross_entropy_with_logits(input, target, F::BinaryCrossEntropyWithLogitsFuncOptions().pos_weight(pos_weight).reduction(torch::kSum));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor binary_cross_entropy_with_logits(
  @Const @ByRef Tensor input, @Const @ByRef Tensor target,
  @Cast("const torch::nn::functional::BinaryCrossEntropyWithLogitsFuncOptions*") @ByRef(nullValue = "torch::nn::functional::BinaryCrossEntropyWithLogitsFuncOptions{}") BCEWithLogitsLossOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/normalization.h

// #pragma once

// #include <torch/nn/options/normalization.h>
// #include <torch/nn/functional/padding.h>
// #include <torch/nn/functional/pooling.h>
// #include <torch/types.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor normalize(
    @Const @ByRef Tensor input,
    double p,
    @Cast("int64_t") long dim,
    double eps,
    @ByVal TensorOptional out);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.normalize
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::NormalizeFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::normalize(input, F::NormalizeFuncOptions().p(1).dim(-1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor normalize(
    @Const @ByRef Tensor input,
    @ByVal(nullValue = "torch::nn::functional::NormalizeFuncOptions{}") NormalizeFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor normalize(
    @Const @ByRef Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor layer_norm(@Const @ByRef Tensor input,
                         @Cast("const std::vector<int64_t>*") @ByRef LongVector normalized_shape,
                         @Const @ByRef Tensor weight,
                         @Const @ByRef Tensor bias,
                         double eps);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.layer_norm
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::LayerNormFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::layer_norm(input, F::LayerNormFuncOptions({2, 2}).eps(2e-5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor layer_norm(@Const @ByRef Tensor input,
    @Const @ByRef LayerNormFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor local_response_norm(
    @Const @ByRef Tensor input,
    @Cast("int64_t") long size,
    double alpha,
    double beta,
    double k);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.local_response_norm
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::LocalResponseNormFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::local_response_norm(x, F::LocalResponseNormFuncOptions(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor local_response_norm(
    @Const @ByRef Tensor input,
    @Cast("const torch::nn::functional::LocalResponseNormFuncOptions*") @ByRef LocalResponseNormOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor group_norm(
    @Const @ByRef Tensor input,
    @Cast("int64_t") long num_groups,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    double eps);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.group_norm
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::GroupNormFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::group_norm(input, F::GroupNormFuncOptions(2).eps(2e-5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor group_norm(
    @Const @ByRef Tensor input,
    @Const @ByRef GroupNormFuncOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/padding.h

// #pragma once

// #include <torch/nn/options/padding.h>

@Namespace("torch::nn::functional") public static native @ByVal Tensor _narrow_with_range(@Const @ByRef Tensor input, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long end);

@Namespace("torch::nn::functional") public static native @ByVal Tensor _pad_circular(@ByVal Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("torch::nn::functional") public static native @ByVal Tensor _pad_circular(@ByVal Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor pad(@Const @ByRef Tensor input,
                  @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef pad,
                  @ByVal pad_mode_t mode,
                  double value);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor pad(@Const @ByRef Tensor input,
                  @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] pad,
                  @ByVal pad_mode_t mode,
                  double value);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.pad
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::PadFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::pad(input, F::PadFuncOptions({1, 2, 2, 1, 1, 2}).mode(torch::kReplicate));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor pad(@Const @ByRef Tensor input, @Const @ByRef PadFuncOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/pixelshuffle.h

// #pragma once

// #include <torch/nn/options/pixelshuffle.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.pixel_shuffle
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::PixelShuffleFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::pixel_shuffle(x, F::PixelShuffleFuncOptions(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor pixel_shuffle(
    @Const @ByRef Tensor input,
    @Cast("const torch::nn::functional::PixelShuffleFuncOptions*") @ByRef PixelShuffleOptions options);

@Namespace("torch::nn::functional") public static native @ByVal Tensor pixel_unshuffle(
    @Const @ByRef Tensor input,
    @Cast("const torch::nn::functional::PixelUnshuffleFuncOptions*") @ByRef PixelUnshuffleOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/pooling.h

// #pragma once

// #include <c10/util/irange.h>
// #include <torch/nn/functional/activation.h>
// #include <torch/nn/options/pooling.h>
// #include <torch/nn/modules/utils.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor avg_pool1d(@Const @ByRef Tensor input,
                         @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer kernel_size,
                         @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
                         @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer padding,
                         @Cast("bool") boolean ceil_mode,
                         @Cast("bool") boolean count_include_pad);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.avg_pool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AvgPool1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::avg_pool1d(x, F::AvgPool1dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor avg_pool1d(@Const @ByRef Tensor input, @Const @ByRef AvgPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor avg_pool2d(@Const @ByRef Tensor input,
                         @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
                         @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
                         @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
                         @Cast("bool") boolean ceil_mode,
                         @Cast("bool") boolean count_include_pad,
                         @ByVal LongOptional divisor_override);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.avg_pool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AvgPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::avg_pool2d(x, F::AvgPool2dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor avg_pool2d(@Const @ByRef Tensor input, @Const @ByRef AvgPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor avg_pool3d(@Const @ByRef Tensor input,
                         @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer kernel_size,
                         @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer stride,
                         @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer padding,
                         @Cast("bool") boolean ceil_mode,
                         @Cast("bool") boolean count_include_pad,
                         @ByVal LongOptional divisor_override);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.avg_pool3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AvgPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::avg_pool3d(x, F::AvgPool3dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor avg_pool3d(@Const @ByRef Tensor input, @Const @ByRef AvgPool3dOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_pool1d(@Const @ByRef Tensor input,
                         @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer kernel_size,
                         @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
                         @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer padding,
                         @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer dilation,
                         @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_pool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxPool1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool1d(x, F::MaxPool1dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_pool1d(@Const @ByRef Tensor input, @Const @ByRef MaxPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal TensorTensorTuple max_pool1d_with_indices(
  @Const @ByRef Tensor input,
  @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer kernel_size,
  @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
  @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer padding,
  @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer dilation,
  @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::MaxPool1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool1d_with_indices(x, F::MaxPool1dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal TensorTensorTuple max_pool1d_with_indices(@Const @ByRef Tensor input, @Const @ByRef MaxPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_pool2d(@Const @ByRef Tensor input,
                         @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
                         @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
                         @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
                         @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer dilation,
                         @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_pool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool2d(x, F::MaxPool2dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_pool2d(@Const @ByRef Tensor input, @Const @ByRef MaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal TensorTensorTuple max_pool2d_with_indices(
  @Const @ByRef Tensor input,
  @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
  @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
  @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
  @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer dilation,
  @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::MaxPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool2d_with_indices(x, F::MaxPool2dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal TensorTensorTuple max_pool2d_with_indices(@Const @ByRef Tensor input, @Const @ByRef MaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_pool3d(@Const @ByRef Tensor input,
                         @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer kernel_size,
                         @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer stride,
                         @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer padding,
                         @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer dilation,
                         @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_pool3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool3d(x, F::MaxPool3dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_pool3d(@Const @ByRef Tensor input, @Const @ByRef MaxPool3dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal TensorTensorTuple max_pool3d_with_indices(
  @Const @ByRef Tensor input,
  @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer kernel_size,
  @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer stride,
  @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer padding,
  @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer dilation,
  @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::MaxPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool3d_with_indices(x, F::MaxPool3dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal TensorTensorTuple max_pool3d_with_indices(@Const @ByRef Tensor input, @Const @ByRef MaxPool3dOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal TensorTensorTuple adaptive_max_pool1d_with_indices(
    @Const @ByRef Tensor input, @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer output_size);
 // namespace detail

/** See the documentation for {@code torch::nn::functional::AdaptiveMaxPool1dFuncOptions} class to learn what
 *  optional arguments are supported for this functional.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_max_pool1d_with_indices(x, F::AdaptiveMaxPool1dFuncOptions(3));
 *  }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal TensorTensorTuple adaptive_max_pool1d_with_indices(
    @Const @ByRef Tensor input, @Const @ByRef AdaptiveMaxPool1dOptions options);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_max_pool1d(@Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_max_pool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AdaptiveMaxPool1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_max_pool1d(x, F::AdaptiveMaxPool1dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_max_pool1d(@Const @ByRef Tensor input,
    @Const @ByRef AdaptiveMaxPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal TensorTensorTuple adaptive_max_pool2d_with_indices(
    @Const @ByRef Tensor input, @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<2>*") LongPointer output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::AdaptiveMaxPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_max_pool2d_with_indices(x, F::AdaptiveMaxPool2dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal TensorTensorTuple adaptive_max_pool2d_with_indices(
    @Const @ByRef Tensor input, @Const @ByRef AdaptiveMaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_max_pool2d(@Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<2>*") LongPointer output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_max_pool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AdaptiveMaxPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_max_pool2d(x, F::AdaptiveMaxPool2dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_max_pool2d(@Const @ByRef Tensor input,
    @Const @ByRef AdaptiveMaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal TensorTensorTuple adaptive_max_pool3d_with_indices(
    @Const @ByRef Tensor input, @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<3>*") LongPointer output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::AdaptiveMaxPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_max_pool3d_with_indices(x, F::AdaptiveMaxPool3dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal TensorTensorTuple adaptive_max_pool3d_with_indices(
    @Const @ByRef Tensor input, @Const @ByRef AdaptiveMaxPool3dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_max_pool3d(@Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<3>*") LongPointer output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_max_pool3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AdaptiveMaxPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_max_pool3d(x, F::AdaptiveMaxPool3dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_max_pool3d(@Const @ByRef Tensor input,
  @Const @ByRef AdaptiveMaxPool3dOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_avg_pool1d(@Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_avg_pool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AdaptiveAvgPool1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_avg_pool1d(x, F::AdaptiveAvgPool1dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_avg_pool1d(@Const @ByRef Tensor input,
    @Const @ByRef AdaptiveAvgPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_avg_pool2d(@Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<2>*") LongPointer output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_avg_pool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AdaptiveAvgPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_avg_pool2d(x, F::AdaptiveAvgPool2dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_avg_pool2d(@Const @ByRef Tensor input,
    @Const @ByRef AdaptiveAvgPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_avg_pool3d(@Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<3>*") LongPointer output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_avg_pool3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AdaptiveAvgPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_avg_pool3d(x, F::AdaptiveAvgPool3dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_avg_pool3d(@Const @ByRef Tensor input,
    @Const @ByRef AdaptiveAvgPool3dOptions options);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector _unpool_output_size(@Const @ByRef Tensor input,
  @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride,
  @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Const @ByRef LongVectorOptional output_size);
@Namespace("torch::nn::functional") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector _unpool_output_size(@Const @ByRef Tensor input,
  @ByRef @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByRef @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride,
  @ByRef @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Const @ByRef LongVectorOptional output_size);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_unpool1d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor indices,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer padding,
    @Const @ByRef LongVectorOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_unpool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxUnpool1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_unpool1d(x, indices, F::MaxUnpool1dFuncOptions(3).stride(2).padding(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_unpool1d(@Const @ByRef Tensor input, @Const @ByRef Tensor indices,
    @Const @ByRef MaxUnpool1dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_unpool2d(
  @Const @ByRef Tensor input,
  @Const @ByRef Tensor indices,
  @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
  @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
  @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
  @Const @ByRef LongVectorOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_unpool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxUnpool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_unpool2d(x, indices, F::MaxUnpool2dFuncOptions(3).stride(2).padding(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_unpool2d(@Const @ByRef Tensor input, @Const @ByRef Tensor indices,
  @Const @ByRef MaxUnpool2dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_unpool3d(
  @Const @ByRef Tensor input,
  @Const @ByRef Tensor indices,
  @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer kernel_size,
  @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer stride,
  @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer padding,
  @Const @ByRef LongVectorOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_unpool3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxUnpool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_unpool3d(x, indices, F::MaxUnpool3dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_unpool3d(@Const @ByRef Tensor input, @Const @ByRef Tensor indices,
  @Const @ByRef MaxUnpool3dFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal TensorTensorTuple fractional_max_pool2d_with_indices(
    @Const @ByRef Tensor input,
    @Cast("const torch::ExpandingArray<2>*") @ByRef LongPointer kernel_size,
    @Cast("const c10::optional<torch::ExpandingArray<2> >*") @ByRef LongExpandingArrayOptional output_size,
    @Cast("const c10::optional<torch::ExpandingArray<2,double> >*") @ByRef DoubleExpandingArrayOptional output_ratio,
    @Const @ByRef Tensor _random_samples);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::FractionalMaxPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::fractional_max_pool2d_with_indices(x, F::FractionalMaxPool2dFuncOptions(3).output_size(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal TensorTensorTuple fractional_max_pool2d_with_indices(@Const @ByRef Tensor input, @Const @ByRef FractionalMaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor fractional_max_pool2d(@Const @ByRef Tensor input,
                                    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
                                    @ByVal @Cast("c10::optional<torch::ExpandingArray<2> >*") LongExpandingArrayOptional output_size,
                                    @ByVal @Cast("c10::optional<torch::ExpandingArray<2,double> >*") DoubleExpandingArrayOptional output_ratio,
                                    @Const @ByRef Tensor _random_samples);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::FractionalMaxPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::fractional_max_pool2d(x, F::FractionalMaxPool2dFuncOptions(3).output_size(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor fractional_max_pool2d(@Const @ByRef Tensor input, @Const @ByRef FractionalMaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal TensorTensorTuple fractional_max_pool3d_with_indices(
    @Const @ByRef Tensor input,
    @Cast("const torch::ExpandingArray<3>*") @ByRef LongPointer kernel_size,
    @Cast("const c10::optional<torch::ExpandingArray<3> >*") @ByRef LongExpandingArrayOptional output_size,
    @Cast("const c10::optional<torch::ExpandingArray<3,double> >*") @ByRef DoubleExpandingArrayOptional output_ratio,
    @Const @ByRef Tensor _random_samples);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::FractionalMaxPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::fractional_max_pool3d_with_indices(x, F::FractionalMaxPool3dFuncOptions(3).output_size(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal TensorTensorTuple fractional_max_pool3d_with_indices(@Const @ByRef Tensor input, @Const @ByRef FractionalMaxPool3dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor fractional_max_pool3d(@Const @ByRef Tensor input,
                                    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer kernel_size,
                                    @ByVal @Cast("c10::optional<torch::ExpandingArray<3> >*") LongExpandingArrayOptional output_size,
                                    @ByVal @Cast("c10::optional<torch::ExpandingArray<3,double> >*") DoubleExpandingArrayOptional output_ratio,
                                    @Const @ByRef Tensor _random_samples);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::FractionalMaxPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::fractional_max_pool3d(x, F::FractionalMaxPool3dFuncOptions(3).output_size(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor fractional_max_pool3d(@Const @ByRef Tensor input, @Const @ByRef FractionalMaxPool3dOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor lp_pool1d(
  @Const @ByRef Tensor input,
  double norm_type,
  @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer kernel_size,
  @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
  @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.lp_pool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::LPPool1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::lp_pool1d(x, F::LPPool1dFuncOptions(2, 3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor lp_pool1d(@Const @ByRef Tensor input, @Const @ByRef LPPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor lp_pool2d(
  @Const @ByRef Tensor input,
  double norm_type,
  @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
  @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
  @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.lp_pool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::LPPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::lp_pool2d(x, F::LPPool2dFuncOptions(2, {2, 3}).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor lp_pool2d(@Const @ByRef Tensor input, @Const @ByRef LPPool2dOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/upsampling.h

// #pragma once

// #include <c10/util/irange.h>
// #include <torch/nn/functional/pooling.h>
// #include <torch/nn/options/upsampling.h>

// #include <cmath>

@Namespace("torch::nn::functional") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector _interp_output_size(
  @Cast("int64_t") long dim,
  @ByVal @Cast("std::tuple<at::Tensor,c10::optional<std::vector<int64_t> >,c10::optional<std::vector<double> >,c10::optional<bool> >*") Pointer closed_over_args);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor interpolate(
  @Const @ByRef Tensor input,
  @Const @ByRef LongVectorOptional size,
  @Const @ByRef DoubleVectorOptional scale_factor,
  @ByVal interpolate_mode_t mode,
  @ByVal BoolOptional align_corners,
  @ByVal BoolOptional recompute_scale_factor);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.interpolate
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::InterpolateFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::interpolate(input, F::InterpolateFuncOptions().size({4}).mode(torch::kNearest));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor interpolate(@Const @ByRef Tensor input, @Const @ByRef(nullValue = "torch::nn::functional::InterpolateFuncOptions{}") InterpolateFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor interpolate(@Const @ByRef Tensor input);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/vision.h

// #pragma once

// #include <torch/nn/options/vision.h>
// #include <torch/types.h>

@Namespace("torch::nn::functional") public static native @ByVal Tensor affine_grid(
    @Const @ByRef Tensor theta,
    @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size,
    @Cast("bool") boolean align_corners/*=false*/);
@Namespace("torch::nn::functional") public static native @ByVal Tensor affine_grid(
    @Const @ByRef Tensor theta,
    @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("torch::nn::functional") public static native @ByVal Tensor affine_grid(
    @Const @ByRef Tensor theta,
    @ByRef @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size,
    @Cast("bool") boolean align_corners/*=false*/);
@Namespace("torch::nn::functional") public static native @ByVal Tensor affine_grid(
    @Const @ByRef Tensor theta,
    @ByRef @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor grid_sample(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor grid,
    @ByVal grid_sample_mode_t mode,
    @ByVal grid_sample_padding_mode_t padding_mode,
    @ByVal BoolOptional align_corners);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.grid_sample
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::GridSampleFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::grid_sample(input, grid, F::GridSampleFuncOptions().mode(torch::kBilinear).padding_mode(torch::kZeros).align_corners(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor grid_sample(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor grid,
    @Const @ByRef(nullValue = "torch::nn::functional::GridSampleFuncOptions{}") GridSampleFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor grid_sample(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor grid);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/instancenorm.h

// #pragma once

// #include <torch/nn/options/instancenorm.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor instance_norm(@Const @ByRef Tensor input, @Const @ByRef Tensor running_mean,
    @Const @ByRef Tensor running_var, @Const @ByRef Tensor weight, @Const @ByRef Tensor bias,
    @Cast("bool") boolean use_input_stats, double momentum, double eps);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.instance_norm
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::InstanceNormFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::instance_norm(input, F::InstanceNormFuncOptions().running_mean(mean).running_var(variance).weight(weight).bias(bias).momentum(0.1).eps(1e-5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor instance_norm(@Const @ByRef Tensor input, @Const @ByRef(nullValue = "torch::nn::functional::InstanceNormFuncOptions{}") InstanceNormFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor instance_norm(@Const @ByRef Tensor input);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/module.h

// #pragma once

// #include <torch/nn/modules/container/any_module_holder.h>
// #include <torch/nn/modules/container/any_value.h>
// #include <torch/nn/pimpl.h>
// #include <torch/ordered_dict.h>
// #include <torch/serialize/archive.h>
// #include <torch/types.h>

// #include <ATen/ATen.h>

// #include <functional>
// #include <iosfwd>
// #include <map>
// #include <memory>
// #include <string>
// #include <type_traits>
// Targeting ../Module.java


@Namespace("torch::nn") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer stream, @Const @ByRef Module module);


/** Serialize a {@code Module} pointer into an {@code OutputArchive}. */
@Namespace("torch::nn") public static native @ByRef @Name("operator <<") OutputArchive shiftLeft(
    @ByRef OutputArchive archive,
    @SharedPtr @Cast({"", "std::shared_ptr<torch::nn::Module>"}) Module module);

/** Deserializes a {@code Module} from an {@code InputArchive}. */
@Namespace("torch::nn") public static native @ByRef @Name("operator >>") InputArchive shiftRight(
    @ByRef InputArchive archive,
    @SharedPtr @Cast({"", "std::shared_ptr<torch::nn::Module>"}) Module module);

// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ nn::Module ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



















 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules.h

// #pragma once

// Common
// #include <torch/nn/modules/common.h>

// Containers
// #include <torch/nn/modules/container/any.h>
// #include <torch/nn/modules/container/functional.h>
// #include <torch/nn/modules/container/moduledict.h>
// #include <torch/nn/modules/container/modulelist.h>
// #include <torch/nn/modules/container/named_any.h>
// #include <torch/nn/modules/container/sequential.h>
// #include <torch/nn/modules/container/parameterdict.h>
// #include <torch/nn/modules/container/parameterlist.h>

// Layers
// #include <torch/nn/modules/adaptive.h>
// #include <torch/nn/modules/batchnorm.h>
// #include <torch/nn/modules/instancenorm.h>
// #include <torch/nn/modules/conv.h>
// #include <torch/nn/modules/dropout.h>
// #include <torch/nn/modules/distance.h>
// #include <torch/nn/modules/embedding.h>
// #include <torch/nn/modules/fold.h>
// #include <torch/nn/modules/linear.h>
// #include <torch/nn/modules/loss.h>
// #include <torch/nn/modules/padding.h>
// #include <torch/nn/modules/pooling.h>
// #include <torch/nn/modules/rnn.h>
// #include <torch/nn/modules/pixelshuffle.h>
// #include <torch/nn/modules/upsampling.h>
// #include <torch/nn/modules/activation.h>
// #include <torch/nn/modules/normalization.h>
// #include <torch/nn/modules/transformerlayer.h>
// #include <torch/nn/modules/transformercoder.h>
// #include <torch/nn/modules/transformer.h>


// Parsed from torch/nn/modules/common.h

// #pragma once


///
///
///
///
///
// #include <c10/util/irange.h>

/** This macro enables a module with default arguments in its forward method
 *  to be used in a Sequential module.
 * 
 *  Example usage:
 * 
 *  Let's say we have a module declared like this:
 *  <pre>{@code
 *  struct MImpl : torch::nn::Module {
 *   public:
 *    explicit MImpl(int value_) : value(value_) {}
 *    torch::Tensor forward(int a, int b = 2, double c = 3.0) {
 *      return torch::tensor(a + b + c);
 *    }
 *   private:
 *    int value;
 *  };
 *  TORCH_MODULE(M);
 *  }</pre>
 * 
 *  If we try to use it in a Sequential module and run forward:
 *  <pre>{@code
 *  torch::nn::Sequential seq(M(1));
 *  seq->forward(1);
 *  }</pre>
 * 
 *  We will receive the following error message:
 *  <pre>{@code
 *  MImpl's forward() method expects 3 argument(s), but received 1.
 *  If MImpl's forward() method has default arguments, please make sure
 *  the forward() method is declared with a corresponding
 *  `FORWARD_HAS_DEFAULT_ARGS` macro.
 *  }</pre>
 * 
 *  The right way to fix this error is to use the {@code FORWARD_HAS_DEFAULT_ARGS} macro when
 *  declaring the module:
 *  <pre>{@code
 *  struct MImpl : torch::nn::Module {
 *   public:
 *    explicit MImpl(int value_) : value(value_) {}
 *    torch::Tensor forward(int a, int b = 2, double c = 3.0) {
 *      return torch::tensor(a + b + c);
 *    }
 *   protected:
 *    /*
 *    NOTE: looking at the argument list of `forward`:
 *    `forward(int a, int b = 2, double c = 3.0)`
 *    we saw the following default arguments:
 *    ----------------------------------------------------------------
 *    0-based index of default |         Default value of arg
 *    arg in forward arg list  |  (wrapped by `torch::nn::AnyValue()`)
 *    ----------------------------------------------------------------
 *                1            |       torch::nn::AnyValue(2)
 *                2            |       torch::nn::AnyValue(3.0)
 *    ----------------------------------------------------------------
 *    Thus we pass the following arguments to the `FORWARD_HAS_DEFAULT_ARGS` macro:
 *    * /
 *    FORWARD_HAS_DEFAULT_ARGS({1, torch::nn::AnyValue(2)}, {2, torch::nn::AnyValue(3.0)})
 *   private:
 *    int value;
 *  };
 *  TORCH_MODULE(M);
 *  }</pre>
 *  Now, running the following would work:
 *  <pre>{@code
 *  torch::nn::Sequential seq(M(1));
 *  seq->forward(1);  // This correctly populates the default arguments for `MImpl::forward`
 *  }</pre> */
// #define FORWARD_HAS_DEFAULT_ARGS(...)
//   template <typename ModuleType, typename... ArgumentTypes>
//   friend struct torch::nn::AnyModuleHolder;
//   bool _forward_has_default_args() override {
//     return true;
//   }
//   unsigned int _forward_num_required_args() override {
//     std::vector<std::pair<unsigned int, torch::nn::AnyValue>> args_info = {__VA_ARGS__};
//     return args_info[0].first;
//   }
//   std::vector<torch::nn::AnyValue> _forward_populate_default_args(std::vector<torch::nn::AnyValue>&& arguments) override {
//     std::vector<std::pair<unsigned int, torch::nn::AnyValue>> args_info = {__VA_ARGS__};
//     unsigned int num_all_args = args_info[args_info.size() - 1].first + 1;
//     TORCH_INTERNAL_ASSERT(arguments.size() >= _forward_num_required_args() && arguments.size() <= num_all_args);
//     std::vector<torch::nn::AnyValue> ret;
//     ret.reserve(num_all_args);
//     for (const auto i : c10::irange(arguments.size())) {
//       ret.emplace_back(std::move(arguments[i]));
//     }
//     for (auto& arg_info : args_info) {
//       if (arg_info.first > ret.size() - 1) ret.emplace_back(std::move(arg_info.second));
//     }
//     return ret;
//   }


// Parsed from torch/nn/modules/container/any.h

// #pragma once

// #include <torch/detail/static.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/container/any_module_holder.h>
// #include <torch/nn/modules/container/any_value.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/autograd/variable.h>
// #include <torch/csrc/utils/memory.h>
// #include <torch/csrc/utils/variadic.h>

// #include <ATen/Device.h>

// #include <memory>
// #include <type_traits>
// #include <typeinfo>
// #include <utility>
// #include <vector>
// Targeting ../AnyModule.java



// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ AnyModule ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

































// Private Methods







 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/container/moduledict.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/ordered_dict.h>
// #include <vector>
// Targeting ../ModuleDictImpl.java


// Targeting ../ModuleDict.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/container/modulelist.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>

// #include <vector>
// Targeting ../ModuleListImpl.java


// Targeting ../ModuleList.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/container/named_any.h

// #pragma once

// #include <torch/detail/static.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/container/any.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/autograd/variable.h>
// #include <torch/csrc/utils/memory.h>
// #include <torch/csrc/utils/variadic.h>

// #include <ATen/Device.h>

// #include <initializer_list>
// #include <memory>
// #include <type_traits>
// #include <typeinfo>
// #include <utility>
// #include <vector>
// Targeting ../NamedAnyModule.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/container/sequential.h

// #pragma once

// #include <torch/detail/static.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/container/any.h>
// #include <torch/nn/modules/container/named_any.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <c10/util/Exception.h>

// #include <cstdint>
// #include <memory>
// #include <ostream>
// #include <string>
// #include <type_traits>
// #include <utility>
// #include <vector>
// Targeting ../SequentialImpl.java


// Targeting ../Sequential.java


 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/container/parameterdict.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/pimpl.h>
// #include <torch/ordered_dict.h>
// #include <vector>
// Targeting ../ParameterDictImpl.java


// Targeting ../ParameterDict.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/container/parameterlist.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>

// #include <vector>
// Targeting ../ParameterListImpl.java


// Targeting ../ParameterList.java


 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/adaptive.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/linear.h>
// #include <torch/nn/modules/container/modulelist.h>
// #include <torch/nn/modules/container/sequential.h>
// #include <torch/nn/functional/activation.h>
// #include <torch/nn/options/adaptive.h>
// Targeting ../ASMoutput.java


// Targeting ../AdaptiveLogSoftmaxWithLossImpl.java


// Targeting ../AdaptiveLogSoftmaxWithLoss.java



 // namespace nn
 // namespace torc


// Parsed from torch/nn/modules/batchnorm.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/batchnorm.h>
// #include <torch/nn/options/batchnorm.h>
// #include <torch/nn/init.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <cstdint>
// Targeting ../BatchNorm1dImplBaseBase.java


// Targeting ../InstanceNorm1dImplBaseBase.java


// Targeting ../BatchNorm2dImplBaseBase.java


// Targeting ../InstanceNorm2dImplBaseBase.java


// Targeting ../BatchNorm3dImplBaseBase.java


// Targeting ../InstanceNorm3dImplBaseBase.java


// Targeting ../BatchNorm1dImplBase.java


// Targeting ../BatchNorm2dImplBase.java


// Targeting ../BatchNorm3dImplBase.java


// Targeting ../BatchNorm1dImpl.java


// Targeting ../BatchNorm1d.java


// Targeting ../BatchNorm2dImpl.java


// Targeting ../BatchNorm2d.java


// Targeting ../BatchNorm3dImpl.java


// Targeting ../BatchNorm3d.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/instancenorm.h

// #pragma once

// #include <torch/nn/modules/batchnorm.h>
// #include <torch/nn/options/instancenorm.h>
// Targeting ../InstanceNorm1dImplBase.java


// Targeting ../InstanceNorm2dImplBase.java


// Targeting ../InstanceNorm3dImplBase.java


// Targeting ../InstanceNorm1dImpl.java


// Targeting ../InstanceNorm1d.java


// Targeting ../InstanceNorm2dImpl.java


// Targeting ../InstanceNorm2d.java


// Targeting ../InstanceNorm3dImpl.java


// Targeting ../InstanceNorm3d.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/conv.h

// #pragma once


// #include <c10/util/irange.h>
// #include <c10/util/overloaded.h>

// #include <torch/expanding_array.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/init.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/modules/utils.h>
// #include <torch/nn/options/conv.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>

// #include <cstddef>
// #include <vector>
// Targeting ../Conv1dImplBase.java


// Targeting ../ConvTranspose1dImplBaseBase.java


// Targeting ../Conv2dImplBase.java


// Targeting ../ConvTranspose2dImplBaseBase.java


// Targeting ../Conv3dImplBase.java


// Targeting ../ConvTranspose3dImplBaseBase.java


// Targeting ../Conv1dImpl.java


// Targeting ../Conv1d.java


// Targeting ../Conv2dImpl.java


// Targeting ../Conv2d.java


// Targeting ../Conv3dImpl.java


// Targeting ../Conv3d.java


// Targeting ../ConvTranspose1dImplBase.java


// Targeting ../ConvTranspose2dImplBase.java


// Targeting ../ConvTranspose3dImplBase.java


// Targeting ../ConvTranspose1dImpl.java


// Targeting ../ConvTranspose1d.java


// Targeting ../ConvTranspose2dImpl.java


// Targeting ../ConvTranspose2d.java


// Targeting ../ConvTranspose3dImpl.java


// Targeting ../ConvTranspose3d.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/dropout.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/options/dropout.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>

// #include <cstddef>
// #include <vector>
// Targeting ../DropoutImplBase.java


// Targeting ../Dropout2dImplBase.java


// Targeting ../Dropout3dImplBase.java


// Targeting ../AlphaDropoutImplBase.java


// Targeting ../FeatureAlphaDropoutImplBase.java




// Targeting ../DropoutImpl.java


// Targeting ../Dropout.java


// Targeting ../Dropout2dImpl.java


// Targeting ../Dropout2d.java


// Targeting ../Dropout3dImpl.java


// Targeting ../Dropout3d.java


// Targeting ../AlphaDropoutImpl.java


// Targeting ../AlphaDropout.java


// Targeting ../FeatureAlphaDropoutImpl.java


// Targeting ../FeatureAlphaDropout.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/distance.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/distance.h>
// #include <torch/nn/options/distance.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>
// Targeting ../CosineSimilarityImpl.java


// Targeting ../CosineSimilarity.java


// Targeting ../PairwiseDistanceImpl.java


// Targeting ../PairwiseDistance.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/embedding.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/options/embedding.h>
// #include <torch/nn/functional/embedding.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <cstddef>
// Targeting ../EmbeddingImpl.java


// Targeting ../Embedding.java


// Targeting ../EmbeddingBagImpl.java


// Targeting ../EmbeddingBag.java


 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/fold.h

// #pragma once

// #include <torch/expanding_array.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/fold.h>
// #include <torch/nn/options/fold.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>
// Targeting ../FoldImpl.java


// Targeting ../Fold.java


// Targeting ../UnfoldImpl.java


// Targeting ../Unfold.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/linear.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/nn/options/linear.h>
// #include <torch/nn/pimpl.h>
// #include <torch/nn/functional/linear.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <vector>
// Targeting ../IdentityImpl.java


// Targeting ../Identity.java


// Targeting ../LinearImpl.java


// Targeting ../Linear.java


// Targeting ../FlattenImpl.java


// Targeting ../Flatten.java


// Targeting ../UnflattenImpl.java


// Targeting ../Unflatten.java


// Targeting ../BilinearImpl.java


// Targeting ../Bilinear.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/loss.h

// #pragma once

// #include <torch/expanding_array.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/loss.h>
// #include <torch/nn/options/loss.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>

// #include <cstddef>
// #include <vector>
// Targeting ../L1LossImpl.java


// Targeting ../L1Loss.java


// Targeting ../KLDivLossImpl.java


// Targeting ../KLDivLoss.java


// Targeting ../MSELossImpl.java


// Targeting ../MSELoss.java


// Targeting ../BCELossImpl.java


// Targeting ../BCELoss.java


// Targeting ../HingeEmbeddingLossImpl.java


// Targeting ../HingeEmbeddingLoss.java


// Targeting ../MultiMarginLossImpl.java


// Targeting ../MultiMarginLoss.java


// Targeting ../CosineEmbeddingLossImpl.java


// Targeting ../CosineEmbeddingLoss.java


// Targeting ../SmoothL1LossImpl.java


// Targeting ../SmoothL1Loss.java


// Targeting ../HuberLossImpl.java


// Targeting ../HuberLoss.java


// Targeting ../MultiLabelMarginLossImpl.java


// Targeting ../MultiLabelMarginLoss.java


// Targeting ../SoftMarginLossImpl.java


// Targeting ../SoftMarginLoss.java


// Targeting ../MultiLabelSoftMarginLossImpl.java


// Targeting ../MultiLabelSoftMarginLoss.java


// Targeting ../TripletMarginLossImpl.java


// Targeting ../TripletMarginLoss.java


// Targeting ../TripletMarginWithDistanceLossImpl.java


// Targeting ../TripletMarginWithDistanceLoss.java


// Targeting ../CTCLossImpl.java


// Targeting ../CTCLoss.java


// Targeting ../PoissonNLLLossImpl.java


// Targeting ../PoissonNLLLoss.java


// Targeting ../MarginRankingLossImpl.java


// Targeting ../MarginRankingLoss.java


// Targeting ../NLLLossImpl.java


// Targeting ../NLLLoss.java


// Targeting ../CrossEntropyLossImpl.java


// Targeting ../CrossEntropyLoss.java


// Targeting ../BCEWithLogitsLossImpl.java


// Targeting ../BCEWithLogitsLoss.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/padding.h

// #pragma once

// #include <torch/expanding_array.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/padding.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>
// Targeting ../ReflectionPad1dImplBase.java


// Targeting ../ReflectionPad2dImplBase.java


// Targeting ../ReflectionPad3dImplBase.java


// Targeting ../ReflectionPad1dImpl.java


// Targeting ../ReflectionPad1d.java


// Targeting ../ReflectionPad2dImpl.java


// Targeting ../ReflectionPad2d.java


// Targeting ../ReflectionPad3dImpl.java


// Targeting ../ReflectionPad3d.java


// Targeting ../ReplicationPad1dImplBase.java


// Targeting ../ReplicationPad2dImplBase.java


// Targeting ../ReplicationPad3dImplBase.java


// Targeting ../ReplicationPad1dImpl.java


// Targeting ../ReplicationPad1d.java


// Targeting ../ReplicationPad2dImpl.java


// Targeting ../ReplicationPad2d.java


// Targeting ../ReplicationPad3dImpl.java


// Targeting ../ReplicationPad3d.java


// Targeting ../ZeroPad2dImpl.java


// Targeting ../ZeroPad2d.java


// Targeting ../ConstantPad1dImplBase.java


// Targeting ../ConstantPad2dImplBase.java


// Targeting ../ConstantPad3dImplBase.java


// Targeting ../ConstantPad1dImpl.java


// Targeting ../ConstantPad1d.java


// Targeting ../ConstantPad2dImpl.java


// Targeting ../ConstantPad2d.java


// Targeting ../ConstantPad3dImpl.java


// Targeting ../ConstantPad3d.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/pooling.h

// #pragma once

// #include <torch/expanding_array.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/options/pooling.h>
// #include <torch/nn/functional/pooling.h>
// #include <torch/nn/modules/common.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>
// Targeting ../AvgPool1dImplBase.java


// Targeting ../AvgPool2dImplBase.java


// Targeting ../AvgPool3dImplBase.java


// Targeting ../AvgPool1dImpl.java


// Targeting ../AvgPool1d.java


// Targeting ../AvgPool2dImpl.java


// Targeting ../AvgPool2d.java


// Targeting ../AvgPool3dImpl.java


// Targeting ../AvgPool3d.java


// Targeting ../MaxPool1dImplBase.java


// Targeting ../MaxPool2dImplBase.java


// Targeting ../MaxPool3dImplBase.java


// Targeting ../MaxPool1dImpl.java


// Targeting ../MaxPool1d.java


// Targeting ../MaxPool2dImpl.java


// Targeting ../MaxPool2d.java


// Targeting ../MaxPool3dImpl.java


// Targeting ../MaxPool3d.java


// Targeting ../AdaptiveMaxPool1dImplBase.java


// Targeting ../AdaptiveMaxPool2dImplBase.java


// Targeting ../AdaptiveMaxPool3dImplBase.java


// Targeting ../AdaptiveMaxPool1dImpl.java


// Targeting ../AdaptiveMaxPool1d.java


// Targeting ../AdaptiveMaxPool2dImpl.java


// Targeting ../AdaptiveMaxPool2d.java


// Targeting ../AdaptiveMaxPool3dImpl.java


// Targeting ../AdaptiveMaxPool3d.java


// Targeting ../AdaptiveAvgPool1dImplBase.java


// Targeting ../AdaptiveAvgPool2dImplBase.java


// Targeting ../AdaptiveAvgPool3dImplBase.java


// Targeting ../AdaptiveAvgPool1dImpl.java


// Targeting ../AdaptiveAvgPool1d.java


// Targeting ../AdaptiveAvgPool2dImpl.java


// Targeting ../AdaptiveAvgPool2d.java


// Targeting ../AdaptiveAvgPool3dImpl.java


// Targeting ../AdaptiveAvgPool3d.java


// Targeting ../MaxUnpool1dImplBase.java


// Targeting ../MaxUnpool2dImplBase.java


// Targeting ../MaxUnpool3dImplBase.java


// Targeting ../MaxUnpool1dImpl.java


// Targeting ../MaxUnpool1d.java


// Targeting ../MaxUnpool2dImpl.java


// Targeting ../MaxUnpool2d.java


// Targeting ../MaxUnpool3dImpl.java


// Targeting ../MaxUnpool3d.java


// Targeting ../FractionalMaxPool2dImpl.java


// Targeting ../FractionalMaxPool2d.java


// Targeting ../FractionalMaxPool3dImpl.java


// Targeting ../FractionalMaxPool3d.java


// Targeting ../LPPool1dImplBase.java


// Targeting ../LPPool2dImplBase.java


// Targeting ../LPPool1dImpl.java


// Targeting ../LPPool1d.java


// Targeting ../LPPool2dImpl.java


// Targeting ../LPPool2d.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/rnn.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/options/rnn.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/modules/dropout.h>
// #include <torch/nn/pimpl.h>
// #include <torch/nn/utils/rnn.h>
// #include <torch/types.h>

// #include <ATen/ATen.h>
// #include <c10/util/Exception.h>

// #include <cstddef>
// #include <functional>
// #include <memory>
// #include <vector>
// Targeting ../RNNImplBase.java


// Targeting ../LSTMImplBase.java


// Targeting ../GRUImplBase.java



// Targeting ../RNNImpl.java


// Targeting ../RNN.java


// Targeting ../LSTMImpl.java


// Targeting ../LSTM.java


// Targeting ../GRUImpl.java


// Targeting ../GRU.java



// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ RNNCellImplBase ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Targeting ../RNNCellImplBase.java


// Targeting ../LSTMCellImplBase.java


// Targeting ../GRUCellImplBase.java



// Targeting ../RNNCellImpl.java


// Targeting ../RNNCell.java


// Targeting ../LSTMCellImpl.java


// Targeting ../LSTMCell.java


// Targeting ../GRUCellImpl.java


// Targeting ../GRUCell.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/pixelshuffle.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/pixelshuffle.h>
// #include <torch/nn/options/pixelshuffle.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>
// Targeting ../PixelShuffleImpl.java


// Targeting ../PixelShuffle.java


// Targeting ../PixelUnshuffleImpl.java


// Targeting ../PixelUnshuffle.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/upsampling.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/upsampling.h>
// #include <torch/nn/options/upsampling.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>

// #include <cstddef>
// #include <ostream>
// Targeting ../UpsampleImpl.java


// Targeting ../Upsample.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/activation.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/options/activation.h>
// #include <torch/nn/functional/activation.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/modules/linear.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>
// Targeting ../ELUImpl.java


// Targeting ../ELU.java


// Targeting ../SELUImpl.java


// Targeting ../SELU.java


// Targeting ../HardshrinkImpl.java


// Targeting ../Hardshrink.java


// Targeting ../HardtanhImpl.java


// Targeting ../Hardtanh.java


// Targeting ../LeakyReLUImpl.java


// Targeting ../LeakyReLU.java


// Targeting ../LogSigmoidImpl.java


// Targeting ../LogSigmoid.java


// Targeting ../SoftmaxImpl.java


// Targeting ../Softmax.java


// Targeting ../SoftminImpl.java


// Targeting ../Softmin.java


// Targeting ../LogSoftmaxImpl.java


// Targeting ../LogSoftmax.java


// Targeting ../Softmax2dImpl.java


// Targeting ../Softmax2d.java


// Targeting ../PReLUImpl.java


// Targeting ../PReLU.java


// Targeting ../ReLUImpl.java


// Targeting ../ReLU.java


// Targeting ../ReLU6Impl.java


// Targeting ../ReLU6.java


// Targeting ../RReLUImpl.java


// Targeting ../RReLU.java


// Targeting ../CELUImpl.java


// Targeting ../CELU.java


// Targeting ../GLUImpl.java


// Targeting ../GLU.java


// Targeting ../GELUImpl.java


// Targeting ../GELU.java


// Targeting ../SiLUImpl.java


// Targeting ../SiLU.java


// Targeting ../MishImpl.java


// Targeting ../Mish.java


// Targeting ../SigmoidImpl.java


// Targeting ../Sigmoid.java


// Targeting ../SoftplusImpl.java


// Targeting ../Softplus.java


// Targeting ../SoftshrinkImpl.java


// Targeting ../Softshrink.java


// Targeting ../SoftsignImpl.java


// Targeting ../Softsign.java


// Targeting ../TanhImpl.java


// Targeting ../Tanh.java


// Targeting ../TanhshrinkImpl.java


// Targeting ../Tanhshrink.java


// Targeting ../ThresholdImpl.java


// Targeting ../Threshold.java


// Targeting ../MultiheadAttentionImpl.java


// Targeting ../MultiheadAttention.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/normalization.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/modules/_functions.h>
// #include <torch/nn/functional/normalization.h>
// #include <torch/nn/options/normalization.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <vector>
// Targeting ../LayerNormImpl.java


// Targeting ../LayerNorm.java


// Targeting ../LocalResponseNormImpl.java


// Targeting ../LocalResponseNorm.java


// Targeting ../CrossMapLRN2dImpl.java


// Targeting ../CrossMapLRN2d.java


// Targeting ../GroupNormImpl.java


// Targeting ../GroupNorm.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/transformerlayer.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/nn/options/transformerlayer.h>
// #include <torch/nn/pimpl.h>
// #include <torch/nn/modules/linear.h>
// #include <torch/nn/modules/dropout.h>
// #include <torch/nn/modules/normalization.h>
// #include <torch/nn/modules/activation.h>
// #include <torch/nn/modules/common.h>

// #include <torch/types.h>

// #include <ostream>
// Targeting ../TransformerEncoderLayerImpl.java


// Targeting ../TransformerEncoderLayer.java


// Targeting ../TransformerDecoderLayerImpl.java


// Targeting ../TransformerDecoderLayer.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/transformercoder.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/container/any.h>
// #include <torch/nn/modules/container/modulelist.h>
// #include <torch/nn/options/transformercoder.h>
// #include <torch/nn/pimpl.h>
// #include <torch/nn/modules/common.h>

// #include <torch/types.h>

// #include <ostream>
// Targeting ../TransformerEncoderImpl.java


// Targeting ../TransformerEncoder.java


// Targeting ../TransformerDecoderImpl.java


// Targeting ../TransformerDecoder.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/transformer.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/nn/options/transformer.h>
// #include <torch/nn/pimpl.h>
// #include <torch/nn/modules/common.h>

// #include <torch/types.h>

// #include <ostream>
// Targeting ../TransformerImpl.java


// Targeting ../Transformer.java



 // namespace nn
 // namespace torch


// Parsed from torch/optim.h

// #pragma once

// #include <torch/optim/adagrad.h>
// #include <torch/optim/adam.h>
// #include <torch/optim/adamw.h>
// #include <torch/optim/lbfgs.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/rmsprop.h>
// #include <torch/optim/sgd.h>

// #include <torch/optim/schedulers/lr_scheduler.h>
// #include <torch/optim/schedulers/step_lr.h>


// Parsed from torch/optim/optimizer.h

// #pragma once

// #include <ATen/Tensor.h>
// #include <c10/util/flat_hash_map.h>
// #include <c10/util/Exception.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/arg.h>

// #include <algorithm>
// #include <functional>
// #include <iterator>
// #include <memory>
// #include <string>
// #include <vector>

// Forward declarations confuse Doxygen
// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace at
 // namespace serialize

// Targeting ../OptimizerParamState.java


// Targeting ../OptimizerCloneableAdagradParamState.java


// Targeting ../OptimizerCloneableAdamParamState.java


// Targeting ../OptimizerCloneableAdamWParamState.java


// Targeting ../OptimizerCloneableLBFGSParamState.java


// Targeting ../OptimizerCloneableRMSpropParamState.java


// Targeting ../OptimizerCloneableSGDParamState.java


// Targeting ../OptimizerOptions.java


// Targeting ../OptimizerCloneableAdagradOptions.java


// Targeting ../OptimizerCloneableAdamOptions.java


// Targeting ../OptimizerCloneableAdamWOptions.java


// Targeting ../OptimizerCloneableLBFGSOptions.java


// Targeting ../OptimizerCloneableRMSpropOptions.java


// Targeting ../OptimizerCloneableSGDOptions.java


// Targeting ../OptimizerParamGroup.java


// Targeting ../Optimizer.java



/* How do we decide whether to serialize undefined tensors or
  c10::nullopt values into the output archive?
Answer: we strictly follow the behavior of Python API. To be more specific:

For optimizer options:
a) For undefined tensor: currently no tensor is used as an options argument in Python API,
   so we don't need to worry about it now.
b) For c10::nullopt value: we serialize c10::nullopt values into the output archive,
   to follow the exact same behavior as Python API.

For optimizer param state:
a) For undefined tensor: in param state, undefined tensor in C++ impl is equivalent to
   missing key in Python impl. Since we don't serialize missing keys in Python API,
   we skip undefined tensors when serializing the param state.
b) For c10::nullopt value: in param state, c10::nullopt value in C++ impl is equivalent to
   missing key in Python impl. Since we don't serialize missing keys in Python API,
   we skip c10::nullopt values when serializing the param state. */

/** Serializes an {@code Optimizer} into an {@code OutputArchive}. */
@Namespace("torch::optim") public static native @ByRef @Name("operator <<") OutputArchive shiftLeft(
    @ByRef OutputArchive archive,
    @Const @ByRef Optimizer optimizer);

/** Deserializes a {@code Tensor} from an {@code InputArchive}. */
@Namespace("torch::optim") public static native @ByRef @Name("operator >>") InputArchive shiftRight(
    @ByRef InputArchive archive,
    @ByRef Optimizer optimizer);

 // namespace optim
 // namespace torch


// Parsed from torch/optim/serialize.h

// #pragma once

// #include <c10/util/irange.h>
// #include <torch/serialize/archive.h>
// #include <torch/types.h>
// #include <torch/optim/optimizer.h>
// #include <cstddef>
// #include <cstdint>
// #include <deque>
// #include <string>
// #include <vector>
  // Utility function to save state
  

  // Utility function to load state
  

  // Utility function to save param_groups
  

  // Utility function to load param_groups
  // We take as input vector of pair of string and unique_ptr to optimizer options so that we can retain the state
  // for each param by using the old tensor impl keys (saved during serialization) and map the new tensor impl keys to
  // the correct state for each param
  
 // namespace detail


// Note: These functions are all called `serialize()` so they can be called
// inside a template where the archive type is a template type and can thus be
// passed such that the appropriate overload is selected.

/** Utility function to save a value of {@code int64_t} type. */


/** Utility function to load a value of {@code int64_t} type. */


/** Utility function to save a vector of step buffers. */


/** Utility function to load a vector of step buffers. */


// Utility function to save state and param_groups


// Utility function to load state and param_groups and update state


/** Utility function to save a vector of buffers. */


/** Utility function to load a vector of buffers. */


// #define _TORCH_OPTIM_SERIALIZE(name)
//   torch::optim::serialize(archive, #name, self.name)

// #define _TORCH_OPTIM_SERIALIZE_WITH_TEMPLATE_ARG(OptimizerName)
//   torch::optim::serialize<OptimizerName##ParamState, OptimizerName##Options>(archive, self)

// #define _TORCH_OPTIM_SERIALIZE_TORCH_ARG(name) {
//   auto ivalue = torch::IValue(name());
//   /* do not serialize if name is an undefined tensor*/
//   if (!(ivalue.isTensor() && ivalue.nsafeToTensorImpl() == at::UndefinedTensorImpl::singleton())) {
//     archive.write(#name, ivalue);
//   }
// }

// #define _TORCH_OPTIM_SERIALIZE_TORCH_ARG_DEQUE(name) {
//   c10::IValue ivalue = torch::IValue(deque_to_list(name()));
//   archive.write(#name, ivalue);
// }

// #define _TORCH_OPTIM_DESERIALIZE_TORCH_ARG(T, name) {
//   c10::IValue ivalue;
//   bool exists = archive.try_read(#name, ivalue);
//   if (exists) {
//     name(ivalue.to<T>());
//   } else {
//     bool is_tensor_type = std::is_base_of<torch::Tensor, T>::value;
//     TORCH_INTERNAL_ASSERT(is_tensor_type);
//   }
// }

// #define _TORCH_OPTIM_DESERIALIZE_TORCH_ARG_OPTIONAL(T, name) {
//   c10::IValue ivalue;
//   bool exists = archive.try_read(#name, ivalue);
//   if (exists) {
//     name(ivalue.toOptional<T>());
//   }
// }

// #define _TORCH_OPTIM_DESERIALIZE_TORCH_ARG_DEQUE(T, name) {
//   c10::IValue ivalue;
//   archive.read(#name, ivalue);
//   auto list = ivalue.to<c10::List<T::value_type>>();
//   name(list_to_deque(list));
// }

 // namespace optim
 // namespace torch


// Parsed from torch/optim/adagrad.h

// #pragma once

// #include <torch/nn/pimpl.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>
// #include <torch/serialize/archive.h>
// #include <torch/types.h>

// #include <utility>
// #include <vector>
 // namespace serialize

// Targeting ../AdagradOptions.java


// Targeting ../AdagradParamState.java


// Targeting ../Adagrad.java


 // namespace optim
 // namespace torch


// Parsed from torch/optim/adam.h

// #pragma once

// #include <torch/nn/module.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>

// #include <utility>
// #include <vector>
 // namespace serialize

// Targeting ../AdamOptions.java


// Targeting ../AdamParamState.java


// Targeting ../Adam.java


 // namespace optim
 // namespace torch


// Parsed from torch/optim/adamw.h

// #pragma once

// #include <torch/nn/module.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>

// #include <utility>
// #include <vector>
 // namespace serialize

// Targeting ../AdamWOptions.java


// Targeting ../AdamWParamState.java


// Targeting ../AdamW.java


 // namespace optim
 // namespace torch


// Parsed from torch/optim/lbfgs.h

// #pragma once

// #include <torch/nn/module.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>
// #include <torch/serialize/archive.h>

// #include <deque>
// #include <functional>
// #include <memory>
// #include <vector>
// Targeting ../LBFGSOptions.java


// Targeting ../LBFGSParamState.java


// Targeting ../LBFGS.java


 // namespace optim
 // namespace torch


// Parsed from torch/optim/rmsprop.h

// #pragma once

// #include <torch/nn/module.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>
// #include <torch/serialize/archive.h>
// #include <torch/types.h>

// #include <functional>
// #include <memory>
// #include <string>
// #include <vector>
 // namespace serialize

// Targeting ../RMSpropOptions.java


// Targeting ../RMSpropParamState.java


// Targeting ../RMSprop.java


 // namespace optim
 // namespace torch


// Parsed from torch/optim/sgd.h

// #pragma once

// #include <torch/nn/module.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>
// #include <torch/serialize/archive.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <utility>
// #include <vector>
 // namespace serialize

// Targeting ../SGDOptions.java


// Targeting ../SGDParamState.java


// Targeting ../SGD.java


 // namespace optim
 // namespace torch


// Parsed from torch/optim/schedulers/lr_scheduler.h

// #pragma once

// #include <torch/optim/optimizer.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>
// Targeting ../LRScheduler.java


 // namespace optim
 // namspace torch


// Parsed from torch/optim/schedulers/step_lr.h

// #pragma once

// #include <torch/optim/schedulers/lr_scheduler.h>
// Targeting ../StepLR.java


 // namespace optim
 // namespace torch


}

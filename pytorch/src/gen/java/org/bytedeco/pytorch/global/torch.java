// Targeted by JavaCPP version 1.5.9: DO NOT EDIT THIS FILE

package org.bytedeco.pytorch.global;

import org.bytedeco.pytorch.*;

import org.bytedeco.pytorch.Allocator;
import org.bytedeco.pytorch.Function;
import org.bytedeco.pytorch.Module;
import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.javacpp.presets.javacpp.*;
import static org.bytedeco.openblas.global.openblas_nolapack.*;
import static org.bytedeco.openblas.global.openblas.*;

public class torch extends org.bytedeco.pytorch.presets.torch {
    static { Loader.load(); }

// Targeting ../BoolOptional.java


// Targeting ../ByteOptional.java


// Targeting ../IntOptional.java


// Targeting ../LongOptional.java


// Targeting ../DoubleOptional.java


// Targeting ../SizeTOptional.java


// Targeting ../StringOptional.java


// Targeting ../BoolVectorOptional.java


// Targeting ../LongVectorOptional.java


// Targeting ../DoubleVectorOptional.java


// Targeting ../SizeTVectorOptional.java


// Targeting ../StringVectorOptional.java


// Targeting ../StrideVectorOptional.java


// Targeting ../ShapeSymbolVectorOptional.java


// Targeting ../TensorVectorOptional.java


// Targeting ../DeviceOptional.java


// Targeting ../LongArrayRefOptional.java


// Targeting ../DoubleArrayRefOptional.java


// Targeting ../SymIntArrayRefOptional.java


// Targeting ../LayoutOptional.java


// Targeting ../MemoryFormatOptional.java


// Targeting ../ScalarOptional.java


// Targeting ../ScalarTypeOptional.java


// Targeting ../AliasInfoOptional.java


// Targeting ../IValueOptional.java


// Targeting ../CppSignatureOptional.java


// Targeting ../DispatchKeyOptional.java


// Targeting ../OperatorHandleOptional.java


// Targeting ../OperatorNameOptional.java


// Targeting ../QualifiedNameOptional.java


// Targeting ../StreamOptional.java


// Targeting ../StrideOptional.java


// Targeting ../TypePtrOptional.java


// Targeting ../ClassTypePropertyOptional.java


// Targeting ../AliasTypeSetOptional.java


// Targeting ../FunctionSchemaOptional.java


// Targeting ../SymDimVectorOptional.java


// Targeting ../SymIntOptional.java


// Targeting ../SymIntArrayRefOptional.java


// Targeting ../IValueOptional.java


// Targeting ../DimVectorOptional.java


// Targeting ../DimnameOptional.java


// Targeting ../DimnameListOptional.java


// Targeting ../GeneratorOptional.java


// Targeting ../TensorOptional.java


// Targeting ../TensorListOptional.java


// Targeting ../ThreadLocalStateOptional.java


// Targeting ../TypeMetaOptional.java


// Targeting ../ExecutorExecutionModeOptional.java


// Targeting ../InlinedCallStackOptional.java


// Targeting ../ScopeOptional.java


// Targeting ../ModuleInstanceInfoOptional.java


// Targeting ../SourceRangeOptional.java


// Targeting ../MethodOptional.java


// Targeting ../OperatorOptional.java


// Targeting ../NamedValueOptional.java


// Targeting ../ValueOptional.java


// Targeting ../LongExpandingArrayOptional.java


// Targeting ../DoubleExpandingArrayOptional.java


// Targeting ../StringSizeTSizeTTupleOptional.java


// Targeting ../ExampleVectorOptional.java


// Targeting ../ExampleOptional.java


// Targeting ../BatchSizeOptional.java


// Targeting ../TensorTensorOptional.java


// Targeting ../NonlinearityType.java


// Targeting ../FanModeType.java


// Targeting ../conv_padding_mode_t.java


// Targeting ../conv_padding_t1.java


// Targeting ../conv_padding_t2.java


// Targeting ../conv_padding_t3.java


// Targeting ../EmbeddingBagMode.java


// Targeting ../pad_mode_t.java


// Targeting ../loss_reduction_t.java


// Targeting ../kldiv_loss_reduction_t.java


// Targeting ../grid_sample_mode_t.java


// Targeting ../grid_sample_padding_mode_t.java


// Targeting ../rnn_options_base_mode_t.java


// Targeting ../rnn_nonlinearity_t.java


// Targeting ../upsample_mode_t.java


// Targeting ../interpolate_mode_t.java


// Targeting ../transformer_activation_t.java


// Targeting ../TensorDeque.java


// Targeting ../StringStringMap.java


// Targeting ../StringIntMap.java


// Targeting ../StringLongMap.java


// Targeting ../StringTensorMap.java


// Targeting ../RecordFunctionCallbackHandleVector.java


// Targeting ../DimnameVector.java


// Targeting ../FunctionPreHookVector.java


// Targeting ../FunctionPostHookVector.java


// Targeting ../TokenTrieVector.java


// Targeting ../SavedVariableVector.java


// Targeting ../DefVector.java


// Targeting ../PropertyVector.java


// Targeting ../InstructionVector.java


// Targeting ../CompilationUnitVector.java


// Targeting ../OptimizerParamGroupVector.java


// Targeting ../Bool2Vector.java


// Targeting ../BoolVector.java


// Targeting ../BytePointerVector.java


// Targeting ../LongVector.java


// Targeting ../DoubleVector.java


// Targeting ../SizeTVector.java


// Targeting ../StringVector.java


// Targeting ../StringLongVector.java


// Targeting ../ArgumentVector.java


// Targeting ../IValueVector.java


// Targeting ../QEngineVector.java


// Targeting ../ScalarTypeVector.java


// Targeting ../SymbolVector.java


// Targeting ../SymIntVector.java


// Targeting ../LongOptionalVector.java


// Targeting ../IValueOptionalVector.java


// Targeting ../ClassTypeVector.java


// Targeting ../TypeVector.java


// Targeting ../StrideVector.java


// Targeting ../ShapeSymbolVector.java


// Targeting ../TensorImplVector.java


// Targeting ../EdgeVector.java


// Targeting ../TensorVector.java


// Targeting ../TensorIndexVector.java


// Targeting ../TensorOptionalVector.java


// Targeting ../OperatorOptionalVector.java


// Targeting ../FunctionPreVector.java


// Targeting ../FunctionVector.java


// Targeting ../GraphVector.java


// Targeting ../OperatorVector.java


// Targeting ../ResolverVector.java


// Targeting ../SugaredValueVector.java


// Targeting ../StackEntryVector.java


// Targeting ../BlockVector.java


// Targeting ../ValueVector.java


// Targeting ../JitNodeVector.java


// Targeting ../ModuleVector.java


// Targeting ../AnyModuleVector.java


// Targeting ../SharedModuleVector.java


// Targeting ../SharedAnyModuleVector.java


// Targeting ../StringTensorPairVector.java


// Targeting ../StringModulePairVector.java


// Targeting ../StringAnyModulePairVector.java


// Targeting ../StringSharedModulePairVector.java


// Targeting ../FusionStrategy.java


// Targeting ../ExampleVector.java


// Targeting ../TensorExampleVector.java


// Targeting ../EnumNameValue.java


// Targeting ../StringTensorPair.java


// Targeting ../StringModulePair.java


// Targeting ../StringAnyModulePair.java


// Targeting ../StringSharedModulePair.java


// Targeting ../TensorTuple.java


// Targeting ../TensorTensorTuple.java


// Targeting ../TensorTensorTensorTuple.java


// Targeting ../TensorTensorTensorTensorTuple.java


// Targeting ../TensorTensorTensorTensorTensorTuple.java


// Targeting ../TensorTensorTensorTensorVectorTuple.java


// Targeting ../TensorTensorTensorTensorLongTuple.java


// Targeting ../TensorTensorLongLongTensorTuple.java


// Targeting ../TensorTensorTensorTensorTensorTensorTuple.java


// Targeting ../TensorTensorTensorTensorTensorTensorTensorTuple.java


// Targeting ../TensorTensorDoubleLongTuple.java


// Targeting ../TensorTensorTensorTupleTuple.java


// Targeting ../TensorMaybeOwnedTensorMaybeOwnedTuple.java


// Targeting ../TensorMaybeOwnedTensorMaybeOwnedTensorMaybeOwnedTuple.java


// Targeting ../PackedSequenceTensorTuple.java


// Targeting ../PackedSequenceTensorTensorTupleTuple.java


// Targeting ../StringSizeTSizeTTuple.java


// Targeting ../TensorTensorVectorTuple.java


// Targeting ../TensorVectorTensorTuple.java


// Targeting ../TensorVectorTensorVectorTensorVectorTensorVectorTensorVectorTuple.java


// Targeting ../TensorTensorVectorTensorVectorTuple.java


// Targeting ../IntFunctionPreHookMap.java


// Targeting ../HashAliasedIValueMap.java


// Targeting ../LongStringMap.java


// Targeting ../StringBoolMap.java


// Targeting ../StringSizeTMap.java


// Targeting ../ExtraFilesMap.java


// Targeting ../TypeEnv.java


// Targeting ../StringIValueMap.java


// Targeting ../StringFunctionMap.java


// Targeting ../StringValueMap.java


// Targeting ../StringLongStringMapMap.java


// Targeting ../ArgumentSpecExecutionPlanMap.java


// Targeting ../ValueValueMap.java


// Targeting ../StringSet.java


// Targeting ../HashAliasedIValues.java


// Targeting ../SymbolSet.java


// Targeting ../TensorImplSet.java


// Targeting ../RecordScopeSet.java


// Parsed from c10/macros/cmake_macros.h

// #ifndef C10_MACROS_CMAKE_MACROS_H_
// #define C10_MACROS_CMAKE_MACROS_H_

// Automatically generated header file for the C10 library.
// Do not include this file directly. Instead, include c10/macros/Macros.h.

// #define C10_BUILD_SHARED_LIBS
/* #undef C10_USE_GLOG */
/* #undef C10_USE_GFLAGS */
// #define C10_USE_NUMA
/* #undef C10_USE_MSVC_STATIC_RUNTIME */

// #endif // C10_MACROS_CMAKE_MACROS_H_


// Parsed from c10/macros/Export.h

// #ifndef C10_MACROS_EXPORT_H_
// #define C10_MACROS_EXPORT_H_

/* Header file to define the common scaffolding for exported symbols.
 *
 * Export is by itself a quite tricky situation to deal with, and if you are
 * hitting this file, make sure you start with the background here:
 * - Linux: https://gcc.gnu.org/wiki/Visibility
 * - Windows:
 * https://docs.microsoft.com/en-us/cpp/cpp/dllexport-dllimport?view=vs-2017
 *
 * Do NOT include this file directly. Instead, use c10/macros/Macros.h
 */

// You do not need to edit this part of file unless you are changing the core
// pytorch export abstractions.
//
// This part defines the C10 core export and import macros. This is controlled
// by whether we are building shared libraries or not, which is determined
// during build time and codified in c10/core/cmake_macros.h.
// When the library is built as a shared lib, EXPORT and IMPORT will contain
// visibility attributes. If it is being built as a static lib, then EXPORT
// and IMPORT basically have no effect.

// As a rule of thumb, you should almost NEVER mix static and shared builds for
// libraries that depend on c10. AKA, if c10 is built as a static library, we
// recommend everything dependent on c10 to be built statically. If c10 is built
// as a shared library, everything dependent on it should be built as shared. In
// the PyTorch project, all native libraries shall use the macro
// C10_BUILD_SHARED_LIB to check whether pytorch is building shared or static
// libraries.

// For build systems that do not directly depend on CMake and directly build
// from the source directory (such as Buck), one may not have a cmake_macros.h
// file at all. In this case, the build system is responsible for providing
// correct macro definitions corresponding to the cmake_macros.h.in file.
//
// In such scenarios, one should define the macro
//     C10_USING_CUSTOM_GENERATED_MACROS
// to inform this header that it does not need to include the cmake_macros.h
// file.

// #ifndef C10_USING_CUSTOM_GENERATED_MACROS
// #include <c10/macros/cmake_macros.h>
// #endif // C10_USING_CUSTOM_GENERATED_MACROS

// #ifdef _WIN32
// #else // _WIN32
// #if defined(__GNUC__)
// #define C10_EXPORT __attribute__((__visibility__("default")))
// #define C10_HIDDEN __attribute__((__visibility__("hidden")))
// #else // defined(__GNUC__)
// #define C10_EXPORT
// #define C10_HIDDEN
// #endif // defined(__GNUC__)
// #define C10_IMPORT C10_EXPORT
// #endif // _WIN32

// #ifdef NO_EXPORT
// #undef C10_EXPORT
// #define C10_EXPORT
// #endif

// Definition of an adaptive XX_API macro, that depends on whether you are
// building the library itself or not, routes to XX_EXPORT and XX_IMPORT.
// Basically, you will need to do this for each shared library that you are
// building, and the instruction is as follows: assuming that you are building
// a library called libawesome.so. You should:
// (1) for your cmake target (usually done by "add_library(awesome, ...)"),
//     define a macro called AWESOME_BUILD_MAIN_LIB using
//     target_compile_options.
// (2) define the AWESOME_API macro similar to the one below.
// And in the source file of your awesome library, use AWESOME_API to
// annotate public symbols.

// Here, for the C10 library, we will define the macro C10_API for both import
// and export.

// This one is being used by libc10.so
// #ifdef C10_BUILD_MAIN_LIB
// #define C10_API C10_EXPORT
// #else
// #define C10_API C10_IMPORT
// #endif

// This one is being used by libtorch.so
// #ifdef CAFFE2_BUILD_MAIN_LIB
// #define TORCH_API C10_EXPORT
// #else
// #define TORCH_API C10_IMPORT
// #endif

// You may be wondering: Whose brilliant idea was it to split torch_cuda into
// two pieces with confusing names?
// Once upon a time, there _was_ only TORCH_CUDA_API. All was happy until we
// tried to compile PyTorch for CUDA 11.1, which ran into relocation marker
// issues when linking big binaries.
// (https://github.com/pytorch/pytorch/issues/39968) We had two choices:
//    (1) Stop supporting so many GPU architectures
//    (2) Do something else
// We chose #2 and decided to split the behemoth that was torch_cuda into two
// smaller libraries, one with most of the core kernel functions (torch_cuda_cu)
// and the other that had..well..everything else (torch_cuda_cpp). The idea was
// this: instead of linking our static libraries (like the hefty
// libcudnn_static.a) with another huge library, torch_cuda, and run into pesky
// relocation marker issues, we could link our static libraries to a smaller
// part of torch_cuda (torch_cuda_cpp) and avoid the issues.

// libtorch_cuda_cu.so
// #ifdef TORCH_CUDA_CU_BUILD_MAIN_LIB
// #define TORCH_CUDA_CU_API C10_EXPORT
// #elif defined(BUILD_SPLIT_CUDA)
// #define TORCH_CUDA_CU_API C10_IMPORT
// #endif

// libtorch_cuda_cpp.so
// #ifdef TORCH_CUDA_CPP_BUILD_MAIN_LIB
// #define TORCH_CUDA_CPP_API C10_EXPORT
// #elif defined(BUILD_SPLIT_CUDA)
// #define TORCH_CUDA_CPP_API C10_IMPORT
// #endif

// libtorch_cuda.so (where torch_cuda_cu and torch_cuda_cpp are a part of the
// same api)
// #ifdef TORCH_CUDA_BUILD_MAIN_LIB
// #define TORCH_CUDA_CPP_API C10_EXPORT
// #define TORCH_CUDA_CU_API C10_EXPORT
// #elif !defined(BUILD_SPLIT_CUDA)
// #define TORCH_CUDA_CPP_API C10_IMPORT
// #define TORCH_CUDA_CU_API C10_IMPORT
// #endif

// #if defined(TORCH_HIP_BUILD_MAIN_LIB)
// #define TORCH_HIP_API C10_EXPORT
// #else
// #define TORCH_HIP_API C10_IMPORT
// #endif

// Enums only need to be exported on windows for non-CUDA files
// #if defined(_WIN32) && defined(__CUDACC__)
// #define C10_API_ENUM C10_API
// #else
// #define C10_API_ENUM
// #endif

// #endif // C10_MACROS_MACROS_H_


// Parsed from c10/macros/Macros.h

// #ifndef C10_MACROS_MACROS_H_
// #define C10_MACROS_MACROS_H_
// #include <cassert>

/* Main entry for c10/macros.
 *
 * In your code, include c10/macros/Macros.h directly, instead of individual
 * files in this folder.
 */

// For build systems that do not directly depend on CMake and directly build
// from the source directory (such as Buck), one may not have a cmake_macros.h
// file at all. In this case, the build system is responsible for providing
// correct macro definitions corresponding to the cmake_macros.h.in file.
//
// In such scenarios, one should define the macro
//     C10_USING_CUSTOM_GENERATED_MACROS
// to inform this header that it does not need to include the cmake_macros.h
// file.

// #ifndef C10_USING_CUSTOM_GENERATED_MACROS
// #include <c10/macros/cmake_macros.h>
// #endif // C10_USING_CUSTOM_GENERATED_MACROS

// #include <c10/macros/Export.h>

// #if defined(__clang__)
// #define __ubsan_ignore_float_divide_by_zero__
//   __attribute__((no_sanitize("float-divide-by-zero")))
// #define __ubsan_ignore_undefined__ __attribute__((no_sanitize("undefined")))
// #define __ubsan_ignore_signed_int_overflow__
//   __attribute__((no_sanitize("signed-integer-overflow")))
// #define __ubsan_ignore_function__ __attribute__((no_sanitize("function")))
// #else
// #define __ubsan_ignore_float_divide_by_zero__
// #define __ubsan_ignore_undefined__
// #define __ubsan_ignore_signed_int_overflow__
// #define __ubsan_ignore_function__
// #endif

// Detect address sanitizer as some stuff doesn't work with it
// #undef C10_ASAN_ENABLED

// for clang
// #if defined(__has_feature)
// #if ((__has_feature(address_sanitizer)))
public static final int C10_ASAN_ENABLED = 1;
// #endif
// #endif

// for gcc
// #if defined(__SANITIZE_ADDRESS__)
// #if __SANITIZE_ADDRESS__
// #if !defined(C10_ASAN_ENABLED)
// #endif
// #endif
// #endif

// #if !defined(C10_ASAN_ENABLED)
// #endif

// Disable the copy and assignment operator for a class. Note that this will
// disable the usage of the class in std containers.
// #define C10_DISABLE_COPY_AND_ASSIGN(classname)
//   classname(const classname&) = delete;
//   classname& operator=(const classname&) = delete

// #define C10_CONCATENATE_IMPL(s1, s2) s1##s2
// #define C10_CONCATENATE(s1, s2) C10_CONCATENATE_IMPL(s1, s2)

// #define C10_MACRO_EXPAND(args) args

// #define C10_STRINGIZE_IMPL(x) #x
// #define C10_STRINGIZE(x) C10_STRINGIZE_IMPL(x)

/**
 * C10_ANONYMOUS_VARIABLE(str) introduces an identifier starting with
 * str and ending with a number that varies with the line.
 */
// #ifdef __COUNTER__
// #else
// #define C10_UID __LINE__
// #define C10_ANONYMOUS_VARIABLE(str) C10_CONCATENATE(str, __LINE__)
// #endif

// #ifdef __has_cpp_attribute
// #define C10_HAS_CPP_ATTRIBUTE(x) __has_cpp_attribute(x)
// #else
// #define C10_HAS_CPP_ATTRIBUTE(x) (0)
// #endif

/** C10_NODISCARD - Warn if a type or return value is discarded. */

// Technically, we should check if __cplusplus > 201402L here, because
// [[nodiscard]] is only defined in C++17.  However, some compilers
// we care about don't advertise being C++17 (e.g., clang), but
// support the attribute anyway.  In fact, this is not just a good idea,
// it's the law: clang::warn_unused_result doesn't work on nvcc + clang
// and the best workaround for this case is to use [[nodiscard]]
// instead; see https://github.com/pytorch/pytorch/issues/13118
//
// Note to future editors: if you have noticed that a compiler is
// misbehaving (e.g., it advertises support, but the support doesn't
// actually work, or it is emitting warnings).  Some compilers which
// are strict about the matter include MSVC, which will complain:
//
//  error C2429: attribute 'nodiscard' requires compiler flag '/std:c++latest'
//
// Exhibits:
//  - MSVC 19.14: https://godbolt.org/z/Dzd7gn (requires /std:c++latest)
//  - Clang 8.0.0: https://godbolt.org/z/3PYL4Z (always advertises support)
//  - gcc 8.3: https://godbolt.org/z/4tLMQS (always advertises support)
// #if C10_HAS_CPP_ATTRIBUTE(nodiscard)
// #define C10_NODISCARD [[nodiscard]]
// Workaround for llvm.org/PR23435, since clang 3.6 and below emit a spurious
// error when __has_cpp_attribute is given a scoped attribute in C mode.
// #elif __cplusplus && C10_HAS_CPP_ATTRIBUTE(clang::warn_unused_result)
// TODO: It's possible this is still triggering
// https://github.com/pytorch/pytorch/issues/13118 on Windows; if it is, better
// fix it.
// #define C10_NODISCARD [[clang::warn_unused_result]]
// #else
// #define C10_NODISCARD
// #endif

// suppress an unused variable.
// #if defined(_MSC_VER) && !defined(__clang__)
// #define C10_UNUSED __pragma(warning(suppress : 4100 4101))
// #else
// #define C10_UNUSED __attribute__((__unused__))
// #endif //_MSC_VER

// Direct port of LLVM_ATTRIBUTE_USED.
// #if __has_attribute(used)
// #define C10_USED __attribute__((__used__))
// #else
// #define C10_USED
// #endif

// #define C10_RESTRICT __restrict

// Simply define the namespace, in case a dependent library want to refer to
// the c10 namespace but not any nontrivial files.
 // namespace c10

 // namespace c10

 // namespace c10

// Since C10 is the core library for caffe2 (and aten), we will simply reroute
// all abstractions defined in c10 to be available in caffe2 as well.
// This is only for backwards compatibility. Please use the symbols from the
// c10 namespace where possible.



 // namespace at

// WARNING!!! THIS IS A GIANT HACK!!!
// This line means you cannot simultaneously include c10/hip
// and c10/cuda and then use them from the at::cuda namespace.
// This is true in practice, because HIPIFY works inplace on
// files in ATen/cuda, so it assumes that c10::hip is available
// from at::cuda.  This namespace makes that happen.  When
// HIPIFY is no longer out-of-place, we can switch the cuda
// here to hip and everyone is happy.

 // namespace at

// C10_LIKELY/C10_UNLIKELY
//
// These macros provide parentheses, so you can use these macros as:
//
//    if C10_LIKELY(some_expr) {
//      ...
//    }
//
// NB: static_cast to boolean is mandatory in C++, because __builtin_expect
// takes a long argument, which means you may trigger the wrong conversion
// without it.
//
// #if defined(__GNUC__) || defined(__ICL) || defined(__clang__)
// #define C10_LIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 1))
// #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))
// #else
// #define C10_LIKELY(expr) (expr)
// #define C10_UNLIKELY(expr) (expr)
// #endif

/** C10_NOINLINE - Functions whose declaration is annotated with this will not
 *  be inlined. */
// #ifdef __GNUC__
// #define C10_NOINLINE __attribute__((noinline))
// #elif _MSC_VER
// #define C10_NOINLINE __declspec(noinline)
// #else
// #define C10_NOINLINE
// #endif

// #if defined(_MSC_VER)
// #elif __has_attribute(always_inline) || defined(__GNUC__)
// #define C10_ALWAYS_INLINE __attribute__((__always_inline__)) inline
// #else
// #define C10_ALWAYS_INLINE inline
// #endif

// #if defined(_MSC_VER)
// #elif defined(__GNUC__)
// #define C10_ATTR_VISIBILITY_HIDDEN __attribute__((__visibility__("hidden")))
// #else
// #define C10_ATTR_VISIBILITY_HIDDEN
// #endif

// #define C10_ERASE C10_ALWAYS_INLINE C10_ATTR_VISIBILITY_HIDDEN

// C10_FALLTHROUGH - Annotate fallthrough to the next case in a switch.
// #if C10_HAS_CPP_ATTRIBUTE(fallthrough)
// #define C10_FALLTHROUGH [[fallthrough]]
// #else
// #define C10_FALLTHROUGH
// #endif

// #include <cstdint>

// #ifdef __HIPCC__
// Unlike CUDA, HIP requires a HIP header to be included for __host__ to work.
// We do this #include here so that C10_HOST_DEVICE and friends will Just Work.
// See https://github.com/ROCm-Developer-Tools/HIP/issues/441
// #include <hip/hip_runtime.h>
// #endif

// #if defined(__CUDACC__) || defined(__HIPCC__)
// #else
// #define C10_HOST_DEVICE
// #define C10_HOST
// #define C10_DEVICE
// #endif

// #if defined(USE_ROCM)
// #else
// #define C10_HIP_HOST_DEVICE
// #endif

// #if defined(USE_ROCM)
// #else
// #define C10_WARP_SIZE 32
// #endif

// #if defined(_MSC_VER) && _MSC_VER <= 1900
// #endif

// CUDA_KERNEL_ASSERT checks the assertion
// even when NDEBUG is defined. This is useful for important assertions in CUDA
// code that would otherwise be suppressed when building Release.
// #if defined(__ANDROID__) || defined(__APPLE__) ||
//     (defined(USE_ROCM) && ROCM_VERSION < 40100)
// Those platforms do not support assert()
// #define CUDA_KERNEL_ASSERT(cond)
// #define SYCL_KERNEL_ASSERT(cond)
// #elif defined(_MSC_VER)
// #else // __APPLE__, _MSC_VER
// #if defined(NDEBUG)
// #endif // NDEBUG
// #define CUDA_KERNEL_ASSERT(cond)
//   if (C10_UNLIKELY(!(cond))) {
//     __assert_fail(
//         #cond, __FILE__, static_cast<unsigned int>(__LINE__), __func__);
//   }
// #define SYCL_KERNEL_ASSERT(cond)
//   if (C10_UNLIKELY(!(cond))) {
//     __assert_fail(
//         #cond, __FILE__, static_cast<unsigned int>(__LINE__), __func__);
//   }
// #endif // __APPLE__

// #ifdef __APPLE__
// #include <TargetConditionals.h>
// #endif

// #if defined(__ANDROID__)
// #elif (
//     defined(__APPLE__) &&
//     (TARGET_IPHONE_SIMULATOR || TARGET_OS_SIMULATOR || TARGET_OS_IPHONE))
// #define C10_IOS 1
// #define C10_MOBILE 1
// #endif // ANDROID / IOS

// #if defined(C10_MOBILE) && C10_MOBILE
// #define C10_ALWAYS_INLINE_UNLESS_MOBILE inline
// #else
// #define C10_ALWAYS_INLINE_UNLESS_MOBILE C10_ALWAYS_INLINE
// #endif

// Portable determination of whether type T is trivially copyable.
// Warning: __has_trivial_copy for GCC may not always detect the non-POD
// correctly. For example, T = std::unique_ptr may evaluate to true and be
// treated as POD. This can cause unexpected behavior.
// #if defined(__GNUG__) && __GNUC__ < 5 && !defined(__clang__)
// #define C10_IS_TRIVIALLY_COPYABLE(T) __has_trivial_copy(T)
// #else
// #define C10_IS_TRIVIALLY_COPYABLE(T) std::is_trivially_copyable<T>::value
// #endif

// #if defined(__CUDA_ARCH__)
// #if defined(_MSC_VER) && defined(__CUDACC__)
// #define CONSTEXPR_EXCEPT_WIN_CUDA const
// #define C10_HOST_CONSTEXPR_EXCEPT_WIN_CUDA __host__

// Note [static constexpr char* members for windows NVCC]
// The Windows NVCC compiler doesn't handle static constexpr class members,
// although it's fixed in a later version.
// (see
// https://developercommunity.visualstudio.com/t/intellisense-error-c11-static-constexpr-member-ini/245425)
//
// If we want to ensure that our field is static under all builds, then we need
// to work around it specifically for windows NVCC by making it (a) const, (b)
// defined outside of the class definition We need to define it outside of the
// class definition because of the C++ standard; char* is not an integral type
// (see
// https://stackoverflow.com/questions/24278473/intellisense-a-member-of-type-const-char-const-cannot-have-an-in-class-in)
//
// So instead of this:
// struct Foo {
//     static constexpr const char* name = "foo";
// }
// In Windows NVCC, we end up with this:
// struct Foo {
//     static const char* name;
// }
// const char* Foo::name = "foo";
//
// This gives us a small perf hit for any code that wants to access these field
// members, but right now it isn't used in any perf-critical code paths.
// #define STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(field, val)
//   static const char* field;
// #define STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cls, field, val)
//   const char* cls::field = val;
// #else
// #define CONSTEXPR_EXCEPT_WIN_CUDA constexpr
// #define C10_HOST_CONSTEXPR_EXCEPT_WIN_CUDA __host__

// #define STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(field, val)
//   static constexpr const char* field = val;
// #define STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cls, field, val)
// #endif
// #else
// #if defined(_MSC_VER) && defined(__CUDACC__)
// #define CONSTEXPR_EXCEPT_WIN_CUDA const
// #define C10_HOST_CONSTEXPR_EXCEPT_WIN_CUDA

// #define STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(field, val)
//   static const char* field;
// #define STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cls, field, val)
//   const char* cls::field = val;
// #else
// #define CONSTEXPR_EXCEPT_WIN_CUDA constexpr
// #define C10_HOST_CONSTEXPR_EXCEPT_WIN_CUDA constexpr

// #define STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(field, val)
//   static constexpr const char* field = val;
// #define STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cls, field, val)
// #endif
// #endif

// #ifndef HAS_DEMANGLE
// #if defined(__ANDROID__) || defined(_WIN32) || defined(__EMSCRIPTEN__)
public static final int HAS_DEMANGLE = 0;
// #elif defined(__APPLE__) &&
//     (TARGET_IPHONE_SIMULATOR || TARGET_OS_SIMULATOR || TARGET_OS_IPHONE)
// #else
// #endif
// #endif // HAS_DEMANGLE

// #define _C10_PRAGMA__(string) _Pragma(#string)
// #define _C10_PRAGMA_(string) _C10_PRAGMA__(string)

// #ifdef __clang__
// #define C10_CLANG_DIAGNOSTIC_PUSH() _Pragma("clang diagnostic push")
// #define C10_CLANG_DIAGNOSTIC_POP() _Pragma("clang diagnostic pop")
// #define C10_CLANG_DIAGNOSTIC_IGNORE(flag)
//   _C10_PRAGMA_(clang diagnostic ignored flag)
// #define C10_CLANG_HAS_WARNING(flag) __has_warning(flag)
// #else
// #define C10_CLANG_DIAGNOSTIC_PUSH()
// #define C10_CLANG_DIAGNOSTIC_POP()
// #define C10_CLANG_DIAGNOSTIC_IGNORE(flag)
// #define C10_CLANG_HAS_WARNING(flag) 0
// #endif

// #ifdef __clang__

// #define C10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED(warning)
//   _C10_PRAGMA_(clang diagnostic push)
//   _C10_PRAGMA_(clang diagnostic ignored "-Wunknown-warning-option")
//   _C10_PRAGMA_(clang diagnostic ignored warning)

// #define C10_DIAGNOSTIC_POP() _C10_PRAGMA_(clang diagnostic pop)

// #elif __GNUC__

// #define C10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED(warning)
//   _C10_PRAGMA_(GCC diagnostic push)
//   _C10_PRAGMA_(GCC diagnostic ignored "-Wpragmas")
//   _C10_PRAGMA_(GCC diagnostic ignored warning)

// #define C10_DIAGNOSTIC_POP() _C10_PRAGMA_(GCC diagnostic pop)

// #else

// #define C10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED(warning)
// #define C10_DIAGNOSTIC_POP()

// #endif

// #endif // C10_MACROS_MACROS_H_


// Parsed from c10/util/IdWrapper.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <cstddef>
// #include <functional>
// #include <utility>
// Targeting ../TypeIdentifierIdWrapper.java



 // namespace c10

// #define C10_DEFINE_HASH_FOR_IDWRAPPER(ClassName)
//   namespace std {
//   template <>
//   struct hash<ClassName> {
//     size_t operator()(ClassName x) const {
//       return hash_value(x);
//     }
//   };
//   }


// Parsed from c10/util/MaybeOwned.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/in_place.h>

// #include <type_traits>

/** MaybeOwnedTraits<T> describes how to borrow from T.  Here is how we
 *  can implement borrowing from an arbitrary type T using a raw
 *  pointer to const: */

/** It is possible to eliminate the extra layer of indirection for
 *  borrows for some types that we control. For examples, see
 *  intrusive_ptr.h and TensorBody.h. */

// Explicitly enable MaybeOwned<shared_ptr<T>>, rather than allowing
// MaybeOwned to be used for any type right away.
// Targeting ../TensorMaybeOwned.java



 // namespace c10


// Parsed from c10/util/typeid.h

// #pragma once

// #include <atomic>
// #include <cassert>
// #include <complex>
// #include <cstdlib>
// #include <memory>
// #include <mutex>
// #include <type_traits>
// #include <unordered_map>
// #include <unordered_set>
// #include <vector>
// #ifdef __GXX_RTTI
// #include <typeinfo>
// #endif

// #include <exception>

// #include <c10/macros/Macros.h>
// #include <c10/util/Backtrace.h>
// #include <c10/util/C++17.h>
// #include <c10/util/Exception.h>
// #include <c10/util/IdWrapper.h>
// #include <c10/util/Type.h>
// #include <c10/util/TypeIndex.h>
// #include <c10/util/TypeTraits.h>
// #include <c10/util/flat_hash_map.h>

// #include <c10/core/ScalarType.h>
// #include <c10/util/irange.h>

/*
 * TypeIdentifier is a small type containing an id.
 * Types must be registered using CAFFE_DECLARE_KNOWN_TYPE() (in their header)
 * and CAFFE_DEFINE_KNOWN_TYPE() (in their .cpp file) for them to have a type
 * id. If a type is registered, you can also create an object containing meta
 * data like constructor, destructor, stringified name, ... about the type by
 * calling TypeMeta::Make<T>. This returns a TypeMeta() object, which is
 * basically just a pointer to the type information, so it's cheap to pass
 * around.
 */

// TODO: This file is still in the caffe2 namespace, despite living
// in the ATen directory.  This is because the macro
// CAFFE_KNOWN_TYPE (and CAFFE_DECLARE_KNOWN_TYPE) defines a template
// specialization, which relies
// on the namespace of TypeMeta matching the namespace where the macro is
// called.  This requires us to fix all of the call-sites, which I want to do
// later.  So the namespace is not fixed at the moment.

// Make at::Half a fundamental type.
 // namespace guts
 // namespace c10
// Targeting ../TypeIdentifier.java



// Allow usage in std::map / std::set
// TODO Disallow this and rather use std::unordered_map/set everywhere
@Namespace("caffe2") public static native @Cast("const bool") @Name("operator <") boolean lessThan(@ByVal TypeIdentifier lhs, @ByVal TypeIdentifier rhs);

@Namespace("caffe2") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(
    @Cast("std::ostream*") @ByRef Pointer stream,
    @ByVal TypeIdentifier typeId);

 // namespace caffe2

// Targeting ../DeviceTypeHash.java


  
// Targeting ../TypeMetaData.java



// Mechanism for throwing errors which can't be prevented at compile time
// due to type erasure. E.g. somebody calling TypeMeta::copy() for
// non-copyable type. Right now just throws exception but is implemented
// in .cpp to manage dependencies
@Namespace("caffe2::detail") public static native void _ThrowRuntimeTypeLogicError(@StdString BytePointer msg);
@Namespace("caffe2::detail") public static native void _ThrowRuntimeTypeLogicError(@StdString String msg);

/**
 * Placement new function for the type.
 */

/**
 * Typed copy function for classes.
 */

/**
 * A placeholder function for types that do not allow assignment.
 */
// Targeting ../_Uninitialized.java



 // namespace detail

//
// note: this is outside TypeMeta bc gcc seems to have trouble
// with scalarTypeItemSizes as a constexpr static member used by
// a public inline instance method
//

// item sizes for TypeMeta::itemsize() fast path
@Namespace("caffe2") @MemberGetter public static native @Cast("const uint8_t") byte scalarTypeItemSizes(int i);
@Namespace("caffe2") @MemberGetter public static native @Cast("const uint8_t*") BytePointer scalarTypeItemSizes();
// Targeting ../TypeMeta.java



// specializations of TypeMeta::_typeMetaData for ScalarType types

// #define DEFINE_SCALAR_METADATA_INSTANCE(T, name)
//   template <>
//   constexpr uint16_t TypeMeta::_typeMetaData<T>() noexcept {
//     return static_cast<uint16_t>(ScalarType::name);
//   }




@Namespace("caffe2") public static native @Cast("bool") @Name("operator ==") @NoException(true) boolean equals(@Const @ByVal TypeMeta lhs, @Const @ByVal TypeMeta rhs);
@Namespace("caffe2") public static native @Cast("bool") @Name("operator !=") @NoException(true) boolean notEquals(@Const @ByVal TypeMeta lhs, @Const @ByVal TypeMeta rhs);

@Namespace("caffe2") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(
    @Cast("std::ostream*") @ByRef Pointer stream,
    @ByVal TypeMeta typeMeta);

/**
 * Register unique id for a type so it can be used in TypeMeta context, e.g. be
 * used as a type for Blob or for Tensor elements.
 *
 * CAFFE_KNOWN_TYPE is deprecated; prefer CAFFE_DECLARE_KNOWN_TYPE and
 * CAFFE_DEFINE_KNOWN_TYPE.
 *
 * CAFFE_KNOWN_TYPE does explicit instantiation of TypeIdentifier::Get<T>
 * template function and thus needs to be put in a single translation unit (.cpp
 * file) for a given type T. Other translation units that use type T as a type
 * of the caffe2::Blob or element type of caffe2::Tensor need to depend on the
 * translation unit that contains CAFFE_KNOWN_TYPE declaration via regular
 * linkage dependencies.
 *
 * NOTE: the macro needs to be invoked in ::caffe2 namespace
 */
// Implementation note: in MSVC, we will need to prepend the C10_API
// keyword in order to get things compiled properly. in Linux, gcc seems to
// create attribute ignored error for explicit template instantiations, see
//   http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0537r0.html
//   https://gcc.gnu.org/bugzilla/show_bug.cgi?id=51930
// and as a result, we define these two macros slightly differently.
// #if defined(_MSC_VER) || defined(__clang__)
// #define EXPORT_IF_NOT_GCC C10_EXPORT
// #else
// #define EXPORT_IF_NOT_GCC
// #endif

// CAFFE_KNOWN_TYPE is deprecated! Use CAFFE_DECLARE_KNOWN_TYPE and
// CAFFE_DEFINE_KNOWN_TYPE instead.
// #define CAFFE_KNOWN_TYPE(T)
//   template uint16_t TypeMeta::addTypeMetaData<T>();
//   template <>
//   EXPORT_IF_NOT_GCC uint16_t TypeMeta::_typeMetaData<T>() noexcept {
//     static const uint16_t index = addTypeMetaData<T>();
//     return index;
//   }

// #define CAFFE_DEFINE_KNOWN_TYPE(T)
//   template uint16_t TypeMeta::addTypeMetaData<T>();

// Unlike CAFFE_KNOWN_TYPE, CAFFE_DECLARE_KNOWN_TYPE avoids a function
// call to access _typeMetaData in the common case.
// #ifdef __CUDACC__
// nvcc needs its own specialization that doesn't use
// C10_ALWAYS_INLINE so that it doesn't need to see a definition for
// _addTypeMeta. See NOTE [ TypeIdentifier::Get nvcc/clang discrepancy
// ].
// #define CAFFE_DECLARE_KNOWN_TYPE(T)
//   extern template uint16_t TypeMeta::addTypeMetaData<T>();
//   template <>
//   EXPORT_IF_NOT_GCC inline uint16_t TypeMeta::_typeMetaData<T>() noexcept {
//     static const uint16_t index = addTypeMetaData<T>();
//     return index;
//   }
// #else
// #define CAFFE_DECLARE_KNOWN_TYPE(T)
//   extern template uint16_t TypeMeta::addTypeMetaData<T>();
//   template <>
//   EXPORT_IF_NOT_GCC C10_ALWAYS_INLINE uint16_t
//   TypeMeta::_typeMetaData<T>() noexcept {
//     static const uint16_t index = addTypeMetaData<T>();
//     return index;
//   }
// #endif

// #define CAFFE_KNOWN_TYPE_NOEXPORT(T)
//   template <>
//   uint16_t TypeMeta::_typeMetaData<T>() noexcept {
//     static const uint16_t index = addTypeMetaData<T>();
//     return index;
//   }


  

  

  

  

  

  

  

  

  

  

  

// For some of the compilers, long is defined separately from int32_t and
// int64_t. As a result we will need to actually define them separately.
// It is recommended that one does NOT use long - use int32_t and int64_t
// explicitly. Explicit long type annotation may go away in the future.
// details: This hack works by defining a _guard_long_unique type, which is
// long iff the compiler has a separate long type and is a dummy type otherwise.
// we then allocate a type id to that _guard_long_unique. If the compiler has a
// separate long type, this allocates a type id for long. Otherwise, it
// allocates a type id for the dummy type, which doesn't matter.
 // namespace detail


  

  


  

  

 // namespace caffe2


// Parsed from c10/util/AlignOf.h

//===--- AlignOf.h - Portable calculation of type alignment -----*- C++ -*-===//
//
//                     The LLVM Compiler Infrastructure
//
// This file is distributed under the University of Illinois Open Source
// License. See LICENSE.TXT for details.
//
//===----------------------------------------------------------------------===//
//
// This file defines the AlignedCharArray and AlignedCharArrayUnion classes.
//
//===----------------------------------------------------------------------===//

// ATen: modified from llvm::AlignOf
// replaced LLVM_ALIGNAS with alignas

// #pragma once

// #include <cstddef>

/** \struct AlignedCharArray
 *  \brief Helper for building an aligned character array type.
 * 
 *  This template is used to explicitly build up a collection of aligned
 *  character array types. We have to build these up using a macro and explicit
 *  specialization to cope with MSVC (at least till 2015) where only an
 *  integer literal can be used to specify an alignment constraint. Once built
 *  up here, we can then begin to indirect between these using normal C++
 *  template parameters. */

// MSVC requires special handling here.
// #ifndef _MSC_VER

// #else // _MSC_VER

/** \brief Create a type with an aligned char buffer. */

// We provide special variations of this template for the most common
// alignments because __declspec(align(...)) doesn't actually work when it is
// a member of a by-value function argument in MSVC, even if the alignment
// request is something reasonably like 8-byte or 16-byte. Note that we can't
// even include the declspec with the union that forces the alignment because
// MSVC warns on the existence of the declspec despite the union member forcing
// proper alignment.

// The rest of these are provided with a __declspec(align(...)) and we simply
// can't pass them by-value as function arguments on MSVC.

// #define AT_ALIGNEDCHARARRAY_TEMPLATE_ALIGNMENT(x)
//   template <size_t Size>
//   struct AlignedCharArray<x, Size> {
//     __declspec(align(x)) char buffer[Size];
//   };

// #undef AT_ALIGNEDCHARARRAY_TEMPLATE_ALIGNMENT

// #endif // _MSC_VER
 // end namespace detail

/** \brief This union template exposes a suitably aligned and sized character
 *  array member which can hold elements of any of up to ten types.
 * 
 *  These types may be arrays, structs, or any other types. The goal is to
 *  expose a char array buffer member which can be used as suitable storage for
 *  a placement new of any of these types. Support for more than ten types can
 *  be added at the cost of more boilerplate. */
 // end namespace c10


// Parsed from c10/util/Deprecated.h

// #pragma once

/**
 * This file provides portable macros for marking declarations
 * as deprecated.  You should generally use C10_DEPRECATED,
 * except when marking 'using' declarations as deprecated,
 * in which case you should use C10_DEFINE_DEPRECATED_USING
 * (due to portability concerns).
 */

// Sample usage:
//
//    C10_DEPRECATED void bad_func();
//    struct C10_DEPRECATED BadStruct {
//      ...
//    };

// NB: __cplusplus doesn't work for MSVC, so for now MSVC always uses
// the "__declspec(deprecated)" implementation and not the C++14
// "[[deprecated]]" attribute. We tried enabling "[[deprecated]]" for C++14 on
// MSVC, but ran into issues with some older MSVC versions.
// #if (defined(__cplusplus) && __cplusplus >= 201402L)
// #define C10_DEPRECATED [[deprecated]]
// #define C10_DEPRECATED_MESSAGE(message) [[deprecated(message)]]
// #elif defined(__GNUC__)
// #define C10_DEPRECATED __attribute__((deprecated))
// TODO Is there some way to implement this?
// #define C10_DEPRECATED_MESSAGE(message) __attribute__((deprecated))

// #elif defined(_MSC_VER)
// #else
// #warning "You need to implement C10_DEPRECATED for this compiler"
// #define C10_DEPRECATED
// #endif

// Sample usage:
//
//    C10_DEFINE_DEPRECATED_USING(BadType, int)
//
//   which is the portable version of
//
//    using BadType [[deprecated]] = int;

// technically [[deprecated]] syntax is from c++14 standard, but it works in
// many compilers.
// #if defined(__has_cpp_attribute)
// #if __has_cpp_attribute(deprecated) && !defined(__CUDACC__)
// #define C10_DEFINE_DEPRECATED_USING(TypeName, TypeThingy)
//   using TypeName [[deprecated]] = TypeThingy;
// #endif
// #endif

// #if defined(_MSC_VER)
// #endif

// #if !defined(C10_DEFINE_DEPRECATED_USING) && defined(__GNUC__)
// nvcc has a bug where it doesn't understand __attribute__((deprecated))
// declarations even when the host compiler supports it. We'll only use this gcc
// attribute when not cuda, and when using a GCC compiler that doesn't support
// the c++14 syntax we checked for above (available in __GNUC__ >= 5)
// #if !defined(__CUDACC__)
// #define C10_DEFINE_DEPRECATED_USING(TypeName, TypeThingy)
//   using TypeName __attribute__((deprecated)) = TypeThingy;
// #else
// using cuda + gcc < 5, neither deprecated syntax is available so turning off.
// #define C10_DEFINE_DEPRECATED_USING(TypeName, TypeThingy)
//   using TypeName = TypeThingy;
// #endif
// #endif

// #if !defined(C10_DEFINE_DEPRECATED_USING)
// #warning "You need to implement C10_DEFINE_DEPRECATED_USING for this compiler"
// #define C10_DEFINE_DEPRECATED_USING
// #endif


// Parsed from c10/util/StringUtil.h

// #ifndef C10_UTIL_STRINGUTIL_H_
// #define C10_UTIL_STRINGUTIL_H_

// #include <c10/macros/Macros.h>
// #include <c10/util/string_utils.h>
// #include <c10/util/string_view.h>

// #include <cstddef>
// #include <ostream>
// #include <sstream>
// #include <string>
// #include <vector>

// #if C10_CLANG_HAS_WARNING("-Wshorten-64-to-32")
// #endif

// Obtains the base name from a full path.
@Namespace("c10::detail") public static native @StdString BytePointer StripBasename(@StdString BytePointer full_path);
@Namespace("c10::detail") public static native @StdString String StripBasename(@StdString String full_path);

@Namespace("c10::detail") public static native @StdString BytePointer ExcludeFileExtension(@StdString BytePointer full_path);
@Namespace("c10::detail") public static native @StdString String ExcludeFileExtension(@StdString String full_path);
// Targeting ../CompileTimeEmptyString.java



@Namespace("c10::detail") public static native @Cast("std::ostream*") @ByRef Pointer _str(@Cast("std::ostream*") @ByRef Pointer ss);

@Namespace("c10::detail") public static native @Cast("std::ostream*") @ByRef @Name("_str") Pointer _strCompileTimeEmptyString(@Cast("std::ostream*") @ByRef Pointer ss, @Const @ByRef CompileTimeEmptyString t);
// Targeting ../_str_wrapper.java



// For c10::str() with an empty argument list (which is common in our assert
// macros), we don't want to pay the binary size for constructing and
// destructing a stringstream or even constructing a string.

 // namespace detail

// Convert a list of string-like arguments into a single string.

// Replace all occurrences of "from" substring to "to" string.
// Returns number of replacements
@Namespace("c10") public static native @Cast("size_t") long ReplaceAll(@StdString @ByRef BytePointer s, @ByVal @Cast("c10::string_view*") Pointer from, @ByVal @Cast("c10::string_view*") Pointer to);
// Targeting ../SourceLocation.java





// unix isprint but insensitive to locale
@Namespace("c10") public static native @Cast("bool") boolean isPrint(@Cast("char") byte s);

@Namespace("c10") public static native void printQuotedString(@Cast("std::ostream*") @ByRef Pointer stmt, @ByVal @Cast("const c10::string_view*") Pointer str);

 // namespace c10

// #endif // C10_UTIL_STRINGUTIL_H_


// Parsed from c10/util/SmallVector.h

//===- llvm/ADT/SmallVector.h - 'Normally small' vectors --------*- C++ -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
//
// This file defines the SmallVector class.
//
//===----------------------------------------------------------------------===//

// ATen: modified from llvm::SmallVector.
// used std::is_trivially_{copy,move}_constructible
// replaced iterator_range constructor with inline Container&& constructor
// replaced LLVM_NODISCARD, LLVM_LIKELY, and LLVM_UNLIKELY with c10 equivalents
// removed LLVM_GSL_OWNER
// added SmallVector::at
// added operator<< for std::ostream
// added C10_API to export SmallVectorBase

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/util/AlignOf.h>

// #include <algorithm>
// #include <cassert>
// #include <cstddef>
// #include <cstdlib>
// #include <cstring>
// #include <functional>
// #include <initializer_list>
// #include <iterator>
// #include <limits>
// #include <memory>
// #include <new>
// #include <ostream>
// #include <type_traits>
// #include <utility>

// #if C10_CLANG_HAS_WARNING("-Wshorten-64-to-32")
// #endif

/** This is all the stuff common to all SmallVectors.
 * 
 *  The template parameter specifies the type which should be used to hold the
 *  Size and Capacity of the SmallVector, so it can be adjusted.
 *  Using 32 bit size is desirable to shrink the size of the SmallVector.
 *  Using 64 bit size is desirable for cases like SmallVector<char>, where a
 *  32 bit size would limit the vector to ~4GB. SmallVectors are used for
 *  buffering bitcode output - which can exceed 4GB. */

/** Figure out the offset of the first element. */

/** This is the part of SmallVectorTemplateBase which does not depend on whether
 *  the type T is a POD. The extra dummy template argument is used by ArrayRef
 *  to avoid unnecessarily requiring T to be complete. */
// Targeting ../SmallVectorBase.java


// Targeting ../SymSmallVectorBase.java



// Define this out-of-line to dissuade the C++ compiler from inlining it.


// Define this out-of-line to dissuade the C++ compiler from inlining it.


// Define this out-of-line to dissuade the C++ compiler from inlining it.


/** SmallVectorTemplateBase<TriviallyCopyable = true> - This is where we put
 *  method implementations that are designed to work with trivially copyable
 *  T's. This allows using memcpy in place of copy/move construction and
 *  skipping destruction. */
// Targeting ../DimVectorImpl.java


// Targeting ../SymDimVectorImpl.java









/** Storage for the SmallVector elements.  This is specialized for the N=0 case
 *  to avoid allocating unnecessary storage. */

/** We need the storage to be properly aligned even for small-size of 0 so that
 *  the pointer math in \a SmallVectorTemplateCommon::getFirstEl() is
 *  well-defined. */

/** Forward declaration of SmallVector so that
 *  calculateSmallVectorDefaultInlinedElements can reference
 *  {@code sizeof(SmallVector<T, 0>)}. */

/** Helper class for calculating the default number of inline elements for
 *  {@code SmallVector<T>}.
 * 
 *  This should be migrated to a constexpr function when our minimum
 *  compiler support is enough for multi-statement constexpr functions. */
// Targeting ../DimVector.java


// Targeting ../SymDimVector.java





/** Given a range of type R, iterate the entire range and return a
 *  SmallVector with elements of the vector.  This is useful, for example,
 *  when you want to iterate a range and then sort the results. */

 // end namespace c10

/** Implement std::swap in terms of SmallVector swap. */

/** Implement std::swap in terms of SmallVector swap. */

 // end namespace std



// Parsed from c10/util/DimVector.h

// #pragma once

// #include <c10/core/SymInt.h>
// #include <c10/core/impl/SizesAndStrides.h>
// #include <c10/util/SmallVector.h>
// #include <cstdint>

@Namespace("c10") @MemberGetter public static native @Cast("const size_t") long kDimVectorStaticSize();

/** A container for sizes or strides */

 // namespace c10


// Parsed from c10/util/Exception.h

// #ifndef C10_UTIL_EXCEPTION_H_
// #define C10_UTIL_EXCEPTION_H_

// #include <c10/macros/Macros.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/StringUtil.h>
// #include <c10/util/variant.h>

// #include <cstddef>
// #include <exception>
// #include <ostream>
// #include <sstream>
// #include <string>
// #include <vector>

// #if defined(_MSC_VER) && _MSC_VER <= 1900
// #endif
// Targeting ../Error.java


// Targeting ../Warning.java



// Issue a warning with a given message. Dispatched to the current
// warning handler.
@Namespace("c10") public static native void warn(@Const @ByRef Warning warning);
// Targeting ../WarningHandler.java



// Note: [Verbatim Warnings]
// Warnings originating in C++ code can appear out-of-place to Python users:
// a user runs a line in Python, but the warning references a line in C++.
// Some parts of PyTorch, like the JIT, are cognizant of this mismatch
// and take care to map warnings back to the user's program, but most
// of PyTorch simply throws a context-free warning. To allow warning
// handlers to add context where appropriate, warn takes the
// "verbatim" flag. When this is false a warning handler might append
// the C++ warning to a Python warning message that relates the warning
// back to the user's program. Callers who have already accounted for
// context in their warnings should set verbatim to true so their warnings
// appear without modification.

/** Sets the global warning handler. This is not thread-safe, so it should
 *  generally be called once during initialization or while holding the GIL
 *  for programs that use python.
 *  User is responsible for keeping the WarningHandler alive until
 *  it is not needed. */
@Namespace("c10::WarningUtils") public static native @NoException(true) void set_warning_handler(WarningHandler handler);
/** Gets the global warning handler. */
@Namespace("c10::WarningUtils") public static native @NoException(true) WarningHandler get_warning_handler();
// Targeting ../WarningHandlerGuard.java



/** The TORCH_WARN_ONCE macro is difficult to test for. Use
 *  setWarnAlways(true) to turn it into TORCH_WARN, which can be
 *  tested for more easily. */
@Namespace("c10::WarningUtils") public static native @NoException(true) void set_warnAlways(@Cast("bool") boolean arg0);
@Namespace("c10::WarningUtils") public static native @Cast("bool") @NoException(true) boolean get_warnAlways();
// Targeting ../WarnAlways.java




// Targeting ../IndexError.java


// Targeting ../ValueError.java


// Targeting ../TypeError.java


// Targeting ../NotImplementedError.java


// Targeting ../EnforceFiniteError.java


// Targeting ../OnnxfiBackendSystemError.java


// Targeting ../LinAlgError.java


// Targeting ../OutOfMemoryError.java


// Targeting ../DistBackendError.java



// A utility function to return an exception std::string by prepending its
// exception type before its what() content
@Namespace("c10") public static native @StdString BytePointer GetExceptionString(@Cast("const std::exception*") @ByRef Pointer e);

 // namespace c10

// Private helper macro for implementing TORCH_INTERNAL_ASSERT and TORCH_CHECK
//
// Note: In the debug build With MSVC, __LINE__ might be of long type (a.k.a
// int32_t), which is different from the definition of `SourceLocation` that
// requires unsigned int (a.k.a uint32_t) and may cause a compile error with the
// message: error C2397: conversion from 'long' to 'uint32_t' requires a
// narrowing conversion Here the static cast is used to pass the build. if this
// is used inside a lambda the __func__ macro expands to operator(), which isn't
// very useful, but hard to fix in a macro so suppressing the warning.
// #define C10_THROW_ERROR(err_type, msg)
//   throw ::c10::err_type(
//       {__func__, __FILE__, static_cast<uint32_t>(__LINE__)}, msg)

// Private helper macro for workaround MSVC misexpansion of nested macro
// invocations involving __VA_ARGS__.  See
// https://stackoverflow.com/questions/5134523/msvc-doesnt-expand-va-args-correctly
// #define C10_EXPAND_MSVC_WORKAROUND(x) x

// On nvcc, C10_UNLIKELY thwarts missing return statement analysis.  In cases
// where the unlikely expression may be a constant, use this macro to ensure
// return statement analysis keeps working (at the cost of not getting the
// likely/unlikely annotation on nvcc).
// https://github.com/pytorch/pytorch/issues/21418
//
// Currently, this is only used in the error reporting macros below.  If you
// want to use it more generally, move me to Macros.h
//
// TODO: Brian Vaughan observed that we might be able to get this to work on
// nvcc by writing some sort of C++ overload that distinguishes constexpr inputs
// from non-constexpr.  Since there isn't any evidence that losing C10_UNLIKELY
// in nvcc is causing us perf problems, this is not yet implemented, but this
// might be an interesting piece of C++ code for an intrepid bootcamper to
// write.
// #if defined(__CUDACC__)
// #define C10_UNLIKELY_OR_CONST(e) e
// #else
// #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)
// #endif

// ----------------------------------------------------------------------------
// Error reporting macros
// ----------------------------------------------------------------------------

// #ifdef STRIP_ERROR_MESSAGES
// #define TORCH_RETHROW(e, ...) throw
// #else
// #define TORCH_RETHROW(e, ...)
//   do {
//     e.add_context(::c10::str(__VA_ARGS__));
//     throw;
//   } while (false)
// #endif

// A utility macro to provide assert()-like functionality; that is, enforcement
// of internal invariants in code.  It supports an arbitrary number of extra
// arguments (evaluated only on failure), which will be printed in the assert
// failure message using operator<< (this is useful to print some variables
// which may be useful for debugging.)
//
// Usage:
//    TORCH_INTERNAL_ASSERT(should_be_true);
//    TORCH_INTERNAL_ASSERT(x == 0, "x = ", x);
//
// Assuming no bugs in PyTorch, the conditions tested by this macro should
// always be true; e.g., it should be possible to disable all of these
// conditions without changing observable user behavior.  If you would like to
// do error reporting for user input, please use TORCH_CHECK instead.
//
// NOTE: It is SAFE to use this macro in production code; on failure, this
// simply raises an exception, it does NOT unceremoniously quit the process
// (unlike assert()).
//
// #ifdef STRIP_ERROR_MESSAGES
// #define TORCH_INTERNAL_ASSERT(cond, ...)
//   if (C10_UNLIKELY_OR_CONST(!(cond))) {
//     ::c10::detail::torchCheckFail(
//         __func__,
//         __FILE__,
//         static_cast<uint32_t>(__LINE__),
//         #cond " INTERNAL ASSERT FAILED at " C10_STRINGIZE(__FILE__));
//   }
// #else
// It would be nice if we could build a combined string literal out of
// the TORCH_INTERNAL_ASSERT prefix and a user-provided string literal
// as the first argument, but there doesn't seem to be any good way to
// do that while still supporting having a first argument that isn't a
// string literal.
// #define TORCH_INTERNAL_ASSERT(cond, ...)
//   if (C10_UNLIKELY_OR_CONST(!(cond))) {
//     ::c10::detail::torchInternalAssertFail(
//         __func__,
//         __FILE__,
//         static_cast<uint32_t>(__LINE__),
//         #cond
//         " INTERNAL ASSERT FAILED at " C10_STRINGIZE(__FILE__) ":" C10_STRINGIZE(
//             __LINE__) ", please report a bug to PyTorch. ",
//         c10::str(__VA_ARGS__));
//   }
// #endif

// A utility macro to make it easier to test for error conditions from user
// input.  Like TORCH_INTERNAL_ASSERT, it supports an arbitrary number of extra
// arguments (evaluated only on failure), which will be printed in the error
// message using operator<< (e.g., you can pass any object which has
// operator<< defined.  Most objects in PyTorch have these definitions!)
//
// Usage:
//    TORCH_CHECK(should_be_true); // A default error message will be provided
//                                 // in this case; but we recommend writing an
//                                 // explicit error message, as it is more
//                                 // user friendly.
//    TORCH_CHECK(x == 0, "Expected x to be 0, but got ", x);
//
// On failure, this macro will raise an exception.  If this exception propagates
// to Python, it will convert into a Python RuntimeError.
//
// NOTE: It is SAFE to use this macro in production code; on failure, this
// simply raises an exception, it does NOT unceremoniously quit the process
// (unlike CHECK() from glog.)
//
// #define TORCH_CHECK_WITH(error_t, cond, ...)
//   TORCH_CHECK_WITH_MSG(error_t, cond, "", __VA_ARGS__)

// #ifdef STRIP_ERROR_MESSAGES
// #define TORCH_CHECK_MSG(cond, type, ...)
//   (#cond #type " CHECK FAILED at " C10_STRINGIZE(__FILE__))
// #define TORCH_CHECK_WITH_MSG(error_t, cond, type, ...)
//   if (C10_UNLIKELY_OR_CONST(!(cond))) {
//     C10_THROW_ERROR(Error, TORCH_CHECK_MSG(cond, type, __VA_ARGS__));
//   }
// #else
@Namespace("c10::detail") public static native @Cast("const char*") BytePointer torchCheckMsgImpl(@Cast("const char*") BytePointer msg);
@Namespace("c10::detail") public static native String torchCheckMsgImpl(String msg);
// If there is just 1 user-provided C-string argument, use it.
@Namespace("c10::detail") public static native @Cast("const char*") BytePointer torchCheckMsgImpl(
    @Cast("const char*") BytePointer arg0,
    @Cast("const char*") BytePointer args);
@Namespace("c10::detail") public static native String torchCheckMsgImpl(
    String arg0,
    String args);
 // namespace detail
 // namespace c10

// #define TORCH_CHECK_MSG(cond, type, ...)
//   (::c10::detail::torchCheckMsgImpl(
//       "Expected " #cond
//       " to be true, but got false.  "
//       "(Could this error message be improved?  If so, "
//       "please report an enhancement request to PyTorch.)",
//       ##__VA_ARGS__))
// #define TORCH_CHECK_WITH_MSG(error_t, cond, type, ...)
//   if (C10_UNLIKELY_OR_CONST(!(cond))) {
//     C10_THROW_ERROR(error_t, TORCH_CHECK_MSG(cond, type, __VA_ARGS__));
//   }
// #endif

@Namespace("c10::detail") public static native void torchCheckFail(
    @Cast("const char*") BytePointer func,
    @Cast("const char*") BytePointer file,
    @Cast("uint32_t") int line,
    @StdString BytePointer msg);
@Namespace("c10::detail") public static native void torchCheckFail(
    String func,
    String file,
    @Cast("uint32_t") int line,
    @StdString String msg);

// The c10::str() call that creates userMsg can have 1 of 3 return
// types depending on the number and types of arguments passed to
// TORCH_INTERNAL_ASSERT.  0 arguments will get a
// CompileTimeEmptyString, 1 const char * will be passed straight
// through, and anything else will get converted to std::string.
@Namespace("c10::detail") public static native void torchInternalAssertFail(
    @Cast("const char*") BytePointer func,
    @Cast("const char*") BytePointer file,
    @Cast("uint32_t") int line,
    @Cast("const char*") BytePointer condMsg,
    @Cast("const char*") BytePointer userMsg);
@Namespace("c10::detail") public static native void torchInternalAssertFail(
    String func,
    String file,
    @Cast("uint32_t") int line,
    String condMsg,
    String userMsg);
@Namespace("c10::detail") public static native void torchInternalAssertFail(
    @Cast("const char*") BytePointer func,
    @Cast("const char*") BytePointer file,
    @Cast("uint32_t") int line,
    @Cast("const char*") BytePointer condMsg,
    @ByVal CompileTimeEmptyString arg4);
@Namespace("c10::detail") public static native void torchInternalAssertFail(
    String func,
    String file,
    @Cast("uint32_t") int line,
    String condMsg,
    @ByVal CompileTimeEmptyString arg4);

 // namespace detail
 // namespace c10

// #ifdef STRIP_ERROR_MESSAGES
// #define TORCH_CHECK(cond, ...)
//   if (C10_UNLIKELY_OR_CONST(!(cond))) {
//     ::c10::detail::torchCheckFail(
//         __func__,
//         __FILE__,
//         static_cast<uint32_t>(__LINE__),
//         TORCH_CHECK_MSG(cond, "", __VA_ARGS__));
//   }
// #else
// #define TORCH_CHECK(cond, ...)
//   if (C10_UNLIKELY_OR_CONST(!(cond))) {
//     ::c10::detail::torchCheckFail(
//         __func__,
//         __FILE__,
//         static_cast<uint32_t>(__LINE__),
//         TORCH_CHECK_MSG(cond, "", ##__VA_ARGS__));
//   }
// #endif

// An utility macro that does what `TORCH_CHECK` does if compiled in the host
// code, otherwise does nothing. Supposed to be used in the code shared between
// host and device code as an alternative for `TORCH_CHECK`.
// #if defined(__CUDACC__) || defined(__HIPCC__)
// #else
// #define TORCH_CHECK_IF_NOT_ON_CUDA(cond, ...) TORCH_CHECK(cond, ##__VA_ARGS__)
// #endif

// Debug only version of TORCH_INTERNAL_ASSERT. This macro only checks in debug
// build, and does nothing in release build.  It is appropriate to use
// in situations where you want to add an assert to a hotpath, but it is
// too expensive to run this assert on production builds.
// #ifdef NDEBUG
// Optimized version - generates no code.
// #define TORCH_INTERNAL_ASSERT_DEBUG_ONLY(...)
//   while (false)
//   C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(__VA_ARGS__))
// #else
// #define TORCH_INTERNAL_ASSERT_DEBUG_ONLY(...)
//   C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(__VA_ARGS__))
// #endif

// TODO: We're going to get a lot of similar looking string literals
// this way; check if this actually affects binary size.

// Like TORCH_CHECK, but raises LinAlgError instead of Error.
// #define TORCH_CHECK_LINALG(cond, ...)
//   TORCH_CHECK_WITH_MSG(LinAlgError, cond, "LINALG", __VA_ARGS__)

// Like TORCH_CHECK, but raises IndexErrors instead of Errors.
// #define TORCH_CHECK_INDEX(cond, ...)
//   TORCH_CHECK_WITH_MSG(IndexError, cond, "INDEX", __VA_ARGS__)

// Like TORCH_CHECK, but raises ValueErrors instead of Errors.
// #define TORCH_CHECK_VALUE(cond, ...)
//   TORCH_CHECK_WITH_MSG(ValueError, cond, "VALUE", __VA_ARGS__)

// Like TORCH_CHECK, but raises TypeErrors instead of Errors.
// #define TORCH_CHECK_TYPE(cond, ...)
//   TORCH_CHECK_WITH_MSG(TypeError, cond, "TYPE", __VA_ARGS__)

// Like TORCH_CHECK, but raises NotImplementedErrors instead of Errors.
// #define TORCH_CHECK_NOT_IMPLEMENTED(cond, ...)
//   TORCH_CHECK_WITH_MSG(NotImplementedError, cond, "TYPE", __VA_ARGS__)

// #ifdef STRIP_ERROR_MESSAGES
// #define WARNING_MESSAGE_STRING(...)
//   ::c10::detail::CompileTimeEmptyString {}
// #else
// #define WARNING_MESSAGE_STRING(...) ::c10::str(__VA_ARGS__)
// #endif

// Report a warning to the user.  Accepts an arbitrary number of extra
// arguments which are concatenated into the warning message using operator<<
//
// #ifdef DISABLE_WARN
// #define _TORCH_WARN_WITH(...) ((void)0);
// #else
// #define _TORCH_WARN_WITH(warning_t, ...)
//   ::c10::warn(::c10::Warning(
//       warning_t(),
//       {__func__, __FILE__, static_cast<uint32_t>(__LINE__)},
//       WARNING_MESSAGE_STRING(__VA_ARGS__),
//       false));
// #endif

// #define TORCH_WARN(...) _TORCH_WARN_WITH(::c10::UserWarning, __VA_ARGS__);

// #define TORCH_WARN_DEPRECATION(...)
//   _TORCH_WARN_WITH(::c10::DeprecationWarning, __VA_ARGS__);

// Report a warning to the user only once.  Accepts an arbitrary number of extra
// arguments which are concatenated into the warning message using operator<<
//
// #define _TORCH_WARN_ONCE(...)
//   C10_UNUSED static const auto C10_ANONYMOUS_VARIABLE(torch_warn_once_) =
//       [&] {
//         TORCH_WARN(__VA_ARGS__);
//         return true;
//       }()

// #ifdef DISABLE_WARN
// #define TORCH_WARN_ONCE(...) ((void)0);
// #else
// #define TORCH_WARN_ONCE(...)
//   if (::c10::WarningUtils::get_warnAlways()) {
//     TORCH_WARN(__VA_ARGS__);
//   } else {
//     _TORCH_WARN_ONCE(__VA_ARGS__);
//   }
// #endif

// Report an error with a specific argument
// NOTE: using the argument name in TORCH_CHECK's message is preferred
// #define TORCH_CHECK_ARG(cond, argN, ...)
//   TORCH_CHECK(cond, "invalid argument ", argN, ": ", __VA_ARGS__)

// ----------------------------------------------------------------------------
// Deprecated macros
// ----------------------------------------------------------------------------

/*
// Deprecation disabled until we fix sites in our codebase
C10_DEPRECATED_MESSAGE("AT_ERROR(msg) is deprecated, use TORCH_CHECK(false, msg)
instead.")
*/
@Namespace("c10::detail") public static native void deprecated_AT_ERROR();

/*
// Deprecation disabled until we fix sites in our codebase
C10_DEPRECATED_MESSAGE("AT_ASSERT is deprecated, if you mean to indicate an
internal invariant failure, use " \
                       "TORCH_INTERNAL_ASSERT instead; if you mean to do user
error checking, use " \ "TORCH_CHECK.  See
https://github.com/pytorch/pytorch/issues/20287 for more details.")
*/
@Namespace("c10::detail") public static native void deprecated_AT_ASSERT();

/*
// Deprecation disabled until we fix sites in our codebase
C10_DEPRECATED_MESSAGE("AT_ASSERTM is deprecated, if you mean to indicate an
internal invariant failure, use " \
                       "TORCH_INTERNAL_ASSERT instead; if you mean to do user
error checking, use " \ "TORCH_CHECK.  See
https://github.com/pytorch/pytorch/issues/20287 for more details.")
*/
@Namespace("c10::detail") public static native void deprecated_AT_ASSERTM();

 // namespace detail
 // namespace c10

// Deprecated alias; this alias was deprecated because people kept mistakenly
// using it for user error checking.  Use TORCH_INTERNAL_ASSERT or TORCH_CHECK
// instead. See https://github.com/pytorch/pytorch/issues/20287 for more
// details.
// #define AT_ASSERT(...)
//   do {
//     ::c10::detail::deprecated_AT_ASSERT();
//     C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(__VA_ARGS__));
//   } while (false)

// Deprecated alias, like AT_ASSERT.  The new TORCH_INTERNAL_ASSERT macro
// supports both 0-ary and variadic calls, so having a separate
// message-accepting macro is not necessary.
//
// NB: we MUST include cond explicitly here, as MSVC will miscompile the macro
// expansion, shunting all of __VA_ARGS__ to cond.  An alternate workaround
// can be seen at
// https://stackoverflow.com/questions/5134523/msvc-doesnt-expand-va-args-correctly
// #define AT_ASSERTM(cond, ...)
//   do {
//     ::c10::detail::deprecated_AT_ASSERTM();
//     C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__));
//   } while (false)

// Deprecated alias; this alias was deprecated because it represents extra API
// surface that makes it hard for people to understand what macro to use.
// Use TORCH_CHECK(false, ...) or TORCH_INTERNAL_ASSERT(false, ...) to
// unconditionally fail at a line of code.
// #define AT_ERROR(...)
//   do {
//     ::c10::detail::deprecated_AT_ERROR();
//     C10_EXPAND_MSVC_WORKAROUND(TORCH_CHECK(false, ::c10::str(__VA_ARGS__)));
//   } while (false)

// #endif // C10_UTIL_EXCEPTION_H_


// Parsed from c10/util/ArrayRef.h

//===--- ArrayRef.h - Array Reference Wrapper -------------------*- C++ -*-===//
//
//                     The LLVM Compiler Infrastructure
//
// This file is distributed under the University of Illinois Open Source
// License. See LICENSE.TXT for details.
//
//===----------------------------------------------------------------------===//

// ATen: modified from llvm::ArrayRef.
// removed llvm-specific functionality
// removed some implicit const -> non-const conversions that rely on
// complicated std::enable_if meta-programming
// removed a bunch of slice variants for simplicity...

// #pragma once

// #include <c10/util/C++17.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Exception.h>
// #include <c10/util/SmallVector.h>

// #include <array>
// #include <iterator>
// #include <vector>
// Targeting ../ByteArrayRef.java


// Targeting ../ShortArrayRef.java


// Targeting ../IntArrayRef.java


// Targeting ../LongArrayRef.java


// Targeting ../FloatArrayRef.java


// Targeting ../DoubleArrayRef.java


// Targeting ../SizeTArrayRef.java


// Targeting ../SymIntRef.java


// Targeting ../SymNodeRef.java


// Targeting ../StringArrayRef.java


// Targeting ../BoolArrayRef.java


// Targeting ../HalfArrayRef.java


// Targeting ../BFloat16ArrayRef.java


// Targeting ../FloatComplexrrayRef.java


// Targeting ../DoubleComplexrrayRef.java


// Targeting ../ScalarTypeArrayRef.java


// Targeting ../IValueArrayRef.java


// Targeting ../EnumNameValueArrayRef.java


// Targeting ../TypeArrayRef.java


// Targeting ../SymbolArrayRef.java


// Targeting ../StrideArrayRef.java


// Targeting ../DimnameArrayRef.java


// Targeting ../ScalarArrayRef.java


// Targeting ../TensorArrayRef.java


// Targeting ../TensorArgArrayRef.java


// Targeting ../TensorIndexArrayRef.java


// Targeting ../TensorOptionalArrayRef.java


// Targeting ../SavedVariableArrayRef.java


// Targeting ../SugaredValueArrayRef.java


// Targeting ../NamedValueArrayRef.java


// Targeting ../BlockArrayRef.java


// Targeting ../ValueArrayRef.java





/** \name ArrayRef Convenience constructors
 *  \{
 <p>
 *  Construct an ArrayRef from a single element. */

/** Construct an ArrayRef from a pointer and length. */

/** Construct an ArrayRef from a range. */

/** Construct an ArrayRef from a SmallVector. */

/** Construct an ArrayRef from a SmallVector. */

/** Construct an ArrayRef from a std::vector. */

/** Construct an ArrayRef from a std::array. */

/** Construct an ArrayRef from an ArrayRef (no-op) (const) */

/** Construct an ArrayRef from an ArrayRef (no-op) */

/** Construct an ArrayRef from a C array. */

// WARNING: Template instantiation will NOT be willing to do an implicit
// conversions to get you to an c10::ArrayRef, which is why we need so
// many overloads.













// This alias is deprecated because it doesn't make ownership
// semantics obvious.  Use IntArrayRef instead!
 // namespace c10


// Parsed from c10/util/complex.h

// #pragma once

// #include <complex>

// #include <c10/macros/Macros.h>

// #if defined(__CUDACC__) || defined(__HIPCC__)
// #endif

// #if C10_CLANG_HAS_WARNING("-Wimplicit-float-conversion")
// #endif
// #if C10_CLANG_HAS_WARNING("-Wfloat-conversion")
// #endif

// c10::complex is an implementation of complex numbers that aims
// to work on all devices supported by PyTorch
//
// Most of the APIs duplicates std::complex
// Reference: https://en.cppreference.com/w/cpp/numeric/complex
//
// [NOTE: Complex Operator Unification]
// Operators currently use a mix of std::complex, thrust::complex, and
// c10::complex internally. The end state is that all operators will use
// c10::complex internally.  Until then, there may be some hacks to support all
// variants.
//
//
// [Note on Constructors]
//
// The APIs of constructors are mostly copied from C++ standard:
//   https://en.cppreference.com/w/cpp/numeric/complex/complex
//
// Since C++14, all constructors are constexpr in std::complex
//
// There are three types of constructors:
// - initializing from real and imag:
//     `constexpr complex( const T& re = T(), const T& im = T() );`
// - implicitly-declared copy constructor
// - converting constructors
//
// Converting constructors:
// - std::complex defines converting constructor between float/double/long
// double,
//   while we define converting constructor between float/double.
// - For these converting constructors, upcasting is implicit, downcasting is
//   explicit.
// - We also define explicit casting from std::complex/thrust::complex
//   - Note that the conversion from thrust is not constexpr, because
//     thrust does not define them as constexpr ????
//
//
// [Operator =]
//
// The APIs of operator = are mostly copied from C++ standard:
//   https://en.cppreference.com/w/cpp/numeric/complex/operator%3D
//
// Since C++20, all operator= are constexpr. Although we are not building with
// C++20, we also obey this behavior.
//
// There are three types of assign operator:
// - Assign a real value from the same scalar type
//   - In std, this is templated as complex& operator=(const T& x)
//     with specialization `complex& operator=(T x)` for float/double/long
//     double Since we only support float and double, on will use `complex&
//     operator=(T x)`
// - Copy assignment operator and converting assignment operator
//   - There is no specialization of converting assignment operators, which type
//   is
//     convertible is solely dependent on whether the scalar type is convertible
//
// In addition to the standard assignment, we also provide assignment operators
// with std and thrust
//
//
// [Casting operators]
//
// std::complex does not have casting operators. We define casting operators
// casting to std::complex and thrust::complex
//
//
// [Operator ""]
//
// std::complex has custom literals `i`, `if` and `il` defined in namespace
// `std::literals::complex_literals`. We define our own custom literals in the
// namespace `c10::complex_literals`. Our custom literals does not follow the
// same behavior as in std::complex, instead, we define _if, _id to construct
// float/double complex literals.
//
//
// [real() and imag()]
//
// In C++20, there are two overload of these functions, one it to return the
// real/imag, another is to set real/imag, they are both constexpr. We follow
// this design.
//
//
// [Operator +=,-=,*=,/=]
//
// Since C++20, these operators become constexpr. In our implementation, they
// are also constexpr.
//
// There are two types of such operators: operating with a real number, or
// operating with another complex number. For the operating with a real number,
// the generic template form has argument type `const T &`, while the overload
// for float/double/long double has `T`. We will follow the same type as
// float/double/long double in std.
//
// [Unary operator +-]
//
// Since C++20, they are constexpr. We also make them expr
//
// [Binary operators +-*/]
//
// Each operator has three versions (taking + as example):
// - complex + complex
// - complex + real
// - real + complex
//
// [Operator ==, !=]
//
// Each operator has three versions (taking == as example):
// - complex == complex
// - complex == real
// - real == complex
//
// Some of them are removed on C++20, but we decide to keep them
//
// [Operator <<, >>]
//
// These are implemented by casting to std::complex
//
//
//
// TODO(@zasdfgbnm): c10::complex<c10::Half> is not currently supported,
// because:
//  - lots of members and functions of c10::Half are not constexpr
//  - thrust::complex only support float and double









 // namespace complex_literals

// Define operators between integral scalars and c10::complex. std::complex does
// not support this when T is a floating-point number. This is useful because it
// saves a lot of "static_cast" when operate a complex and an integer. This
// makes the code both less verbose and potentially more efficient.
// #define COMPLEX_INTEGER_OP_TEMPLATE_CONDITION
//   typename std::enable_if_t<
//       std::is_floating_point<fT>::value && std::is_integral<iT>::value,
//       int> = 0

// #undef COMPLEX_INTEGER_OP_TEMPLATE_CONDITION















 // namespace c10

// std functions
//
// The implementation of these functions also follow the design of C++20

// #if defined(USE_ROCM)
// #else
// #define ROCm_Bug(x) x
// #endif

// #undef ROCm_Bug

// For std::conj, there are other versions of it:
//   constexpr std::complex<float> conj( float z );
//   template< class DoubleOrInteger >
//   constexpr std::complex<double> conj( DoubleOrInteger z );
//   constexpr std::complex<long double> conj( long double z );
// These are not implemented
// TODO(@zasdfgbnm): implement them as c10::conj

// Thrust does not have complex --> complex version of thrust::proj,
// so this function is not implemented at c10 right now.
// TODO(@zasdfgbnm): implement it by ourselves

// There is no c10 version of std::polar, because std::polar always
// returns std::complex. Use c10::polar instead;

 // namespace std

 // namespace c10

// #define C10_INTERNAL_INCLUDE_COMPLEX_REMAINING_H
// math functions are included in a separate file
// #include <c10/util/complex_math.h> // IWYU pragma: keep
// utilities for complex types
// #include <c10/util/complex_utils.h> // IWYU pragma: keep
// #undef C10_INTERNAL_INCLUDE_COMPLEX_REMAINING_H


// Parsed from c10/util/Half.h

// #pragma once

/** Defines the Half type (half-precision floating-point) including conversions
 *  to standard C types and basic arithmetic operations. Note that arithmetic
 *  operations are implemented by converting to floating point and
 *  performing the operation in float32, instead of using CUDA half intrinsics.
 *  Most uses of this type within ATen are memory bound, including the
 *  element-wise kernels, and the half intrinsics aren't efficient on all GPUs.
 *  If you are writing a compute bound kernel, you can use the CUDA half
 *  intrinsics directly on the Half type from device code. */

// #include <c10/macros/Macros.h>
// #include <c10/util/C++17.h>
// #include <c10/util/TypeSafeSignMath.h>
// #include <c10/util/complex.h>
// #include <type_traits>

// #if defined(__cplusplus) && (__cplusplus >= 201103L)
// #include <cmath>
// #include <cstdint>
// #elif !defined(__OPENCL_VERSION__)
// #include <math.h>
// #include <stdint.h>
// #endif

// #ifdef _MSC_VER
// #include <intrin.h>
// #endif

// #include <complex>
// #include <cstdint>
// #include <cstring>
// #include <iosfwd>
// #include <limits>
// #include <sstream>
// #include <stdexcept>
// #include <string>
// #include <utility>

// #ifdef __CUDACC__
// #include <cuda_fp16.h>
// #endif

// #ifdef __HIPCC__
// #include <hip/hip_fp16.h>
// #endif

// #if defined(CL_SYCL_LANGUAGE_VERSION)
// #include <CL/sycl.hpp> // for SYCL 1.2.1
// #elif defined(SYCL_LANGUAGE_VERSION)
// #include <sycl/sycl.hpp> // for SYCL 2020
// #endif

// Standard check for compiling CUDA with clang
// #if defined(__clang__) && defined(__CUDA__) && defined(__CUDA_ARCH__)
// #define C10_DEVICE_HOST_FUNCTION __device__ __host__
// #else
// #define C10_DEVICE_HOST_FUNCTION
// #endif

// #include <typeinfo> // operator typeid

@Namespace("c10::detail") public static native float fp32_from_bits(@Cast("uint32_t") int w);

@Namespace("c10::detail") public static native @Cast("uint32_t") int fp32_to_bits(float f);

/*
 * Convert a 16-bit floating-point number in IEEE half-precision format, in bit
 * representation, to a 32-bit floating-point number in IEEE single-precision
 * format, in bit representation.
 *
 * @note The implementation doesn't use any floating-point operations.
 */
@Namespace("c10::detail") public static native @Cast("uint32_t") int fp16_ieee_to_fp32_bits(@Cast("uint16_t") short h);

/*
 * Convert a 16-bit floating-point number in IEEE half-precision format, in bit
 * representation, to a 32-bit floating-point number in IEEE single-precision
 * format.
 *
 * @note The implementation relies on IEEE-like (no assumption about rounding
 * mode and no operations on denormals) floating-point operations and bitcasts
 * between integer and floating-point variables.
 */
@Namespace("c10::detail") public static native float fp16_ieee_to_fp32_value(@Cast("uint16_t") short h);

/*
 * Convert a 32-bit floating-point number in IEEE single-precision format to a
 * 16-bit floating-point number in IEEE half-precision format, in bit
 * representation.
 *
 * @note The implementation relies on IEEE-like (no assumption about rounding
 * mode and no operations on denormals) floating-point operations and bitcasts
 * between integer and floating-point variables.
 */
@Namespace("c10::detail") public static native @Cast("uint16_t") short fp16_ieee_from_fp32_value(float f);


// Targeting ../Half.java



// TODO : move to complex.h

// In some versions of MSVC, there will be a compiler error when building.
// C4146: unary minus operator applied to unsigned type, result still unsigned
// C4804: unsafe use of type 'bool' in operation
// It can be addressed by disabling the following warning.
// #ifdef _MSC_VER
// #pragma warning(push)
// #pragma warning(disable : 4146)
// #pragma warning(disable : 4804)
// #pragma warning(disable : 4018)
// #endif

// The overflow checks may involve float to int conversion which may
// trigger precision loss warning. Re-enable the warning once the code
// is fixed. See T58053069.
// #ifdef __clang__
// #pragma GCC diagnostic push
// #pragma GCC diagnostic ignored "-Wunknown-warning-option"
// #pragma GCC diagnostic ignored "-Wimplicit-int-float-conversion"
// #endif

// bool can be converted to any type.
// Without specializing on bool, in pytorch_linux_trusty_py2_7_9_build:
// `error: comparison of constant '255' with boolean expression is always false`
// for `f > limit::max()` below

// skip isnan and isinf check for integral types

// #ifdef __clang__
// #pragma GCC diagnostic pop
// #endif

// #ifdef _MSC_VER
// #pragma warning(pop)
// #endif



 // namespace c10

// #include <c10/util/Half-inl.h> // IWYU pragma: keep


// Parsed from c10/util/qint32.h

// #pragma once
// #include <cstdint>

// #include <c10/macros/Macros.h>
// Targeting ../qint32.java



 // namespace c10


// Parsed from c10/util/qint8.h

// #pragma once
// #include <cstdint>

// #include <c10/macros/Macros.h>
// Targeting ../qint8.java



 // namespace c10


// Parsed from c10/util/quint8.h

// #pragma once
// #include <cstdint>

// #include <c10/macros/Macros.h>
// Targeting ../quint8.java



 // namespace c10


// Parsed from c10/util/BFloat16.h

// #pragma once

// Defines the bloat16 type (brain floating-point). This representation uses
// 1 bit for the sign, 8 bits for the exponent and 7 bits for the mantissa.

// #include <c10/macros/Macros.h>
// #include <cmath>
// #include <cstring>

// #if defined(__CUDACC__) && !defined(USE_ROCM)
// #endif

// #if defined(SYCL_EXT_ONEAPI_BFLOAT16_MATH_FUNCTIONS)
// #endif
@Namespace("c10::detail") public static native float f32_from_bits(@Cast("uint16_t") short src);

@Namespace("c10::detail") public static native @Cast("uint16_t") short bits_from_f32(float src);

@Namespace("c10::detail") public static native @Cast("uint16_t") short round_to_nearest_even(float src);

// Targeting ../BFloat16.java



 // namespace c10

// #include <c10/util/BFloat16-inl.h> // IWYU pragma: keep


// Parsed from c10/util/quint2x4.h

// #pragma once
// #include <cstdint>

// #include <c10/macros/Macros.h>
// Targeting ../quint2x4.java



 // namespace c10


// Parsed from c10/util/quint4x2.h

// #pragma once
// #include <cstdint>

// #include <c10/macros/Macros.h>
// Targeting ../quint4x2.java



 // namespace c10


// Parsed from c10/util/ThreadLocalDebugInfo.h

// #pragma once

// #include <c10/macros/Export.h>

// #include <memory>
// #include <string>

@Namespace("c10") public enum DebugInfoKind {
  PRODUCER_INFO((byte)(0)),
  MOBILE_RUNTIME_INFO((byte)(1)),
  PROFILER_STATE((byte)(2)),
  INFERENCE_CONTEXT((byte)(3)), // for inference usage
  PARAM_COMMS_INFO((byte)(4)),

  TEST_INFO((byte)(5)), // used only in tests
  TEST_INFO_2((byte)(6));// used only in tests

    public final byte value;
    private DebugInfoKind(byte v) { this.value = v; }
    private DebugInfoKind(DebugInfoKind e) { this.value = e.value; }
    public DebugInfoKind intern() { for (DebugInfoKind e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../DebugInfoBase.java


// Targeting ../ThreadLocalDebugInfo.java


// Targeting ../DebugInfoGuard.java



 // namespace c10


// Parsed from c10/util/Type.h

// #ifndef C10_UTIL_TYPE_H_
// #define C10_UTIL_TYPE_H_

// #include <cstddef>
// #include <string>
// #include <typeinfo>

// #include <c10/macros/Macros.h>

/** Utility to demangle a C++ symbol name. */
@Namespace("c10") public static native @StdString BytePointer demangle(@Cast("const char*") BytePointer name);
@Namespace("c10") public static native @StdString String demangle(String name);

/** Returns the printable name of the type. */

 // namespace c10

// #endif // C10_UTIL_TYPE_H_


// Parsed from c10/util/TypeCast.h

// #pragma once
// #include <c10/macros/Macros.h>
// #include <c10/util/BFloat16.h>
// #include <c10/util/Half.h>

// #include <type_traits>

// #if C10_CLANG_HAS_WARNING("-Wimplicit-float-conversion")
// #endif
// #if C10_CLANG_HAS_WARNING("-Wimplicit-int-float-conversion")
// #endif

// Note: deliberately ignores undefined behavior, consistent with NumPy.
// PyTorch's type conversions can cause a variety of undefined behavior,
// including float to integral overflow and signed to unsigned integer overflow.
// Some of this undefined behavior is addressed below.
// Targeting ../static_cast_with_inter_type.java



// Define separately to avoid being inlined and prevent code-size bloat
@Namespace("c10") public static native void report_overflow(@Cast("const char*") BytePointer name);
@Namespace("c10") public static native void report_overflow(String name);

 // namespace c10

// Trigger tests for D25440771. TODO: Remove this line any time you want.


// Parsed from c10/util/Registry.h

// #ifndef C10_UTIL_REGISTRY_H_
// #define C10_UTIL_REGISTRY_H_

/**
 * Simple registry implementation that uses static variables to
 * register object creators during program initialization time.
 */

// NB: This Registry works poorly when you have other namespaces.
// Make all macro invocations from inside the at namespace.

// #include <algorithm>
// #include <cstdio>
// #include <cstdlib>
// #include <functional>
// #include <memory>
// #include <mutex>
// #include <string>
// #include <unordered_map>
// #include <vector>

// #include <c10/macros/Macros.h>
// #include <c10/util/Type.h>

@Namespace("c10") public static native @StdString BytePointer KeyStrRepr(@StdString BytePointer key);
@Namespace("c10") public static native @StdString String KeyStrRepr(@StdString String key);

@Namespace("c10") public enum RegistryPriority {
  REGISTRY_FALLBACK(1),
  REGISTRY_DEFAULT(2),
  REGISTRY_PREFERRED(3);

    public final int value;
    private RegistryPriority(int v) { this.value = v; }
    private RegistryPriority(RegistryPriority e) { this.value = e.value; }
    public RegistryPriority intern() { for (RegistryPriority e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

/**
 * \brief A template class that allows one to register classes by keys.
 *
 * The keys are usually a std::string specifying the name, but can be anything
 * that can be used in a std::map.
 *
 * You should most likely not use the Registry class explicitly, but use the
 * helper macros below to declare specific registries as well as registering
 * objects.
 */

/**
 * C10_DECLARE_TYPED_REGISTRY is a macro that expands to a function
 * declaration, as well as creating a convenient typename for its corresponding
 * registerer.
 */
// Note on C10_IMPORT and C10_EXPORT below: we need to explicitly mark DECLARE
// as import and DEFINE as export, because these registry macros will be used
// in downstream shared libraries as well, and one cannot use *_API - the API
// macro will be defined on a per-shared-library basis. Semantically, when one
// declares a typed registry it is always going to be IMPORT, and when one
// defines a registry (which should happen ONLY ONCE and ONLY IN SOURCE FILE),
// the instantiation unit is always going to be exported.
//
// The only unique condition is when in the same file one does DECLARE and
// DEFINE - in Windows compilers, this generates a warning that dllimport and
// dllexport are mixed, but the warning is fine and linker will be properly
// exporting the symbol. Same thing happens in the gflags flag declaration and
// definition caes.
// #define C10_DECLARE_TYPED_REGISTRY(
//     RegistryName, SrcType, ObjectType, PtrType, ...)
//   C10_IMPORT ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>*
//   RegistryName();
//   typedef ::c10::Registerer<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>
//       Registerer##RegistryName

// #define C10_DEFINE_TYPED_REGISTRY(
//     RegistryName, SrcType, ObjectType, PtrType, ...)
//   C10_EXPORT ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>*
//   RegistryName() {
//     static ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>*
//         registry = new ::c10::
//             Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>();
//     return registry;
//   }

// #define C10_DEFINE_TYPED_REGISTRY_WITHOUT_WARNING(
//     RegistryName, SrcType, ObjectType, PtrType, ...)
//   C10_EXPORT ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>*
//   RegistryName() {
//     static ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>*
//         registry =
//             new ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>(
//                 false);
//     return registry;
//   }

// Note(Yangqing): The __VA_ARGS__ below allows one to specify a templated
// creator with comma in its templated arguments.
// #define C10_REGISTER_TYPED_CREATOR(RegistryName, key, ...)
//   static Registerer##RegistryName C10_ANONYMOUS_VARIABLE(g_##RegistryName)(
//       key, RegistryName(), ##__VA_ARGS__);

// #define C10_REGISTER_TYPED_CREATOR_WITH_PRIORITY(
//     RegistryName, key, priority, ...)
//   static Registerer##RegistryName C10_ANONYMOUS_VARIABLE(g_##RegistryName)(
//       key, priority, RegistryName(), ##__VA_ARGS__);

// #define C10_REGISTER_TYPED_CLASS(RegistryName, key, ...)
//   static Registerer##RegistryName C10_ANONYMOUS_VARIABLE(g_##RegistryName)(
//       key,
//       RegistryName(),
//       Registerer##RegistryName::DefaultCreator<__VA_ARGS__>,
//       ::c10::demangle_type<__VA_ARGS__>());

// #define C10_REGISTER_TYPED_CLASS_WITH_PRIORITY(
//     RegistryName, key, priority, ...)
//   static Registerer##RegistryName C10_ANONYMOUS_VARIABLE(g_##RegistryName)(
//       key,
//       priority,
//       RegistryName(),
//       Registerer##RegistryName::DefaultCreator<__VA_ARGS__>,
//       ::c10::demangle_type<__VA_ARGS__>());

// C10_DECLARE_REGISTRY and C10_DEFINE_REGISTRY are hard-wired to use
// std::string as the key type, because that is the most commonly used cases.
// #define C10_DECLARE_REGISTRY(RegistryName, ObjectType, ...)
//   C10_DECLARE_TYPED_REGISTRY(
//       RegistryName, std::string, ObjectType, std::unique_ptr, ##__VA_ARGS__)

// #define C10_DEFINE_REGISTRY(RegistryName, ObjectType, ...)
//   C10_DEFINE_TYPED_REGISTRY(
//       RegistryName, std::string, ObjectType, std::unique_ptr, ##__VA_ARGS__)

// #define C10_DEFINE_REGISTRY_WITHOUT_WARNING(RegistryName, ObjectType, ...)
//   C10_DEFINE_TYPED_REGISTRY_WITHOUT_WARNING(
//       RegistryName, std::string, ObjectType, std::unique_ptr, ##__VA_ARGS__)

// #define C10_DECLARE_SHARED_REGISTRY(RegistryName, ObjectType, ...)
//   C10_DECLARE_TYPED_REGISTRY(
//       RegistryName, std::string, ObjectType, std::shared_ptr, ##__VA_ARGS__)

// #define C10_DEFINE_SHARED_REGISTRY(RegistryName, ObjectType, ...)
//   C10_DEFINE_TYPED_REGISTRY(
//       RegistryName, std::string, ObjectType, std::shared_ptr, ##__VA_ARGS__)

// #define C10_DEFINE_SHARED_REGISTRY_WITHOUT_WARNING(
//     RegistryName, ObjectType, ...)
//   C10_DEFINE_TYPED_REGISTRY_WITHOUT_WARNING(
//       RegistryName, std::string, ObjectType, std::shared_ptr, ##__VA_ARGS__)

// C10_REGISTER_CREATOR and C10_REGISTER_CLASS are hard-wired to use std::string
// as the key
// type, because that is the most commonly used cases.
// #define C10_REGISTER_CREATOR(RegistryName, key, ...)
//   C10_REGISTER_TYPED_CREATOR(RegistryName, #key, __VA_ARGS__)

// #define C10_REGISTER_CREATOR_WITH_PRIORITY(RegistryName, key, priority, ...)
//   C10_REGISTER_TYPED_CREATOR_WITH_PRIORITY(
//       RegistryName, #key, priority, __VA_ARGS__)

// #define C10_REGISTER_CLASS(RegistryName, key, ...)
//   C10_REGISTER_TYPED_CLASS(RegistryName, #key, __VA_ARGS__)

// #define C10_REGISTER_CLASS_WITH_PRIORITY(RegistryName, key, priority, ...)
//   C10_REGISTER_TYPED_CLASS_WITH_PRIORITY(
//       RegistryName, #key, priority, __VA_ARGS__)

 // namespace c10

// #endif // C10_UTIL_REGISTRY_H_


// Parsed from c10/util/Flags.h

// #ifndef C10_UTIL_FLAGS_H_
// #define C10_UTIL_FLAGS_H_

/* Commandline flags support for C10.
 *
 * This is a portable commandline flags tool for c10, so we can optionally
 * choose to use gflags or a lightweight custom implementation if gflags is
 * not possible on a certain platform. If you have gflags installed, set the
 * macro C10_USE_GFLAGS will seamlessly route everything to gflags.
 *
 * To define a flag foo of type bool default to true, do the following in the
 * *global* namespace:
 *     C10_DEFINE_bool(foo, true, "An example.");
 *
 * To use it in another .cc file, you can use C10_DECLARE_* as follows:
 *     C10_DECLARE_bool(foo);
 *
 * In both cases, you can then access the flag via FLAGS_foo.
 *
 * It is recommended that you build with gflags. To learn more about the flags
 * usage, refer to the gflags page here:
 *
 * https://gflags.github.io/gflags/
 *
 * Note about Python users / devs: gflags is initiated from a C++ function
 * ParseCommandLineFlags, and is usually done in native binaries in the main
 * function. As Python does not have a modifiable main function, it is usually
 * difficult to change the flags after Python starts. Hence, it is recommended
 * that one sets the default value of the flags to one that's acceptable in
 * general - that will allow Python to run without wrong flags.
 */

// #include <string>

// #include <c10/macros/Macros.h>
// #include <c10/util/Registry.h>
/**
 * Sets the usage message when a commandline tool is called with "--help".
 */
@Namespace("c10") public static native void SetUsageMessage(@StdString BytePointer str);
@Namespace("c10") public static native void SetUsageMessage(@StdString String str);

/**
 * Returns the usage message for the commandline tool set by SetUsageMessage.
 */
@Namespace("c10") public static native @Cast("const char*") BytePointer UsageMessage();

/**
 * Parses the commandline flags.
 *
 * This command parses all the commandline arguments passed in via pargc
 * and argv. Once it is finished, partc and argv will contain the remaining
 * commandline args that c10 does not deal with. Note that following
 * convention, argv[0] contains the binary name and is not parsed.
 */
@Namespace("c10") public static native @Cast("bool") boolean ParseCommandLineFlags(IntPointer pargc, @Cast("char***") @ByPtrPtr PointerPointer pargv);
@Namespace("c10") public static native @Cast("bool") boolean ParseCommandLineFlags(IntBuffer pargc, @Cast("char***") @ByPtrPtr PointerPointer pargv);
@Namespace("c10") public static native @Cast("bool") boolean ParseCommandLineFlags(int[] pargc, @Cast("char***") @ByPtrPtr PointerPointer pargv);

/**
 * Checks if the commandline flags has already been passed.
 */
@Namespace("c10") public static native @Cast("bool") boolean CommandLineFlagsHasBeenParsed();

 // namespace c10

////////////////////////////////////////////////////////////////////////////////
// Below are gflags and non-gflags specific implementations.
// In general, they define the following macros for one to declare (use
// C10_DECLARE) or define (use C10_DEFINE) flags:
// C10_{DECLARE,DEFINE}_{int,int64,double,bool,string}
////////////////////////////////////////////////////////////////////////////////

// #ifdef C10_USE_GFLAGS

////////////////////////////////////////////////////////////////////////////////
// Begin gflags section: most functions are basically rerouted to gflags.
////////////////////////////////////////////////////////////////////////////////
// #include <gflags/gflags.h>

// C10 uses hidden visibility by default. However, in gflags, it only uses
// export on Windows platform (with dllexport) but not on linux/mac (with
// default visibility). As a result, to ensure that we are always exporting
// global variables, we will redefine the GFLAGS_DLL_DEFINE_FLAG macro if we
// are building C10 as a shared libray.
// This has to be done after the inclusion of gflags, because some early
// versions of gflags.h (e.g. 2.0 on ubuntu 14.04) directly defines the
// macros, so we need to do definition after gflags is done.
// #ifdef GFLAGS_DLL_DEFINE_FLAG
// #endif // GFLAGS_DLL_DEFINE_FLAG
// #ifdef GFLAGS_DLL_DECLARE_FLAG
// #endif // GFLAGS_DLL_DECLARE_FLAG
// #define GFLAGS_DLL_DEFINE_FLAG C10_EXPORT
// #define GFLAGS_DLL_DECLARE_FLAG C10_IMPORT

// gflags before 2.0 uses namespace google and after 2.1 uses namespace gflags.
// Using GFLAGS_GFLAGS_H_ to capture this change.
// #ifndef GFLAGS_GFLAGS_H_
// #endif // GFLAGS_GFLAGS_H_

// Motivation about the gflags wrapper:
// (1) We would need to make sure that the gflags version and the non-gflags
// version of C10 are going to expose the same flags abstraction. One should
// explicitly use FLAGS_flag_name to access the flags.
// (2) For flag names, it is recommended to start with c10_ to distinguish it
// from regular gflags flags. For example, do
//    C10_DEFINE_BOOL(c10_my_flag, true, "An example");
// to allow one to use FLAGS_c10_my_flag.
// (3) Gflags has a design issue that does not properly expose the global flags,
// if one builds the library with -fvisibility=hidden. The current gflags (as of
// Aug 2018) only deals with the Windows case using dllexport, and not the Linux
// counterparts. As a result, we will explciitly use C10_EXPORT to export the
// flags defined in C10. This is done via a global reference, so the flag
// itself is not duplicated - under the hood it is the same global gflags flag.
// #define C10_GFLAGS_DEF_WRAPPER(type, real_type, name, default_value, help_str)
//   DEFINE_##type(name, default_value, help_str);

// #define C10_DEFINE_int(name, default_value, help_str)
//   C10_GFLAGS_DEF_WRAPPER(int32, gflags::int32, name, default_value, help_str)
// #define C10_DEFINE_int32(name, default_value, help_str)
//   C10_DEFINE_int(name, default_value, help_str)
// #define C10_DEFINE_int64(name, default_value, help_str)
//   C10_GFLAGS_DEF_WRAPPER(int64, gflags::int64, name, default_value, help_str)
// #define C10_DEFINE_double(name, default_value, help_str)
//   C10_GFLAGS_DEF_WRAPPER(double, double, name, default_value, help_str)
// #define C10_DEFINE_bool(name, default_value, help_str)
//   C10_GFLAGS_DEF_WRAPPER(bool, bool, name, default_value, help_str)
// #define C10_DEFINE_string(name, default_value, help_str)
//   C10_GFLAGS_DEF_WRAPPER(string, ::fLS::clstring, name, default_value, help_str)

// DECLARE_typed_var should be used in header files and in the global namespace.
// #define C10_GFLAGS_DECLARE_WRAPPER(type, real_type, name) DECLARE_##type(name);

// #define C10_DECLARE_int(name)
//   C10_GFLAGS_DECLARE_WRAPPER(int32, gflags::int32, name)
// #define C10_DECLARE_int32(name) C10_DECLARE_int(name)
// #define C10_DECLARE_int64(name)
//   C10_GFLAGS_DECLARE_WRAPPER(int64, gflags::int64, name)
// #define C10_DECLARE_double(name)
//   C10_GFLAGS_DECLARE_WRAPPER(double, double, name)
// #define C10_DECLARE_bool(name) C10_GFLAGS_DECLARE_WRAPPER(bool, bool, name)
// #define C10_DECLARE_string(name)
//   C10_GFLAGS_DECLARE_WRAPPER(string, ::fLS::clstring, name)
// Targeting ../C10FlagParser.java





 // namespace c10

// The macros are defined outside the c10 namespace. In your code, you should
// write the C10_DEFINE_* and C10_DECLARE_* macros outside any namespace
// as well.

// #define C10_DEFINE_typed_var(type, name, default_value, help_str)
//   C10_EXPORT type FLAGS_##name = default_value;
//   namespace c10 {
//   namespace {
//   class C10FlagParser_##name : public C10FlagParser {
//    public:
//     explicit C10FlagParser_##name(const std::string& content) {
//       success_ = C10FlagParser::Parse<type>(content, &FLAGS_##name);
//     }
//   };
//   }
//   RegistererC10FlagsRegistry g_C10FlagsRegistry_##name(
//       #name,
//       C10FlagsRegistry(),
//       RegistererC10FlagsRegistry::DefaultCreator<C10FlagParser_##name>,
//       "(" #type ", default " #default_value ") " help_str);
//   }

// #define C10_DEFINE_int(name, default_value, help_str)
//   C10_DEFINE_typed_var(int, name, default_value, help_str)
// #define C10_DEFINE_int32(name, default_value, help_str)
//   C10_DEFINE_int(name, default_value, help_str)
// #define C10_DEFINE_int64(name, default_value, help_str)
//   C10_DEFINE_typed_var(int64_t, name, default_value, help_str)
// #define C10_DEFINE_double(name, default_value, help_str)
//   C10_DEFINE_typed_var(double, name, default_value, help_str)
// #define C10_DEFINE_bool(name, default_value, help_str)
//   C10_DEFINE_typed_var(bool, name, default_value, help_str)
// #define C10_DEFINE_string(name, default_value, help_str)
//   C10_DEFINE_typed_var(std::string, name, default_value, help_str)

// DECLARE_typed_var should be used in header files and in the global namespace.
// #define C10_DECLARE_typed_var(type, name) C10_IMPORT extern type FLAGS_##name

// #define C10_DECLARE_int(name) C10_DECLARE_typed_var(int, name)
// #define C10_DECLARE_int32(name) C10_DECLARE_int(name)
// #define C10_DECLARE_int64(name) C10_DECLARE_typed_var(int64_t, name)
// #define C10_DECLARE_double(name) C10_DECLARE_typed_var(double, name)
// #define C10_DECLARE_bool(name) C10_DECLARE_typed_var(bool, name)
// #define C10_DECLARE_string(name) C10_DECLARE_typed_var(std::string, name)

////////////////////////////////////////////////////////////////////////////////
// End non-gflags section.
////////////////////////////////////////////////////////////////////////////////

// #endif // C10_USE_GFLAGS

// #endif // C10_UTIL_FLAGS_H_


// Parsed from c10/util/Logging.h

// #ifndef C10_UTIL_LOGGING_H_
// #define C10_UTIL_LOGGING_H_

// #include <climits>
// #include <exception>
// #include <functional>
// #include <limits>
// #include <sstream>

// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Flags.h>
// #include <c10/util/StringUtil.h>

// CAFFE2_LOG_THRESHOLD is a compile time flag that would allow us to turn off
// logging at compile time so no logging message below that level is produced
// at all. The value should be between INT_MIN and CAFFE_FATAL.
// #ifndef CAFFE2_LOG_THRESHOLD
// If we have not defined the compile time log threshold, we keep all the
// log cases.
public static native @MemberGetter int CAFFE2_LOG_THRESHOLD();
public static final int CAFFE2_LOG_THRESHOLD = CAFFE2_LOG_THRESHOLD();
// #endif // CAFFE2_LOG_THRESHOLD

// Below are different implementations for glog and non-glog cases.
// #ifdef C10_USE_GLOG
// #include <c10/util/logging_is_google_glog.h>
// #else // !C10_USE_GLOG
// #include <c10/util/logging_is_not_google_glog.h>
// #endif // C10_USE_GLOG




// Some versions of GLOG support less-spammy version of LOG_EVERY_MS. If it's
// not available - just short-circuit to the always working one one.
// We define the C10_ name to avoid confusing other files
// #ifdef LOG_EVERY_MS
// #define C10_LOG_EVERY_MS(severity, ms) LOG_EVERY_MS(severity, ms)
// #else
// #define C10_LOG_EVERY_MS(severity, ms) LOG(severity)
// #endif

// Same for LOG_FIRST_N
// #ifdef LOG_FIRST_N
// #define C10_LOG_FIRST_N(severity, n) LOG_FIRST_N(severity, n)
// #else
// #define C10_LOG_FIRST_N(severity, n) LOG(severity)
// #endif

// Same for LOG_EVERY_N
// #ifdef LOG_EVERY_N
// #define C10_LOG_EVERY_N(severity, n) LOG_EVERY_N(severity, n)
// #else
// #define C10_LOG_EVERY_N(severity, n) LOG(severity)
// #endif

// Functions that we use for initialization.
@Namespace("c10") public static native @Cast("bool") boolean InitCaffeLogging(IntPointer argc, @Cast("char**") PointerPointer argv);
@Namespace("c10") public static native @Cast("bool") boolean InitCaffeLogging(IntPointer argc, @Cast("char**") @ByPtrPtr BytePointer argv);
@Namespace("c10") public static native @Cast("bool") boolean InitCaffeLogging(IntBuffer argc, @Cast("char**") @ByPtrPtr ByteBuffer argv);
@Namespace("c10") public static native @Cast("bool") boolean InitCaffeLogging(int[] argc, @Cast("char**") @ByPtrPtr byte[] argv);
@Namespace("c10") public static native void UpdateLoggingLevelsFromFlags();

@Namespace("c10") public static native void ThrowEnforceNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @StdString BytePointer msg,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @StdString BytePointer msg);
@Namespace("c10") public static native void ThrowEnforceNotMet(
    String file,
    int line,
    String condition,
    @StdString String msg,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceNotMet(
    String file,
    int line,
    String condition,
    @StdString String msg);

@Namespace("c10") public static native void ThrowEnforceNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @ByVal CompileTimeEmptyString arg3,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @ByVal CompileTimeEmptyString arg3);
@Namespace("c10") public static native void ThrowEnforceNotMet(
    String file,
    int line,
    String condition,
    @ByVal CompileTimeEmptyString arg3,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceNotMet(
    String file,
    int line,
    String condition,
    @ByVal CompileTimeEmptyString arg3);

@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @StdString BytePointer msg,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @StdString BytePointer msg);
@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    String file,
    int line,
    String condition,
    @StdString String msg,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    String file,
    int line,
    String condition,
    @StdString String msg);

@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @ByVal CompileTimeEmptyString arg3,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @ByVal CompileTimeEmptyString arg3);
@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    String file,
    int line,
    String condition,
    @ByVal CompileTimeEmptyString arg3,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    String file,
    int line,
    String condition,
    @ByVal CompileTimeEmptyString arg3);

@Namespace("c10") public static native @Cast("const bool") boolean IsUsingGoogleLogging();

/**
 * A utility to allow one to show log info to stderr after the program starts.
 *
 * This is similar to calling GLOG's --logtostderr, or setting caffe2_log_level
 * to smaller than INFO. You are recommended to only use this in a few sparse
 * cases, such as when you want to write a tutorial or something. Normally, use
 * the commandline flags to set the log level.
 */
@Namespace("c10") public static native void ShowLogInfoToStderr();

@Namespace("c10") public static native void SetStackTraceFetcher(@ByVal Fetcher fetcher);

// #define CAFFE_ENFORCE(condition, ...)
//   do {
//     if (C10_UNLIKELY(!(condition))) {
//       ::c10::ThrowEnforceNotMet(
//           __FILE__, __LINE__, #condition, ::c10::str(__VA_ARGS__));
//     }
//   } while (false)

// #define CAFFE_ENFORCE_FINITE(condition, ...)
//   do {
//     if (C10_UNLIKELY(!(condition))) {
//       ::c10::ThrowEnforceFiniteNotMet(
//           __FILE__, __LINE__, #condition, ::c10::str(__VA_ARGS__));
//     }
//   } while (false)

// #define CAFFE_ENFORCE_WITH_CALLER(condition, ...)
//   do {
//     if (C10_UNLIKELY(!(condition))) {
//       ::c10::ThrowEnforceNotMet(
//           __FILE__, __LINE__, #condition, ::c10::str(__VA_ARGS__), this);
//     }
//   } while (false)

// #define CAFFE_THROW(...)
//   ::c10::ThrowEnforceNotMet(__FILE__, __LINE__, "", ::c10::str(__VA_ARGS__))

/**
 * Rich logging messages
 *
 * CAFFE_ENFORCE_THAT can be used with one of the "checker functions" that
 * capture input argument values and add it to the exception message. E.g.
 * {@code CAFFE_ENFORCE_THAT(Equals(foo(x), bar(y)), "Optional additional message")}
 * would evaluate both foo and bar only once and if the results are not equal -
 * include them in the exception message.
 *
 * Some of the basic checker functions like Equals or Greater are already
 * defined below. Other header might define customized checkers by adding
 * functions to caffe2::enforce_detail namespace. For example:
 *
 *   namespace caffe2 { namespace enforce_detail {
 *   inline EnforceFailMessage IsVector(const vector<int64_t>& shape) {
 *     if (shape.size() == 1) { return EnforceOK(); }
 *     return c10::str("Shape ", shape, " is not a vector");
 *   }
 *   }}
 *
 * With further usages like {@code CAFFE_ENFORCE_THAT(IsVector(Input(0).dims()))}
 *
 * Convenient wrappers for binary operations like CAFFE_ENFORCE_EQ are provided
 * too. Please use them instead of TORCH_CHECK_EQ and friends for failures in
 * user-provided input.
 */
// #define CAFFE_ENFORCE_THAT_IMPL(op, lhs, rhs, expr, ...)
//   ::c10::enforce_detail::enforceThatImpl(
//       op, lhs, rhs, __FILE__, __LINE__, expr, nullptr, ##__VA_ARGS__)

// #define CAFFE_ENFORCE_THAT_IMPL_WITH_CALLER(op, lhs, rhs, expr, ...)
//   ::c10::enforce_detail::enforceThatImpl(
//       op, (lhs), (rhs), __FILE__, __LINE__, expr, this, ##__VA_ARGS__)

 // namespace enforce_detail

// #define CAFFE_ENFORCE_THAT(cmp, op, lhs, rhs, ...)
//   CAFFE_ENFORCE_THAT_IMPL(cmp, lhs, rhs, #lhs " " #op " " #rhs, ##__VA_ARGS__)

// #define CAFFE_ENFORCE_BINARY_OP(cmp, op, x, y, ...)
//   CAFFE_ENFORCE_THAT_IMPL(cmp, x, y, #x " " #op " " #y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_EQ(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP(std::equal_to<void>(), ==, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_NE(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP(std::not_equal_to<void>(), !=, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_LE(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP(std::less_equal<void>(), <=, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_LT(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP(std::less<void>(), <, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_GE(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP(std::greater_equal<void>(), >=, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_GT(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP(std::greater<void>(), >, x, y, ##__VA_ARGS__)

// #define CAFFE_ENFORCE_BINARY_OP_WITH_CALLER(cmp, op, x, y, ...)
//   CAFFE_ENFORCE_THAT_IMPL_WITH_CALLER(
//       cmp, x, y, #x " " #op " " #y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_EQ_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP_WITH_CALLER(
//       std::equal_to<void>(), ==, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_NE_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP_WITH_CALLER(
//       std::not_equal_to<void>(), !=, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_LE_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP_WITH_CALLER(
//       std::less_equal<void>(), <=, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_LT_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP_WITH_CALLER(std::less<void>(), <, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_GE_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP_WITH_CALLER(
//       std::greater_equal<void>(), >=, x, y, ##__VA_ARGS__)
// #define CAFFE_ENFORCE_GT_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_BINARY_OP_WITH_CALLER(
//       std::greater<void>(), >, x, y, ##__VA_ARGS__)

/**
 * Very lightweight logging for the first time API usage. It's beneficial for
 * tracking of individual functionality usage in larger applications.
 *
 * In order to ensure light-weightedness of logging, we utilize static variable
 * trick - LogAPIUsage will be invoked only once and further invocations will
 * just do an atomic check.
 *
 * Example:
 *   // Logs caller info with an arbitrary text event, if there is a usage.
 *   C10_LOG_API_USAGE_ONCE("my_api");
 */
// #define C10_LOG_API_USAGE_ONCE(...)
//   C10_UNUSED static bool C10_ANONYMOUS_VARIABLE(logFlag) =
//       ::c10::detail::LogAPIUsageFakeReturn(__VA_ARGS__);

// API usage logging capabilities
@Namespace("c10") public static native void SetAPIUsageLogger(@ByVal Logger logger);
@Namespace("c10") public static native void LogAPIUsage(@StdString BytePointer context);
@Namespace("c10") public static native void LogAPIUsage(@StdString String context);
// Targeting ../DDPLoggingData.java



@Namespace("c10") public static native void SetPyTorchDDPUsageLogger(
    @ByVal DataLogger logger);
@Namespace("c10") public static native void LogPyTorchDDPUsage(@Const @ByRef DDPLoggingData ddpData);
// Return value is needed to do the static variable initialization trick
@Namespace("c10::detail") public static native @Cast("bool") boolean LogAPIUsageFakeReturn(@StdString BytePointer context);
@Namespace("c10::detail") public static native @Cast("bool") boolean LogAPIUsageFakeReturn(@StdString String context);
 // namespace detail

// Initializes the c10 logger.
@Namespace("c10") public static native void initLogging();

 // namespace c10

// #endif // C10_UTIL_LOGGING_H_


// Parsed from c10/core/DeviceType.h

// #pragma once

// This is directly synchronized with caffe2/proto/caffe2.proto, but
// doesn't require me to figure out how to get Protobuf headers into
// ATen/core (which would require a lot more build system hacking.)
// If you modify me, keep me synchronized with that file.

// #include <c10/macros/Macros.h>

// #include <functional>
// #include <ostream>

// These contains all device types that also have a BackendComponent
// and therefore participate in per-backend functionality dispatch keys.
// This is most backends except PrivateUse2 and PrivateUse3
// #define C10_FORALL_BACKEND_DEVICE_TYPES(_, extra)
//   _(CPU, extra)
//   _(CUDA, extra)
//   _(HIP, extra)
//   _(XLA, extra)
//   _(MPS, extra)
//   _(IPU, extra)
//   _(XPU, extra)
//   _(HPU, extra)
//   _(VE, extra)
//   _(Lazy, extra)
//   _(Meta, extra)
//   _(MTIA, extra)
//   _(PrivateUse1, extra)

@Namespace("c10") public enum DeviceType {
  CPU((byte)(0)),
  CUDA((byte)(1)), // CUDA.
  MKLDNN((byte)(2)), // Reserved for explicit MKLDNN
  OPENGL((byte)(3)), // OpenGL
  OPENCL((byte)(4)), // OpenCL
  IDEEP((byte)(5)), // IDEEP.
  HIP((byte)(6)), // AMD HIP
  FPGA((byte)(7)), // FPGA
  ORT((byte)(8)), // ONNX Runtime / Microsoft
  XLA((byte)(9)), // XLA / TPU
  Vulkan((byte)(10)), // Vulkan
  Metal((byte)(11)), // Metal
  XPU((byte)(12)), // XPU
  MPS((byte)(13)), // MPS
  Meta((byte)(14)), // Meta (tensors with no data)
  HPU((byte)(15)), // HPU / HABANA
  VE((byte)(16)), // SX-Aurora / NEC
  Lazy((byte)(17)), // Lazy Tensors
  IPU((byte)(18)), // Graphcore IPU
  MTIA((byte)(19)), // Meta training and inference devices
  PrivateUse1((byte)(20)), // PrivateUse1 device
  // NB: If you add more devices:
  //  - Change the implementations of DeviceTypeName and isValidDeviceType
  //    in DeviceType.cpp
  //  - Change the number below
  COMPILE_TIME_MAX_DEVICE_TYPES((byte)(21));

    public final byte value;
    private DeviceType(byte v) { this.value = v; }
    private DeviceType(DeviceType e) { this.value = e.value; }
    public DeviceType intern() { for (DeviceType e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") @MemberGetter public static native DeviceType kCPU();
@Namespace("c10") @MemberGetter public static native DeviceType kCUDA();
@Namespace("c10") @MemberGetter public static native DeviceType kHIP();
@Namespace("c10") @MemberGetter public static native DeviceType kFPGA();
@Namespace("c10") @MemberGetter public static native DeviceType kORT();
@Namespace("c10") @MemberGetter public static native DeviceType kXLA();
@Namespace("c10") @MemberGetter public static native DeviceType kMPS();
@Namespace("c10") @MemberGetter public static native DeviceType kMeta();
@Namespace("c10") @MemberGetter public static native DeviceType kVulkan();
@Namespace("c10") @MemberGetter public static native DeviceType kMetal();
@Namespace("c10") @MemberGetter public static native DeviceType kXPU();
@Namespace("c10") @MemberGetter public static native DeviceType kHPU();
@Namespace("c10") @MemberGetter public static native DeviceType kVE();
@Namespace("c10") @MemberGetter public static native DeviceType kLazy();
@Namespace("c10") @MemberGetter public static native DeviceType kIPU();
@Namespace("c10") @MemberGetter public static native DeviceType kMTIA();
@Namespace("c10") @MemberGetter public static native DeviceType kPrivateUse1();

// define explicit int constant
@Namespace("c10") @MemberGetter public static native int COMPILE_TIME_MAX_DEVICE_TYPES();

@Namespace("c10") public static native @StdString BytePointer DeviceTypeName(DeviceType d, @Cast("bool") boolean lower_case/*=false*/);
@Namespace("c10") public static native @StdString BytePointer DeviceTypeName(DeviceType d);
@Namespace("c10") public static native @StdString String DeviceTypeName(@Cast("c10::DeviceType") byte d, @Cast("bool") boolean lower_case/*=false*/);
@Namespace("c10") public static native @StdString String DeviceTypeName(@Cast("c10::DeviceType") byte d);

@Namespace("c10") public static native @Cast("bool") boolean isValidDeviceType(DeviceType d);
@Namespace("c10") public static native @Cast("bool") boolean isValidDeviceType(@Cast("c10::DeviceType") byte d);



@Namespace("c10") public static native void register_privateuse1_backend(@StdString BytePointer backend_name);
@Namespace("c10") public static native void register_privateuse1_backend(@StdString String backend_name);
@Namespace("c10") public static native @StdString BytePointer get_privateuse1_backend(@Cast("bool") boolean lower_case/*=true*/);
@Namespace("c10") public static native @StdString BytePointer get_privateuse1_backend();

 // namespace c10
 // namespace std



// Parsed from c10/core/Device.h

// #pragma once

// #include <c10/core/DeviceType.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>

// #include <cstddef>
// #include <functional>
// #include <iosfwd>
// #include <string>

/** An index representing a specific device; e.g., the 1 in GPU 1.
 *  A DeviceIndex is not independently meaningful without knowing
 *  the DeviceType it is associated; try to use Device rather than
 *  DeviceIndex directly. */
// Targeting ../Device.java






// Targeting ../DeviceHash.java


 // namespace std


// Parsed from c10/core/DeviceGuard.h

// #pragma once

// #include <c10/core/impl/InlineDeviceGuard.h>
// Targeting ../DeviceGuard.java


// Targeting ../OptionalDeviceGuard.java



// Note [Whither the DeviceGuard boilerplate]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Design note: in principle, we could avoid these wrappers using:
//
// using DeviceGuard = impl::InlineDeviceGuard<impl::VirtualGuardImpl>;
// using OptionalDeviceGuard =
// impl::InlineOptionalDeviceGuard<impl::VirtualGuardImpl>;
//
// But the error messages are worse, and our users can't just look at the
// header file to find out what's going on.  Furthermore, for specializations
// like CUDAStreamGuard, it can be profitable to replace some interfaces with
// refined types (e.g., return CUDAStream instead of Stream).  So, we eat
// the boilerplate and write out the API explicitly.

 // namespace c10


// Parsed from c10/core/DispatchKey.h

// #pragma once

// #include <c10/core/DeviceType.h>
// #include <c10/macros/Macros.h>
// #include <ostream>
// #include <string>

// Semantically, each value of BackendComponent identifies a "backend" for our
// dispatch. Some functionalities that we may dispatch to are allowed to
// register different handlers for each backend. The BackendComponent is then
// used to figure out which backend implementation to dispatch to.

// In implementation terms, the backend component identifies a specific "bit" in
// a DispatchKeySet. The bits in the DispatchKeySet are split between the bottom
// ~12 "BackendComponent" bits, while the remaining upper bits are assigned to
// functionalities. When we encounter a functionality bit that is known to be
// customizeable per-backend, then we also look at the lower BackendComponent
// bits and take the highest bit to determine which backend's implementation to
// use.

// WARNING!  If you add a new backend component to the end of this list,
// make sure you update PrivateUse3Bit.  (But you shouldn't: private use
// keys should have higher precedence than all built-in keys)

// If you add a new (non-privateuse) backend here,
// make sure to add an Autograd<Backend> fallthrough kernel
// in aten/src/ATen/core/VariableFallbackKernel.cpp

// #define C10_FORALL_BACKEND_COMPONENTS(_, extra)
//   _(CPU, extra)
//   _(CUDA, extra)
//   _(HIP, extra)
//   _(XLA, extra)
//   _(MPS, extra)
//   _(IPU, extra)
//   _(XPU, extra)
//   _(HPU, extra)
//   _(VE, extra)
//   _(Lazy, extra)
//   _(Meta, extra)
//   _(MTIA, extra)
//   _(PrivateUse1, extra)
//   _(PrivateUse2, extra)
//   _(PrivateUse3, extra)

// WARNING!  If we add a new per-backend functionality key that has higher
// priority than Autograd, then make sure you update EndOfRuntimeBackendKeys

// #define C10_FORALL_FUNCTIONALITY_KEYS(_)
//   _(Dense, )
//   _(Quantized, Quantized)
//   _(Sparse, Sparse)
//   _(NestedTensor, NestedTensor)
//   _(AutogradFunctionality, Autograd)

@Namespace("c10") public enum BackendComponent {

  // A "backend" is colloquially used to refer to handlers for dispatch
  // which actually implement the numerics of an operation in question.
  //
  // Due to the nature of the enum, these backends are specified in
  // an ordered way, but for most backends this order is not semantically
  // meaningful (e.g., it's valid to reorder these backends without changing
  // semantics).  The only situation when backend ordering is meaningful
  // is when the backend participates in multiple dispatch with another
  // backend; e.g., CPU and CUDA (cuda must have higher priority).

  // These keys don't correspond to individual kernels.
  // Instead, they represent the backends that are allowed to override specific
  // pieces of functionality:
  // - dense kernels (e.g. DispatchKey::CPU)
  // - sparse kernels (e.g. DispatchKey::SparseCPU)
  // - quantized kernels (e.g. DispatchKey::QuantizedCPU)
  // - autograd kernels (e.g. DispatchKey::AutogradCPU)
  // We reserve space in the runtime operator table for this full cross product
  // of
  // [backends in this enum] x [keys below that are explicitly marked as having
  // per-backend functionality]
  //
  // A meta tensor is a tensor without any data associated with it.  (They
  // have also colloquially been referred to as tensors on the "null" device).
  // A meta tensor can be used to dry run operators without actually doing any
  // computation, e.g., add on two meta tensors would give you another meta
  // tensor with the output shape and dtype, but wouldn't actually add anything.

  InvalidBit((byte)(0)),
  CPUBit((byte)(1)),
  CUDABit((byte)(2)),
  HIPBit((byte)(3)),
  XLABit((byte)(4)),
  MPSBit((byte)(5)),
  IPUBit((byte)(6)),
  XPUBit((byte)(7)),
  HPUBit((byte)(8)),
  VEBit((byte)(9)),
  LazyBit((byte)(10)),
  MetaBit((byte)(11)),
  MTIABit((byte)(12)),
  PrivateUse1Bit((byte)(13)),
  PrivateUse2Bit((byte)(14)),
  PrivateUse3Bit((byte)(15)),

  // Define an alias to represent end of backend dispatch keys.
  // If you add new backend keys after PrivateUse3, please also update it here.
  EndOfBackendKeys((byte)(PrivateUse3Bit.value));

    public final byte value;
    private BackendComponent(byte v) { this.value = v; }
    private BackendComponent(BackendComponent e) { this.value = e.value; }
    public BackendComponent intern() { for (BackendComponent e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

// Semantically, a dispatch key identifies a possible "level" in our
// dispatch, for which a handler may be registered. Each handler corresponds
// to a type of functionality.
//
// In implementation terms, the dispatch key identifies a specific "bit" in a
// DispatchKeySet.  Higher bit indexes get handled by dispatching first (because
// we "count leading zeros" when we extract the highest priority dispatch
// key.)
//
// Note [DispatchKey Classification]
// This enum actually contains several types of keys, which are explained
// in more detail further down:
// (1) non-customizable backends (e.g. FPGA)
// (2) non-customizable functionalities (e.g. Functionalize)
// (3) functionalized that are customizable per backend (e.g. Dense, Sparse,
// AutogradFunctionality) (4) per-backend instances of customizable
// functionalities (e.g. CPU, SparseCPU, AutogradCPU) (5) alias keys (e.g.
// CompositeImplicitAutograd)
//
// Of the categories above, it's important to note:
// (a) which keys are assigned individual bits in a DispatchKeySet
// (b) which keys are assigned individual slots in the runtime operator table
// ("Runtime keys")
//
// (1), (2) and (3) all get their own dedicated bits in the DispatchKeySet.
// (1), (2) and (4) all get their own dedicated slots in the runtime operator
// table.

// See Note [DispatchKeySet Internal Representation] for more details.
//
// NOTE: Keep the list in sync with `DispatchKey` in torchgen/model.py
@Namespace("c10") public enum DispatchKey {

  // ~~~~~~~~~~~~~~~~~~~~~~~~~~ UNDEFINED ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ //
  // This is not a "real" functionality, but it exists to give us a "nullopt"
  // element we can return for cases when a DispatchKeySet contains no elements.
  // You can think a more semantically accurate definition of DispatchKey is:
  //
  //    using DispatchKey = optional<RealDispatchKey>
  //
  // and Undefined == nullopt.  We didn't actually represent
  // it this way because optional<RealDispatchKey> would take two
  // words, when DispatchKey fits in eight bits.

  Undefined((short)(0)),

  // Define an alias for Undefined to represent CatchAll (long term
  // this will get eliminated, but for now it's convenient)
  CatchAll((short)(Undefined.value)),

  // ~~~~~~~~~~~~~~~~~~~~~~~~~~ Functionality Keys ~~~~~~~~~~~~~~~~~~~~~~ //
  // Every value in the enum (up to EndOfFunctionalityKeys)
  // corresponds to an individual "functionality" that can be dispatched to.
  // This is represented in the DispatchKeySet by assigning each of these enum
  // values
  // to each of the remaining (64 - len(BackendComponent)) bits.
  //
  // Most of these functionalities have a single handler assigned to them,
  // making them "runtime keys".
  // That map to a single slot in the runtime operator table.
  //
  // A few functionalities are allowed to be customizable per backend.
  // See [Note: Per-Backend Functionality Dispatch Keys] for details.

  // See [Note: Per-Backend Functionality Dispatch Keys]
  Dense((short)(Undefined.value + 1)),

  // Below are non-extensible backends.
  // These are backends that currently don't have their own overrides for
  // Autograd/Sparse/Quantized kernels,
  // and we therefore don't waste space in the runtime operator table allocating
  // space for them.
  // If any of these backends ever need to customize, e.g., Autograd, then we'll
  // need to add a DispatchKey::*Bit for them.

  // TODO: put this in BackendComponents
  FPGA((short)(Undefined.value + 2)), // Xilinx support lives out of tree at
  // https://gitlab.com/pytorch-complex/vitis_kernels

  // TODO: put this in BackendComponents
  // ONNX Runtime, lives out of tree at https://github.com/pytorch/ort and
  // https://github.com/microsoft/onnxruntime, and is also used to test general
  // backend/extension machinery in the core. cf:
  // - test/cpp_extensions/ort_extension.cpp
  // - test/test_torch.py
  // - aten/src/ATen/test/extension_backend_test.cpp
  ORT((short)(Undefined.value + 3)),

  Vulkan((short)(Undefined.value + 4)), // TODO: put this in BackendComponents
  Metal((short)(Undefined.value + 5)), // TODO: put this in BackendComponents

  // See [Note: Per-Backend Functionality Dispatch Keys]
  Quantized((short)(Undefined.value + 6)),

  // This backend is to support custom RNGs; it lets you go
  // to a different kernel if you pass in a generator that is not a
  // traditional CPUGeneratorImpl/CUDAGeneratorImpl.  To make use of this
  // key:
  //  1) set it as a second parameter of at::Generator constructor call in
  //     the user-defined PRNG class.
  //  2) use it as a dispatch key while registering custom kernels
  //     (templatized kernels specialized for user-defined PRNG class)
  // intended for out of tree use; tested by aten/src/ATen/test/rng_test.cpp
  CustomRNGKeyId((short)(Undefined.value + 7)),

  // TODO: Make Mkldnn a functionality key, so we can give it Meta
  // support
  // Here are backends which specify more specialized operators
  // based on the layout of the tensor.  Note that the sparse backends
  // are one case where ordering matters: sparse multi-dispatches with
  // the corresponding dense tensors, and must be handled before them.
  MkldnnCPU((short)(Undefined.value + 8)), // registered at build/aten/src/ATen/RegisterMkldnnCPU.cpp
  // NB: not to be confused with MKLDNN, which is Caffe2 only

  // See [Note: Per-Backend Functionality Dispatch Keys]
  Sparse((short)(Undefined.value + 9)),

  // TODO: Make SparseCsr a functionality key
  SparseCsrCPU((short)(Undefined.value + 10)),
  SparseCsrCUDA((short)(Undefined.value + 11)),

  NestedTensor((short)(Undefined.value + 12)),

  // In some situations, it is not immediately obvious what the correct
  // backend for function is, because the function in question doesn't
  // have any "tensor" arguments.  In this case, a BackendSelect function
  // can be registered to implement the custom determination of the
  // correct backend.
  BackendSelect((short)(Undefined.value + 13)),

  Python((short)(Undefined.value + 14)),

  // Out-of-core key for Fake Tensor in torchdistx.
  // See https://pytorch.org/torchdistx/latest/fake_tensor.html
  // TODO: delete this in favor of Python-implemented fake tensor
  Fake((short)(Undefined.value + 15)),
  // See Note [Out-of-tree vmap+grad prototype]. The purpose of this key
  // is to insert code after the "autograd subsystem" runs, so this key should
  // be directly after ADInplaceOrView and all of the autograd keys.
  FuncTorchDynamicLayerBackMode((short)(Undefined.value + 16)),

  // Alias and mutation removal.
  // If some backends want to opt into only alias removal or only mutation
  // removal,
  // we can consider adding separate keys dedicated to those individual passes.
  // See Note [Functionalization Pass In Core] for details.
  Functionalize((short)(Undefined.value + 17)),

  // The named dispatch key is set for any tensors with named dimensions.
  // Although we have a dispatch key for named tensors, for historical reasons,
  // this dispatch key doesn't do any of the substantive functionality for named
  // tensor (though, hypothetically, it could!)  At the moment, it's just
  // responsible for letting us give good error messages when operations
  // don't support named tensors.
  //
  // NB: If you ever consider moving named tensor functionality into
  // this dispatch key, note that it might be necessary add another dispatch
  // key that triggers before composite operators, in case a composite operator
  // has named dimension propagation that doesn't match that of its
  // constituent parts.
  // TODO: delete this once torchdim lands in functorch
  Named((short)(Undefined.value + 18)),

  // The Conjugate dispatch key is set for any tensors that need to perform
  // conjugation
  // This is implemented at a dispatch level right before any backends run
  Conjugate((short)(Undefined.value + 19)),

  // The Negative dispatch key is set for any tensors that need to perform
  // negation
  // This is implemented at a dispatch level right before any backends run
  Negative((short)(Undefined.value + 20)),

  ZeroTensor((short)(Undefined.value + 21)), // registered at build/aten/src/ATen/RegisterZeroTensor.cpp

  // Note [ADInplaceOrView key]
  // ADInplaceOrView key is used by inplace or view ops to register a kernel
  // that does additional setup for future autograd computation.
  //
  // 1. For inplace ops this kernel does version bump
  // 2. For view ops this kernel does `as_view` setup where we properly setup
  //    DifferentiableViewMeta on the view tensors.
  //
  // For other ops it's fallthrough kernel since there's no extra
  // work to do.
  //
  // Note [Dream: skip VariableType kernel when requires_grad=false]
  //
  // In an ideal world where we can skip VariableType kernel for inputs
  // with requires_grad=false, instead of a fallthrough kernel, we'll
  // register a kernel shown below to all functional ops as well:
  // torch::Tensor my_functional_op(...) {
  //   {
  //     // Note for every op in VariableType, you need to go through
  //     // `AutoDispatchBelowADInplaceOrView` guard exactly once to add the
  //     // key to TLS excluded set. If you don't go through it at all,
  //     // inplace/view ops called through `at::` inside your backend
  //     // kernel will dispatch to ADInplaceOrView kernels and do a lot
  //     // of extra work.
  //     at::AutoDispatchBelowADInplaceOrView guard;
  //     at::redispatch::my_functional_op(...);
  //   }
  // }
  // But this work is currently blocked since it adds an extra dispatch
  // for all ops and it's non-trivial overhead at model level(a few percents).
  // Thus our current approach takes advantage of the fact every kernel go
  // through VariableType kernel first and pulls the
  // `at::AutoDispatchBelowADInplaceOrView` guard of functional ops
  // up to the `VariableType` kernel. Thus we only add the extra dispatch
  // to view/inplace ops to minimize its perf impact to real models.
  ADInplaceOrView((short)(Undefined.value + 22)),
  // Note [Alias Dispatch Key : Autograd]
  // All backends are oblivious to autograd; autograd is handled as a
  // layer which happens on top of all backends. It inspects the autograd
  // metadata of all inputs, determines what autograd metadata should be
  // constructed by the output, and otherwise defers to the backend to
  // actually do the numeric computation.  Autograd contains
  // the bulk of this logic.

  // Autograd is now an alias dispatch key which by default maps to all
  // backend-specific autograd keys.
  // Backend-specific allow backends to override the default kernel registered
  // to Autograd key as needed.
  // For example, XLA wants to define autograd for einsum directly.
  // Registering a custom autograd implementation at the XLA key won't work
  // because we process Autograd before XLA.  This key has higher priority and
  // gets processed first.  You generally should NOT redispatch after handling
  // autograd here (since that would result in execution of the Autograd
  // operator, which you're trying to skip).  In AutogradXLA implementations,
  // you are responsible for handling autograd yourself, or deferring to other
  // operators which support autograd.

  // Currently we only have backend-specific autograd keys for CPU/CUDA/XLA and
  // reserved user-defined backends. All other in-tree backends share the
  // AutogradOther key. We can add specific autograd key for those backends
  // upon request.
  AutogradOther((short)(Undefined.value + 23)),

  // See [Note: Per-Backend Functionality Dispatch Keys]
  AutogradFunctionality((short)(Undefined.value + 24)),

  // NestedTensor is an example of something that isn't a "real backend"
  // (because it mostly consists of redispatching kernels)
  // but it would like to override autograd functionality in C++.
  // We can handle cases like this by adding an extra functionality key
  // exclusively for handling autograd for NestedTensor.
  // lives out of tree at
  // https://github.com/pytorch/nestedtensor
  AutogradNestedTensor((short)(Undefined.value + 25)),

  Tracer((short)(Undefined.value + 26)),

  // TODO: make Autocast a functionality key
  // Autocasting precedes VariableTypeId, to ensure casts are autograd-exposed
  // and inputs are saved for backward in the post-autocast type.
  AutocastCPU((short)(Undefined.value + 27)),
  AutocastXPU((short)(Undefined.value + 28)),
  AutocastHPU((short)(Undefined.value + 29)),
  // Naughtily, AutocastCUDA is also being used for XLA.  In the terminal state,
  // it probably should get its own Autocast key
  AutocastCUDA((short)(Undefined.value + 30)),

  // ~~~~~~~~~~~~~~~~~~~~~~~~~~~ WRAPPERS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ //
  // There are a number of alternative modes which may want to handle before
  // autograd; for example, error checking, tracing, profiling or vmap.  They
  // go here.

  FuncTorchBatched((short)(Undefined.value + 31)), // See Note [Out-of-tree vmap+grad prototype]
  FuncTorchVmapMode((short)(Undefined.value + 32)), // See Note [Out-of-tree vmap+grad prototype]

  // This is the dispatch key for BatchedTensorImpl, which is used to implement
  // batching rules for vmap.
  Batched((short)(Undefined.value + 33)),

  // When we are inside a vmap, all tensors dispatch on this key.
  // See Note: [DispatchKey::VmapMode usage] for more details.
  VmapMode((short)(Undefined.value + 34)),

  FuncTorchGradWrapper((short)(Undefined.value + 35)), // See Note [Out-of-tree vmap+grad prototype]

  // Out-of-core key for Deferred Module Initialization in torchdistx.
  // See https://pytorch.org/torchdistx/latest/deferred_init.html
  DeferredInit((short)(Undefined.value + 36)),

  // Used by Python key logic to know the set of tls on entry to the dispatcher
  // This kernel assumes it is the top-most non-functorch-related DispatchKey.
  // If you add a key above, make sure to update the fallback implementation for
  // this.
  PythonTLSSnapshot((short)(Undefined.value + 37)),

  // This key should be at the very top of the dispatcher
  FuncTorchDynamicLayerFrontMode((short)(Undefined.value + 38)), // See Note [Out-of-tree vmap+grad prototype]

  // TESTING: This is intended to be a generic testing tensor type id.
  // Don't use it for anything real; its only acceptable use is within a single
  // process test.  Use it by creating a TensorImpl with this DispatchKey, and
  // then registering operators to operate on this type id.  See
  // aten/src/ATen/core/dispatch/backend_fallback_test.cpp for a usage example.
  TESTING_ONLY_GenericWrapper((short)(Undefined.value + 39)),

  // TESTING: This is intended to be a generic testing tensor type id.
  // Don't use it for anything real; its only acceptable use is within a ingle
  // process test.  Use it by toggling the mode on and off via
  // TESTING_ONLY_tls_generic_mode_set_enabled and then registering operators
  // to operate on this type id.  See
  // aten/src/ATen/core/dispatch/backend_fallback_test.cpp
  // for a usage example
  TESTING_ONLY_GenericMode((short)(Undefined.value + 40)),

  // This is a bypass that allows you to skip running the C++ dispatcher
  // entirely
  PythonDispatcher((short)(Undefined.value + 41)),

  // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FIN ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ //
  EndOfFunctionalityKeys((short)(Undefined.value + 42)),

  StartOfDenseBackends((short)(Undefined.value + 43)),
      CPU((short)(Undefined.value + 44)),
          
  CUDA((short)(Undefined.value + 45)),
          
  HIP((short)(Undefined.value + 46)),
          
  XLA((short)(Undefined.value + 47)),
          
  MPS((short)(Undefined.value + 48)),
          
  IPU((short)(Undefined.value + 49)),
          
  XPU((short)(Undefined.value + 50)),
          
  HPU((short)(Undefined.value + 51)),
          
  VE((short)(Undefined.value + 52)),
          
  Lazy((short)(Undefined.value + 53)),
          
  Meta((short)(Undefined.value + 54)),
          
  MTIA((short)(Undefined.value + 55)),
          
  PrivateUse1((short)(Undefined.value + 56)),
          
  PrivateUse2((short)(Undefined.value + 57)),
          
  PrivateUse3((short)(Undefined.value + 58)),
          EndOfDenseBackends((short)(0)),
  StartOfQuantizedBackends((short)(1)),
      QuantizedCPU((short)(2)),
          
  QuantizedCUDA((short)(3)),
          
  QuantizedHIP((short)(4)),
          
  QuantizedXLA((short)(5)),
          
  QuantizedMPS((short)(6)),
          
  QuantizedIPU((short)(7)),
          
  QuantizedXPU((short)(8)),
          
  QuantizedHPU((short)(9)),
          
  QuantizedVE((short)(10)),
          
  QuantizedLazy((short)(11)),
          
  QuantizedMeta((short)(12)),
          
  QuantizedMTIA((short)(13)),
          
  QuantizedPrivateUse1((short)(14)),
          
  QuantizedPrivateUse2((short)(15)),
          
  QuantizedPrivateUse3((short)(16)),
          EndOfQuantizedBackends((short)( QuantizedPrivateUse3.value)),
  StartOfSparseBackends((short)( QuantizedPrivateUse3.value + 1)),
      SparseCPU((short)( QuantizedPrivateUse3.value + 2)),
          
  SparseCUDA((short)( QuantizedPrivateUse3.value + 3)),
          
  SparseHIP((short)( QuantizedPrivateUse3.value + 4)),
          
  SparseXLA((short)( QuantizedPrivateUse3.value + 5)),
          
  SparseMPS((short)( QuantizedPrivateUse3.value + 6)),
          
  SparseIPU((short)( QuantizedPrivateUse3.value + 7)),
          
  SparseXPU((short)( QuantizedPrivateUse3.value + 8)),
          
  SparseHPU((short)( QuantizedPrivateUse3.value + 9)),
          
  SparseVE((short)( QuantizedPrivateUse3.value + 10)),
          
  SparseLazy((short)( QuantizedPrivateUse3.value + 11)),
          
  SparseMeta((short)( QuantizedPrivateUse3.value + 12)),
          
  SparseMTIA((short)( QuantizedPrivateUse3.value + 13)),
          
  SparsePrivateUse1((short)( QuantizedPrivateUse3.value + 14)),
          
  SparsePrivateUse2((short)( QuantizedPrivateUse3.value + 15)),
          
  SparsePrivateUse3((short)( QuantizedPrivateUse3.value + 16)),
          EndOfSparseBackends((short)( SparsePrivateUse3.value)),
  StartOfNestedTensorBackends((short)( SparsePrivateUse3.value + 1)),
      NestedTensorCPU((short)( SparsePrivateUse3.value + 2)),
          
  NestedTensorCUDA((short)( SparsePrivateUse3.value + 3)),
          
  NestedTensorHIP((short)( SparsePrivateUse3.value + 4)),
          
  NestedTensorXLA((short)( SparsePrivateUse3.value + 5)),
          
  NestedTensorMPS((short)( SparsePrivateUse3.value + 6)),
          
  NestedTensorIPU((short)( SparsePrivateUse3.value + 7)),
          
  NestedTensorXPU((short)( SparsePrivateUse3.value + 8)),
          
  NestedTensorHPU((short)( SparsePrivateUse3.value + 9)),
          
  NestedTensorVE((short)( SparsePrivateUse3.value + 10)),
          
  NestedTensorLazy((short)( SparsePrivateUse3.value + 11)),
          
  NestedTensorMeta((short)( SparsePrivateUse3.value + 12)),
          
  NestedTensorMTIA((short)( SparsePrivateUse3.value + 13)),
          
  NestedTensorPrivateUse1((short)( SparsePrivateUse3.value + 14)),
          
  NestedTensorPrivateUse2((short)( SparsePrivateUse3.value + 15)),
          
  NestedTensorPrivateUse3((short)( SparsePrivateUse3.value + 16)),
          EndOfNestedTensorBackends((short)( NestedTensorPrivateUse3.value)),
  StartOfAutogradFunctionalityBackends((short)( NestedTensorPrivateUse3.value + 1)),
      AutogradCPU((short)( NestedTensorPrivateUse3.value + 2)),
          
  AutogradCUDA((short)( NestedTensorPrivateUse3.value + 3)),
          
  AutogradHIP((short)( NestedTensorPrivateUse3.value + 4)),
          
  AutogradXLA((short)( NestedTensorPrivateUse3.value + 5)),
          
  AutogradMPS((short)( NestedTensorPrivateUse3.value + 6)),
          
  AutogradIPU((short)( NestedTensorPrivateUse3.value + 7)),
          
  AutogradXPU((short)( NestedTensorPrivateUse3.value + 8)),
          
  AutogradHPU((short)( NestedTensorPrivateUse3.value + 9)),
          
  AutogradVE((short)( NestedTensorPrivateUse3.value + 10)),
          
  AutogradLazy((short)( NestedTensorPrivateUse3.value + 11)),
          
  AutogradMeta((short)( NestedTensorPrivateUse3.value + 12)),
          
  AutogradMTIA((short)( NestedTensorPrivateUse3.value + 13)),
          
  AutogradPrivateUse1((short)( NestedTensorPrivateUse3.value + 14)),
          
  AutogradPrivateUse2((short)( NestedTensorPrivateUse3.value + 15)),
          
  AutogradPrivateUse3((short)( NestedTensorPrivateUse3.value + 16)),
          EndOfAutogradFunctionalityBackends((short)( AutogradPrivateUse3.value)),

      EndOfRuntimeBackendKeys((short)(EndOfAutogradFunctionalityBackends.value)),

  // ~~~~~~~~~~~~~~~~~~~~~~ Alias Dispatch Keys ~~~~~~~~~~~~~~~~~~~~~~~~~~ //
  // Note [Alias Dispatch Keys]
  // Alias dispatch keys are synthetic dispatch keys which map to multiple
  // runtime dispatch keys. Alisa keys have precedence, but they are always
  // lower precedence than runtime keys. You can register a kernel to an
  // alias key, the kernel might be populated to the mapped runtime keys
  // during dispatch table computation.
  // If a runtime dispatch key has multiple kernels from alias keys, which
  // kernel wins is done based on the precedence of alias keys (but runtime
  // keys always have precedence over alias keys).
  // Alias keys won't be directly called during runtime.

  // See Note [Alias Dispatch Key : Autograd]
  Autograd((short)(EndOfAutogradFunctionalityBackends.value + 1)),
  CompositeImplicitAutograd((short)(EndOfAutogradFunctionalityBackends.value + 2)), // registered at
  // build/aten/src/ATen/RegisterCompositeImplicitAutograd.cpp

  // Note: The alias keyset for FuncTorchBatchedDecomposition is disjoint from
  // all
  // other alias keysets
  // and so precedence order doesn't matter
  FuncTorchBatchedDecomposition((short)(EndOfAutogradFunctionalityBackends.value + 3)), // registered at
  // build/aten/src/ATen/RegisterFuncTorchBatchedDecomposition.cpp
  // Note: The alias keyset for CompositeImplicitAutogradNestedTensor is
  // disjoint from all other alias keysets
  CompositeImplicitAutogradNestedTensor((short)(EndOfAutogradFunctionalityBackends.value + 4)), // registered at
  // build/aten/src/ATen/RegisterCompositeImplicitAutogradNestedTensor.cpp
  CompositeExplicitAutograd((short)(EndOfAutogradFunctionalityBackends.value + 5)), // registered at
  // build/aten/src/ATen/RegisterCompositeExplicitAutograd.cpp
  // See Note [CompositeExplicitAutogradNonFunctional Key]
  CompositeExplicitAutogradNonFunctional((short)(EndOfAutogradFunctionalityBackends.value + 6)), // registered at
  // build/aten/src/ATen/RegisterCompositeExplicitAutograd.cpp

  // Define an alias key to represent end of alias dispatch keys.
  // If you add new alias keys after Autograd, please also update it here.
  StartOfAliasKeys((short)(Autograd.value)),
  EndOfAliasKeys((short)(CompositeExplicitAutogradNonFunctional.value)), //

  // ~~~~~~~~~~~~~~~~~~~~~~~~~ BC ALIASES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ //
  // The aliases exist for backwards compatibility reasons, they shouldn't
  // be used
  CPUTensorId((short)(CPU.value)),
  CUDATensorId((short)(CUDA.value)),
  DefaultBackend((short)(CompositeExplicitAutograd.value)),
  PrivateUse1_PreAutograd((short)(AutogradPrivateUse1.value)),
  PrivateUse2_PreAutograd((short)(AutogradPrivateUse2.value)),
  PrivateUse3_PreAutograd((short)(AutogradPrivateUse3.value)),
  Autocast((short)(AutocastCUDA.value));

    public final short value;
    private DispatchKey(short v) { this.value = v; }
    private DispatchKey(DispatchKey e) { this.value = e.value; }
    public DispatchKey intern() { for (DispatchKey e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

// Note [Private use DispatchKey]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Private use tensor IDs are preallocated tensor type IDs for use in user
// applications.  Similar to private use fields in HTTP, they can be used
// by end users for experimental or private applications, without needing
// to "standardize" the tensor ID (which would be done by submitting a PR
// to PyTorch to add your type ID).
//
// Private use tensor IDs are appropriate to use if you want to experiment
// with adding a new tensor type (without having to patch PyTorch first) or
// have a private, non-distributed application that needs to make use of a
// new tensor type.  Private use tensor IDs are NOT appropriate to use for
// libraries intended to be distributed to further users: please contact
// the PyTorch developers to get a type ID registered in this case.
//
// We provide two classes of private user tensor id: regular DispatchKeys
// and Autograd DispatchKeys.  DispatchKeys serve the role of ordinary "backend"
// DispatchKeys; if you were adding support for a new type of accelerator, you
// would use a backend DispatchKey, and ideally automatically reuse
// AutogradOther definitions already defined in PyTorch.  AutogradPrivateUse
// DispatchKeys serve as "wrapper" DispatchKeys: they are only necessary for
// tensors that compose multiple internal tensors, and for cases when the
// built-in autograd formulas for operators are not appropriate.

// Check if a DispatchKey is an alias mapping to other runtime keys.
@Namespace("c10") public static native @Cast("const bool") boolean isAliasDispatchKey(DispatchKey k);
@Namespace("c10") public static native @Cast("const bool") boolean isAliasDispatchKey(@Cast("c10::DispatchKey") short k);

// [Note: Per-Backend Functionality Dispatch Keys]
// Check if a DispatchKey is a per-backend functionality key
// Any functionalities that can be customized per-backend should be added here.
// These keys correspond to functionalities that can be customized indivually
// per backend. While they only take up one bit in the `DispatchKeySet` bitset,
// they map to (# backends) slots in the operator table.
// Each of these keys also has a separate set of "runtime keys" in the dispatch
// key enum, per backend, which *do* map to the individual operator table slots.
// For example, the "Sparse" key maps to an individual bit in the
// DispatchKeySet, while `SparseCPU`, `SparseCUDA`, etc all map to individual
// slots in the runtime operator table.

@Namespace("c10") public static native @Cast("const bool") boolean isPerBackendFunctionalityKey(DispatchKey k);
@Namespace("c10") public static native @Cast("const bool") boolean isPerBackendFunctionalityKey(@Cast("c10::DispatchKey") short k);

// Note that this includes Undefined in the total count.
// BUT EndOfFunctionalityKeys is its own (placeholder) key.
// e.g. Undefined=0, Dense=1, Sparse=2, EndOfFunctionalityKeys=3.
// In the above example, there are 3 total functionality keys.
@Namespace("c10") @MemberGetter public static native @Cast("const uint8_t") byte num_functionality_keys();

@Namespace("c10") @MemberGetter public static native @Cast("const uint8_t") byte num_backends();

// Note [No More Than 16 Backends]
// Search for this note to find places in the code where the "no more than 16
// backends" invariant is baked in.

@Namespace("c10") public static native @Cast("const uint8_t") byte numPerBackendFunctionalityKeys();

// #if defined(C10_MOBILE_TRIM_DISPATCH_KEYS)
// See [Note: Trimmed Mobile Dispatch Keys]
@Namespace("c10") @MemberGetter public static native @Cast("const uint16_t") short num_runtime_entries();
// #else
// #endif

// See Note [No More Than 16 Backends]
@Namespace("c10") @MemberGetter public static native @Cast("const uint16_t") short full_backend_mask();

@Namespace("c10") public static native @Cast("const char*") BytePointer toString(DispatchKey arg0);
@Namespace("c10") public static native String toString(@Cast("c10::DispatchKey") short arg0);
@Namespace("c10") public static native @Cast("const char*") BytePointer toString(BackendComponent arg0);
@Namespace("c10") public static native String toString(@Cast("c10::BackendComponent") byte arg0);



@Namespace("c10") public static native DispatchKey getAutogradKeyFromBackend(BackendComponent k);
@Namespace("c10") public static native @Cast("c10::DispatchKey") short getAutogradKeyFromBackend(@Cast("c10::BackendComponent") byte k);

// Parses a string into a dispatch key.
// If the string cannot be correctly parsed, throws an exception.
@Namespace("c10") public static native DispatchKey parseDispatchKey(@StdString BytePointer k);
@Namespace("c10") public static native @Cast("c10::DispatchKey") short parseDispatchKey(@StdString String k);

// These are some convenience identifiers for dispatch keys which are
// shorter to type than their long counterparts.  Note that some of these
// dispatch keys directly correspond to DeviceType; and most APIs that
// accept DispatchKey also accept DeviceType; e.g.,
// torch::dispatch(torch::kCPU, ...) is also valid.
@Namespace("c10") @MemberGetter public static native DispatchKey kAutograd();

// See Note [The Ordering of Per-Backend Dispatch Keys Matters!]
// This function relies on the invariant that the dispatch keys between
// StartOfDenseBackends and EndOfRuntimeBackendKeys are ordered by backend
// in the same order as `BackendComponent`.


@Namespace("c10") public static native DispatchKey toFunctionalityKey(DispatchKey k);
@Namespace("c10") public static native @Cast("c10::DispatchKey") short toFunctionalityKey(@Cast("c10::DispatchKey") short k);



// Given (DispatchKey::Dense, BackendComponent::CUDABit), returns
// DispatchKey::CUDA.
// See Note [The Ordering of Per-Backend Dispatch Keys Matters!]
// This function relies on the invariant that the dispatch keys between
// StartOfDenseBackends and EndOfRuntimeBackendKeys are ordered by backend
// in the same order as `BackendComponent`.
@Namespace("c10") public static native DispatchKey toRuntimePerBackendFunctionalityKey(
    DispatchKey functionality_k,
    BackendComponent backend_k);
@Namespace("c10") public static native @Cast("c10::DispatchKey") short toRuntimePerBackendFunctionalityKey(
    @Cast("c10::DispatchKey") short functionality_k,
    @Cast("c10::BackendComponent") byte backend_k);

 // namespace c10
// Expose the constant, but not the TYPE (DispatchKey is an implementation
// detail!)
 // namespace torch

// NB: You really shouldn't use this instance; this enum is guaranteed
// to be pretty small so a regular array should be acceptable.
 // namespace std


// Parsed from c10/core/DispatchKeySet.h

// #pragma once
// #include <c10/core/DispatchKey.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Metaprogramming.h>
// #include <c10/util/llvmMathExtras.h>
// #include <ostream>
// Targeting ../FunctionalityOffsetAndMask.java


// Targeting ../DispatchKeySet.java



@Namespace("c10") public static native @StdString BytePointer toString(@ByVal DispatchKeySet arg0);


@Namespace("c10") public static native int getDispatchTableIndexForDispatchKey(DispatchKey k);
@Namespace("c10") public static native int getDispatchTableIndexForDispatchKey(@Cast("c10::DispatchKey") short k);

// Alias key DispatchKey::Autograd maps to
// (autograd_dispatch_keyset x full_backend_mask)
// NB: keys in this set also get associated with CompositeImplicitAutograd
//
// Note [autograd_dispatch_keyset Does Not Include Backend Bits]
// We don't want to include any backend bits (BackendComponent::CPUBit, etc)
// directly in autograd_dispatch_keyset.
// Why? keysets like autograd_dispatch_keyset are commonly used to remove
// autograd keys from a DispatchKeySet throughout the code base. However, you
// are only allowed to remove functionality bits from a keyset, not backend
// bits. See Note [Removing keys from DispatchKeySet Only Affects Functionality
// Keys] for details. To be consistent and avoid confusion, we're explicitly
// setting up autograd_dispatch_keyset to not have any backend bits.
@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet autograd_dispatch_keyset();

@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet autocast_dispatch_keyset();

// See Note [TLS Initialization]
@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet default_included_set();

@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet default_excluded_set();

@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet autograd_dispatch_keyset_with_ADInplaceOrView();

@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet python_ks();

@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet sparse_ks();

@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet sparse_csr_ks();

@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet mkldnn_ks();

// backend dispatch keys that map to DispatchKey::AutogradOther
// NB: keys in this set also get associated with CompositeImplicitAutograd
@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet autogradother_backends();

// The set of dispatch keys that come after autograd
// n.b. this relies on the fact that AutogradOther is currently the lowest
// Autograd key
@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet after_autograd_keyset();

// The set of dispatch keys that come after ADInplaceOrView
@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet after_ADInplaceOrView_keyset();

// The set of dispatch keys that come after Functionalize
@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet after_func_keyset();

@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet backend_bitset_mask();
// keyset correpsonding to functorch keys that have their own dedicated
// TensorImpl subclass.

// This keyset has:
// (1) the functionality bits corresponding to backends (dense, sparse,
// quantized) (2) all of the backend bits set
@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet backend_functionality_keys();
// Targeting ../OpTableOffsetAndMask.java



// true if t is a backend dispatch key
@Namespace("c10") public static native @Cast("bool") boolean isBackendDispatchKey(DispatchKey t);
@Namespace("c10") public static native @Cast("bool") boolean isBackendDispatchKey(@Cast("c10::DispatchKey") short t);

// Resolve alias dispatch key to DispatchKeySet if applicable
@Namespace("c10") public static native @ByVal DispatchKeySet getRuntimeDispatchKeySet(DispatchKey t);
@Namespace("c10") public static native @ByVal DispatchKeySet getRuntimeDispatchKeySet(@Cast("c10::DispatchKey") short t);

// Resolve alias dispatch key to DispatchKeySet if applicable,
// and chek if k is a part of that set
@Namespace("c10") public static native @Cast("bool") boolean runtimeDispatchKeySetHas(DispatchKey t, DispatchKey k);
@Namespace("c10") public static native @Cast("bool") boolean runtimeDispatchKeySetHas(@Cast("c10::DispatchKey") short t, @Cast("c10::DispatchKey") short k);

// Returns a DispatchKeySet of all backend keys mapped to Autograd dispatch key
// t, DispatchKeySet is empty if t is not alias of DispatchKey::Autograd.
@Namespace("c10") public static native @ByVal DispatchKeySet getBackendKeySetFromAutograd(DispatchKey t);
@Namespace("c10") public static native @ByVal DispatchKeySet getBackendKeySetFromAutograd(@Cast("c10::DispatchKey") short t);

// Returns a DispatchKeySet of autograd related keys mapped to backend.
// for a given backend key, use the associated autograd key.
// for non-backend keys, use AutogradOther as a default.
// Note: it's convenient and fast to return a default here rather than (say)
// returning an optional<DispatchKey>, or throwing. But it makes callers
// responsible for either a) enforcing the invariant that only backend keys
// be passed as arguments, or b) interpreting our return value carefully.
@Namespace("c10") public static native @ByVal DispatchKeySet getAutogradRelatedKeySetFromBackend(BackendComponent t);
@Namespace("c10") public static native @ByVal DispatchKeySet getAutogradRelatedKeySetFromBackend(@Cast("c10::BackendComponent") byte t);

// Returns a DispatchKeySet of autocast related keys mapped to backend.
@Namespace("c10") public static native @ByVal DispatchKeySet getAutocastRelatedKeySetFromBackend(BackendComponent t);
@Namespace("c10") public static native @ByVal DispatchKeySet getAutocastRelatedKeySetFromBackend(@Cast("c10::BackendComponent") byte t);

// returns the "backend" DispatchKey of highest priority in the set.
// This is basically like highestBackendKey(), except that we have some
// "functionality" bits that correspond to backends (Sparse, Quantized)
@Namespace("c10") public static native DispatchKey highestPriorityBackendTypeId(@ByVal DispatchKeySet ks);

// This API exists because we have a use case for checking
// getRuntimeDispatchKeySet(alias).has(DispatchKey::Undefined)
// in OperatorEntry.cpp but we disallow it in has() API.
@Namespace("c10") public static native @Cast("bool") boolean isIncludedInAlias(DispatchKey k, DispatchKey alias);
@Namespace("c10") public static native @Cast("bool") boolean isIncludedInAlias(@Cast("c10::DispatchKey") short k, @Cast("c10::DispatchKey") short alias);

// Historically, every tensor only had a single DispatchKey, and it was always
// something like CPU, and there wasn't any of this business where TLS
// could cause the DispatchKey of a tensor to change.  But we still have some
// legacy code that is still using DispatchKey for things like instanceof
// checks; if at all possible, refactor the code to stop using DispatchKey in
// those cases.
@Namespace("c10") public static native DispatchKey legacyExtractDispatchKey(@ByVal DispatchKeySet s);

// Given a function type, constructs a function_traits type that drops the first
// parameter type if the first parameter is of type DispatchKeySet. NB:
// DispatchKeySet is currently explicitly hidden from JIT (mainly to avoid
// pushing unnecessary arguments on the stack - see Note [ Plumbing Keys Through
// the Dispatcher] for details). If at any point in the future we need to expose
// this type to JIT, revisit the usage of this type alias.
 // namespace c10


// Parsed from c10/core/Backend.h

// #pragma once

// #include <c10/core/DeviceType.h>
// #include <c10/core/DispatchKey.h>
// #include <c10/core/DispatchKeySet.h>
// #include <c10/util/Exception.h>

// #include <stdexcept>

/**
 * This legacy enum class defines the set of backends supported by old school,
 * code generated Type-based ATen.  A "backend" in this sense roughly
 * corresponds to the cartesian product of (device type, layout), but restricted
 * only to combinations which we actually have kernels for.  Backend does NOT
 * include dtype.
 *
 * The reason we are sunsetting this enum class is because it doesn't allow for
 * open registration; e.g., if you want to add SparseXLA, you'd have to
 * edit this enum; you wouldn't be able to do it out of tree.  DispatchKey is
 * the replacement for Backend which supports open registration.
 *
 * NB: The concept of 'Backend' here disagrees with the notion of backend
 * exposed to users in torch.backends.  Backend here is something like "CPU"
 * or "SparseCUDA"; backend in torch.backends is something like "MKL" or
 * "CUDNN".
 */
@Namespace("c10") public enum Backend {
  CPU(0),
  CUDA(1),
  HIP(2),
  VE(3),
  FPGA(4),
  IPU(5),
  XPU(6),
  SparseCPU(7),
  SparseCUDA(8),
  SparseCsrCPU(9),
  SparseCsrCUDA(10),
  SparseHIP(11),
  SparseVE(12),
  SparseXPU(13),
  ORT(14),
  XLA(15),
  Vulkan(16),
  Metal(17),
  Meta(18),
  QuantizedCPU(19),
  QuantizedCUDA(20),
  QuantizedXPU(21),
  Undefined(22),
  MkldnnCPU(23),
  MPS(24),
  HPU(25),
  Lazy(26),
  MTIA(27),
  PrivateUse1(28),
  NumOptions(29);

    public final int value;
    private Backend(int v) { this.value = v; }
    private Backend(Backend e) { this.value = e.value; }
    public Backend intern() { for (Backend e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") public static native Backend dispatchKeyToBackend(DispatchKey t);
@Namespace("c10") public static native @Cast("c10::Backend") int dispatchKeyToBackend(@Cast("c10::DispatchKey") short t);

@Namespace("c10") public static native DispatchKey backendToDispatchKey(Backend b);
@Namespace("c10") public static native @Cast("c10::DispatchKey") short backendToDispatchKey(@Cast("c10::Backend") int b);

@Namespace("c10") public static native DeviceType backendToDeviceType(Backend b);
@Namespace("c10") public static native @Cast("c10::DeviceType") byte backendToDeviceType(@Cast("c10::Backend") int b);

// TODO: This probably shouldn't actually be static inline
@Namespace("c10") public static native @Cast("const char*") BytePointer toString(Backend b);
@Namespace("c10") public static native String toString(@Cast("c10::Backend") int b);

@Namespace("c10") public static native @Cast("bool") boolean isSparse(Backend b);
@Namespace("c10") public static native @Cast("bool") boolean isSparse(@Cast("c10::Backend") int b);

@Namespace("c10") public static native @Cast("bool") boolean isSparseCsr(Backend b);
@Namespace("c10") public static native @Cast("bool") boolean isSparseCsr(@Cast("c10::Backend") int b);

 // namespace c10


// Parsed from c10/core/CopyBytes.h

// #pragma once

// #include <c10/core/Device.h>
// Targeting ../CopyBytesFunction.java


// Targeting ../_CopyBytesFunctionRegisterer.java



// #define REGISTER_COPY_BYTES_FUNCTION(from, to, ...)
//   namespace {
//   static _CopyBytesFunctionRegisterer C10_ANONYMOUS_VARIABLE(
//       g_copy_function)(from, to, __VA_ARGS__);
//   }

/*
 * WARNING: Implementations for this function are currently registered from
 * ATen and caffe2, not yet from c10. Don't use this if not either ATen
 * or caffe2 is present as well.
 * We can't move them yet, because the CUDA implementations aren't unified yet
 * between ATen and caffe2.
 * We're planning to move the implementations into c10/backend/xxx
 * to make c10 self contained again.
 */
@Namespace("c10") public static native void CopyBytes(
    @Cast("size_t") long nbytes,
    @Const Pointer src,
    @ByVal Device src_device,
    Pointer dst,
    @ByVal Device dst_device,
    @Cast("bool") boolean async);
 // namespace c10


// Parsed from c10/core/GradMode.h

// #pragma once

// #include <c10/core/AutogradState.h>
// #include <c10/macros/Macros.h>
// Targeting ../GradMode.java


// Targeting ../AutoGradMode.java


// Targeting ../NoGradGuard.java


// Targeting ../AutoFwGradMode.java



 // namespace c10


// Parsed from c10/core/InferenceMode.h

// #pragma once

// #include <c10/core/AutogradState.h>
// #include <c10/core/GradMode.h>
// #include <c10/core/impl/LocalDispatchKeySet.h>
// #include <c10/macros/Macros.h>
// Targeting ../InferenceMode.java


 // namespace c10


// Parsed from c10/core/Layout.h

// #pragma once

// #include <c10/core/Backend.h>
// #include <c10/util/Exception.h>

// #include <ostream>
@Namespace("c10") public enum Layout {
  Strided((byte)(0)),
  Sparse((byte)(1)),
  SparseCsr((byte)(2)),
  Mkldnn((byte)(3)),
  SparseCsc((byte)(4)),
  SparseBsr((byte)(5)),
  SparseBsc((byte)(6)),
  NumOptions((byte)(7));

    public final byte value;
    private Layout(byte v) { this.value = v; }
    private Layout(Layout e) { this.value = e.value; }
    public Layout intern() { for (Layout e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") public static native Layout layout_from_backend(Backend backend);
@Namespace("c10") public static native @Cast("c10::Layout") byte layout_from_backend(@Cast("c10::Backend") int backend);



 // namespace c10


// Parsed from c10/core/MemoryFormat.h

// #pragma once

// #include <c10/core/Backend.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>

// #include <ostream>

// Memory format is not the property of a Tensor. It is the way to tell an
// operator how the result should be organized in memory and nothing more. That
// means memory format should never be used as return value for any tensor state
// interrogation functions (internally and externally).
//
// Possible options are:
//  Preserve:
//    If any of the input tensors is in channels_last format, operator output
//    should be in channels_last format
//
//  Contiguous:
//    Regardless of input tensors format, the output should be contiguous
//    Tensor.
//
//  ChannelsLast:
//    Regardless of input tensors format, the output should be in channels_last
//    format.
@Namespace("c10") public enum MemoryFormat {
  Contiguous((byte)(0)),
  Preserve((byte)(1)),
  ChannelsLast((byte)(2)),
  ChannelsLast3d((byte)(3)),
  NumOptions((byte)(4));

    public final byte value;
    private MemoryFormat(byte v) { this.value = v; }
    private MemoryFormat(MemoryFormat e) { this.value = e.value; }
    public MemoryFormat intern() { for (MemoryFormat e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

// If you are seeing this, it means that this call site was not checked if
// the memory format could be preserved, and it was switched to old default
// behaviour of contiguous
// #define LEGACY_CONTIGUOUS_MEMORY_FORMAT c10::get_contiguous_memory_format()

@Namespace("c10") public static native MemoryFormat get_contiguous_memory_format();



// Note: Hardcoded the channel last stride indices here to get better
// performance

@Namespace("c10") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector get_channels_last_strides_2d(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("c10") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector get_channels_last_strides_2d(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sizes);

@Namespace("c10") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector get_channels_last_strides_3d(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("c10") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector get_channels_last_strides_3d(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sizes);

// NOTE:
// Below are Helper functions for is_channels_last_strides_xd.
// 1. Please do not combine these helper functions, each helper function handles
// exactly one case of sizes + memory_format, by doing this, the strides indices
// will be a constant array and we can access it using constant index number,
// the compiler will fully unroll the loop on strides indices to gain a better
// performance.
// 2. No error check in helper function, caller ensures the correctness of the
// input
// 3. All helper functions have similar comments, only 1st helper function is
// commented here.

// Note [Ambiguous is_channels_last_strides_xd]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// The flaw of carrying memory_format implicitly through strides is very hard
// to WAR properly. issue #24090
// Without the history of permutation, we can't infer the memory_format of a
// tensor from the snapshot of its size & stride
// e.g.
//
// 1. We can NOT specify the memory_format of N111 tensor through strides in a
//  meaningful way;
//
// 2. Two path that ended up with identical size/stride
//  N11W contiguous tensor sliced at w-dimension becomes [N,1,1,1]@[W,W,W,W]
//  NC11 channels_last tensor sliced at c-dimension becomes [N,1,1,1]@[C,C,C,C]
//    So if we see a tensor [N,1,1,1]@[X,X,X,X], there's no way for us to infer
//    the memory_format of the original tensor.
//
// Due to the limitations, our temporary WAR `is_channels_last_strides` does the
// best effort to infer whether the original memory_format of a tensor is
// at::MemoryFormat::ChannelsLast. The two objectives of this function (ordered
// by their importance):
//   1. Ensure that normal shape manipulation does not accidentally change the
//      MemoryFormat of an existing tensor.
//   2. Allows user to mark MemoryFormat::ChannelsLast to tensors;
//
// The function does so via checking strides of the tensor, including strides of
// size-1 dimensions. Although conventionally PyTorch implies no restriction on
// trivial stride (stride for size-1 dimension).
//
// Note that this approach is a compromise. We did not solve the problem
// completely. Many cases we will not be able to infer the correct memory
// format.
// The implementation of `is_channels_last_strides` is to serve the objectives:
// MemoryFormat::ChannelsLast has to be explicitly opted-in (no accidental
// conversion); Best effort to maintain the ChannelsLast flag.
//
// Due to the fact that this is not a bulletproof solution, through testing
// (aten/src/ATen/test/memory_format_test.cpp)
//   a. we ensure that the common tasks are supported;
//   a. we identify corner cases where the implementation compromises on.
//
// By the time accumulated permutation is enabled to replace implicit
// memory_format through strides, we should be updating our tests and fix the
// issues in our tests.
//
// We use Channels Last 2d as an example above.
// This is a general problem for all the is_channels_last_strides_xd
// implementation. Please check the helper functions
// (is_channels_last_strides_*d_s*) for more details.

@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_2d(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides);
@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_2d(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... strides);

@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_3d(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides);
@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_3d(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... strides);

 // namespace c10


// Parsed from c10/core/QEngine.h

// #pragma once

// #include <c10/core/DeviceType.h>
// #include <c10/core/DispatchKey.h>
// #include <c10/util/Exception.h>

/**
 * QEngine is an enum that is used to select the engine to run quantized ops.
 * Keep this enum in sync with get_qengine_id() in
 * torch/backends/quantized/__init__.py
 */
@Namespace("c10") public enum QEngine {
  NoQEngine((byte)(0)),
  FBGEMM((byte)(1)),
  QNNPACK((byte)(2)),
  ONEDNN((byte)(3)),
  X86((byte)(4));

    public final byte value;
    private QEngine(byte v) { this.value = v; }
    private QEngine(QEngine e) { this.value = e.value; }
    public QEngine intern() { for (QEngine e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") public static native @StdString BytePointer toString(QEngine qengine);

 // namespace c10


// Parsed from c10/core/QScheme.h

// #pragma once

// #include <c10/core/DeviceType.h>
// #include <c10/util/Exception.h>

/**
 * QScheme is an enum that specifies the type of quantization. This has a one
 * to one correspondence with Quantizer
 * Please refer to ATen/quantized/Quantizer.h to see the Quantizers classes.
 * Keep this file in sync with torch/nn/_qscheme.py
 */
@Namespace("c10") public enum QScheme {
  PER_TENSOR_AFFINE((byte)(0)),
  PER_CHANNEL_AFFINE((byte)(1)),
  PER_TENSOR_SYMMETRIC((byte)(2)),
  PER_CHANNEL_SYMMETRIC((byte)(3)),
  PER_CHANNEL_AFFINE_FLOAT_QPARAMS((byte)(4)),
  COMPILE_TIME_NUM_QSCHEMES((byte)(5));

    public final byte value;
    private QScheme(byte v) { this.value = v; }
    private QScheme(QScheme e) { this.value = e.value; }
    public QScheme intern() { for (QScheme e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
@Namespace("c10") @MemberGetter public static native int COMPILE_TIME_NUM_QSCHEMES();

@Namespace("c10") public static native @StdString BytePointer toString(QScheme qscheme);

 // namespace c10


// Parsed from c10/core/Stream.h

// #pragma once

// #include <c10/core/Device.h>

/** An index representing a specific stream.  A StreamId is not independently
 *  meaningful without knowing the Device it is associated with; try to
 *  use Stream rather than StreamId directly.
 * 
 *  StreamIds are opaque; they are assigned by some DeviceType-specific
 *  numbering system which is not visible to the user.  HOWEVER, we
 *  guarantee that StreamId 0 is always a valid stream, and corresponds
 *  to some sort of "default" stream. */
// Targeting ../StreamData3.java


// Targeting ../Stream.java






// Targeting ../StreamHash.java


 // namespace std


// Parsed from c10/core/ScalarType.h

// #pragma once

// #include <c10/util/BFloat16.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Half.h>
// #include <c10/util/complex.h>
// #include <c10/util/qint32.h>
// #include <c10/util/qint8.h>
// #include <c10/util/quint2x4.h>
// #include <c10/util/quint4x2.h>
// #include <c10/util/quint8.h>

// #include <complex>
// #include <cstdint>
// #include <ostream>

// For the macros below:
// NB: If you want to macro some code for all non-QInt scalar types (i.e. types
// with complete information, you probably want one of the
// AT_FORALL_SCALAR_TYPES / AT_FORALL_SCALAR_TYPES_AND
// macros below, which are designed to behave similarly to the Dispatch macros
// with the same name.

// NB: Order matters for this macro; it is relied upon in
// _promoteTypesLookup and the serialization format.
// #define AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS(_)
//   _(uint8_t, Byte) /* 0 */
//   _(int8_t, Char) /* 1 */
//   _(int16_t, Short) /* 2 */
//   _(int, Int) /* 3 */
//   _(int64_t, Long) /* 4 */
//   _(at::Half, Half) /* 5 */
//   _(float, Float) /* 6 */
//   _(double, Double) /* 7 */
//   _(c10::complex<c10::Half>, ComplexHalf) /* 8 */
//   _(c10::complex<float>, ComplexFloat) /* 9 */
//   _(c10::complex<double>, ComplexDouble) /* 10 */
//   _(bool, Bool) /* 11 */
//   _(c10::qint8, QInt8) /* 12 */
//   _(c10::quint8, QUInt8) /* 13 */
//   _(c10::qint32, QInt32) /* 14 */
//   _(at::BFloat16, BFloat16) /* 15 */
//   _(c10::quint4x2, QUInt4x2) /* 16 */
//   _(c10::quint2x4, QUInt2x4) /* 17 */

// If you want to support ComplexHalf for real, add ComplexHalf
// into this macro (and change the name).  But beware: convert()
// doesn't work for all the conversions you need...
// #define AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_EXCEPT_COMPLEX_HALF(_)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(at::Half, Half)
//   _(float, Float)
//   _(double, Double)
//   _(c10::complex<float>, ComplexFloat)
//   _(c10::complex<double>, ComplexDouble)
//   _(bool, Bool)
//   _(at::BFloat16, BFloat16)

// #define AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(_)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(at::Half, Half)
//   _(float, Float)
//   _(double, Double)
//   _(c10::complex<c10::Half>, ComplexHalf)
//   _(c10::complex<float>, ComplexFloat)
//   _(c10::complex<double>, ComplexDouble)
//   _(bool, Bool)
//   _(at::BFloat16, BFloat16)

@Namespace("c10") public enum ScalarType {
  Byte((byte)(0)), /* 0 */
  Char((byte)(1)), /* 1 */
  Short((byte)(2)), /* 2 */
  Int((byte)(3)), /* 3 */
  Long((byte)(4)), /* 4 */
  Half((byte)(5)), /* 5 */
  Float((byte)(6)), /* 6 */
  Double((byte)(7)), /* 7 */
  ComplexHalf((byte)(8)), /* 8 */
  ComplexFloat((byte)(9)), /* 9 */
  ComplexDouble((byte)(10)), /* 10 */
  Bool((byte)(11)), /* 11 */
  QInt8((byte)(12)), /* 12 */
  QUInt8((byte)(13)), /* 13 */
  QInt32((byte)(14)), /* 14 */
  BFloat16((byte)(15)), /* 15 */
  QUInt4x2((byte)(16)), /* 16 */
  QUInt2x4((byte)(17)),
      Undefined((byte)(18)),
  NumOptions((byte)(19));

    public final byte value;
    private ScalarType(byte v) { this.value = v; }
    private ScalarType(ScalarType e) { this.value = e.value; }
    public ScalarType intern() { for (ScalarType e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") @MemberGetter public static native @Cast("const uint16_t") short NumScalarTypes();

// These are used to map ScalarTypes to C++ types.

// #define SPECIALIZE_ScalarTypeToCPPType(cpp_type, scalar_type)
//   template <>
//   struct ScalarTypeToCPPType<c10::ScalarType::scalar_type> {
//     using type = cpp_type;
// 
//     /* This is a workaround for the CUDA bug which prevents */
//     /* ::detail::ScalarTypeToCType<T>::type being used directly due to */
//     /* ambiguous reference which can't to be resolved. For some reason it */
//     /* can't pick between at::detail and at::cuda::detail. */
//     /* For repro example, please see: */
//     /* https://gist.github.com/izdeby/952ae7cf256ddb740a73776d39a7e7ba */
//     /* TODO: remove once the bug is fixed. */
//     static type t;
//   }; /* 0 */ /* 1 */ /* 2 */ /* 3 */ /* 4 */ /* 5 */ /* 6 */ /* 7 */ /* 8 */ /* 9 */ /* 10 */ /* 11 */ /* 12 */ /* 13 */ /* 14 */ /* 15 */ /* 16 */ /* 17 */

// #undef SPECIALIZE_ScalarTypeToCPPType

 // namespace impl

// #define SPECIALIZE_CppTypeToScalarType(cpp_type, scalar_type)
//   template <>
//   struct CppTypeToScalarType<cpp_type>
//       : std::
//             integral_constant<c10::ScalarType, c10::ScalarType::scalar_type> {
//   }; /* 0 */ /* 1 */ /* 2 */ /* 3 */ /* 4 */ /* 5 */ /* 6 */ /* 7 */ /* 8 */ /* 9 */ /* 10 */ /* 11 */ /* 12 */ /* 13 */ /* 14 */ /* 15 */ /* 16 */ /* 17 */

// #undef SPECIALIZE_CppTypeToScalarType

// #define AT_FORALL_INT_TYPES(_)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)

// #define AT_FORALL_SCALAR_TYPES(_)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(float, Float)
//   _(double, Double)

// #define AT_FORALL_SCALAR_TYPES_AND(SCALARTYPE, _)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(float, Float)
//   _(double, Double)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE>::t),
//     SCALARTYPE)

// #define AT_FORALL_SCALAR_TYPES_AND2(SCALARTYPE1, SCALARTYPE2, _)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(float, Float)
//   _(double, Double)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE1>::t),
//     SCALARTYPE1)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE2>::t),
//     SCALARTYPE2)

// #define AT_FORALL_SCALAR_TYPES_AND3(SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, _)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(float, Float)
//   _(double, Double)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE1>::t),
//     SCALARTYPE1)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE2>::t),
//     SCALARTYPE2)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<
//              ::c10::ScalarType::SCALARTYPE3>::t),
//     SCALARTYPE3)

// #define AT_FORALL_QINT_TYPES(_)
//   _(c10::qint8, QInt8)
//   _(c10::quint8, QUInt8)
//   _(c10::qint32, QInt32)
//   _(c10::quint4x2, QUInt4x2)
//   _(c10::quint2x4, QUInt2x4)

// #define AT_FORALL_COMPLEX_TYPES(_)
//   _(c10::complex<float>, ComplexFloat)
//   _(c10::complex<double>, ComplexDouble)

// #define DEFINE_CONSTANT(_, name)
//   constexpr ScalarType k##name = ScalarType::name;

@Namespace("c10") @MemberGetter public static native ScalarType kByte(); /* 0 */
  @Namespace("c10") @MemberGetter public static native ScalarType kChar(); /* 1 */
  @Namespace("c10") @MemberGetter public static native ScalarType kShort(); /* 2 */
  @Namespace("c10") @MemberGetter public static native ScalarType kInt(); /* 3 */
  @Namespace("c10") @MemberGetter public static native ScalarType kLong(); /* 4 */
  @Namespace("c10") @MemberGetter public static native ScalarType kHalf(); /* 5 */
  @Namespace("c10") @MemberGetter public static native ScalarType kFloat(); /* 6 */
  @Namespace("c10") @MemberGetter public static native ScalarType kDouble(); /* 7 */
  @Namespace("c10") @MemberGetter public static native ScalarType kComplexHalf(); /* 8 */
  @Namespace("c10") @MemberGetter public static native ScalarType kComplexFloat(); /* 9 */
  @Namespace("c10") @MemberGetter public static native ScalarType kComplexDouble(); /* 10 */
  @Namespace("c10") @MemberGetter public static native ScalarType kBool(); /* 11 */
  @Namespace("c10") @MemberGetter public static native ScalarType kQInt8(); /* 12 */
  @Namespace("c10") @MemberGetter public static native ScalarType kQUInt8(); /* 13 */
  @Namespace("c10") @MemberGetter public static native ScalarType kQInt32(); /* 14 */
  @Namespace("c10") @MemberGetter public static native ScalarType kBFloat16(); /* 15 */
  @Namespace("c10") @MemberGetter public static native ScalarType kQUInt4x2(); /* 16 */
  @Namespace("c10") @MemberGetter public static native ScalarType kQUInt2x4(); /* 17 */
// #undef DEFINE_CONSTANT

@Namespace("c10") public static native @Cast("const char*") BytePointer toString(ScalarType t);

@Namespace("c10") public static native @Cast("size_t") long elementSize(ScalarType t);

@Namespace("c10") public static native @Cast("bool") boolean isIntegralType(ScalarType t, @Cast("bool") boolean includeBool);

@Namespace("c10") public static native @Cast("bool") @Deprecated boolean isIntegralType(ScalarType t);

@Namespace("c10") public static native @Cast("bool") boolean isFloatingType(ScalarType t);

@Namespace("c10") public static native @Cast("bool") boolean isComplexType(ScalarType t);

@Namespace("c10") public static native @Cast("bool") boolean isQIntType(ScalarType t);

@Namespace("c10") public static native ScalarType toQIntType(ScalarType t);

@Namespace("c10") public static native ScalarType toUnderlying(ScalarType t);

@Namespace("c10") public static native @Cast("bool") boolean isSignedType(ScalarType t);

@Namespace("c10") public static native @Cast("bool") boolean isUnderlying(ScalarType type, ScalarType qtype);

@Namespace("c10") public static native ScalarType toRealValueType(ScalarType t);

@Namespace("c10") public static native ScalarType toComplexType(ScalarType t);

// see tensor_attributes.rst for detailed explanation and examples
// of casting rules.
@Namespace("c10") public static native @Cast("bool") boolean canCast(ScalarType from, ScalarType to);

@Namespace("c10") public static native ScalarType promoteTypes(ScalarType a, ScalarType b);



// #define AT_FORAUTOCAST_SCALAR_TYPES(_)
//   _(half, Half) /* 0 */
//   _(bfloat16, BFloat16) /* 1 */

 // namespace c10


// Parsed from c10/core/ScalarTypeToTypeMeta.h

// #pragma once

// #include <c10/core/ScalarType.h>
// #include <c10/util/Optional.h>
// #include <c10/util/typeid.h>

// these just expose TypeMeta/ScalarType bridge functions in c10
// TODO move to typeid.h (or codemod away) when TypeMeta et al
// are moved from caffe2 to c10 (see note at top of typeid.h)

/**
 * convert ScalarType enum values to TypeMeta handles
 */
@Namespace("c10") public static native @ByVal TypeMeta scalarTypeToTypeMeta(ScalarType scalar_type);

/**
 * convert TypeMeta handles to ScalarType enum values
 */
@Namespace("c10") public static native ScalarType typeMetaToScalarType(@ByVal TypeMeta dtype);

/**
 * typeMetaToScalarType(), lifted to optional
 */
@Namespace("c10") public static native @ByVal ScalarTypeOptional optTypeMetaToScalarType(
    @ByVal TypeMetaOptional type_meta);

/**
 * convenience: equality across TypeMeta/ScalarType conversion
 */








 // namespace c10


// Parsed from c10/core/Scalar.h

// #pragma once

// #include <assert.h>
// #include <stdint.h>
// #include <stdexcept>
// #include <string>
// #include <type_traits>
// #include <utility>

// #include <c10/core/OptionalRef.h>
// #include <c10/core/ScalarType.h>
// #include <c10/core/SymFloat.h>
// #include <c10/core/SymInt.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Half.h>
// #include <c10/util/TypeCast.h>
// #include <c10/util/intrusive_ptr.h>

// #if C10_CLANG_HAS_WARNING("-Wimplicit-int-float-conversion")
// #endif
// Targeting ../Scalar.java



// define the scalar.to<int64_t>() specializations
// #define DEFINE_TO(T, name)
//   template <>
//   inline T Scalar::to<T>() const {
//     return to##name();
//   }

  
  
  
  
  
  
  
  
  
  
  
  
// #undef DEFINE_TO

 // namespace c10



// Parsed from c10/core/SymNodeImpl.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>
// #include <c10/util/intrusive_ptr.h>
// #include <memory>
// Targeting ../SymNodeImpl.java



 // namespace c10


// Parsed from c10/core/SymBool.h

// #pragma once

// #include <c10/core/SymNodeImpl.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/intrusive_ptr.h>
// Targeting ../SymBool.java




 // namespace c10


// Parsed from c10/core/SymFloat.h

// #pragma once

// #include <c10/core/SymNodeImpl.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/intrusive_ptr.h>

// #include <limits>
// #include <memory>
// Targeting ../SymFloat.java




 // namespace c10


// Parsed from c10/core/SymInt.h

// #pragma once

// #include <c10/core/SymBool.h>
// #include <c10/core/SymNodeImpl.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>

// #include <numeric>
// Targeting ../SymInt.java



/** Sum of a list of SymInt; accumulates into the c10::SymInt expression */

@Namespace("c10") public static native @ByVal @Name("operator +") SymInt add(@Cast("int64_t") long a, @Const @ByRef SymInt b);
@Namespace("c10") public static native @ByVal @Name("operator -") SymInt subtract(@Cast("int64_t") long a, @Const @ByRef SymInt b);
@Namespace("c10") public static native @ByVal @Name("operator *") SymInt multiply(@Cast("int64_t") long a, @Const @ByRef SymInt b);
@Namespace("c10") public static native @ByVal @Name("operator /") SymInt divide(@Cast("int64_t") long a, @Const @ByRef SymInt b);
@Namespace("c10") public static native @ByVal @Name("operator %") SymInt mod(@Cast("int64_t") long a, @Const @ByRef SymInt b);


@Namespace("c10") public static native @Cast("bool") @Name("operator <") boolean lessThan(@Cast("int64_t") long a, @Const @ByRef SymInt b);
@Namespace("c10") public static native @Cast("bool") @Name("operator <=") boolean lessThanEquals(@Cast("int64_t") long a, @Const @ByRef SymInt b);
@Namespace("c10") public static native @Cast("bool") @Name("operator >") boolean greaterThan(@Cast("int64_t") long a, @Const @ByRef SymInt b);
@Namespace("c10") public static native @Cast("bool") @Name("operator >=") boolean greaterThanEquals(@Cast("int64_t") long a, @Const @ByRef SymInt b);


@Namespace("c10") public static native @ByVal @Name("operator -") SymInt subtract(@Const @ByRef SymInt s);
 // namespace c10


// Parsed from c10/core/SymIntArrayRef.h

// #pragma once

// #include <c10/core/SymInt.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Optional.h>

@Namespace("c10") public static native @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef asIntArrayRefUnchecked(@ByVal SymIntRef ar);

@Namespace("c10") public static native @ByVal LongArrayRefOptional asIntArrayRefSlowOpt(
    @ByVal SymIntRef ar);



// #define C10_AS_INTARRAYREF_SLOW(a) c10::asIntArrayRefSlow(a, __FILE__, __LINE__)

// Prefer using a more semantic constructor, like
// fromIntArrayRefKnownNonNegative
@Namespace("c10") public static native @ByVal SymIntRef fromIntArrayRefUnchecked(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef array_ref);
@Namespace("c10") public static native @ByVal SymIntRef fromIntArrayRefUnchecked(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... array_ref);

@Namespace("c10") public static native @ByVal SymIntRef fromIntArrayRefKnownNonNegative(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef array_ref);
@Namespace("c10") public static native @ByVal SymIntRef fromIntArrayRefKnownNonNegative(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... array_ref);

@Namespace("c10") public static native @ByVal SymIntRef fromIntArrayRefSlow(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef array_ref);
@Namespace("c10") public static native @ByVal SymIntRef fromIntArrayRefSlow(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... array_ref);

 // namespace c10


// Parsed from c10/core/Allocator.h

// #pragma once

// #include <stddef.h>
// #include <memory>

// #include <c10/core/Device.h>
// #include <c10/util/Exception.h>
// #include <c10/util/ThreadLocalDebugInfo.h>
// #include <c10/util/UniqueVoidPtr.h>
// Targeting ../DataPtr.java



// NB: Device is NOT tested for here; a CUDA nullptr is as much a nullptr as a
// CPU nullptr





// Targeting ../Allocator.java



// This context is used to generate DataPtr which have arbitrary
// std::function deleters associated with them.  In some user facing
// functions, we give a (user-friendly) interface for constructing
// tensors from external data which take an arbitrary std::function
// deleter.  Grep for InefficientStdFunctionContext to find these
// occurrences.
//
// This context is inefficient because we have to do a dynamic
// allocation InefficientStdFunctionContext, on top of the dynamic
// allocation which is implied by std::function itself.

/** Set the allocator for DeviceType {@code t}. The passed in allocator pointer is
 *  expected to have static lifetime; this function does NOT take ownership
 *  of the raw pointer. (The reason for this is to prevent existing pointers
 *  to an allocator of a particular device from being invalidated when
 *  SetAllocator is called.)
 *
 *  Also note that this is not thread-safe, and we assume this function will
 *  only be called during initialization.
 *
 *  The 'priority' flag is introduced when we want to overwrite the default
 *  allocator, since the allocators are set statically. The default priority
 *  is 0, which means the lowest. Only higher or equal priority can overwrite
 *  existing ones.
 */
@Namespace("c10") public static native void SetAllocator(DeviceType t, Allocator alloc, @Cast("uint8_t") byte priority/*=0*/);
@Namespace("c10") public static native void SetAllocator(DeviceType t, Allocator alloc);
@Namespace("c10") public static native void SetAllocator(@Cast("c10::DeviceType") byte t, Allocator alloc, @Cast("uint8_t") byte priority/*=0*/);
@Namespace("c10") public static native void SetAllocator(@Cast("c10::DeviceType") byte t, Allocator alloc);
@Namespace("c10") public static native Allocator GetAllocator(DeviceType t);
@Namespace("c10") public static native Allocator GetAllocator(@Cast("c10::DeviceType") byte t);

// #define REGISTER_ALLOCATOR(t, f)
//   namespace {
//   static c10::AllocatorRegisterer<t> g_allocator_d(f);
//   }
// Targeting ../MemoryReportingInfoBase.java



@Namespace("c10") public static native @Cast("bool") boolean memoryProfilingEnabled();
@Namespace("c10") public static native void reportMemoryUsageToProfiler(
    Pointer ptr,
    @Cast("int64_t") long alloc_size,
    @Cast("size_t") long total_allocated,
    @Cast("size_t") long total_reserved,
    @ByVal Device device);

@Namespace("c10") public static native void reportOutOfMemoryToProfiler(
    @Cast("int64_t") long alloc_size,
    @Cast("size_t") long total_allocated,
    @Cast("size_t") long total_reserved,
    @ByVal Device device);

 // namespace c10


// Parsed from c10/core/DefaultDtype.h

// #pragma once

// #include <c10/core/ScalarType.h>
// #include <c10/macros/Macros.h>
 // namespace caffe2
@Namespace("c10") public static native void set_default_dtype(@ByVal TypeMeta dtype);
@Namespace("c10") public static native @Const @ByVal TypeMeta get_default_dtype();
@Namespace("c10") public static native ScalarType get_default_dtype_as_scalartype();
@Namespace("c10") public static native @Const @ByVal TypeMeta get_default_complex_dtype();
 // namespace c10


// Parsed from c10/core/StorageImpl.h

// #pragma once

// #include <c10/core/Allocator.h>
// #include <c10/core/ScalarType.h>
// #include <c10/core/SymInt.h>

// #include <c10/util/intrusive_ptr.h>
// Targeting ../StorageImpl.java


 // namespace c10


// Parsed from c10/core/Storage.h

// #pragma once

// #include <c10/core/StorageImpl.h>
// Targeting ../Storage.java



 // namespace c10


// Parsed from c10/core/TensorOptions.h

// #pragma once

// #include <c10/core/Backend.h>
// #include <c10/core/DefaultDtype.h>
// #include <c10/core/Device.h>
// #include <c10/core/DispatchKeySet.h>
// #include <c10/core/Layout.h>
// #include <c10/core/MemoryFormat.h>
// #include <c10/core/ScalarType.h>
// #include <c10/core/ScalarTypeToTypeMeta.h>

// #include <c10/macros/Macros.h>
// #include <c10/util/C++17.h>
// #include <c10/util/Optional.h>

// #include <cstddef>
// #include <iosfwd>
// #include <utility>

@Namespace("c10") public static native DispatchKey computeDispatchKey(
    @ByVal ScalarTypeOptional dtype,
    @ByVal LayoutOptional layout,
    @ByVal DeviceOptional device);

@Namespace("c10") public static native ScalarType dtype_or_default(@ByVal ScalarTypeOptional dtype);

@Namespace("c10") public static native @ByVal TypeMeta dtype_or_default(
    @ByVal TypeMetaOptional dtype);

@Namespace("c10") public static native Layout layout_or_default(@ByVal LayoutOptional layout);

@Namespace("c10") public static native @ByVal Device device_or_default(@ByVal DeviceOptional device);


///
///
///
///
///
///
///
///
///
///
///
///
///
///
///
///
///
@Namespace("c10") public static native @Cast("bool") boolean pinned_memory_or_default(@ByVal BoolOptional pinned_memory);
// Targeting ../TensorOptions.java



// We should aspire to fit in one machine-size word; but a size greater than two
// words is too much.  (We are doing terribly on 32-bit archs, where we require
// three machine size words to store tensor options.  Eek!)

/** Convenience function that returns a {@code TensorOptions} object with the {@code dtype}
 *  set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions dtype(@ByVal TypeMeta dtype);

// legacy function to support ScalarType
@Namespace("c10") public static native @ByVal TensorOptions dtype(ScalarType dtype);

/** Convenience function that returns a {@code TensorOptions} object with the {@code layout}
 *  set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions layout(Layout layout);
@Namespace("c10") public static native @ByVal TensorOptions layout(@Cast("c10::Layout") byte layout);

/** Convenience function that returns a {@code TensorOptions} object with the {@code device}
 *  set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions device(@ByVal Device device);

/** Convenience function that returns a {@code TensorOptions} object with the
 *  {@code device} set to CUDA and the {@code device_index} set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions device_index(short device_index);

/** Convenience function that returns a {@code TensorOptions} object with the
 *  {@code requires_grad} set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions requires_grad(@Cast("bool") boolean requires_grad/*=true*/);

/** Convenience function that returns a {@code TensorOptions} object with the
 *  {@code memory_format} set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions memory_format(MemoryFormat memory_format);
@Namespace("c10") public static native @ByVal TensorOptions memory_format(@Cast("c10::MemoryFormat") byte memory_format);



@Namespace("c10") public static native @StdString BytePointer toString(@Const @ByVal TensorOptions options);

// This is intended to be a centralized location by which we can determine
// what an appropriate DispatchKey for a tensor is.

@Namespace("c10") public static native Layout dispatchKeyToLayout(DispatchKey dispatch_key);
@Namespace("c10") public static native @Cast("c10::Layout") byte dispatchKeyToLayout(@Cast("c10::DispatchKey") short dispatch_key);

@Namespace("c10") public static native DeviceType dispatchKeyToDeviceType(DispatchKey dispatch_key);
@Namespace("c10") public static native @Cast("c10::DeviceType") byte dispatchKeyToDeviceType(@Cast("c10::DispatchKey") short dispatch_key);

@Namespace("c10") public static native @ByVal TensorOptions dispatchKeyToTensorOptions(DispatchKey dispatch_key);
@Namespace("c10") public static native @ByVal TensorOptions dispatchKeyToTensorOptions(@Cast("c10::DispatchKey") short dispatch_key);
@Namespace("c10::detail") public static native @Cast("bool") boolean backend_supports_empty_operator(@Const @ByVal TensorOptions options);

 // namespace detail

 // namespace c10


// Parsed from c10/core/TensorImpl.h

// #pragma once

// #include <c10/core/Backend.h>
// #include <c10/core/CopyBytes.h>
// #include <c10/core/DispatchKeySet.h>
// #include <c10/core/InferenceMode.h>
// #include <c10/core/MemoryFormat.h>
// #include <c10/core/Storage.h>
// #include <c10/core/SymBool.h>
// #include <c10/core/SymIntArrayRef.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/core/WrapDimMinimal.h>
// #include <c10/core/impl/LocalDispatchKeySet.h>
// #include <c10/core/impl/PyObjectSlot.h>
// #include <c10/core/impl/SizesAndStrides.h>
// #include <c10/util/DimVector.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Flags.h>
// #include <c10/util/Logging.h>
// #include <c10/util/Optional.h>
// #include <c10/util/accumulate.h>
// #include <c10/util/irange.h>
// #include <c10/util/python_stub.h>
// #include <c10/util/safe_numerics.h>

// #include <algorithm>
// #include <atomic>
// #include <limits>
// #include <memory>
// #include <numeric>
// #include <utility>

// A global boolean variable to control whether we free memory when a Tensor
// is shrunk to a smaller size. As a result, a Tensor is always going to
// keep the memory allocated for its maximum capacity reshaped to so far.
//
// This parameter is respected "upper-case" methods which call Resize()
// (e.g., CopyFrom, ResizeLike); it is NOT respected by Tensor::resize_
// or ShrinkTo, both of which guarantee to never to free memory.


// Since we can have high variance in blob memory allocated across different
// inputs in the same run, we will shrink the blob only if the memory gain
// is larger than this flag in bytes.  This only applies to functions which
// respect caffe2_keep_on_shrink.


// #if C10_CLANG_HAS_WARNING("-Wimplicit-int-float-conversion")
// #endif
 // namespace at
 // namespace c10

/**
 * A utility function to convert vector<int> to vector<int64_t>.
 */
@Namespace("c10") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector ToVectorint64_t(@Cast("const c10::ArrayRef<int>*") @ByRef IntArrayRef src);

/**
 * Return product of all dimensions starting from k
 */
@Namespace("c10") public static native @Cast("int64_t") long size_from_dim_(int k, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("c10") public static native @Cast("int64_t") long size_from_dim_(int k, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);

// Product of all dims up to k (not including dims[k])
@Namespace("c10") public static native @Cast("int64_t") long size_to_dim_(int k, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("c10") public static native @Cast("int64_t") long size_to_dim_(int k, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);

// Product of all dims between k and l (not including dims[k] and dims[l])
@Namespace("c10") public static native @Cast("int64_t") long size_between_dim_(int k, int l, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("c10") public static native @Cast("int64_t") long size_between_dim_(int k, int l, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);

// Wrap around axis_index if it is negative, s.t., -1 is the last dim
@Namespace("c10") public static native int canonical_axis_index_(int axis_index, int ndims);
// Targeting ../PlacementDtor.java


// Targeting ../PlacementDeleteContext.java


// Targeting ../AutogradMetaInterface.java


// Targeting ../AutogradMetaFactory.java



@Namespace("c10::impl") public static native void SetAutogradMetaFactory(AutogradMetaFactory factory);
@Namespace("c10::impl") public static native AutogradMetaFactory GetAutogradMetaFactory();
// Targeting ../AutogradMetaFactoryRegisterer.java




// Targeting ../NamedTensorMetaInterface.java



// For ease of copy pasting
// #if 0
// #endif
// Targeting ../VariableVersion.java



// Forward declaration of TensorImpl needed for forward declaration of
// C10_TensorImpl_Size_Check_Dummy_Class

// Forward declaration needed because TensorImpl needs to be friends with
// C10_TensorImpl_Size_Check_Dummy_Class in order to check the size
// of its private fields.

/**
 * NOTE: Some TensorImpl methods are small and not overridden in the
 * PyTorch codebase itself, but may theoretically need to be
 * overridden by third-party TensorImpl subclasses. This macro allows
 * users that need maximum performance and don't need these extension
 * points to disable them with a build-time flag. (In particular,
 * XLA's XLATensorImpl currently overrides these methods, so we can't
 * enable this flag by default.)
 */
// #ifdef C10_DISABLE_TENSORIMPL_EXTENSIBILITY
// #define TENSORIMPL_MAYBE_VIRTUAL
// #else
// #define TENSORIMPL_MAYBE_VIRTUAL virtual
// Targeting ../TensorImpl.java



// Note [TensorImpl size constraints]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Changed the size of TensorImpl?  If the size went down, good for
// you!  Adjust the documentation below and the expected size.
// Did it go up?  Read on...
//
// Struct size matters.  In some production systems at Facebook, we have
// 400M live tensors during a training run.  Do the math: every 64-bit
// word you add to Tensor is an extra 3.2 gigabytes in RAM.
//
// If you are a Facebook employee, you can check if the run in question
// has tipped you over the point using the command here:
// https://fburl.com/q5enpv98
//
// For reference, we OOMed at 160 bytes (20 words) per TensorImpl.
// This is not counting overhead from strides out-of-line allocation and
// StorageImpl space and this is from before we inlined sizes and strides
// directly into TensorImpl as SmallVectors.
//
// Our memory usage on 32-bit systems is suboptimal, but we're not checking
// for it at the moment (to help avoid rage inducing cycles when the
// 32-bit number is wrong).
//
// Current breakdown:
//
//    vtable pointer
//    strong refcount           TODO: pack these into one word
//    weak refcount
//    storage pointer
//    autograd metadata pointer
//    named tensor metadata pointer
//    version counter pointer
//    PyObjectSlot
//    SizesAndStrides size/pointer
//    SizesAndStrides sizes (pre-allocated 0)
//    SizesAndStrides sizes (pre-allocated 1)
//    SizesAndStrides sizes (pre-allocated 2)
//    SizesAndStrides sizes (pre-allocated 3)
//    SizesAndStrides sizes (pre-allocated 4)
//    SizesAndStrides strides (pre-allocated 0)
//    SizesAndStrides strides (pre-allocated 1)
//    SizesAndStrides strides (pre-allocated 2)
//    SizesAndStrides strides (pre-allocated 3)
//    SizesAndStrides strides (pre-allocated 4)
//    storage offset
//    numel
//    data type, device, is_contiguous, storage_access_should_throw_, bitfields
//    DispatchKeySet
//

// Various preprocessor macros we use to check that the
// TensorImpl size hasn't changed unexpectedly. We undef
// these later.
// #ifndef __NVCC__
public static final int C10_NVCC = 0;
// #else
// #endif

// #ifndef __CUDA_VER_MAJOR__
public static final int C10_CUDA_VERSION_MAJOR = 0;
// #else
// #endif

// #ifndef CUDA_VERSION
public static final int C10_CUDA_VERSION = 0;
// #else
// #endif

// #ifndef __clang_major__
public static final int C10_CLANG_MAJOR_VERSION = 0;
// #else
// #endif

// #ifndef __GNUC__
public static final int C10_GCC_VERSION = 0;
// #else
// #endif

// #ifndef __GNUC_MINOR__
public static final int C10_GCC_VERSION_MINOR = 0;
// #else
// #endif

// We use a templatized class to both contain the logic of checking the sizes
// as well as to provide compile-time information that might be useful in
// figuring out why sizes may have changed.
// All the compile time information is given by the template fields that are
// always printed by the compiler when the static_assert fails.

// We use a class to encapsulate size-checking logic with
// templates to capture sizes and flags. We call this within
// a static assert to prove there is no run-time behaviour.
// Since the methods we call return either true or fail their
// own static_asserts, we should never see the error messages
// below. We have to provide it though for c++ <17.

// Clean up after ourselves
// #undef C10_NVCC
// #undef C10_CUDA_VERSION_MAJOR
// #undef C10_CUDA_VERSION
// #undef C10_CLANG_MAJOR_VERSION
// #undef C10_GCC_VERSION
// #undef C10_GCC_VERSION_MINOR

 // namespace c10



// Parsed from c10/core/UndefinedTensorImpl.h

// #pragma once

// #include <c10/core/TensorImpl.h>
// Targeting ../UndefinedTensorImpl.java



 // namespace c10


// Parsed from c10/core/WrapDimMinimal.h

// #pragma once

// #include <c10/core/SymInt.h>
// #include <c10/util/Exception.h>
// This template can only be specialized at int64_t and c10::SymInt;
// you'll get linker errors otherwise
 // namespace detail

@Namespace("c10") public static native @Cast("int64_t") long maybe_wrap_dim(
    @Cast("int64_t") long dim,
    @Cast("int64_t") long dim_post_expr,
    @Cast("bool") boolean wrap_scalar/*=true*/);
@Namespace("c10") public static native @Cast("int64_t") long maybe_wrap_dim(
    @Cast("int64_t") long dim,
    @Cast("int64_t") long dim_post_expr);

@Namespace("c10") public static native @ByVal SymInt maybe_wrap_dim(
    @ByVal SymInt dim,
    @ByVal SymInt dim_post_expr,
    @Cast("bool") boolean wrap_scalar/*=true*/);
@Namespace("c10") public static native @ByVal SymInt maybe_wrap_dim(
    @ByVal SymInt dim,
    @ByVal SymInt dim_post_expr);

 // namespace c10


// Parsed from ATen/core/symbol.h

// #pragma once
// #include <c10/macros/Export.h>
// #include <cstdint>
// #include <functional>  // For std::hash
// #include <string>

// 'prim' symbols are synthetic operators that occur only in the IR
// and don't have corresponding implementations in ATen.

// 'onnx' symbols correspond to ONNX operators.  Their semantics
// are defined in https://github.com/onnx/onnx/blob/master/docs/Operators.md
// The particular version we are targeting is specified by '_onnx_opset_version'
// in torch.onnx.symbolic_helper
//
// In general, most ONNX operators won't get an entry here, because they
// are handled from the Python end.  However, you may occasionally need
// to intern an ONNX symbol here so that you can conveniently write an
// optimization on ONNX operations.

// 'attr' symbols are attribute keys.  They are shared between both ONNX and ATen
// operators (you disambiguate their meaning by looking at the operator itself).
// In general, you only need to define attribute keys that are used by
// onnx or prim; ATen attributes are automatically generated in FORALL_ATTR_BASE_SYMBOLS.

// Note [Symbol allocation]
// ~~~~~~~~~~~~~~~~~~~~~~~~
//
//  1. Symbol namespace is split up into namespaces.
//
//  2. The intended access pattern for built-in symbols is onnx::MatMul
//  in the c10 namespace (this is a Symbol).
//

// Built-in constant definition strategy:
// - Enum is the most convenient way to generate a contiguous sequence
//   of numbers for an identifier.
// - However, an enum gives you a fresh type.  We want onnx::MatMul to
//   be type Symbol, not some random enum type!
// - Therefore, after using enums to generate the sequence of integers,
//   we then declare constexpr Symbols to get everything the actual Symbol
//   type we want.  Symbols must be constexpr to be valid to be "case"ed on.


// Targeting ../Symbol.java
















// Targeting ../SymbolHash.java





// Parsed from ATen/core/aten_interned_strings.h

// #pragma once

// @generated by torchgen/gen.py from aten_interned_strings.h

// #if defined(TORCH_ASSERT_NO_OPERATORS) || defined(TORCH_ASSERT_ONLY_METHOD_OPERATORS)
// #error This change adds a dependency on native_functions.yaml,
//   meaning the file will need to be re-compiled every time an operator
//   is changed or added. Consider if including <ATen/core/symbol.h> for
//   the c10::Symbol class would be sufficient, or if your change would be
//   better placed in another file.
// #endif

// ATen symbols correspond exactly to operators defined in ATen. Every
// symbol here corresponds exactly to an ATen operation defined in
// native_functions.yaml; attributes are in one-to-one correspondence
// with their ATen name.

// #define FORALL_ATEN_BASE_SYMBOLS(_)
// _(aten, __and__)
// _(aten, __iand__)
// _(aten, __ilshift__)
// _(aten, __ior__)
// _(aten, __irshift__)
// _(aten, __ixor__)
// _(aten, __lshift__)
// _(aten, __or__)
// _(aten, __rshift__)
// _(aten, __xor__)
// _(aten, _adaptive_avg_pool2d)
// _(aten, _adaptive_avg_pool2d_backward)
// _(aten, _adaptive_avg_pool3d)
// _(aten, _adaptive_avg_pool3d_backward)
// _(aten, _add_batch_dim)
// _(aten, _add_relu)
// _(aten, _add_relu_)
// _(aten, _addmm_activation)
// _(aten, _aminmax)
// _(aten, _amp_foreach_non_finite_check_and_unscale)
// _(aten, _amp_foreach_non_finite_check_and_unscale_)
// _(aten, _amp_update_scale)
// _(aten, _amp_update_scale_)
// _(aten, _assert_async)
// _(aten, _assert_tensor_metadata)
// _(aten, _autocast_to_full_precision)
// _(aten, _autocast_to_reduced_precision)
// _(aten, _backward)
// _(aten, _batch_norm_impl_index)
// _(aten, _batch_norm_impl_index_backward)
// _(aten, _cast_Byte)
// _(aten, _cast_Char)
// _(aten, _cast_Double)
// _(aten, _cast_Float)
// _(aten, _cast_Half)
// _(aten, _cast_Int)
// _(aten, _cast_Long)
// _(aten, _cast_Short)
// _(aten, _cdist_backward)
// _(aten, _cdist_forward)
// _(aten, _cholesky_solve_helper)
// _(aten, _choose_qparams_per_tensor)
// _(aten, _chunk_grad_outputs_efficient_attention)
// _(aten, _coalesce)
// _(aten, _coalesced)
// _(aten, _coalesced_)
// _(aten, _compute_linear_combination)
// _(aten, _conj)
// _(aten, _conj_copy)
// _(aten, _conj_physical)
// _(aten, _conv_depthwise2d)
// _(aten, _convert_indices_from_coo_to_csr)
// _(aten, _convert_indices_from_csr_to_coo)
// _(aten, _convolution)
// _(aten, _convolution_double_backward)
// _(aten, _convolution_mode)
// _(aten, _copy_from)
// _(aten, _copy_from_and_resize)
// _(aten, _ctc_loss)
// _(aten, _ctc_loss_backward)
// _(aten, _cudnn_ctc_loss)
// _(aten, _cudnn_init_dropout_state)
// _(aten, _cudnn_rnn)
// _(aten, _cudnn_rnn_backward)
// _(aten, _cudnn_rnn_flatten_weight)
// _(aten, _cufft_clear_plan_cache)
// _(aten, _cufft_get_plan_cache_max_size)
// _(aten, _cufft_get_plan_cache_size)
// _(aten, _cufft_set_plan_cache_max_size)
// _(aten, _cummax_helper)
// _(aten, _cummin_helper)
// _(aten, _debug_has_internal_overlap)
// _(aten, _dimI)
// _(aten, _dimV)
// _(aten, _dim_arange)
// _(aten, _dirichlet_grad)
// _(aten, _efficient_attention_backward)
// _(aten, _efficient_attention_forward)
// _(aten, _efficientzerotensor)
// _(aten, _embedding_bag)
// _(aten, _embedding_bag_backward)
// _(aten, _embedding_bag_dense_backward)
// _(aten, _embedding_bag_forward_only)
// _(aten, _embedding_bag_per_sample_weights_backward)
// _(aten, _embedding_bag_sparse_backward)
// _(aten, _empty_affine_quantized)
// _(aten, _empty_per_channel_affine_quantized)
// _(aten, _euclidean_dist)
// _(aten, _fake_quantize_learnable_per_channel_affine)
// _(aten, _fake_quantize_learnable_per_channel_affine_backward)
// _(aten, _fake_quantize_learnable_per_tensor_affine)
// _(aten, _fake_quantize_learnable_per_tensor_affine_backward)
// _(aten, _fake_quantize_per_tensor_affine_cachemask_tensor_qparams)
// _(aten, _fft_c2c)
// _(aten, _fft_c2r)
// _(aten, _fft_r2c)
// _(aten, _flash_attention_backward)
// _(aten, _flash_attention_forward)
// _(aten, _foobar)
// _(aten, _foreach_abs)
// _(aten, _foreach_abs_)
// _(aten, _foreach_acos)
// _(aten, _foreach_acos_)
// _(aten, _foreach_add)
// _(aten, _foreach_add_)
// _(aten, _foreach_addcdiv)
// _(aten, _foreach_addcdiv_)
// _(aten, _foreach_addcmul)
// _(aten, _foreach_addcmul_)
// _(aten, _foreach_asin)
// _(aten, _foreach_asin_)
// _(aten, _foreach_atan)
// _(aten, _foreach_atan_)
// _(aten, _foreach_ceil)
// _(aten, _foreach_ceil_)
// _(aten, _foreach_clamp_max)
// _(aten, _foreach_clamp_max_)
// _(aten, _foreach_clamp_min)
// _(aten, _foreach_clamp_min_)
// _(aten, _foreach_cos)
// _(aten, _foreach_cos_)
// _(aten, _foreach_cosh)
// _(aten, _foreach_cosh_)
// _(aten, _foreach_div)
// _(aten, _foreach_div_)
// _(aten, _foreach_erf)
// _(aten, _foreach_erf_)
// _(aten, _foreach_erfc)
// _(aten, _foreach_erfc_)
// _(aten, _foreach_exp)
// _(aten, _foreach_exp_)
// _(aten, _foreach_expm1)
// _(aten, _foreach_expm1_)
// _(aten, _foreach_floor)
// _(aten, _foreach_floor_)
// _(aten, _foreach_frac)
// _(aten, _foreach_frac_)
// _(aten, _foreach_lerp)
// _(aten, _foreach_lerp_)
// _(aten, _foreach_lgamma)
// _(aten, _foreach_lgamma_)
// _(aten, _foreach_log)
// _(aten, _foreach_log10)
// _(aten, _foreach_log10_)
// _(aten, _foreach_log1p)
// _(aten, _foreach_log1p_)
// _(aten, _foreach_log2)
// _(aten, _foreach_log2_)
// _(aten, _foreach_log_)
// _(aten, _foreach_maximum)
// _(aten, _foreach_maximum_)
// _(aten, _foreach_minimum)
// _(aten, _foreach_minimum_)
// _(aten, _foreach_mul)
// _(aten, _foreach_mul_)
// _(aten, _foreach_neg)
// _(aten, _foreach_neg_)
// _(aten, _foreach_norm)
// _(aten, _foreach_reciprocal)
// _(aten, _foreach_reciprocal_)
// _(aten, _foreach_round)
// _(aten, _foreach_round_)
// _(aten, _foreach_sigmoid)
// _(aten, _foreach_sigmoid_)
// _(aten, _foreach_sin)
// _(aten, _foreach_sin_)
// _(aten, _foreach_sinh)
// _(aten, _foreach_sinh_)
// _(aten, _foreach_sqrt)
// _(aten, _foreach_sqrt_)
// _(aten, _foreach_sub)
// _(aten, _foreach_sub_)
// _(aten, _foreach_tan)
// _(aten, _foreach_tan_)
// _(aten, _foreach_tanh)
// _(aten, _foreach_tanh_)
// _(aten, _foreach_trunc)
// _(aten, _foreach_trunc_)
// _(aten, _foreach_zero)
// _(aten, _foreach_zero_)
// _(aten, _fused_adam)
// _(aten, _fused_adam_)
// _(aten, _fused_adamw)
// _(aten, _fused_adamw_)
// _(aten, _fused_dropout)
// _(aten, _fused_moving_avg_obs_fq_helper)
// _(aten, _fused_moving_avg_obs_fq_helper_functional)
// _(aten, _fused_sdp_choice)
// _(aten, _fw_primal)
// _(aten, _fw_primal_copy)
// _(aten, _gather_sparse_backward)
// _(aten, _grid_sampler_2d_cpu_fallback)
// _(aten, _grid_sampler_2d_cpu_fallback_backward)
// _(aten, _has_compatible_shallow_copy_type)
// _(aten, _has_same_storage_numel)
// _(aten, _histogramdd_bin_edges)
// _(aten, _histogramdd_from_bin_cts)
// _(aten, _histogramdd_from_bin_tensors)
// _(aten, _index_put_impl)
// _(aten, _index_put_impl_)
// _(aten, _indices)
// _(aten, _indices_copy)
// _(aten, _is_all_true)
// _(aten, _is_any_true)
// _(aten, _is_zerotensor)
// _(aten, _linalg_check_errors)
// _(aten, _linalg_det)
// _(aten, _linalg_eigh)
// _(aten, _linalg_slogdet)
// _(aten, _linalg_solve_ex)
// _(aten, _linalg_svd)
// _(aten, _local_scalar_dense)
// _(aten, _log_softmax)
// _(aten, _log_softmax_backward_data)
// _(aten, _logcumsumexp)
// _(aten, _lstm_mps)
// _(aten, _lu_with_info)
// _(aten, _make_dual)
// _(aten, _make_dual_copy)
// _(aten, _make_per_channel_quantized_tensor)
// _(aten, _make_per_tensor_quantized_tensor)
// _(aten, _masked_scale)
// _(aten, _masked_softmax)
// _(aten, _masked_softmax_backward)
// _(aten, _mkldnn_reshape)
// _(aten, _mkldnn_transpose)
// _(aten, _mkldnn_transpose_)
// _(aten, _mps_convolution)
// _(aten, _mps_convolution_transpose)
// _(aten, _native_batch_norm_legit)
// _(aten, _native_batch_norm_legit_functional)
// _(aten, _native_decoder_only_multi_head_attention)
// _(aten, _native_multi_head_attention)
// _(aten, _neg_view)
// _(aten, _neg_view_copy)
// _(aten, _nested_from_padded)
// _(aten, _nested_from_padded_and_nested_example)
// _(aten, _nested_select_backward)
// _(aten, _nested_sum_backward)
// _(aten, _nested_tensor_from_mask)
// _(aten, _nested_tensor_from_mask_left_aligned)
// _(aten, _nested_tensor_from_tensor_list)
// _(aten, _nested_tensor_offsets)
// _(aten, _nested_tensor_size)
// _(aten, _nested_tensor_softmax_with_shape)
// _(aten, _nested_tensor_strides)
// _(aten, _nested_view_from_buffer)
// _(aten, _nested_view_from_buffer_copy)
// _(aten, _new_zeros_with_same_feature_meta)
// _(aten, _nnpack_available)
// _(aten, _nnpack_spatial_convolution)
// _(aten, _nnz)
// _(aten, _pack_padded_sequence)
// _(aten, _pack_padded_sequence_backward)
// _(aten, _pad_circular)
// _(aten, _pad_enum)
// _(aten, _pad_packed_sequence)
// _(aten, _pdist_backward)
// _(aten, _pdist_forward)
// _(aten, _pin_memory)
// _(aten, _prelu_kernel)
// _(aten, _prelu_kernel_backward)
// _(aten, _remove_batch_dim)
// _(aten, _reshape_alias)
// _(aten, _reshape_alias_copy)
// _(aten, _reshape_copy)
// _(aten, _reshape_from_tensor)
// _(aten, _resize_output)
// _(aten, _resize_output_)
// _(aten, _rowwise_prune)
// _(aten, _sample_dirichlet)
// _(aten, _saturate_weight_to_fp16)
// _(aten, _scaled_dot_product_attention)
// _(aten, _scaled_dot_product_attention_math)
// _(aten, _scaled_dot_product_efficient_attention)
// _(aten, _scaled_dot_product_efficient_attention_backward)
// _(aten, _scaled_dot_product_flash_attention)
// _(aten, _scaled_dot_product_flash_attention_backward)
// _(aten, _segment_reduce_backward)
// _(aten, _shape_as_tensor)
// _(aten, _slow_conv2d_backward)
// _(aten, _slow_conv2d_forward)
// _(aten, _sobol_engine_draw)
// _(aten, _sobol_engine_ff)
// _(aten, _sobol_engine_ff_)
// _(aten, _sobol_engine_initialize_state)
// _(aten, _sobol_engine_initialize_state_)
// _(aten, _sobol_engine_scramble)
// _(aten, _sobol_engine_scramble_)
// _(aten, _softmax)
// _(aten, _softmax_backward_data)
// _(aten, _sparse_addmm)
// _(aten, _sparse_broadcast_to)
// _(aten, _sparse_broadcast_to_copy)
// _(aten, _sparse_bsc_tensor_unsafe)
// _(aten, _sparse_bsr_tensor_unsafe)
// _(aten, _sparse_compressed_tensor_unsafe)
// _(aten, _sparse_coo_tensor_unsafe)
// _(aten, _sparse_coo_tensor_with_dims)
// _(aten, _sparse_coo_tensor_with_dims_and_tensors)
// _(aten, _sparse_csc_tensor_unsafe)
// _(aten, _sparse_csr_prod)
// _(aten, _sparse_csr_sum)
// _(aten, _sparse_csr_tensor_unsafe)
// _(aten, _sparse_log_softmax)
// _(aten, _sparse_log_softmax_backward_data)
// _(aten, _sparse_mm)
// _(aten, _sparse_mm_reduce_impl)
// _(aten, _sparse_mm_reduce_impl_backward)
// _(aten, _sparse_softmax)
// _(aten, _sparse_softmax_backward_data)
// _(aten, _sparse_sparse_matmul)
// _(aten, _sparse_sum)
// _(aten, _sparse_sum_backward)
// _(aten, _spdiags)
// _(aten, _stack)
// _(aten, _standard_gamma)
// _(aten, _standard_gamma_grad)
// _(aten, _test_ambiguous_defaults)
// _(aten, _test_autograd_multiple_dispatch)
// _(aten, _test_autograd_multiple_dispatch_view)
// _(aten, _test_autograd_multiple_dispatch_view_copy)
// _(aten, _test_check_tensor)
// _(aten, _test_optional_filled_intlist)
// _(aten, _test_optional_floatlist)
// _(aten, _test_optional_intlist)
// _(aten, _test_serialization_subcmul)
// _(aten, _test_string_default)
// _(aten, _test_warn_in_autograd)
// _(aten, _thnn_differentiable_gru_cell_backward)
// _(aten, _thnn_differentiable_lstm_cell_backward)
// _(aten, _thnn_fused_gru_cell)
// _(aten, _thnn_fused_gru_cell_backward)
// _(aten, _thnn_fused_lstm_cell)
// _(aten, _thnn_fused_lstm_cell_backward)
// _(aten, _thnn_fused_lstm_cell_backward_impl)
// _(aten, _to_copy)
// _(aten, _to_cpu)
// _(aten, _to_dense)
// _(aten, _transform_bias_rescale_qkv)
// _(aten, _transformer_decoder_only_layer_fwd)
// _(aten, _transformer_encoder_layer_fwd)
// _(aten, _trilinear)
// _(aten, _triton_multi_head_attention)
// _(aten, _triton_scaled_dot_attention)
// _(aten, _unique)
// _(aten, _unique2)
// _(aten, _unpack_dual)
// _(aten, _unsafe_view)
// _(aten, _upsample_bicubic2d_aa)
// _(aten, _upsample_bicubic2d_aa_backward)
// _(aten, _upsample_bilinear2d_aa)
// _(aten, _upsample_bilinear2d_aa_backward)
// _(aten, _upsample_nearest_exact1d)
// _(aten, _upsample_nearest_exact1d_backward)
// _(aten, _upsample_nearest_exact2d)
// _(aten, _upsample_nearest_exact2d_backward)
// _(aten, _upsample_nearest_exact3d)
// _(aten, _upsample_nearest_exact3d_backward)
// _(aten, _use_cudnn_ctc_loss)
// _(aten, _use_cudnn_rnn_flatten_weight)
// _(aten, _validate_compressed_sparse_indices)
// _(aten, _validate_sparse_bsc_tensor_args)
// _(aten, _validate_sparse_bsr_tensor_args)
// _(aten, _validate_sparse_compressed_tensor_args)
// _(aten, _validate_sparse_coo_tensor_args)
// _(aten, _validate_sparse_csc_tensor_args)
// _(aten, _validate_sparse_csr_tensor_args)
// _(aten, _values)
// _(aten, _values_copy)
// _(aten, _version)
// _(aten, _weight_norm)
// _(aten, _weight_norm_differentiable_backward)
// _(aten, _weight_norm_interface)
// _(aten, _weight_norm_interface_backward)
// _(aten, abs)
// _(aten, abs_)
// _(aten, absolute)
// _(aten, absolute_)
// _(aten, acos)
// _(aten, acos_)
// _(aten, acosh)
// _(aten, acosh_)
// _(aten, adaptive_avg_pool1d)
// _(aten, adaptive_avg_pool2d)
// _(aten, adaptive_avg_pool3d)
// _(aten, adaptive_avg_pool3d_backward)
// _(aten, adaptive_max_pool1d)
// _(aten, adaptive_max_pool2d)
// _(aten, adaptive_max_pool2d_backward)
// _(aten, adaptive_max_pool3d)
// _(aten, adaptive_max_pool3d_backward)
// _(aten, add)
// _(aten, add_)
// _(aten, addbmm)
// _(aten, addbmm_)
// _(aten, addcdiv)
// _(aten, addcdiv_)
// _(aten, addcmul)
// _(aten, addcmul_)
// _(aten, addmm)
// _(aten, addmm_)
// _(aten, addmv)
// _(aten, addmv_)
// _(aten, addr)
// _(aten, addr_)
// _(aten, adjoint)
// _(aten, affine_grid_generator)
// _(aten, affine_grid_generator_backward)
// _(aten, alias)
// _(aten, alias_copy)
// _(aten, align_as)
// _(aten, align_tensors)
// _(aten, align_to)
// _(aten, all)
// _(aten, allclose)
// _(aten, alpha_dropout)
// _(aten, alpha_dropout_)
// _(aten, amax)
// _(aten, amin)
// _(aten, aminmax)
// _(aten, angle)
// _(aten, any)
// _(aten, arange)
// _(aten, arccos)
// _(aten, arccos_)
// _(aten, arccosh)
// _(aten, arccosh_)
// _(aten, arcsin)
// _(aten, arcsin_)
// _(aten, arcsinh)
// _(aten, arcsinh_)
// _(aten, arctan)
// _(aten, arctan2)
// _(aten, arctan2_)
// _(aten, arctan_)
// _(aten, arctanh)
// _(aten, arctanh_)
// _(aten, argmax)
// _(aten, argmin)
// _(aten, argsort)
// _(aten, argwhere)
// _(aten, as_strided)
// _(aten, as_strided_)
// _(aten, as_strided_copy)
// _(aten, as_strided_scatter)
// _(aten, asin)
// _(aten, asin_)
// _(aten, asinh)
// _(aten, asinh_)
// _(aten, atan)
// _(aten, atan2)
// _(aten, atan2_)
// _(aten, atan_)
// _(aten, atanh)
// _(aten, atanh_)
// _(aten, atleast_1d)
// _(aten, atleast_2d)
// _(aten, atleast_3d)
// _(aten, avg_pool1d)
// _(aten, avg_pool2d)
// _(aten, avg_pool2d_backward)
// _(aten, avg_pool3d)
// _(aten, avg_pool3d_backward)
// _(aten, baddbmm)
// _(aten, baddbmm_)
// _(aten, bartlett_window)
// _(aten, batch_norm)
// _(aten, batch_norm_backward_elemt)
// _(aten, batch_norm_backward_reduce)
// _(aten, batch_norm_elemt)
// _(aten, batch_norm_gather_stats)
// _(aten, batch_norm_gather_stats_with_counts)
// _(aten, batch_norm_stats)
// _(aten, batch_norm_update_stats)
// _(aten, bernoulli)
// _(aten, bernoulli_)
// _(aten, bilinear)
// _(aten, binary_cross_entropy)
// _(aten, binary_cross_entropy_backward)
// _(aten, binary_cross_entropy_with_logits)
// _(aten, bincount)
// _(aten, binomial)
// _(aten, bitwise_and)
// _(aten, bitwise_and_)
// _(aten, bitwise_left_shift)
// _(aten, bitwise_left_shift_)
// _(aten, bitwise_not)
// _(aten, bitwise_not_)
// _(aten, bitwise_or)
// _(aten, bitwise_or_)
// _(aten, bitwise_right_shift)
// _(aten, bitwise_right_shift_)
// _(aten, bitwise_xor)
// _(aten, bitwise_xor_)
// _(aten, blackman_window)
// _(aten, block_diag)
// _(aten, bmm)
// _(aten, broadcast_tensors)
// _(aten, broadcast_to)
// _(aten, bucketize)
// _(aten, can_cast)
// _(aten, cartesian_prod)
// _(aten, cat)
// _(aten, cauchy)
// _(aten, cauchy_)
// _(aten, ccol_indices)
// _(aten, ccol_indices_copy)
// _(aten, cdist)
// _(aten, ceil)
// _(aten, ceil_)
// _(aten, celu)
// _(aten, celu_)
// _(aten, chain_matmul)
// _(aten, chalf)
// _(aten, channel_shuffle)
// _(aten, cholesky)
// _(aten, cholesky_inverse)
// _(aten, cholesky_solve)
// _(aten, choose_qparams_optimized)
// _(aten, chunk)
// _(aten, clamp)
// _(aten, clamp_)
// _(aten, clamp_max)
// _(aten, clamp_max_)
// _(aten, clamp_min)
// _(aten, clamp_min_)
// _(aten, clip)
// _(aten, clip_)
// _(aten, clone)
// _(aten, coalesce)
// _(aten, col2im)
// _(aten, col_indices)
// _(aten, col_indices_copy)
// _(aten, column_stack)
// _(aten, combinations)
// _(aten, complex)
// _(aten, concat)
// _(aten, concatenate)
// _(aten, conj)
// _(aten, conj_physical)
// _(aten, conj_physical_)
// _(aten, constant_pad_nd)
// _(aten, contiguous)
// _(aten, conv1d)
// _(aten, conv2d)
// _(aten, conv3d)
// _(aten, conv_depthwise3d)
// _(aten, conv_tbc)
// _(aten, conv_tbc_backward)
// _(aten, conv_transpose1d)
// _(aten, conv_transpose2d)
// _(aten, conv_transpose3d)
// _(aten, convolution)
// _(aten, convolution_backward)
// _(aten, convolution_backward_overrideable)
// _(aten, convolution_overrideable)
// _(aten, copy)
// _(aten, copy_)
// _(aten, copy_sparse_to_sparse)
// _(aten, copy_sparse_to_sparse_)
// _(aten, copysign)
// _(aten, copysign_)
// _(aten, corrcoef)
// _(aten, cos)
// _(aten, cos_)
// _(aten, cosh)
// _(aten, cosh_)
// _(aten, cosine_embedding_loss)
// _(aten, cosine_similarity)
// _(aten, count_nonzero)
// _(aten, cov)
// _(aten, cross)
// _(aten, cross_entropy_loss)
// _(aten, crow_indices)
// _(aten, crow_indices_copy)
// _(aten, ctc_loss)
// _(aten, cudnn_affine_grid_generator)
// _(aten, cudnn_affine_grid_generator_backward)
// _(aten, cudnn_batch_norm)
// _(aten, cudnn_batch_norm_backward)
// _(aten, cudnn_convolution)
// _(aten, cudnn_convolution_add_relu)
// _(aten, cudnn_convolution_relu)
// _(aten, cudnn_convolution_transpose)
// _(aten, cudnn_grid_sampler)
// _(aten, cudnn_grid_sampler_backward)
// _(aten, cudnn_is_acceptable)
// _(aten, cummax)
// _(aten, cummaxmin_backward)
// _(aten, cummin)
// _(aten, cumprod)
// _(aten, cumprod_)
// _(aten, cumprod_backward)
// _(aten, cumsum)
// _(aten, cumsum_)
// _(aten, cumulative_trapezoid)
// _(aten, data)
// _(aten, deg2rad)
// _(aten, deg2rad_)
// _(aten, dense_dim)
// _(aten, dequantize)
// _(aten, det)
// _(aten, detach)
// _(aten, detach_)
// _(aten, detach_copy)
// _(aten, diag)
// _(aten, diag_embed)
// _(aten, diagflat)
// _(aten, diagonal)
// _(aten, diagonal_backward)
// _(aten, diagonal_copy)
// _(aten, diagonal_scatter)
// _(aten, diff)
// _(aten, digamma)
// _(aten, digamma_)
// _(aten, dist)
// _(aten, div)
// _(aten, div_)
// _(aten, divide)
// _(aten, divide_)
// _(aten, dot)
// _(aten, dropout)
// _(aten, dropout_)
// _(aten, dsplit)
// _(aten, dstack)
// _(aten, einsum)
// _(aten, elu)
// _(aten, elu_)
// _(aten, elu_backward)
// _(aten, embedding)
// _(aten, embedding_backward)
// _(aten, embedding_bag)
// _(aten, embedding_dense_backward)
// _(aten, embedding_renorm)
// _(aten, embedding_renorm_)
// _(aten, embedding_sparse_backward)
// _(aten, empty)
// _(aten, empty_like)
// _(aten, empty_quantized)
// _(aten, empty_strided)
// _(aten, eq)
// _(aten, eq_)
// _(aten, equal)
// _(aten, erf)
// _(aten, erf_)
// _(aten, erfc)
// _(aten, erfc_)
// _(aten, erfinv)
// _(aten, erfinv_)
// _(aten, exp)
// _(aten, exp2)
// _(aten, exp2_)
// _(aten, exp_)
// _(aten, expand)
// _(aten, expand_as)
// _(aten, expand_copy)
// _(aten, expm1)
// _(aten, expm1_)
// _(aten, exponential)
// _(aten, exponential_)
// _(aten, eye)
// _(aten, fake_quantize_per_channel_affine)
// _(aten, fake_quantize_per_channel_affine_cachemask)
// _(aten, fake_quantize_per_channel_affine_cachemask_backward)
// _(aten, fake_quantize_per_tensor_affine)
// _(aten, fake_quantize_per_tensor_affine_cachemask)
// _(aten, fake_quantize_per_tensor_affine_cachemask_backward)
// _(aten, fbgemm_linear_fp16_weight)
// _(aten, fbgemm_linear_fp16_weight_fp32_activation)
// _(aten, fbgemm_linear_int8_weight)
// _(aten, fbgemm_linear_int8_weight_fp32_activation)
// _(aten, fbgemm_linear_quantize_weight)
// _(aten, fbgemm_pack_gemm_matrix_fp16)
// _(aten, fbgemm_pack_quantized_matrix)
// _(aten, feature_alpha_dropout)
// _(aten, feature_alpha_dropout_)
// _(aten, feature_dropout)
// _(aten, feature_dropout_)
// _(aten, fft_fft)
// _(aten, fft_fft2)
// _(aten, fft_fftfreq)
// _(aten, fft_fftn)
// _(aten, fft_fftshift)
// _(aten, fft_hfft)
// _(aten, fft_hfft2)
// _(aten, fft_hfftn)
// _(aten, fft_ifft)
// _(aten, fft_ifft2)
// _(aten, fft_ifftn)
// _(aten, fft_ifftshift)
// _(aten, fft_ihfft)
// _(aten, fft_ihfft2)
// _(aten, fft_ihfftn)
// _(aten, fft_irfft)
// _(aten, fft_irfft2)
// _(aten, fft_irfftn)
// _(aten, fft_rfft)
// _(aten, fft_rfft2)
// _(aten, fft_rfftfreq)
// _(aten, fft_rfftn)
// _(aten, fill)
// _(aten, fill_)
// _(aten, fill_diagonal)
// _(aten, fill_diagonal_)
// _(aten, fix)
// _(aten, fix_)
// _(aten, flatten)
// _(aten, flatten_dense_tensors)
// _(aten, flip)
// _(aten, fliplr)
// _(aten, flipud)
// _(aten, float_power)
// _(aten, float_power_)
// _(aten, floor)
// _(aten, floor_)
// _(aten, floor_divide)
// _(aten, floor_divide_)
// _(aten, fmax)
// _(aten, fmin)
// _(aten, fmod)
// _(aten, fmod_)
// _(aten, frac)
// _(aten, frac_)
// _(aten, fractional_max_pool2d)
// _(aten, fractional_max_pool2d_backward)
// _(aten, fractional_max_pool3d)
// _(aten, fractional_max_pool3d_backward)
// _(aten, frexp)
// _(aten, frobenius_norm)
// _(aten, from_file)
// _(aten, full)
// _(aten, full_like)
// _(aten, fused_moving_avg_obs_fake_quant)
// _(aten, gather)
// _(aten, gather_backward)
// _(aten, gcd)
// _(aten, gcd_)
// _(aten, ge)
// _(aten, ge_)
// _(aten, gelu)
// _(aten, gelu_)
// _(aten, gelu_backward)
// _(aten, geometric)
// _(aten, geometric_)
// _(aten, geqrf)
// _(aten, ger)
// _(aten, glu)
// _(aten, glu_backward)
// _(aten, glu_backward_jvp)
// _(aten, glu_jvp)
// _(aten, gradient)
// _(aten, greater)
// _(aten, greater_)
// _(aten, greater_equal)
// _(aten, greater_equal_)
// _(aten, grid_sampler)
// _(aten, grid_sampler_2d)
// _(aten, grid_sampler_2d_backward)
// _(aten, grid_sampler_3d)
// _(aten, grid_sampler_3d_backward)
// _(aten, group_norm)
// _(aten, gru)
// _(aten, gru_cell)
// _(aten, gt)
// _(aten, gt_)
// _(aten, hamming_window)
// _(aten, hann_window)
// _(aten, hardshrink)
// _(aten, hardshrink_backward)
// _(aten, hardsigmoid)
// _(aten, hardsigmoid_)
// _(aten, hardsigmoid_backward)
// _(aten, hardswish)
// _(aten, hardswish_)
// _(aten, hardswish_backward)
// _(aten, hardtanh)
// _(aten, hardtanh_)
// _(aten, hardtanh_backward)
// _(aten, heaviside)
// _(aten, heaviside_)
// _(aten, hinge_embedding_loss)
// _(aten, histc)
// _(aten, histogram)
// _(aten, histogramdd)
// _(aten, hsplit)
// _(aten, hspmm)
// _(aten, hstack)
// _(aten, huber_loss)
// _(aten, huber_loss_backward)
// _(aten, hypot)
// _(aten, hypot_)
// _(aten, i0)
// _(aten, i0_)
// _(aten, igamma)
// _(aten, igamma_)
// _(aten, igammac)
// _(aten, igammac_)
// _(aten, im2col)
// _(aten, imag)
// _(aten, index)
// _(aten, index_add)
// _(aten, index_add_)
// _(aten, index_copy)
// _(aten, index_copy_)
// _(aten, index_fill)
// _(aten, index_fill_)
// _(aten, index_put)
// _(aten, index_put_)
// _(aten, index_reduce)
// _(aten, index_reduce_)
// _(aten, index_select)
// _(aten, index_select_backward)
// _(aten, indices)
// _(aten, indices_copy)
// _(aten, infinitely_differentiable_gelu_backward)
// _(aten, inner)
// _(aten, instance_norm)
// _(aten, int_repr)
// _(aten, inverse)
// _(aten, is_coalesced)
// _(aten, is_complex)
// _(aten, is_conj)
// _(aten, is_distributed)
// _(aten, is_floating_point)
// _(aten, is_inference)
// _(aten, is_leaf)
// _(aten, is_neg)
// _(aten, is_nonzero)
// _(aten, is_pinned)
// _(aten, is_same_size)
// _(aten, is_set_to)
// _(aten, is_signed)
// _(aten, is_vulkan_available)
// _(aten, isclose)
// _(aten, isfinite)
// _(aten, isin)
// _(aten, isinf)
// _(aten, isnan)
// _(aten, isneginf)
// _(aten, isposinf)
// _(aten, isreal)
// _(aten, istft)
// _(aten, item)
// _(aten, kaiser_window)
// _(aten, kl_div)
// _(aten, kron)
// _(aten, kthvalue)
// _(aten, l1_loss)
// _(aten, layer_norm)
// _(aten, lcm)
// _(aten, lcm_)
// _(aten, ldexp)
// _(aten, ldexp_)
// _(aten, le)
// _(aten, le_)
// _(aten, leaky_relu)
// _(aten, leaky_relu_)
// _(aten, leaky_relu_backward)
// _(aten, lerp)
// _(aten, lerp_)
// _(aten, less)
// _(aten, less_)
// _(aten, less_equal)
// _(aten, less_equal_)
// _(aten, lgamma)
// _(aten, lgamma_)
// _(aten, lift)
// _(aten, lift_fresh)
// _(aten, lift_fresh_copy)
// _(aten, linalg_cholesky)
// _(aten, linalg_cholesky_ex)
// _(aten, linalg_cond)
// _(aten, linalg_cross)
// _(aten, linalg_det)
// _(aten, linalg_diagonal)
// _(aten, linalg_eig)
// _(aten, linalg_eigh)
// _(aten, linalg_eigvals)
// _(aten, linalg_eigvalsh)
// _(aten, linalg_householder_product)
// _(aten, linalg_inv)
// _(aten, linalg_inv_ex)
// _(aten, linalg_ldl_factor)
// _(aten, linalg_ldl_factor_ex)
// _(aten, linalg_ldl_solve)
// _(aten, linalg_lstsq)
// _(aten, linalg_lu)
// _(aten, linalg_lu_factor)
// _(aten, linalg_lu_factor_ex)
// _(aten, linalg_lu_solve)
// _(aten, linalg_matmul)
// _(aten, linalg_matrix_exp)
// _(aten, linalg_matrix_norm)
// _(aten, linalg_matrix_power)
// _(aten, linalg_matrix_rank)
// _(aten, linalg_multi_dot)
// _(aten, linalg_norm)
// _(aten, linalg_pinv)
// _(aten, linalg_qr)
// _(aten, linalg_slogdet)
// _(aten, linalg_solve)
// _(aten, linalg_solve_ex)
// _(aten, linalg_solve_triangular)
// _(aten, linalg_svd)
// _(aten, linalg_svdvals)
// _(aten, linalg_tensorinv)
// _(aten, linalg_tensorsolve)
// _(aten, linalg_vander)
// _(aten, linalg_vecdot)
// _(aten, linalg_vector_norm)
// _(aten, linear)
// _(aten, linear_backward)
// _(aten, linspace)
// _(aten, log)
// _(aten, log10)
// _(aten, log10_)
// _(aten, log1p)
// _(aten, log1p_)
// _(aten, log2)
// _(aten, log2_)
// _(aten, log_)
// _(aten, log_normal)
// _(aten, log_normal_)
// _(aten, log_sigmoid)
// _(aten, log_sigmoid_backward)
// _(aten, log_sigmoid_forward)
// _(aten, log_softmax)
// _(aten, logaddexp)
// _(aten, logaddexp2)
// _(aten, logcumsumexp)
// _(aten, logdet)
// _(aten, logical_and)
// _(aten, logical_and_)
// _(aten, logical_not)
// _(aten, logical_not_)
// _(aten, logical_or)
// _(aten, logical_or_)
// _(aten, logical_xor)
// _(aten, logical_xor_)
// _(aten, logit)
// _(aten, logit_)
// _(aten, logit_backward)
// _(aten, logspace)
// _(aten, logsumexp)
// _(aten, lshift)
// _(aten, lstm)
// _(aten, lstm_cell)
// _(aten, lstm_mps_backward)
// _(aten, lt)
// _(aten, lt_)
// _(aten, lu_solve)
// _(aten, lu_unpack)
// _(aten, mH)
// _(aten, mT)
// _(aten, margin_ranking_loss)
// _(aten, masked_fill)
// _(aten, masked_fill_)
// _(aten, masked_scatter)
// _(aten, masked_scatter_)
// _(aten, masked_select)
// _(aten, masked_select_backward)
// _(aten, matmul)
// _(aten, matmul_backward)
// _(aten, matrix_H)
// _(aten, matrix_exp)
// _(aten, matrix_exp_backward)
// _(aten, matrix_power)
// _(aten, max)
// _(aten, max_pool1d)
// _(aten, max_pool1d_with_indices)
// _(aten, max_pool2d)
// _(aten, max_pool2d_backward)
// _(aten, max_pool2d_with_indices)
// _(aten, max_pool2d_with_indices_backward)
// _(aten, max_pool3d)
// _(aten, max_pool3d_with_indices)
// _(aten, max_pool3d_with_indices_backward)
// _(aten, max_unpool2d)
// _(aten, max_unpool3d)
// _(aten, maximum)
// _(aten, mean)
// _(aten, median)
// _(aten, meshgrid)
// _(aten, min)
// _(aten, minimum)
// _(aten, miopen_batch_norm)
// _(aten, miopen_batch_norm_backward)
// _(aten, miopen_convolution)
// _(aten, miopen_convolution_add_relu)
// _(aten, miopen_convolution_relu)
// _(aten, miopen_convolution_transpose)
// _(aten, miopen_depthwise_convolution)
// _(aten, miopen_rnn)
// _(aten, miopen_rnn_backward)
// _(aten, mish)
// _(aten, mish_)
// _(aten, mish_backward)
// _(aten, mkldnn_adaptive_avg_pool2d)
// _(aten, mkldnn_adaptive_avg_pool2d_backward)
// _(aten, mkldnn_convolution)
// _(aten, mkldnn_linear)
// _(aten, mkldnn_linear_backward)
// _(aten, mkldnn_linear_backward_input)
// _(aten, mkldnn_linear_backward_weights)
// _(aten, mkldnn_max_pool2d)
// _(aten, mkldnn_max_pool2d_backward)
// _(aten, mkldnn_max_pool3d)
// _(aten, mkldnn_max_pool3d_backward)
// _(aten, mkldnn_reorder_conv2d_weight)
// _(aten, mkldnn_reorder_conv3d_weight)
// _(aten, mkldnn_rnn_layer)
// _(aten, mkldnn_rnn_layer_backward)
// _(aten, mm)
// _(aten, mode)
// _(aten, moveaxis)
// _(aten, movedim)
// _(aten, mps_convolution_backward)
// _(aten, mps_convolution_transpose_backward)
// _(aten, mse_loss)
// _(aten, mse_loss_backward)
// _(aten, msort)
// _(aten, mul)
// _(aten, mul_)
// _(aten, multi_margin_loss)
// _(aten, multi_margin_loss_backward)
// _(aten, multilabel_margin_loss)
// _(aten, multilabel_margin_loss_backward)
// _(aten, multilabel_margin_loss_forward)
// _(aten, multinomial)
// _(aten, multiply)
// _(aten, multiply_)
// _(aten, mv)
// _(aten, mvlgamma)
// _(aten, mvlgamma_)
// _(aten, nan_to_num)
// _(aten, nan_to_num_)
// _(aten, nanmean)
// _(aten, nanmedian)
// _(aten, nanquantile)
// _(aten, nansum)
// _(aten, narrow)
// _(aten, narrow_copy)
// _(aten, native_batch_norm)
// _(aten, native_batch_norm_backward)
// _(aten, native_channel_shuffle)
// _(aten, native_dropout)
// _(aten, native_dropout_backward)
// _(aten, native_group_norm)
// _(aten, native_group_norm_backward)
// _(aten, native_layer_norm)
// _(aten, native_layer_norm_backward)
// _(aten, native_norm)
// _(aten, ne)
// _(aten, ne_)
// _(aten, neg)
// _(aten, neg_)
// _(aten, negative)
// _(aten, negative_)
// _(aten, nested_to_padded_tensor)
// _(aten, new_empty)
// _(aten, new_empty_strided)
// _(aten, new_full)
// _(aten, new_ones)
// _(aten, new_zeros)
// _(aten, nextafter)
// _(aten, nextafter_)
// _(aten, nll_loss)
// _(aten, nll_loss2d)
// _(aten, nll_loss2d_backward)
// _(aten, nll_loss2d_forward)
// _(aten, nll_loss_backward)
// _(aten, nll_loss_forward)
// _(aten, nll_loss_nd)
// _(aten, nonzero)
// _(aten, nonzero_numpy)
// _(aten, norm)
// _(aten, norm_except_dim)
// _(aten, normal)
// _(aten, normal_)
// _(aten, normal_functional)
// _(aten, not_equal)
// _(aten, not_equal_)
// _(aten, nuclear_norm)
// _(aten, numpy_T)
// _(aten, one_hot)
// _(aten, ones)
// _(aten, ones_like)
// _(aten, orgqr)
// _(aten, ormqr)
// _(aten, outer)
// _(aten, output_nr)
// _(aten, pad)
// _(aten, pad_sequence)
// _(aten, pairwise_distance)
// _(aten, pdist)
// _(aten, permute)
// _(aten, permute_copy)
// _(aten, pin_memory)
// _(aten, pinverse)
// _(aten, pixel_shuffle)
// _(aten, pixel_unshuffle)
// _(aten, poisson)
// _(aten, poisson_nll_loss)
// _(aten, polar)
// _(aten, polygamma)
// _(aten, polygamma_)
// _(aten, positive)
// _(aten, pow)
// _(aten, pow_)
// _(aten, prelu)
// _(aten, prod)
// _(aten, promote_types)
// _(aten, put)
// _(aten, put_)
// _(aten, q_per_channel_axis)
// _(aten, q_per_channel_scales)
// _(aten, q_per_channel_zero_points)
// _(aten, q_scale)
// _(aten, q_zero_point)
// _(aten, qr)
// _(aten, qscheme)
// _(aten, quantile)
// _(aten, quantize_per_channel)
// _(aten, quantize_per_tensor)
// _(aten, quantize_per_tensor_dynamic)
// _(aten, quantized_batch_norm)
// _(aten, quantized_gru_cell)
// _(aten, quantized_lstm_cell)
// _(aten, quantized_max_pool1d)
// _(aten, quantized_max_pool2d)
// _(aten, quantized_rnn_relu_cell)
// _(aten, quantized_rnn_tanh_cell)
// _(aten, rad2deg)
// _(aten, rad2deg_)
// _(aten, rand)
// _(aten, rand_like)
// _(aten, randint)
// _(aten, randint_like)
// _(aten, randn)
// _(aten, randn_like)
// _(aten, random)
// _(aten, random_)
// _(aten, randperm)
// _(aten, range)
// _(aten, ravel)
// _(aten, real)
// _(aten, reciprocal)
// _(aten, reciprocal_)
// _(aten, record_stream)
// _(aten, refine_names)
// _(aten, reflection_pad1d)
// _(aten, reflection_pad1d_backward)
// _(aten, reflection_pad2d)
// _(aten, reflection_pad2d_backward)
// _(aten, reflection_pad3d)
// _(aten, reflection_pad3d_backward)
// _(aten, relu)
// _(aten, relu6)
// _(aten, relu6_)
// _(aten, relu_)
// _(aten, remainder)
// _(aten, remainder_)
// _(aten, rename)
// _(aten, rename_)
// _(aten, renorm)
// _(aten, renorm_)
// _(aten, repeat)
// _(aten, repeat_interleave)
// _(aten, replication_pad1d)
// _(aten, replication_pad1d_backward)
// _(aten, replication_pad2d)
// _(aten, replication_pad2d_backward)
// _(aten, replication_pad3d)
// _(aten, replication_pad3d_backward)
// _(aten, requires_grad)
// _(aten, requires_grad_)
// _(aten, reshape)
// _(aten, reshape_as)
// _(aten, resize)
// _(aten, resize_)
// _(aten, resize_as)
// _(aten, resize_as_)
// _(aten, resize_as_sparse)
// _(aten, resize_as_sparse_)
// _(aten, resolve_conj)
// _(aten, resolve_neg)
// _(aten, result_type)
// _(aten, retain_grad)
// _(aten, retains_grad)
// _(aten, rnn_relu)
// _(aten, rnn_relu_cell)
// _(aten, rnn_tanh)
// _(aten, rnn_tanh_cell)
// _(aten, roll)
// _(aten, rot90)
// _(aten, round)
// _(aten, round_)
// _(aten, row_indices)
// _(aten, row_indices_copy)
// _(aten, row_stack)
// _(aten, rrelu)
// _(aten, rrelu_)
// _(aten, rrelu_with_noise)
// _(aten, rrelu_with_noise_)
// _(aten, rrelu_with_noise_backward)
// _(aten, rshift)
// _(aten, rsqrt)
// _(aten, rsqrt_)
// _(aten, rsub)
// _(aten, scalar_tensor)
// _(aten, scaled_dot_product_attention)
// _(aten, scatter)
// _(aten, scatter_)
// _(aten, scatter_add)
// _(aten, scatter_add_)
// _(aten, scatter_reduce)
// _(aten, scatter_reduce_)
// _(aten, searchsorted)
// _(aten, segment_reduce)
// _(aten, select)
// _(aten, select_backward)
// _(aten, select_copy)
// _(aten, select_scatter)
// _(aten, selu)
// _(aten, selu_)
// _(aten, set)
// _(aten, set_)
// _(aten, set_data)
// _(aten, sgn)
// _(aten, sgn_)
// _(aten, sigmoid)
// _(aten, sigmoid_)
// _(aten, sigmoid_backward)
// _(aten, sign)
// _(aten, sign_)
// _(aten, signbit)
// _(aten, silu)
// _(aten, silu_)
// _(aten, silu_backward)
// _(aten, sin)
// _(aten, sin_)
// _(aten, sinc)
// _(aten, sinc_)
// _(aten, sinh)
// _(aten, sinh_)
// _(aten, size)
// _(aten, slice)
// _(aten, slice_backward)
// _(aten, slice_copy)
// _(aten, slice_scatter)
// _(aten, slogdet)
// _(aten, slow_conv3d)
// _(aten, slow_conv3d_forward)
// _(aten, slow_conv_dilated2d)
// _(aten, slow_conv_dilated3d)
// _(aten, slow_conv_transpose2d)
// _(aten, slow_conv_transpose3d)
// _(aten, smm)
// _(aten, smooth_l1_loss)
// _(aten, smooth_l1_loss_backward)
// _(aten, soft_margin_loss)
// _(aten, soft_margin_loss_backward)
// _(aten, softmax)
// _(aten, softplus)
// _(aten, softplus_backward)
// _(aten, softshrink)
// _(aten, softshrink_backward)
// _(aten, sort)
// _(aten, sparse_bsc_tensor)
// _(aten, sparse_bsr_tensor)
// _(aten, sparse_compressed_tensor)
// _(aten, sparse_coo_tensor)
// _(aten, sparse_csc_tensor)
// _(aten, sparse_csr_tensor)
// _(aten, sparse_dim)
// _(aten, sparse_mask)
// _(aten, sparse_resize)
// _(aten, sparse_resize_)
// _(aten, sparse_resize_and_clear)
// _(aten, sparse_resize_and_clear_)
// _(aten, sparse_sampled_addmm)
// _(aten, special_airy_ai)
// _(aten, special_bessel_j0)
// _(aten, special_bessel_j1)
// _(aten, special_bessel_y0)
// _(aten, special_bessel_y1)
// _(aten, special_chebyshev_polynomial_t)
// _(aten, special_chebyshev_polynomial_u)
// _(aten, special_chebyshev_polynomial_v)
// _(aten, special_chebyshev_polynomial_w)
// _(aten, special_digamma)
// _(aten, special_entr)
// _(aten, special_erf)
// _(aten, special_erfc)
// _(aten, special_erfcx)
// _(aten, special_erfinv)
// _(aten, special_exp2)
// _(aten, special_expit)
// _(aten, special_expm1)
// _(aten, special_gammainc)
// _(aten, special_gammaincc)
// _(aten, special_gammaln)
// _(aten, special_hermite_polynomial_h)
// _(aten, special_hermite_polynomial_he)
// _(aten, special_i0)
// _(aten, special_i0e)
// _(aten, special_i1)
// _(aten, special_i1e)
// _(aten, special_laguerre_polynomial_l)
// _(aten, special_legendre_polynomial_p)
// _(aten, special_log1p)
// _(aten, special_log_ndtr)
// _(aten, special_log_softmax)
// _(aten, special_logit)
// _(aten, special_logsumexp)
// _(aten, special_modified_bessel_i0)
// _(aten, special_modified_bessel_i1)
// _(aten, special_modified_bessel_k0)
// _(aten, special_modified_bessel_k1)
// _(aten, special_multigammaln)
// _(aten, special_ndtr)
// _(aten, special_ndtri)
// _(aten, special_polygamma)
// _(aten, special_psi)
// _(aten, special_round)
// _(aten, special_scaled_modified_bessel_k0)
// _(aten, special_scaled_modified_bessel_k1)
// _(aten, special_shifted_chebyshev_polynomial_t)
// _(aten, special_shifted_chebyshev_polynomial_u)
// _(aten, special_shifted_chebyshev_polynomial_v)
// _(aten, special_shifted_chebyshev_polynomial_w)
// _(aten, special_sinc)
// _(aten, special_softmax)
// _(aten, special_spherical_bessel_j0)
// _(aten, special_xlog1py)
// _(aten, special_xlogy)
// _(aten, special_zeta)
// _(aten, split)
// _(aten, split_copy)
// _(aten, split_with_sizes)
// _(aten, split_with_sizes_copy)
// _(aten, sqrt)
// _(aten, sqrt_)
// _(aten, square)
// _(aten, square_)
// _(aten, squeeze)
// _(aten, squeeze_)
// _(aten, squeeze_copy)
// _(aten, sspaddmm)
// _(aten, stack)
// _(aten, std)
// _(aten, std_mean)
// _(aten, stft)
// _(aten, stride)
// _(aten, sub)
// _(aten, sub_)
// _(aten, subtract)
// _(aten, subtract_)
// _(aten, sum)
// _(aten, sum_to_size)
// _(aten, svd)
// _(aten, swapaxes)
// _(aten, swapaxes_)
// _(aten, swapdims)
// _(aten, swapdims_)
// _(aten, t)
// _(aten, t_)
// _(aten, t_copy)
// _(aten, take)
// _(aten, take_along_dim)
// _(aten, tan)
// _(aten, tan_)
// _(aten, tanh)
// _(aten, tanh_)
// _(aten, tanh_backward)
// _(aten, tensor_split)
// _(aten, tensordot)
// _(aten, thnn_conv2d)
// _(aten, threshold)
// _(aten, threshold_)
// _(aten, threshold_backward)
// _(aten, tile)
// _(aten, to)
// _(aten, to_dense)
// _(aten, to_dense_backward)
// _(aten, to_mkldnn)
// _(aten, to_mkldnn_backward)
// _(aten, to_padded_tensor)
// _(aten, to_sparse)
// _(aten, to_sparse_bsc)
// _(aten, to_sparse_bsr)
// _(aten, to_sparse_csc)
// _(aten, to_sparse_csr)
// _(aten, topk)
// _(aten, trace)
// _(aten, trace_backward)
// _(aten, transpose)
// _(aten, transpose_)
// _(aten, transpose_copy)
// _(aten, trapezoid)
// _(aten, trapz)
// _(aten, triangular_solve)
// _(aten, tril)
// _(aten, tril_)
// _(aten, tril_indices)
// _(aten, triplet_margin_loss)
// _(aten, triu)
// _(aten, triu_)
// _(aten, triu_indices)
// _(aten, true_divide)
// _(aten, true_divide_)
// _(aten, trunc)
// _(aten, trunc_)
// _(aten, type_as)
// _(aten, unbind)
// _(aten, unbind_copy)
// _(aten, unflatten)
// _(aten, unflatten_dense_tensors)
// _(aten, unfold)
// _(aten, unfold_backward)
// _(aten, unfold_copy)
// _(aten, uniform)
// _(aten, uniform_)
// _(aten, unique_consecutive)
// _(aten, unique_dim)
// _(aten, unique_dim_consecutive)
// _(aten, unsafe_chunk)
// _(aten, unsafe_split)
// _(aten, unsafe_split_with_sizes)
// _(aten, unsqueeze)
// _(aten, unsqueeze_)
// _(aten, unsqueeze_copy)
// _(aten, upsample_bicubic2d)
// _(aten, upsample_bicubic2d_backward)
// _(aten, upsample_bilinear2d)
// _(aten, upsample_bilinear2d_backward)
// _(aten, upsample_linear1d)
// _(aten, upsample_linear1d_backward)
// _(aten, upsample_nearest1d)
// _(aten, upsample_nearest1d_backward)
// _(aten, upsample_nearest2d)
// _(aten, upsample_nearest2d_backward)
// _(aten, upsample_nearest3d)
// _(aten, upsample_nearest3d_backward)
// _(aten, upsample_trilinear3d)
// _(aten, upsample_trilinear3d_backward)
// _(aten, value_selecting_reduction_backward)
// _(aten, values)
// _(aten, values_copy)
// _(aten, vander)
// _(aten, var)
// _(aten, var_mean)
// _(aten, vdot)
// _(aten, view)
// _(aten, view_as)
// _(aten, view_as_complex)
// _(aten, view_as_complex_copy)
// _(aten, view_as_real)
// _(aten, view_as_real_copy)
// _(aten, view_copy)
// _(aten, vsplit)
// _(aten, vstack)
// _(aten, where)
// _(aten, xlogy)
// _(aten, xlogy_)
// _(aten, zero)
// _(aten, zero_)
// _(aten, zeros)
// _(aten, zeros_like)

// #define FORALL_ATTR_BASE_SYMBOLS(_)
// _(attr, A)
// _(attr, B)
// _(attr, C)
// _(attr, H)
// _(attr, HxW)
// _(attr, K)
// _(attr, L)
// _(attr, LD)
// _(attr, LU)
// _(attr, LU_data)
// _(attr, LU_pivots)
// _(attr, M)
// _(attr, N)
// _(attr, P)
// _(attr, Q)
// _(attr, R)
// _(attr, S)
// _(attr, U)
// _(attr, UPLO)
// _(attr, V)
// _(attr, Vh)
// _(attr, W)
// _(attr, X)
// _(attr, a)
// _(attr, abs)
// _(attr, accumulate)
// _(attr, addends)
// _(attr, adjoint)
// _(attr, align_corners)
// _(attr, allow_tf32)
// _(attr, alpha)
// _(attr, amsgrad)
// _(attr, anchor)
// _(attr, angle)
// _(attr, api_name)
// _(attr, append)
// _(attr, approximate)
// _(attr, arg1)
// _(attr, arg2)
// _(attr, arg3)
// _(attr, arg_out)
// _(attr, assume_unique)
// _(attr, atol)
// _(attr, attn_mask)
// _(attr, average_attn_weights)
// _(attr, averaging_const)
// _(attr, aweights)
// _(attr, axis)
// _(attr, axis0)
// _(attr, axis1)
// _(attr, b)
// _(attr, b_hh)
// _(attr, b_ih)
// _(attr, bag_size)
// _(attr, base)
// _(attr, batch1)
// _(attr, batch2)
// _(attr, batch_dim)
// _(attr, batch_first)
// _(attr, batch_size)
// _(attr, batch_sizes)
// _(attr, benchmark)
// _(attr, beta)
// _(attr, beta1)
// _(attr, beta2)
// _(attr, bias)
// _(attr, bias_defined)
// _(attr, bias_g)
// _(attr, bias_sizes)
// _(attr, bidirectional)
// _(attr, bin_edges)
// _(attr, bins)
// _(attr, bit_width)
// _(attr, blank)
// _(attr, blocksize)
// _(attr, boundaries)
// _(attr, buffer)
// _(attr, causal)
// _(attr, ccol_indices)
// _(attr, cdim)
// _(attr, cdist)
// _(attr, ceil_mode)
// _(attr, cell_state_fwd)
// _(attr, center)
// _(attr, ch_axis)
// _(attr, check_errors)
// _(attr, chunk_grad_outputs)
// _(attr, chunks)
// _(attr, coalesced)
// _(attr, coefficients)
// _(attr, col)
// _(attr, col_indices)
// _(attr, col_offsets)
// _(attr, col_offsets_hh)
// _(attr, col_offsets_ih)
// _(attr, compressed_idx)
// _(attr, compressed_indices)
// _(attr, compressed_indices_dtype)
// _(attr, compute_log_sumexp)
// _(attr, compute_mode)
// _(attr, compute_uv)
// _(attr, compute_v)
// _(attr, condition)
// _(attr, copy)
// _(attr, correction)
// _(attr, count)
// _(attr, count_include_pad)
// _(attr, counts)
// _(attr, cpu_dtype)
// _(attr, cpu_enabled)
// _(attr, cpu_nested_shape_example)
// _(attr, create_graph)
// _(attr, crow_indices)
// _(attr, cu_seqlens_k)
// _(attr, cu_seqlens_q)
// _(attr, cuda_dtype)
// _(attr, cuda_enabled)
// _(attr, cudnn_enable)
// _(attr, cudnn_enabled)
// _(attr, cum_seq_k)
// _(attr, cum_seq_q)
// _(attr, cx)
// _(attr, cx_)
// _(attr, cx_tmp)
// _(attr, cy)
// _(attr, cy_)
// _(attr, d)
// _(attr, data)
// _(attr, decimals)
// _(attr, delta)
// _(attr, dense)
// _(attr, dense_dim)
// _(attr, density)
// _(attr, descending)
// _(attr, destination)
// _(attr, deterministic)
// _(attr, device)
// _(attr, device_index)
// _(attr, dgrad_glu)
// _(attr, diagonal)
// _(attr, diagonals)
// _(attr, dilation)
// _(attr, dim)
// _(attr, dim0)
// _(attr, dim1)
// _(attr, dim2)
// _(attr, dimension)
// _(attr, dims)
// _(attr, dims_other)
// _(attr, dims_self)
// _(attr, divisor_override)
// _(attr, downscale_factor)
// _(attr, driver)
// _(attr, dropout)
// _(attr, dropout_mask)
// _(attr, dropout_p)
// _(attr, dropout_seed)
// _(attr, dropout_state)
// _(attr, dst)
// _(attr, dtype)
// _(attr, dual)
// _(attr, dummy)
// _(attr, dx)
// _(attr, edge_order)
// _(attr, eigenvalues)
// _(attr, eigenvectors)
// _(attr, eigvals)
// _(attr, eigvecs)
// _(attr, element)
// _(attr, elements)
// _(attr, ellipsis_idx)
// _(attr, embed_dim)
// _(attr, end)
// _(attr, end_dim)
// _(attr, eps)
// _(attr, epsilon)
// _(attr, equal_nan)
// _(attr, equation)
// _(attr, exp_avg_sqs)
// _(attr, exp_avgs)
// _(attr, expand1)
// _(attr, expand2)
// _(attr, expand3)
// _(attr, exponent)
// _(attr, exponential_average_factor)
// _(attr, fake_quant_enabled)
// _(attr, fake_quant_on)
// _(attr, ffn_bias_1)
// _(attr, ffn_bias_2)
// _(attr, ffn_weight_1)
// _(attr, ffn_weight_2)
// _(attr, filename)
// _(attr, fill_value)
// _(attr, flat)
// _(attr, forward)
// _(attr, found_inf)
// _(attr, from)
// _(attr, full)
// _(attr, full_matrices)
// _(attr, fuse_transform_0213)
// _(attr, fweights)
// _(attr, g)
// _(attr, gO)
// _(attr, generator)
// _(attr, ggI)
// _(attr, ggW)
// _(attr, ggb)
// _(attr, glu)
// _(attr, grad)
// _(attr, grad_bias)
// _(attr, grad_cy)
// _(attr, grad_factor)
// _(attr, grad_glu)
// _(attr, grad_hy)
// _(attr, grad_in)
// _(attr, grad_input)
// _(attr, grad_out)
// _(attr, grad_out_)
// _(attr, grad_output)
// _(attr, grad_scale)
// _(attr, grad_w)
// _(attr, grad_weight)
// _(attr, grad_x)
// _(attr, grad_y)
// _(attr, gradient)
// _(attr, grads)
// _(attr, grid)
// _(attr, group)
// _(attr, groups)
// _(attr, growth_interval)
// _(attr, growth_tracker)
// _(attr, half_to_float)
// _(attr, has_bias)
// _(attr, has_biases)
// _(attr, hermitian)
// _(attr, hidden_bias)
// _(attr, hidden_gates)
// _(attr, hidden_size)
// _(attr, high)
// _(attr, hist)
// _(attr, hop_length)
// _(attr, hx)
// _(attr, hx_)
// _(attr, hy_)
// _(attr, i1)
// _(attr, i2)
// _(attr, i3)
// _(attr, ignore_index)
// _(attr, imag)
// _(attr, impl_index)
// _(attr, implicit)
// _(attr, include_last_offset)
// _(attr, include_self)
// _(attr, incr_key)
// _(attr, incr_value)
// _(attr, increasing)
// _(attr, ind)
// _(attr, index)
// _(attr, indexing)
// _(attr, indices)
// _(attr, info)
// _(attr, initial)
// _(attr, input)
// _(attr, input1)
// _(attr, input2)
// _(attr, input3)
// _(attr, input_bias)
// _(attr, input_dtype)
// _(attr, input_g)
// _(attr, input_gates)
// _(attr, input_lengths)
// _(attr, input_scale)
// _(attr, input_size)
// _(attr, input_sizes)
// _(attr, inputs)
// _(attr, interpolation)
// _(attr, interpolation_mode)
// _(attr, inv_scale)
// _(attr, inverse)
// _(attr, invert)
// _(attr, invstd)
// _(attr, is_causal)
// _(attr, is_crow)
// _(attr, is_matrix)
// _(attr, is_result)
// _(attr, is_target)
// _(attr, k)
// _(attr, keepdim)
// _(attr, kernel_size)
// _(attr, key)
// _(attr, label_smoothing)
// _(attr, lambd)
// _(attr, largest)
// _(attr, last_dim_size)
// _(attr, layersOutputs)
// _(attr, layout)
// _(attr, left)
// _(attr, length)
// _(attr, lengths)
// _(attr, level)
// _(attr, like)
// _(attr, list)
// _(attr, log_alpha)
// _(attr, log_input)
// _(attr, log_probs)
// _(attr, log_target)
// _(attr, logabsdet)
// _(attr, logsumexp)
// _(attr, low)
// _(attr, lower)
// _(attr, lr)
// _(attr, ltm)
// _(attr, m)
// _(attr, mantissa)
// _(attr, margin)
// _(attr, mask)
// _(attr, mask_check)
// _(attr, mask_type)
// _(attr, mat)
// _(attr, mat1)
// _(attr, mat2)
// _(attr, matrices)
// _(attr, max)
// _(attr, max_exp_avg_sqs)
// _(attr, max_k)
// _(attr, max_norm)
// _(attr, max_q)
// _(attr, max_seqlen_q)
// _(attr, max_size)
// _(attr, max_val)
// _(attr, max_values)
// _(attr, maximize)
// _(attr, maximum_indices)
// _(attr, maxnorm)
// _(attr, mean)
// _(attr, mean_dy)
// _(attr, mean_dy_xmu)
// _(attr, median)
// _(attr, memory_format)
// _(attr, min)
// _(attr, min_indices)
// _(attr, min_val)
// _(attr, minlength)
// _(attr, mode)
// _(attr, momentum)
// _(attr, n)
// _(attr, n_bins)
// _(attr, n_fft)
// _(attr, names)
// _(attr, nan)
// _(attr, need_attn_weights)
// _(attr, need_weights)
// _(attr, neg_log_likelihood)
// _(attr, negative)
// _(attr, negative_slope)
// _(attr, neginf)
// _(attr, nested_size)
// _(attr, nested_strides)
// _(attr, new_data)
// _(attr, nnz)
// _(attr, noise)
// _(attr, non_blocking)
// _(attr, norm)
// _(attr, norm_bias_1)
// _(attr, norm_bias_2)
// _(attr, norm_first)
// _(attr, norm_type)
// _(attr, norm_weight_1)
// _(attr, norm_weight_2)
// _(attr, normalization)
// _(attr, normalized)
// _(attr, normalized_shape)
// _(attr, nt_example)
// _(attr, num_classes)
// _(attr, num_generated)
// _(attr, num_groups)
// _(attr, num_head)
// _(attr, num_heads)
// _(attr, num_layers)
// _(attr, num_samples)
// _(attr, num_weights)
// _(attr, numel)
// _(attr, observer_on)
// _(attr, offset)
// _(attr, offset2bag)
// _(attr, offsets)
// _(attr, onesided)
// _(attr, ord)
// _(attr, order)
// _(attr, other)
// _(attr, out)
// _(attr, out0)
// _(attr, out1)
// _(attr, out2)
// _(attr, out3)
// _(attr, out4)
// _(attr, out5)
// _(attr, out6)
// _(attr, out_dim)
// _(attr, out_int32)
// _(attr, outdim)
// _(attr, output)
// _(attr, output_mask)
// _(attr, output_padding)
// _(attr, output_scale)
// _(attr, output_size)
// _(attr, output_zero_point)
// _(attr, p)
// _(attr, packed)
// _(attr, packed_hh)
// _(attr, packed_ih)
// _(attr, packed_weight)
// _(attr, pad)
// _(attr, pad_mode)
// _(attr, padded)
// _(attr, padding)
// _(attr, padding_idx)
// _(attr, padding_mode)
// _(attr, padding_value)
// _(attr, params)
// _(attr, path)
// _(attr, pdist)
// _(attr, per_row_fake_quant)
// _(attr, per_sample_weights)
// _(attr, periodic)
// _(attr, philox_offset)
// _(attr, philox_seed)
// _(attr, pin_memory)
// _(attr, pivot)
// _(attr, pivots)
// _(attr, plain_idx)
// _(attr, plain_indices)
// _(attr, pos_weight)
// _(attr, posinf)
// _(attr, positive)
// _(attr, pow)
// _(attr, prepend)
// _(attr, primal)
// _(attr, prob)
// _(attr, proj_bias)
// _(attr, proj_size)
// _(attr, proj_weight)
// _(attr, q)
// _(attr, qkv)
// _(attr, qkv_bias)
// _(attr, qkv_weight)
// _(attr, qtensor)
// _(attr, quant_max)
// _(attr, quant_min)
// _(attr, quasi)
// _(attr, query)
// _(attr, r)
// _(attr, random_samples)
// _(attr, range)
// _(attr, rank)
// _(attr, ratio)
// _(attr, rcond)
// _(attr, real)
// _(attr, reduce)
// _(attr, reduce_range)
// _(attr, reduction)
// _(attr, repeats)
// _(attr, replacement)
// _(attr, requires_grad)
// _(attr, reserve)
// _(attr, reserveSpace)
// _(attr, reservedSpace)
// _(attr, residuals)
// _(attr, result)
// _(attr, retain_graph)
// _(attr, return_complex)
// _(attr, return_counts)
// _(attr, return_debug_mask)
// _(attr, return_inverse)
// _(attr, reverse)
// _(attr, right)
// _(attr, rounding_mode)
// _(attr, row)
// _(attr, row_indices)
// _(attr, rstd)
// _(attr, rtol)
// _(attr, running_max)
// _(attr, running_mean)
// _(attr, running_min)
// _(attr, running_var)
// _(attr, s)
// _(attr, save_invstd)
// _(attr, save_mean)
// _(attr, save_var)
// _(attr, save_var_transform)
// _(attr, saved_g)
// _(attr, saved_norms)
// _(attr, saved_v)
// _(attr, scalar)
// _(attr, scalar1)
// _(attr, scalar2)
// _(attr, scalars)
// _(attr, scale)
// _(attr, scale_backoff_factor)
// _(attr, scale_factors)
// _(attr, scale_grad_by_freq)
// _(attr, scale_growth_factor)
// _(attr, scale_hh)
// _(attr, scale_ih)
// _(attr, scales)
// _(attr, scales_d)
// _(attr, scales_h)
// _(attr, scales_w)
// _(attr, sections)
// _(attr, self)
// _(attr, self_is_result)
// _(attr, self_num_batch_dims)
// _(attr, self_or_result)
// _(attr, self_sizes)
// _(attr, sequences)
// _(attr, shape)
// _(attr, shared)
// _(attr, shifts)
// _(attr, side)
// _(attr, sigma)
// _(attr, sign)
// _(attr, singular_values)
// _(attr, size)
// _(attr, sizes)
// _(attr, sobolstate)
// _(attr, solution)
// _(attr, some)
// _(attr, sorted)
// _(attr, sorted_sequence)
// _(attr, sorter)
// _(attr, source)
// _(attr, spacing)
// _(attr, sparse)
// _(attr, sparse_dim)
// _(attr, sparse_grad)
// _(attr, split_size)
// _(attr, split_sizes)
// _(attr, src)
// _(attr, stable)
// _(attr, start)
// _(attr, start_dim)
// _(attr, state_steps)
// _(attr, std)
// _(attr, step)
// _(attr, steps)
// _(attr, storage_offset)
// _(attr, stride)
// _(attr, sumdim)
// _(attr, swap)
// _(attr, symmetric_quant)
// _(attr, t)
// _(attr, tangent)
// _(attr, target)
// _(attr, target_lengths)
// _(attr, targets)
// _(attr, tau)
// _(attr, tensor)
// _(attr, tensor1)
// _(attr, tensor2)
// _(attr, tensor_indices_or_sections)
// _(attr, tensors)
// _(attr, tensors1)
// _(attr, test_element)
// _(attr, test_elements)
// _(attr, the_template)
// _(attr, theta)
// _(attr, threshold)
// _(attr, to)
// _(attr, tol)
// _(attr, total)
// _(attr, total_length)
// _(attr, total_weight)
// _(attr, train)
// _(attr, training)
// _(attr, transpose)
// _(attr, transposed)
// _(attr, type1)
// _(attr, type2)
// _(attr, unbiased)
// _(attr, unitriangular)
// _(attr, unpack_data)
// _(attr, unpack_pivots)
// _(attr, unroll_dim)
// _(attr, unsafe)
// _(attr, upper)
// _(attr, upscale_factor)
// _(attr, use_gelu)
// _(attr, use_input_stats)
// _(attr, v)
// _(attr, value)
// _(attr, values)
// _(attr, var)
// _(attr, vec)
// _(attr, vec1)
// _(attr, vec2)
// _(attr, w_hh)
// _(attr, w_ih)
// _(attr, weight)
// _(attr, weight0)
// _(attr, weight1)
// _(attr, weight2)
// _(attr, weight3)
// _(attr, weight4)
// _(attr, weight_arr)
// _(attr, weight_buf)
// _(attr, weight_decay)
// _(attr, weight_g)
// _(attr, weight_scale)
// _(attr, weight_stride0)
// _(attr, weight_zero_point)
// _(attr, weights)
// _(attr, win_length)
// _(attr, window)
// _(attr, window_length)
// _(attr, with_replacement)
// _(attr, workspace)
// _(attr, wrap)
// _(attr, x)
// _(attr, x1)
// _(attr, x2)
// _(attr, y)
// _(attr, z)
// _(attr, z_state)
// _(attr, zero_infinity)
// _(attr, zero_point)
// _(attr, zero_point_hh)
// _(attr, zero_point_ih)
// _(attr, zero_points)


// Parsed from ATen/core/interned_strings.h

// #pragma once
// #include <vector>
// #include <cstdint>
// #include <string>
// #include <unordered_map>
// #include <algorithm>

// #include <c10/macros/Macros.h>

// #include <ATen/core/aten_interned_strings.h>
// #include <ATen/core/symbol.h>

// #define FORALL_NS_SYMBOLS(_)
//   _(namespaces, prim)
//   _(namespaces, prims)
//   _(namespaces, nvprims)
//   _(namespaces, aten)
//   _(namespaces, cuda)
//   _(namespaces, onnx)
//   _(namespaces, attr)
//   _(namespaces, scope)
//   _(namespaces, user)
//   _(namespaces, _caffe2)
//   _(namespaces, dimname)
//   _(namespaces, namespaces)
//   _(prim, Assign)
//   _(prim, BroadcastingChunk)
//   _(prim, BroadcastSizes)
//   _(prim, ReductionSizes)
//   _(prim, Constant)
//   _(prim, ChunkSizes)
//   _(prim, ConstantMKLDNNTensor)
//   _(prim, BroadcastMKLDNNTensors)
//   _(prim, MKLDNNGroup)
//   _(prim, MKLDNNHardSwish)
//   _(prim, MKLDNNHardSigmoid)
//   _(prim, MKLDNNHardTanh)
//   _(prim, MKLDNNClamp)
//   _(prim, StaticRuntimeCopyOuts)
//   _(prim, Drop)
//   _(prim, Eval)
//   _(prim, Expand) /* onnx */
//   _(prim, FusionGroup)
//   _(prim, CudaFusionGroup)
//   _(prim, CudaFusionGuard)
//   _(prim, oneDNNFusionGroup)
//   _(prim, oneDNNFusionGuard)
//   _(prim, FunctionalGraph)
//   _(prim, add_optional)
//   _(prim, view_copy)
//   _(prim, permute_copy)
//   _(prim, reshape_copy)
//   _(prim, squeeze_copy)
//   _(prim, t_copy)
//   _(prim, transpose_copy)
//   _(prim, unsqueeze_copy)
//   _(prim, flatten_copy)
//   _(prim, expand_copy)
//   _(prim, expand_as_copy)
//   _(prim, DifferentiableGraph)
//   _(prim, TensorExprGroup)
//   _(prim, TensorExprDynamicGroup)
//   _(prim, StaticSubgraph)
//   _(prim, If)
//   _(prim, Jump) /* debug */
//   _(prim, JumpNZ) /* debug */
//   _(prim, JumpZ) /* debug */
//   _(prim, Load)
//   _(prim, Loop)
//   _(prim, Param)
//   _(prim, PackPadded) /* onnx */
//   _(prim, PadPacked) /* onnx */
//   _(prim, Placeholder) /* debug */
//   _(prim, Print)
//   _(prim, EmptyListLiteral)
//   _(prim, LegacyTypedConstructor)
//   _(prim, PythonOp)
//   _(prim, IgnoredPythonOp)
//   _(prim, Reverse)
//   _(prim, Return)
//   _(prim, ReturnStmt)
//   _(prim, BreakStmt)
//   _(prim, ContinueStmt)
//   _(prim, ComprehensionScope)
//   _(prim, Store)
//   _(prim, AutogradZero)
//   _(prim, AutogradAnyNonZero)
//   _(prim, AutogradAllNonZero)
//   _(prim, AutogradAllZero)
//   _(prim, Starred)
//   _(prim, TupleConstruct)
//   _(prim, TupleUnpack)
//   _(prim, TupleIndex)
//   _(prim, TupleSlice)
//   _(prim, ListConstruct)
//   _(prim, ListUnpack)
//   _(prim, DictConstruct)
//   _(prim, ModuleContainerIndex)
//   _(prim, EnumName)
//   _(prim, EnumValue)
//   _(prim, StringIndex)
//   _(prim, NumToTensor)
//   _(prim, Uninitialized)
//   _(prim, VarConcat)
//   _(prim, VarStack)
//   _(prim, With)
//   _(prim, Enter)
//   _(prim, Exit)
//   _(prim, IfThenElse)
//   _(aten, Bool)
//   _(aten, Int)
//   _(aten, FloatImplicit)
//   _(aten, ComplexImplicit)
//   _(aten, IntImplicit)
//   _(aten, ScalarImplicit)
//   _(aten, Float)
//   _(aten, Complex)
//   _(aten, str)
//   _(aten, Delete)
//   _(prim, device)
//   _(prim, dtype)
//   _(prim, layout)
//   _(prim, id)
//   _(prim, requires_grad)
//   _(prim, MakeTestTensor) /* test */
//   _(prim, AutogradAdd)
//   _(prim, GradOf)
//   _(aten, grad)
//   _(aten, backward)
//   _(prim, Guard)
//   _(prim, BailOut)
//   _(prim, TypeCheck)
//   _(prim, RequiresGradCheck)
//   _(prim, FallbackGraph)
//   _(prim, FusedConcat)
//   _(prim, ConstantChunk)
//   _(prim, MMTreeReduce)
//   _(prim, MMBatchSide)
//   _(prim, list)
//   _(prim, dict)
//   _(prim, min)
//   _(prim, max)
//   _(prim, abs)
//   _(aten, divmod)
//   _(prim, zip)
//   _(prim, enumerate)
//   _(prim, range)
//   _(prim, rangelist)
//   _(prim, isinstance)
//   _(prim, tolist)
//   _(prim, unchecked_cast)
//   _(aten, _grad_sum_to_size)
//   _(aten, _size_if_not_equal)
//   _(aten, _ncf_unsqueeze)
//   _(aten, warn)
//   _(aten, sorted)
//   _(aten, floordiv)
//   _(aten, __range_length)
//   _(aten, __derive_index)
//   _(aten, __round_to_zero_floordiv)
//   _(aten, is_scripting)
//   _(aten, _unwrap_optional)
//   _(prim, fork)
//   _(prim, awaitable)
//   _(prim, forkClosure)
//   _(prim, awaitableClosure)
//   _(prim, awaitable_nowait)
//   _(prim, awaitable_wait)
//   _(prim, RaiseException)
//   _(prim, Closure)
//   _(prim, CreateObject)
//   _(prim, SetAttr)
//   _(prim, GetAttr)
//   _(prim, HasAttr)
//   _(prim, profile)
//   _(prim, profile_ivalue)
//   _(prim, AddStatValue)
//   _(prim, TimePoint)
//   _(prim, CallFunction)
//   _(prim, CallMethod)
//   _(prim, LoopContinuation)
//   _(prim, annotate)
//   _(prim, TracedModuleForward)
//   _(prim, TracedFork)
//   _(prim, TracedAttr)
//   _(prim, rpc_async)
//   _(prim, rpc_sync)
//   _(prim, rpc_remote)
//   _(prim, is_cuda)
//   _(aten, append)
//   _(aten, as_tensor)
//   _(aten, adaptive_avg_pool2d_backward)
//   _(aten, dim)
//   _(aten, format)
//   _(aten, percentFormat)
//   _(aten, __not__)
//   _(aten, __is__)
//   _(aten, __isnot__)
//   _(aten, _ger)
//   _(aten, __getitem__)
//   _(aten, _set_item)
//   _(aten, manual_seed)
//   _(aten, device)
//   _(aten, hash)
//   _(aten, len)
//   _(aten, list)
//   _(aten, dict)
//   _(aten, wait)
//   _(aten, save)
//   _(aten, keys)
//   _(aten, ord)
//   _(aten, chr)
//   _(aten, hex)
//   _(aten, oct)
//   _(aten, clear)
//   _(aten, setdefault)
//   _(aten, bin)
//   _(aten, pop)
//   _(aten, insert)
//   _(aten, tensor)
//   _(prim, unchecked_unwrap_optional)
//   _(aten, __contains__)
//   _(prim, BailoutTemplate)
//   _(prim, grad)
//   _(cuda, _set_device)
//   _(cuda, set_stream)
//   _(cuda, _current_device)
//   _(cuda, synchronize)
//   _(aten, has_torch_function)
//   _(aten, is_autocast_enabled)
//   _(aten, is_autocast_cpu_enabled)
//   FORALL_ATEN_BASE_SYMBOLS(_)
//   _(onnx, Add)
//   _(onnx, Concat)
//   _(onnx, Constant)
//   _(onnx, ConstantFill)
//   _(onnx, Div)
//   _(onnx, GRU)
//   _(onnx, Gather)
//   _(onnx, Gemm)
//   _(onnx, LSTM)
//   _(onnx, MatMul)
//   _(onnx, Min)
//   _(onnx, Max)
//   _(onnx, Mul)
//   _(onnx, Pow)
//   _(onnx, RNN)
//   _(onnx, Shape)
//   _(onnx, Size)
//   _(onnx, Slice)
//   _(onnx, Softmax)
//   _(onnx, Squeeze)
//   _(onnx, Sub)
//   _(onnx, Transpose)
//   _(onnx, Unsqueeze)
//   _(onnx, Loop)
//   _(onnx, If)
//   _(onnx, Reshape)
//   _(onnx, Expand)
//   _(onnx, Equal)
//   _(onnx, Greater)
//   _(onnx, GreaterOrEqual)
//   _(onnx, Less)
//   _(onnx, LessOrEqual)
//   _(onnx, Not)
//   _(aten, ATen)
//   _(onnx, Split)
//   _(onnx, ConstantOfShape)
//   _(onnx, Cast)
//   _(onnx, Mod)
//   _(onnx, Sqrt)
//   _(onnx, SplitToSequence)
//   _(onnx, SequenceAt)
//   _(onnx, SequenceConstruct)
//   _(onnx, SequenceEmpty)
//   _(onnx, SequenceInsert)
//   _(onnx, SequenceErase)
//   _(onnx, ConcatFromSequence)
//   _(onnx, Identity)
//   _(onnx, SoftmaxCrossEntropyLoss)
//   _(onnx, NegativeLogLikelihoodLoss)
//   _(onnx, LogSoftmax)
//   _(onnx, ReduceL1)
//   _(onnx, ReduceL2)
//   _(onnx, Conv)
//   _(onnx, BatchNormalization)
//   _(onnx, ReduceMean)
//   _(onnx, ReduceProd)
//   _(onnx, Relu)
//   _(onnx, Neg)
//   _(onnx, NonZero)
//   _(onnx, Range)
//   _(onnx, Tile)
//   _(onnx, Where)
//   _(onnx, Optional)
//   _(onnx, OptionalGetElement)
//   _(onnx, OptionalHasElement)
//   FORALL_ATTR_BASE_SYMBOLS(_)
//   _(attr, Subgraph)
//   _(attr, ReverseSubgraph)
//   _(attr, f_real_outputs)
//   _(attr, df_input_vjps)
//   _(attr, df_input_captured_inputs)
//   _(attr, df_input_captured_outputs)
//   _(attr, df_output_vjps)
//   _(attr, axes)
//   _(attr, symbolic_shape_inputs)
//   _(attr, allow_stack_outputs)
//   _(attr, striding_inputs_desc)
//   _(attr, striding_outputs_desc)
//   _(attr, broadcast)
//   _(attr, direction)
//   _(attr, ends)
//   _(attr, inplace)
//   _(attr, input_as_shape)
//   _(attr, is_zero)
//   _(attr, num_none)
//   _(attr, num_present)
//   _(attr, perm)
//   _(attr, starts)
//   _(attr, profiled_type)
//   _(attr, transA)
//   _(attr, transB)
//   _(attr, name)
//   _(attr, module)
//   _(attr, beg)
//   _(attr, idx)
//   _(attr, split)
//   _(attr, slot)
//   _(attr, kinds)
//   _(attr, types)
//   _(attr, scope)
//   _(attr, keepdims)
//   _(attr, cache_id)
//   _(attr, new_axis)
//   _(attr, warn_id)
//   _(attr, output_layouts)
//   _(attr, allowzero)
//   _(attr, seen_none)
//   _(attr, overload_name)

@Namespace("c10") public enum _keys {
    namespaces_prim(0),
  namespaces_prims(1),
  namespaces_nvprims(2),
  namespaces_aten(3),
  namespaces_cuda(4),
  namespaces_onnx(5),
  namespaces_attr(6),
  namespaces_scope(7),
  namespaces_user(8),
  namespaces__caffe2(9),
  namespaces_dimname(10),
  namespaces_namespaces(11),
  prim_Assign(12),
  prim_BroadcastingChunk(13),
  prim_BroadcastSizes(14),
  prim_ReductionSizes(15),
  prim_Constant(16),
  prim_ChunkSizes(17),
  prim_ConstantMKLDNNTensor(18),
  prim_BroadcastMKLDNNTensors(19),
  prim_MKLDNNGroup(20),
  prim_MKLDNNHardSwish(21),
  prim_MKLDNNHardSigmoid(22),
  prim_MKLDNNHardTanh(23),
  prim_MKLDNNClamp(24),
  prim_StaticRuntimeCopyOuts(25),
  prim_Drop(26),
  prim_Eval(27),
  prim_Expand(28), /* onnx */
  prim_FusionGroup(29),
  prim_CudaFusionGroup(30),
  prim_CudaFusionGuard(31),
  prim_oneDNNFusionGroup(32),
  prim_oneDNNFusionGuard(33),
  prim_FunctionalGraph(34),
  prim_add_optional(35),
  prim_view_copy(36),
  prim_permute_copy(37),
  prim_reshape_copy(38),
  prim_squeeze_copy(39),
  prim_t_copy(40),
  prim_transpose_copy(41),
  prim_unsqueeze_copy(42),
  prim_flatten_copy(43),
  prim_expand_copy(44),
  prim_expand_as_copy(45),
  prim_DifferentiableGraph(46),
  prim_TensorExprGroup(47),
  prim_TensorExprDynamicGroup(48),
  prim_StaticSubgraph(49),
  prim_If(50),
  prim_Jump(51), /* debug */
  prim_JumpNZ(52), /* debug */
  prim_JumpZ(53), /* debug */
  prim_Load(54),
  prim_Loop(55),
  prim_Param(56),
  prim_PackPadded(57), /* onnx */
  prim_PadPacked(58), /* onnx */
  prim_Placeholder(59), /* debug */
  prim_Print(60),
  prim_EmptyListLiteral(61),
  prim_LegacyTypedConstructor(62),
  prim_PythonOp(63),
  prim_IgnoredPythonOp(64),
  prim_Reverse(65),
  prim_Return(66),
  prim_ReturnStmt(67),
  prim_BreakStmt(68),
  prim_ContinueStmt(69),
  prim_ComprehensionScope(70),
  prim_Store(71),
  prim_AutogradZero(72),
  prim_AutogradAnyNonZero(73),
  prim_AutogradAllNonZero(74),
  prim_AutogradAllZero(75),
  prim_Starred(76),
  prim_TupleConstruct(77),
  prim_TupleUnpack(78),
  prim_TupleIndex(79),
  prim_TupleSlice(80),
  prim_ListConstruct(81),
  prim_ListUnpack(82),
  prim_DictConstruct(83),
  prim_ModuleContainerIndex(84),
  prim_EnumName(85),
  prim_EnumValue(86),
  prim_StringIndex(87),
  prim_NumToTensor(88),
  prim_Uninitialized(89),
  prim_VarConcat(90),
  prim_VarStack(91),
  prim_With(92),
  prim_Enter(93),
  prim_Exit(94),
  prim_IfThenElse(95),
  aten_Bool(96),
  aten_Int(97),
  aten_FloatImplicit(98),
  aten_ComplexImplicit(99),
  aten_IntImplicit(100),
  aten_ScalarImplicit(101),
  aten_Float(102),
  aten_Complex(103),
  aten_str(104),
  aten_Delete(105),
  prim_device(106),
  prim_dtype(107),
  prim_layout(108),
  prim_id(109),
  prim_requires_grad(110),
  prim_MakeTestTensor(111), /* test */
  prim_AutogradAdd(112),
  prim_GradOf(113),
  aten_grad(114),
  aten_backward(115),
  prim_Guard(116),
  prim_BailOut(117),
  prim_TypeCheck(118),
  prim_RequiresGradCheck(119),
  prim_FallbackGraph(120),
  prim_FusedConcat(121),
  prim_ConstantChunk(122),
  prim_MMTreeReduce(123),
  prim_MMBatchSide(124),
  prim_list(125),
  prim_dict(126),
  prim_min(127),
  prim_max(128),
  prim_abs(129),
  aten_divmod(130),
  prim_zip(131),
  prim_enumerate(132),
  prim_range(133),
  prim_rangelist(134),
  prim_isinstance(135),
  prim_tolist(136),
  prim_unchecked_cast(137),
  aten__grad_sum_to_size(138),
  aten__size_if_not_equal(139),
  aten__ncf_unsqueeze(140),
  aten_warn(141),
  aten_sorted(142),
  aten_floordiv(143),
  aten___range_length(144),
  aten___derive_index(145),
  aten___round_to_zero_floordiv(146),
  aten_is_scripting(147),
  aten__unwrap_optional(148),
  prim_fork(149),
  prim_awaitable(150),
  prim_forkClosure(151),
  prim_awaitableClosure(152),
  prim_awaitable_nowait(153),
  prim_awaitable_wait(154),
  prim_RaiseException(155),
  prim_Closure(156),
  prim_CreateObject(157),
  prim_SetAttr(158),
  prim_GetAttr(159),
  prim_HasAttr(160),
  prim_profile(161),
  prim_profile_ivalue(162),
  prim_AddStatValue(163),
  prim_TimePoint(164),
  prim_CallFunction(165),
  prim_CallMethod(166),
  prim_LoopContinuation(167),
  prim_annotate(168),
  prim_TracedModuleForward(169),
  prim_TracedFork(170),
  prim_TracedAttr(171),
  prim_rpc_async(172),
  prim_rpc_sync(173),
  prim_rpc_remote(174),
  prim_is_cuda(175),
  aten_append(176),
  aten_as_tensor(177),
  aten_adaptive_avg_pool2d_backward(178),
  aten_dim(179),
  aten_format(180),
  aten_percentFormat(181),
  aten___not__(182),
  aten___is__(183),
  aten___isnot__(184),
  aten__ger(185),
  aten___getitem__(186),
  aten__set_item(187),
  aten_manual_seed(188),
  aten_device(189),
  aten_hash(190),
  aten_len(191),
  aten_list(192),
  aten_dict(193),
  aten_wait(194),
  aten_save(195),
  aten_keys(196),
  aten_ord(197),
  aten_chr(198),
  aten_hex(199),
  aten_oct(200),
  aten_clear(201),
  aten_setdefault(202),
  aten_bin(203),
  aten_pop(204),
  aten_insert(205),
  aten_tensor(206),
  prim_unchecked_unwrap_optional(207),
  aten___contains__(208),
  prim_BailoutTemplate(209),
  prim_grad(210),
  cuda__set_device(211),
  cuda_set_stream(212),
  cuda__current_device(213),
  cuda_synchronize(214),
  aten_has_torch_function(215),
  aten_is_autocast_enabled(216),
  aten_is_autocast_cpu_enabled(217),
  aten___and__(218),
aten___iand__(219),
aten___ilshift__(220),
aten___ior__(221),
aten___irshift__(222),
aten___ixor__(223),
aten___lshift__(224),
aten___or__(225),
aten___rshift__(226),
aten___xor__(227),
aten__adaptive_avg_pool2d(228),
aten__adaptive_avg_pool2d_backward(229),
aten__adaptive_avg_pool3d(230),
aten__adaptive_avg_pool3d_backward(231),
aten__add_batch_dim(232),
aten__add_relu(233),
aten__add_relu_(234),
aten__addmm_activation(235),
aten__aminmax(236),
aten__amp_foreach_non_finite_check_and_unscale(237),
aten__amp_foreach_non_finite_check_and_unscale_(238),
aten__amp_update_scale(239),
aten__amp_update_scale_(240),
aten__assert_async(241),
aten__assert_tensor_metadata(242),
aten__autocast_to_full_precision(243),
aten__autocast_to_reduced_precision(244),
aten__backward(245),
aten__batch_norm_impl_index(246),
aten__batch_norm_impl_index_backward(247),
aten__cast_Byte(248),
aten__cast_Char(249),
aten__cast_Double(250),
aten__cast_Float(251),
aten__cast_Half(252),
aten__cast_Int(253),
aten__cast_Long(254),
aten__cast_Short(255),
aten__cdist_backward(256),
aten__cdist_forward(257),
aten__cholesky_solve_helper(258),
aten__choose_qparams_per_tensor(259),
aten__chunk_grad_outputs_efficient_attention(260),
aten__coalesce(261),
aten__coalesced(262),
aten__coalesced_(263),
aten__compute_linear_combination(264),
aten__conj(265),
aten__conj_copy(266),
aten__conj_physical(267),
aten__conv_depthwise2d(268),
aten__convert_indices_from_coo_to_csr(269),
aten__convert_indices_from_csr_to_coo(270),
aten__convolution(271),
aten__convolution_double_backward(272),
aten__convolution_mode(273),
aten__copy_from(274),
aten__copy_from_and_resize(275),
aten__ctc_loss(276),
aten__ctc_loss_backward(277),
aten__cudnn_ctc_loss(278),
aten__cudnn_init_dropout_state(279),
aten__cudnn_rnn(280),
aten__cudnn_rnn_backward(281),
aten__cudnn_rnn_flatten_weight(282),
aten__cufft_clear_plan_cache(283),
aten__cufft_get_plan_cache_max_size(284),
aten__cufft_get_plan_cache_size(285),
aten__cufft_set_plan_cache_max_size(286),
aten__cummax_helper(287),
aten__cummin_helper(288),
aten__debug_has_internal_overlap(289),
aten__dimI(290),
aten__dimV(291),
aten__dim_arange(292),
aten__dirichlet_grad(293),
aten__efficient_attention_backward(294),
aten__efficient_attention_forward(295),
aten__efficientzerotensor(296),
aten__embedding_bag(297),
aten__embedding_bag_backward(298),
aten__embedding_bag_dense_backward(299),
aten__embedding_bag_forward_only(300),
aten__embedding_bag_per_sample_weights_backward(301),
aten__embedding_bag_sparse_backward(302),
aten__empty_affine_quantized(303),
aten__empty_per_channel_affine_quantized(304),
aten__euclidean_dist(305),
aten__fake_quantize_learnable_per_channel_affine(306),
aten__fake_quantize_learnable_per_channel_affine_backward(307),
aten__fake_quantize_learnable_per_tensor_affine(308),
aten__fake_quantize_learnable_per_tensor_affine_backward(309),
aten__fake_quantize_per_tensor_affine_cachemask_tensor_qparams(310),
aten__fft_c2c(311),
aten__fft_c2r(312),
aten__fft_r2c(313),
aten__flash_attention_backward(314),
aten__flash_attention_forward(315),
aten__foobar(316),
aten__foreach_abs(317),
aten__foreach_abs_(318),
aten__foreach_acos(319),
aten__foreach_acos_(320),
aten__foreach_add(321),
aten__foreach_add_(322),
aten__foreach_addcdiv(323),
aten__foreach_addcdiv_(324),
aten__foreach_addcmul(325),
aten__foreach_addcmul_(326),
aten__foreach_asin(327),
aten__foreach_asin_(328),
aten__foreach_atan(329),
aten__foreach_atan_(330),
aten__foreach_ceil(331),
aten__foreach_ceil_(332),
aten__foreach_clamp_max(333),
aten__foreach_clamp_max_(334),
aten__foreach_clamp_min(335),
aten__foreach_clamp_min_(336),
aten__foreach_cos(337),
aten__foreach_cos_(338),
aten__foreach_cosh(339),
aten__foreach_cosh_(340),
aten__foreach_div(341),
aten__foreach_div_(342),
aten__foreach_erf(343),
aten__foreach_erf_(344),
aten__foreach_erfc(345),
aten__foreach_erfc_(346),
aten__foreach_exp(347),
aten__foreach_exp_(348),
aten__foreach_expm1(349),
aten__foreach_expm1_(350),
aten__foreach_floor(351),
aten__foreach_floor_(352),
aten__foreach_frac(353),
aten__foreach_frac_(354),
aten__foreach_lerp(355),
aten__foreach_lerp_(356),
aten__foreach_lgamma(357),
aten__foreach_lgamma_(358),
aten__foreach_log(359),
aten__foreach_log10(360),
aten__foreach_log10_(361),
aten__foreach_log1p(362),
aten__foreach_log1p_(363),
aten__foreach_log2(364),
aten__foreach_log2_(365),
aten__foreach_log_(366),
aten__foreach_maximum(367),
aten__foreach_maximum_(368),
aten__foreach_minimum(369),
aten__foreach_minimum_(370),
aten__foreach_mul(371),
aten__foreach_mul_(372),
aten__foreach_neg(373),
aten__foreach_neg_(374),
aten__foreach_norm(375),
aten__foreach_reciprocal(376),
aten__foreach_reciprocal_(377),
aten__foreach_round(378),
aten__foreach_round_(379),
aten__foreach_sigmoid(380),
aten__foreach_sigmoid_(381),
aten__foreach_sin(382),
aten__foreach_sin_(383),
aten__foreach_sinh(384),
aten__foreach_sinh_(385),
aten__foreach_sqrt(386),
aten__foreach_sqrt_(387),
aten__foreach_sub(388),
aten__foreach_sub_(389),
aten__foreach_tan(390),
aten__foreach_tan_(391),
aten__foreach_tanh(392),
aten__foreach_tanh_(393),
aten__foreach_trunc(394),
aten__foreach_trunc_(395),
aten__foreach_zero(396),
aten__foreach_zero_(397),
aten__fused_adam(398),
aten__fused_adam_(399),
aten__fused_adamw(400),
aten__fused_adamw_(401),
aten__fused_dropout(402),
aten__fused_moving_avg_obs_fq_helper(403),
aten__fused_moving_avg_obs_fq_helper_functional(404),
aten__fused_sdp_choice(405),
aten__fw_primal(406),
aten__fw_primal_copy(407),
aten__gather_sparse_backward(408),
aten__grid_sampler_2d_cpu_fallback(409),
aten__grid_sampler_2d_cpu_fallback_backward(410),
aten__has_compatible_shallow_copy_type(411),
aten__has_same_storage_numel(412),
aten__histogramdd_bin_edges(413),
aten__histogramdd_from_bin_cts(414),
aten__histogramdd_from_bin_tensors(415),
aten__index_put_impl(416),
aten__index_put_impl_(417),
aten__indices(418),
aten__indices_copy(419),
aten__is_all_true(420),
aten__is_any_true(421),
aten__is_zerotensor(422),
aten__linalg_check_errors(423),
aten__linalg_det(424),
aten__linalg_eigh(425),
aten__linalg_slogdet(426),
aten__linalg_solve_ex(427),
aten__linalg_svd(428),
aten__local_scalar_dense(429),
aten__log_softmax(430),
aten__log_softmax_backward_data(431),
aten__logcumsumexp(432),
aten__lstm_mps(433),
aten__lu_with_info(434),
aten__make_dual(435),
aten__make_dual_copy(436),
aten__make_per_channel_quantized_tensor(437),
aten__make_per_tensor_quantized_tensor(438),
aten__masked_scale(439),
aten__masked_softmax(440),
aten__masked_softmax_backward(441),
aten__mkldnn_reshape(442),
aten__mkldnn_transpose(443),
aten__mkldnn_transpose_(444),
aten__mps_convolution(445),
aten__mps_convolution_transpose(446),
aten__native_batch_norm_legit(447),
aten__native_batch_norm_legit_functional(448),
aten__native_decoder_only_multi_head_attention(449),
aten__native_multi_head_attention(450),
aten__neg_view(451),
aten__neg_view_copy(452),
aten__nested_from_padded(453),
aten__nested_from_padded_and_nested_example(454),
aten__nested_select_backward(455),
aten__nested_sum_backward(456),
aten__nested_tensor_from_mask(457),
aten__nested_tensor_from_mask_left_aligned(458),
aten__nested_tensor_from_tensor_list(459),
aten__nested_tensor_offsets(460),
aten__nested_tensor_size(461),
aten__nested_tensor_softmax_with_shape(462),
aten__nested_tensor_strides(463),
aten__nested_view_from_buffer(464),
aten__nested_view_from_buffer_copy(465),
aten__new_zeros_with_same_feature_meta(466),
aten__nnpack_available(467),
aten__nnpack_spatial_convolution(468),
aten__nnz(469),
aten__pack_padded_sequence(470),
aten__pack_padded_sequence_backward(471),
aten__pad_circular(472),
aten__pad_enum(473),
aten__pad_packed_sequence(474),
aten__pdist_backward(475),
aten__pdist_forward(476),
aten__pin_memory(477),
aten__prelu_kernel(478),
aten__prelu_kernel_backward(479),
aten__remove_batch_dim(480),
aten__reshape_alias(481),
aten__reshape_alias_copy(482),
aten__reshape_copy(483),
aten__reshape_from_tensor(484),
aten__resize_output(485),
aten__resize_output_(486),
aten__rowwise_prune(487),
aten__sample_dirichlet(488),
aten__saturate_weight_to_fp16(489),
aten__scaled_dot_product_attention(490),
aten__scaled_dot_product_attention_math(491),
aten__scaled_dot_product_efficient_attention(492),
aten__scaled_dot_product_efficient_attention_backward(493),
aten__scaled_dot_product_flash_attention(494),
aten__scaled_dot_product_flash_attention_backward(495),
aten__segment_reduce_backward(496),
aten__shape_as_tensor(497),
aten__slow_conv2d_backward(498),
aten__slow_conv2d_forward(499),
aten__sobol_engine_draw(500),
aten__sobol_engine_ff(501),
aten__sobol_engine_ff_(502),
aten__sobol_engine_initialize_state(503),
aten__sobol_engine_initialize_state_(504),
aten__sobol_engine_scramble(505),
aten__sobol_engine_scramble_(506),
aten__softmax(507),
aten__softmax_backward_data(508),
aten__sparse_addmm(509),
aten__sparse_broadcast_to(510),
aten__sparse_broadcast_to_copy(511),
aten__sparse_bsc_tensor_unsafe(512),
aten__sparse_bsr_tensor_unsafe(513),
aten__sparse_compressed_tensor_unsafe(514),
aten__sparse_coo_tensor_unsafe(515),
aten__sparse_coo_tensor_with_dims(516),
aten__sparse_coo_tensor_with_dims_and_tensors(517),
aten__sparse_csc_tensor_unsafe(518),
aten__sparse_csr_prod(519),
aten__sparse_csr_sum(520),
aten__sparse_csr_tensor_unsafe(521),
aten__sparse_log_softmax(522),
aten__sparse_log_softmax_backward_data(523),
aten__sparse_mm(524),
aten__sparse_mm_reduce_impl(525),
aten__sparse_mm_reduce_impl_backward(526),
aten__sparse_softmax(527),
aten__sparse_softmax_backward_data(528),
aten__sparse_sparse_matmul(529),
aten__sparse_sum(530),
aten__sparse_sum_backward(531),
aten__spdiags(532),
aten__stack(533),
aten__standard_gamma(534),
aten__standard_gamma_grad(535),
aten__test_ambiguous_defaults(536),
aten__test_autograd_multiple_dispatch(537),
aten__test_autograd_multiple_dispatch_view(538),
aten__test_autograd_multiple_dispatch_view_copy(539),
aten__test_check_tensor(540),
aten__test_optional_filled_intlist(541),
aten__test_optional_floatlist(542),
aten__test_optional_intlist(543),
aten__test_serialization_subcmul(544),
aten__test_string_default(545),
aten__test_warn_in_autograd(546),
aten__thnn_differentiable_gru_cell_backward(547),
aten__thnn_differentiable_lstm_cell_backward(548),
aten__thnn_fused_gru_cell(549),
aten__thnn_fused_gru_cell_backward(550),
aten__thnn_fused_lstm_cell(551),
aten__thnn_fused_lstm_cell_backward(552),
aten__thnn_fused_lstm_cell_backward_impl(553),
aten__to_copy(554),
aten__to_cpu(555),
aten__to_dense(556),
aten__transform_bias_rescale_qkv(557),
aten__transformer_decoder_only_layer_fwd(558),
aten__transformer_encoder_layer_fwd(559),
aten__trilinear(560),
aten__triton_multi_head_attention(561),
aten__triton_scaled_dot_attention(562),
aten__unique(563),
aten__unique2(564),
aten__unpack_dual(565),
aten__unsafe_view(566),
aten__upsample_bicubic2d_aa(567),
aten__upsample_bicubic2d_aa_backward(568),
aten__upsample_bilinear2d_aa(569),
aten__upsample_bilinear2d_aa_backward(570),
aten__upsample_nearest_exact1d(571),
aten__upsample_nearest_exact1d_backward(572),
aten__upsample_nearest_exact2d(573),
aten__upsample_nearest_exact2d_backward(574),
aten__upsample_nearest_exact3d(575),
aten__upsample_nearest_exact3d_backward(576),
aten__use_cudnn_ctc_loss(577),
aten__use_cudnn_rnn_flatten_weight(578),
aten__validate_compressed_sparse_indices(579),
aten__validate_sparse_bsc_tensor_args(580),
aten__validate_sparse_bsr_tensor_args(581),
aten__validate_sparse_compressed_tensor_args(582),
aten__validate_sparse_coo_tensor_args(583),
aten__validate_sparse_csc_tensor_args(584),
aten__validate_sparse_csr_tensor_args(585),
aten__values(586),
aten__values_copy(587),
aten__version(588),
aten__weight_norm(589),
aten__weight_norm_differentiable_backward(590),
aten__weight_norm_interface(591),
aten__weight_norm_interface_backward(592),
aten_abs(593),
aten_abs_(594),
aten_absolute(595),
aten_absolute_(596),
aten_acos(597),
aten_acos_(598),
aten_acosh(599),
aten_acosh_(600),
aten_adaptive_avg_pool1d(601),
aten_adaptive_avg_pool2d(602),
aten_adaptive_avg_pool3d(603),
aten_adaptive_avg_pool3d_backward(604),
aten_adaptive_max_pool1d(605),
aten_adaptive_max_pool2d(606),
aten_adaptive_max_pool2d_backward(607),
aten_adaptive_max_pool3d(608),
aten_adaptive_max_pool3d_backward(609),
aten_add(610),
aten_add_(611),
aten_addbmm(612),
aten_addbmm_(613),
aten_addcdiv(614),
aten_addcdiv_(615),
aten_addcmul(616),
aten_addcmul_(617),
aten_addmm(618),
aten_addmm_(619),
aten_addmv(620),
aten_addmv_(621),
aten_addr(622),
aten_addr_(623),
aten_adjoint(624),
aten_affine_grid_generator(625),
aten_affine_grid_generator_backward(626),
aten_alias(627),
aten_alias_copy(628),
aten_align_as(629),
aten_align_tensors(630),
aten_align_to(631),
aten_all(632),
aten_allclose(633),
aten_alpha_dropout(634),
aten_alpha_dropout_(635),
aten_amax(636),
aten_amin(637),
aten_aminmax(638),
aten_angle(639),
aten_any(640),
aten_arange(641),
aten_arccos(642),
aten_arccos_(643),
aten_arccosh(644),
aten_arccosh_(645),
aten_arcsin(646),
aten_arcsin_(647),
aten_arcsinh(648),
aten_arcsinh_(649),
aten_arctan(650),
aten_arctan2(651),
aten_arctan2_(652),
aten_arctan_(653),
aten_arctanh(654),
aten_arctanh_(655),
aten_argmax(656),
aten_argmin(657),
aten_argsort(658),
aten_argwhere(659),
aten_as_strided(660),
aten_as_strided_(661),
aten_as_strided_copy(662),
aten_as_strided_scatter(663),
aten_asin(664),
aten_asin_(665),
aten_asinh(666),
aten_asinh_(667),
aten_atan(668),
aten_atan2(669),
aten_atan2_(670),
aten_atan_(671),
aten_atanh(672),
aten_atanh_(673),
aten_atleast_1d(674),
aten_atleast_2d(675),
aten_atleast_3d(676),
aten_avg_pool1d(677),
aten_avg_pool2d(678),
aten_avg_pool2d_backward(679),
aten_avg_pool3d(680),
aten_avg_pool3d_backward(681),
aten_baddbmm(682),
aten_baddbmm_(683),
aten_bartlett_window(684),
aten_batch_norm(685),
aten_batch_norm_backward_elemt(686),
aten_batch_norm_backward_reduce(687),
aten_batch_norm_elemt(688),
aten_batch_norm_gather_stats(689),
aten_batch_norm_gather_stats_with_counts(690),
aten_batch_norm_stats(691),
aten_batch_norm_update_stats(692),
aten_bernoulli(693),
aten_bernoulli_(694),
aten_bilinear(695),
aten_binary_cross_entropy(696),
aten_binary_cross_entropy_backward(697),
aten_binary_cross_entropy_with_logits(698),
aten_bincount(699),
aten_binomial(700),
aten_bitwise_and(701),
aten_bitwise_and_(702),
aten_bitwise_left_shift(703),
aten_bitwise_left_shift_(704),
aten_bitwise_not(705),
aten_bitwise_not_(706),
aten_bitwise_or(707),
aten_bitwise_or_(708),
aten_bitwise_right_shift(709),
aten_bitwise_right_shift_(710),
aten_bitwise_xor(711),
aten_bitwise_xor_(712),
aten_blackman_window(713),
aten_block_diag(714),
aten_bmm(715),
aten_broadcast_tensors(716),
aten_broadcast_to(717),
aten_bucketize(718),
aten_can_cast(719),
aten_cartesian_prod(720),
aten_cat(721),
aten_cauchy(722),
aten_cauchy_(723),
aten_ccol_indices(724),
aten_ccol_indices_copy(725),
aten_cdist(726),
aten_ceil(727),
aten_ceil_(728),
aten_celu(729),
aten_celu_(730),
aten_chain_matmul(731),
aten_chalf(732),
aten_channel_shuffle(733),
aten_cholesky(734),
aten_cholesky_inverse(735),
aten_cholesky_solve(736),
aten_choose_qparams_optimized(737),
aten_chunk(738),
aten_clamp(739),
aten_clamp_(740),
aten_clamp_max(741),
aten_clamp_max_(742),
aten_clamp_min(743),
aten_clamp_min_(744),
aten_clip(745),
aten_clip_(746),
aten_clone(747),
aten_coalesce(748),
aten_col2im(749),
aten_col_indices(750),
aten_col_indices_copy(751),
aten_column_stack(752),
aten_combinations(753),
aten_complex(754),
aten_concat(755),
aten_concatenate(756),
aten_conj(757),
aten_conj_physical(758),
aten_conj_physical_(759),
aten_constant_pad_nd(760),
aten_contiguous(761),
aten_conv1d(762),
aten_conv2d(763),
aten_conv3d(764),
aten_conv_depthwise3d(765),
aten_conv_tbc(766),
aten_conv_tbc_backward(767),
aten_conv_transpose1d(768),
aten_conv_transpose2d(769),
aten_conv_transpose3d(770),
aten_convolution(771),
aten_convolution_backward(772),
aten_convolution_backward_overrideable(773),
aten_convolution_overrideable(774),
aten_copy(775),
aten_copy_(776),
aten_copy_sparse_to_sparse(777),
aten_copy_sparse_to_sparse_(778),
aten_copysign(779),
aten_copysign_(780),
aten_corrcoef(781),
aten_cos(782),
aten_cos_(783),
aten_cosh(784),
aten_cosh_(785),
aten_cosine_embedding_loss(786),
aten_cosine_similarity(787),
aten_count_nonzero(788),
aten_cov(789),
aten_cross(790),
aten_cross_entropy_loss(791),
aten_crow_indices(792),
aten_crow_indices_copy(793),
aten_ctc_loss(794),
aten_cudnn_affine_grid_generator(795),
aten_cudnn_affine_grid_generator_backward(796),
aten_cudnn_batch_norm(797),
aten_cudnn_batch_norm_backward(798),
aten_cudnn_convolution(799),
aten_cudnn_convolution_add_relu(800),
aten_cudnn_convolution_relu(801),
aten_cudnn_convolution_transpose(802),
aten_cudnn_grid_sampler(803),
aten_cudnn_grid_sampler_backward(804),
aten_cudnn_is_acceptable(805),
aten_cummax(806),
aten_cummaxmin_backward(807),
aten_cummin(808),
aten_cumprod(809),
aten_cumprod_(810),
aten_cumprod_backward(811),
aten_cumsum(812),
aten_cumsum_(813),
aten_cumulative_trapezoid(814),
aten_data(815),
aten_deg2rad(816),
aten_deg2rad_(817),
aten_dense_dim(818),
aten_dequantize(819),
aten_det(820),
aten_detach(821),
aten_detach_(822),
aten_detach_copy(823),
aten_diag(824),
aten_diag_embed(825),
aten_diagflat(826),
aten_diagonal(827),
aten_diagonal_backward(828),
aten_diagonal_copy(829),
aten_diagonal_scatter(830),
aten_diff(831),
aten_digamma(832),
aten_digamma_(833),
aten_dist(834),
aten_div(835),
aten_div_(836),
aten_divide(837),
aten_divide_(838),
aten_dot(839),
aten_dropout(840),
aten_dropout_(841),
aten_dsplit(842),
aten_dstack(843),
aten_einsum(844),
aten_elu(845),
aten_elu_(846),
aten_elu_backward(847),
aten_embedding(848),
aten_embedding_backward(849),
aten_embedding_bag(850),
aten_embedding_dense_backward(851),
aten_embedding_renorm(852),
aten_embedding_renorm_(853),
aten_embedding_sparse_backward(854),
aten_empty(855),
aten_empty_like(856),
aten_empty_quantized(857),
aten_empty_strided(858),
aten_eq(859),
aten_eq_(860),
aten_equal(861),
aten_erf(862),
aten_erf_(863),
aten_erfc(864),
aten_erfc_(865),
aten_erfinv(866),
aten_erfinv_(867),
aten_exp(868),
aten_exp2(869),
aten_exp2_(870),
aten_exp_(871),
aten_expand(872),
aten_expand_as(873),
aten_expand_copy(874),
aten_expm1(875),
aten_expm1_(876),
aten_exponential(877),
aten_exponential_(878),
aten_eye(879),
aten_fake_quantize_per_channel_affine(880),
aten_fake_quantize_per_channel_affine_cachemask(881),
aten_fake_quantize_per_channel_affine_cachemask_backward(882),
aten_fake_quantize_per_tensor_affine(883),
aten_fake_quantize_per_tensor_affine_cachemask(884),
aten_fake_quantize_per_tensor_affine_cachemask_backward(885),
aten_fbgemm_linear_fp16_weight(886),
aten_fbgemm_linear_fp16_weight_fp32_activation(887),
aten_fbgemm_linear_int8_weight(888),
aten_fbgemm_linear_int8_weight_fp32_activation(889),
aten_fbgemm_linear_quantize_weight(890),
aten_fbgemm_pack_gemm_matrix_fp16(891),
aten_fbgemm_pack_quantized_matrix(892),
aten_feature_alpha_dropout(893),
aten_feature_alpha_dropout_(894),
aten_feature_dropout(895),
aten_feature_dropout_(896),
aten_fft_fft(897),
aten_fft_fft2(898),
aten_fft_fftfreq(899),
aten_fft_fftn(900),
aten_fft_fftshift(901),
aten_fft_hfft(902),
aten_fft_hfft2(903),
aten_fft_hfftn(904),
aten_fft_ifft(905),
aten_fft_ifft2(906),
aten_fft_ifftn(907),
aten_fft_ifftshift(908),
aten_fft_ihfft(909),
aten_fft_ihfft2(910),
aten_fft_ihfftn(911),
aten_fft_irfft(912),
aten_fft_irfft2(913),
aten_fft_irfftn(914),
aten_fft_rfft(915),
aten_fft_rfft2(916),
aten_fft_rfftfreq(917),
aten_fft_rfftn(918),
aten_fill(919),
aten_fill_(920),
aten_fill_diagonal(921),
aten_fill_diagonal_(922),
aten_fix(923),
aten_fix_(924),
aten_flatten(925),
aten_flatten_dense_tensors(926),
aten_flip(927),
aten_fliplr(928),
aten_flipud(929),
aten_float_power(930),
aten_float_power_(931),
aten_floor(932),
aten_floor_(933),
aten_floor_divide(934),
aten_floor_divide_(935),
aten_fmax(936),
aten_fmin(937),
aten_fmod(938),
aten_fmod_(939),
aten_frac(940),
aten_frac_(941),
aten_fractional_max_pool2d(942),
aten_fractional_max_pool2d_backward(943),
aten_fractional_max_pool3d(944),
aten_fractional_max_pool3d_backward(945),
aten_frexp(946),
aten_frobenius_norm(947),
aten_from_file(948),
aten_full(949),
aten_full_like(950),
aten_fused_moving_avg_obs_fake_quant(951),
aten_gather(952),
aten_gather_backward(953),
aten_gcd(954),
aten_gcd_(955),
aten_ge(956),
aten_ge_(957),
aten_gelu(958),
aten_gelu_(959),
aten_gelu_backward(960),
aten_geometric(961),
aten_geometric_(962),
aten_geqrf(963),
aten_ger(964),
aten_glu(965),
aten_glu_backward(966),
aten_glu_backward_jvp(967),
aten_glu_jvp(968),
aten_gradient(969),
aten_greater(970),
aten_greater_(971),
aten_greater_equal(972),
aten_greater_equal_(973),
aten_grid_sampler(974),
aten_grid_sampler_2d(975),
aten_grid_sampler_2d_backward(976),
aten_grid_sampler_3d(977),
aten_grid_sampler_3d_backward(978),
aten_group_norm(979),
aten_gru(980),
aten_gru_cell(981),
aten_gt(982),
aten_gt_(983),
aten_hamming_window(984),
aten_hann_window(985),
aten_hardshrink(986),
aten_hardshrink_backward(987),
aten_hardsigmoid(988),
aten_hardsigmoid_(989),
aten_hardsigmoid_backward(990),
aten_hardswish(991),
aten_hardswish_(992),
aten_hardswish_backward(993),
aten_hardtanh(994),
aten_hardtanh_(995),
aten_hardtanh_backward(996),
aten_heaviside(997),
aten_heaviside_(998),
aten_hinge_embedding_loss(999),
aten_histc(1000),
aten_histogram(1001),
aten_histogramdd(1002),
aten_hsplit(1003),
aten_hspmm(1004),
aten_hstack(1005),
aten_huber_loss(1006),
aten_huber_loss_backward(1007),
aten_hypot(1008),
aten_hypot_(1009),
aten_i0(1010),
aten_i0_(1011),
aten_igamma(1012),
aten_igamma_(1013),
aten_igammac(1014),
aten_igammac_(1015),
aten_im2col(1016),
aten_imag(1017),
aten_index(1018),
aten_index_add(1019),
aten_index_add_(1020),
aten_index_copy(1021),
aten_index_copy_(1022),
aten_index_fill(1023),
aten_index_fill_(1024),
aten_index_put(1025),
aten_index_put_(1026),
aten_index_reduce(1027),
aten_index_reduce_(1028),
aten_index_select(1029),
aten_index_select_backward(1030),
aten_indices(1031),
aten_indices_copy(1032),
aten_infinitely_differentiable_gelu_backward(1033),
aten_inner(1034),
aten_instance_norm(1035),
aten_int_repr(1036),
aten_inverse(1037),
aten_is_coalesced(1038),
aten_is_complex(1039),
aten_is_conj(1040),
aten_is_distributed(1041),
aten_is_floating_point(1042),
aten_is_inference(1043),
aten_is_leaf(1044),
aten_is_neg(1045),
aten_is_nonzero(1046),
aten_is_pinned(1047),
aten_is_same_size(1048),
aten_is_set_to(1049),
aten_is_signed(1050),
aten_is_vulkan_available(1051),
aten_isclose(1052),
aten_isfinite(1053),
aten_isin(1054),
aten_isinf(1055),
aten_isnan(1056),
aten_isneginf(1057),
aten_isposinf(1058),
aten_isreal(1059),
aten_istft(1060),
aten_item(1061),
aten_kaiser_window(1062),
aten_kl_div(1063),
aten_kron(1064),
aten_kthvalue(1065),
aten_l1_loss(1066),
aten_layer_norm(1067),
aten_lcm(1068),
aten_lcm_(1069),
aten_ldexp(1070),
aten_ldexp_(1071),
aten_le(1072),
aten_le_(1073),
aten_leaky_relu(1074),
aten_leaky_relu_(1075),
aten_leaky_relu_backward(1076),
aten_lerp(1077),
aten_lerp_(1078),
aten_less(1079),
aten_less_(1080),
aten_less_equal(1081),
aten_less_equal_(1082),
aten_lgamma(1083),
aten_lgamma_(1084),
aten_lift(1085),
aten_lift_fresh(1086),
aten_lift_fresh_copy(1087),
aten_linalg_cholesky(1088),
aten_linalg_cholesky_ex(1089),
aten_linalg_cond(1090),
aten_linalg_cross(1091),
aten_linalg_det(1092),
aten_linalg_diagonal(1093),
aten_linalg_eig(1094),
aten_linalg_eigh(1095),
aten_linalg_eigvals(1096),
aten_linalg_eigvalsh(1097),
aten_linalg_householder_product(1098),
aten_linalg_inv(1099),
aten_linalg_inv_ex(1100),
aten_linalg_ldl_factor(1101),
aten_linalg_ldl_factor_ex(1102),
aten_linalg_ldl_solve(1103),
aten_linalg_lstsq(1104),
aten_linalg_lu(1105),
aten_linalg_lu_factor(1106),
aten_linalg_lu_factor_ex(1107),
aten_linalg_lu_solve(1108),
aten_linalg_matmul(1109),
aten_linalg_matrix_exp(1110),
aten_linalg_matrix_norm(1111),
aten_linalg_matrix_power(1112),
aten_linalg_matrix_rank(1113),
aten_linalg_multi_dot(1114),
aten_linalg_norm(1115),
aten_linalg_pinv(1116),
aten_linalg_qr(1117),
aten_linalg_slogdet(1118),
aten_linalg_solve(1119),
aten_linalg_solve_ex(1120),
aten_linalg_solve_triangular(1121),
aten_linalg_svd(1122),
aten_linalg_svdvals(1123),
aten_linalg_tensorinv(1124),
aten_linalg_tensorsolve(1125),
aten_linalg_vander(1126),
aten_linalg_vecdot(1127),
aten_linalg_vector_norm(1128),
aten_linear(1129),
aten_linear_backward(1130),
aten_linspace(1131),
aten_log(1132),
aten_log10(1133),
aten_log10_(1134),
aten_log1p(1135),
aten_log1p_(1136),
aten_log2(1137),
aten_log2_(1138),
aten_log_(1139),
aten_log_normal(1140),
aten_log_normal_(1141),
aten_log_sigmoid(1142),
aten_log_sigmoid_backward(1143),
aten_log_sigmoid_forward(1144),
aten_log_softmax(1145),
aten_logaddexp(1146),
aten_logaddexp2(1147),
aten_logcumsumexp(1148),
aten_logdet(1149),
aten_logical_and(1150),
aten_logical_and_(1151),
aten_logical_not(1152),
aten_logical_not_(1153),
aten_logical_or(1154),
aten_logical_or_(1155),
aten_logical_xor(1156),
aten_logical_xor_(1157),
aten_logit(1158),
aten_logit_(1159),
aten_logit_backward(1160),
aten_logspace(1161),
aten_logsumexp(1162),
aten_lshift(1163),
aten_lstm(1164),
aten_lstm_cell(1165),
aten_lstm_mps_backward(1166),
aten_lt(1167),
aten_lt_(1168),
aten_lu_solve(1169),
aten_lu_unpack(1170),
aten_mH(1171),
aten_mT(1172),
aten_margin_ranking_loss(1173),
aten_masked_fill(1174),
aten_masked_fill_(1175),
aten_masked_scatter(1176),
aten_masked_scatter_(1177),
aten_masked_select(1178),
aten_masked_select_backward(1179),
aten_matmul(1180),
aten_matmul_backward(1181),
aten_matrix_H(1182),
aten_matrix_exp(1183),
aten_matrix_exp_backward(1184),
aten_matrix_power(1185),
aten_max(1186),
aten_max_pool1d(1187),
aten_max_pool1d_with_indices(1188),
aten_max_pool2d(1189),
aten_max_pool2d_backward(1190),
aten_max_pool2d_with_indices(1191),
aten_max_pool2d_with_indices_backward(1192),
aten_max_pool3d(1193),
aten_max_pool3d_with_indices(1194),
aten_max_pool3d_with_indices_backward(1195),
aten_max_unpool2d(1196),
aten_max_unpool3d(1197),
aten_maximum(1198),
aten_mean(1199),
aten_median(1200),
aten_meshgrid(1201),
aten_min(1202),
aten_minimum(1203),
aten_miopen_batch_norm(1204),
aten_miopen_batch_norm_backward(1205),
aten_miopen_convolution(1206),
aten_miopen_convolution_add_relu(1207),
aten_miopen_convolution_relu(1208),
aten_miopen_convolution_transpose(1209),
aten_miopen_depthwise_convolution(1210),
aten_miopen_rnn(1211),
aten_miopen_rnn_backward(1212),
aten_mish(1213),
aten_mish_(1214),
aten_mish_backward(1215),
aten_mkldnn_adaptive_avg_pool2d(1216),
aten_mkldnn_adaptive_avg_pool2d_backward(1217),
aten_mkldnn_convolution(1218),
aten_mkldnn_linear(1219),
aten_mkldnn_linear_backward(1220),
aten_mkldnn_linear_backward_input(1221),
aten_mkldnn_linear_backward_weights(1222),
aten_mkldnn_max_pool2d(1223),
aten_mkldnn_max_pool2d_backward(1224),
aten_mkldnn_max_pool3d(1225),
aten_mkldnn_max_pool3d_backward(1226),
aten_mkldnn_reorder_conv2d_weight(1227),
aten_mkldnn_reorder_conv3d_weight(1228),
aten_mkldnn_rnn_layer(1229),
aten_mkldnn_rnn_layer_backward(1230),
aten_mm(1231),
aten_mode(1232),
aten_moveaxis(1233),
aten_movedim(1234),
aten_mps_convolution_backward(1235),
aten_mps_convolution_transpose_backward(1236),
aten_mse_loss(1237),
aten_mse_loss_backward(1238),
aten_msort(1239),
aten_mul(1240),
aten_mul_(1241),
aten_multi_margin_loss(1242),
aten_multi_margin_loss_backward(1243),
aten_multilabel_margin_loss(1244),
aten_multilabel_margin_loss_backward(1245),
aten_multilabel_margin_loss_forward(1246),
aten_multinomial(1247),
aten_multiply(1248),
aten_multiply_(1249),
aten_mv(1250),
aten_mvlgamma(1251),
aten_mvlgamma_(1252),
aten_nan_to_num(1253),
aten_nan_to_num_(1254),
aten_nanmean(1255),
aten_nanmedian(1256),
aten_nanquantile(1257),
aten_nansum(1258),
aten_narrow(1259),
aten_narrow_copy(1260),
aten_native_batch_norm(1261),
aten_native_batch_norm_backward(1262),
aten_native_channel_shuffle(1263),
aten_native_dropout(1264),
aten_native_dropout_backward(1265),
aten_native_group_norm(1266),
aten_native_group_norm_backward(1267),
aten_native_layer_norm(1268),
aten_native_layer_norm_backward(1269),
aten_native_norm(1270),
aten_ne(1271),
aten_ne_(1272),
aten_neg(1273),
aten_neg_(1274),
aten_negative(1275),
aten_negative_(1276),
aten_nested_to_padded_tensor(1277),
aten_new_empty(1278),
aten_new_empty_strided(1279),
aten_new_full(1280),
aten_new_ones(1281),
aten_new_zeros(1282),
aten_nextafter(1283),
aten_nextafter_(1284),
aten_nll_loss(1285),
aten_nll_loss2d(1286),
aten_nll_loss2d_backward(1287),
aten_nll_loss2d_forward(1288),
aten_nll_loss_backward(1289),
aten_nll_loss_forward(1290),
aten_nll_loss_nd(1291),
aten_nonzero(1292),
aten_nonzero_numpy(1293),
aten_norm(1294),
aten_norm_except_dim(1295),
aten_normal(1296),
aten_normal_(1297),
aten_normal_functional(1298),
aten_not_equal(1299),
aten_not_equal_(1300),
aten_nuclear_norm(1301),
aten_numpy_T(1302),
aten_one_hot(1303),
aten_ones(1304),
aten_ones_like(1305),
aten_orgqr(1306),
aten_ormqr(1307),
aten_outer(1308),
aten_output_nr(1309),
aten_pad(1310),
aten_pad_sequence(1311),
aten_pairwise_distance(1312),
aten_pdist(1313),
aten_permute(1314),
aten_permute_copy(1315),
aten_pin_memory(1316),
aten_pinverse(1317),
aten_pixel_shuffle(1318),
aten_pixel_unshuffle(1319),
aten_poisson(1320),
aten_poisson_nll_loss(1321),
aten_polar(1322),
aten_polygamma(1323),
aten_polygamma_(1324),
aten_positive(1325),
aten_pow(1326),
aten_pow_(1327),
aten_prelu(1328),
aten_prod(1329),
aten_promote_types(1330),
aten_put(1331),
aten_put_(1332),
aten_q_per_channel_axis(1333),
aten_q_per_channel_scales(1334),
aten_q_per_channel_zero_points(1335),
aten_q_scale(1336),
aten_q_zero_point(1337),
aten_qr(1338),
aten_qscheme(1339),
aten_quantile(1340),
aten_quantize_per_channel(1341),
aten_quantize_per_tensor(1342),
aten_quantize_per_tensor_dynamic(1343),
aten_quantized_batch_norm(1344),
aten_quantized_gru_cell(1345),
aten_quantized_lstm_cell(1346),
aten_quantized_max_pool1d(1347),
aten_quantized_max_pool2d(1348),
aten_quantized_rnn_relu_cell(1349),
aten_quantized_rnn_tanh_cell(1350),
aten_rad2deg(1351),
aten_rad2deg_(1352),
aten_rand(1353),
aten_rand_like(1354),
aten_randint(1355),
aten_randint_like(1356),
aten_randn(1357),
aten_randn_like(1358),
aten_random(1359),
aten_random_(1360),
aten_randperm(1361),
aten_range(1362),
aten_ravel(1363),
aten_real(1364),
aten_reciprocal(1365),
aten_reciprocal_(1366),
aten_record_stream(1367),
aten_refine_names(1368),
aten_reflection_pad1d(1369),
aten_reflection_pad1d_backward(1370),
aten_reflection_pad2d(1371),
aten_reflection_pad2d_backward(1372),
aten_reflection_pad3d(1373),
aten_reflection_pad3d_backward(1374),
aten_relu(1375),
aten_relu6(1376),
aten_relu6_(1377),
aten_relu_(1378),
aten_remainder(1379),
aten_remainder_(1380),
aten_rename(1381),
aten_rename_(1382),
aten_renorm(1383),
aten_renorm_(1384),
aten_repeat(1385),
aten_repeat_interleave(1386),
aten_replication_pad1d(1387),
aten_replication_pad1d_backward(1388),
aten_replication_pad2d(1389),
aten_replication_pad2d_backward(1390),
aten_replication_pad3d(1391),
aten_replication_pad3d_backward(1392),
aten_requires_grad(1393),
aten_requires_grad_(1394),
aten_reshape(1395),
aten_reshape_as(1396),
aten_resize(1397),
aten_resize_(1398),
aten_resize_as(1399),
aten_resize_as_(1400),
aten_resize_as_sparse(1401),
aten_resize_as_sparse_(1402),
aten_resolve_conj(1403),
aten_resolve_neg(1404),
aten_result_type(1405),
aten_retain_grad(1406),
aten_retains_grad(1407),
aten_rnn_relu(1408),
aten_rnn_relu_cell(1409),
aten_rnn_tanh(1410),
aten_rnn_tanh_cell(1411),
aten_roll(1412),
aten_rot90(1413),
aten_round(1414),
aten_round_(1415),
aten_row_indices(1416),
aten_row_indices_copy(1417),
aten_row_stack(1418),
aten_rrelu(1419),
aten_rrelu_(1420),
aten_rrelu_with_noise(1421),
aten_rrelu_with_noise_(1422),
aten_rrelu_with_noise_backward(1423),
aten_rshift(1424),
aten_rsqrt(1425),
aten_rsqrt_(1426),
aten_rsub(1427),
aten_scalar_tensor(1428),
aten_scaled_dot_product_attention(1429),
aten_scatter(1430),
aten_scatter_(1431),
aten_scatter_add(1432),
aten_scatter_add_(1433),
aten_scatter_reduce(1434),
aten_scatter_reduce_(1435),
aten_searchsorted(1436),
aten_segment_reduce(1437),
aten_select(1438),
aten_select_backward(1439),
aten_select_copy(1440),
aten_select_scatter(1441),
aten_selu(1442),
aten_selu_(1443),
aten_set(1444),
aten_set_(1445),
aten_set_data(1446),
aten_sgn(1447),
aten_sgn_(1448),
aten_sigmoid(1449),
aten_sigmoid_(1450),
aten_sigmoid_backward(1451),
aten_sign(1452),
aten_sign_(1453),
aten_signbit(1454),
aten_silu(1455),
aten_silu_(1456),
aten_silu_backward(1457),
aten_sin(1458),
aten_sin_(1459),
aten_sinc(1460),
aten_sinc_(1461),
aten_sinh(1462),
aten_sinh_(1463),
aten_size(1464),
aten_slice(1465),
aten_slice_backward(1466),
aten_slice_copy(1467),
aten_slice_scatter(1468),
aten_slogdet(1469),
aten_slow_conv3d(1470),
aten_slow_conv3d_forward(1471),
aten_slow_conv_dilated2d(1472),
aten_slow_conv_dilated3d(1473),
aten_slow_conv_transpose2d(1474),
aten_slow_conv_transpose3d(1475),
aten_smm(1476),
aten_smooth_l1_loss(1477),
aten_smooth_l1_loss_backward(1478),
aten_soft_margin_loss(1479),
aten_soft_margin_loss_backward(1480),
aten_softmax(1481),
aten_softplus(1482),
aten_softplus_backward(1483),
aten_softshrink(1484),
aten_softshrink_backward(1485),
aten_sort(1486),
aten_sparse_bsc_tensor(1487),
aten_sparse_bsr_tensor(1488),
aten_sparse_compressed_tensor(1489),
aten_sparse_coo_tensor(1490),
aten_sparse_csc_tensor(1491),
aten_sparse_csr_tensor(1492),
aten_sparse_dim(1493),
aten_sparse_mask(1494),
aten_sparse_resize(1495),
aten_sparse_resize_(1496),
aten_sparse_resize_and_clear(1497),
aten_sparse_resize_and_clear_(1498),
aten_sparse_sampled_addmm(1499),
aten_special_airy_ai(1500),
aten_special_bessel_j0(1501),
aten_special_bessel_j1(1502),
aten_special_bessel_y0(1503),
aten_special_bessel_y1(1504),
aten_special_chebyshev_polynomial_t(1505),
aten_special_chebyshev_polynomial_u(1506),
aten_special_chebyshev_polynomial_v(1507),
aten_special_chebyshev_polynomial_w(1508),
aten_special_digamma(1509),
aten_special_entr(1510),
aten_special_erf(1511),
aten_special_erfc(1512),
aten_special_erfcx(1513),
aten_special_erfinv(1514),
aten_special_exp2(1515),
aten_special_expit(1516),
aten_special_expm1(1517),
aten_special_gammainc(1518),
aten_special_gammaincc(1519),
aten_special_gammaln(1520),
aten_special_hermite_polynomial_h(1521),
aten_special_hermite_polynomial_he(1522),
aten_special_i0(1523),
aten_special_i0e(1524),
aten_special_i1(1525),
aten_special_i1e(1526),
aten_special_laguerre_polynomial_l(1527),
aten_special_legendre_polynomial_p(1528),
aten_special_log1p(1529),
aten_special_log_ndtr(1530),
aten_special_log_softmax(1531),
aten_special_logit(1532),
aten_special_logsumexp(1533),
aten_special_modified_bessel_i0(1534),
aten_special_modified_bessel_i1(1535),
aten_special_modified_bessel_k0(1536),
aten_special_modified_bessel_k1(1537),
aten_special_multigammaln(1538),
aten_special_ndtr(1539),
aten_special_ndtri(1540),
aten_special_polygamma(1541),
aten_special_psi(1542),
aten_special_round(1543),
aten_special_scaled_modified_bessel_k0(1544),
aten_special_scaled_modified_bessel_k1(1545),
aten_special_shifted_chebyshev_polynomial_t(1546),
aten_special_shifted_chebyshev_polynomial_u(1547),
aten_special_shifted_chebyshev_polynomial_v(1548),
aten_special_shifted_chebyshev_polynomial_w(1549),
aten_special_sinc(1550),
aten_special_softmax(1551),
aten_special_spherical_bessel_j0(1552),
aten_special_xlog1py(1553),
aten_special_xlogy(1554),
aten_special_zeta(1555),
aten_split(1556),
aten_split_copy(1557),
aten_split_with_sizes(1558),
aten_split_with_sizes_copy(1559),
aten_sqrt(1560),
aten_sqrt_(1561),
aten_square(1562),
aten_square_(1563),
aten_squeeze(1564),
aten_squeeze_(1565),
aten_squeeze_copy(1566),
aten_sspaddmm(1567),
aten_stack(1568),
aten_std(1569),
aten_std_mean(1570),
aten_stft(1571),
aten_stride(1572),
aten_sub(1573),
aten_sub_(1574),
aten_subtract(1575),
aten_subtract_(1576),
aten_sum(1577),
aten_sum_to_size(1578),
aten_svd(1579),
aten_swapaxes(1580),
aten_swapaxes_(1581),
aten_swapdims(1582),
aten_swapdims_(1583),
aten_t(1584),
aten_t_(1585),
aten_t_copy(1586),
aten_take(1587),
aten_take_along_dim(1588),
aten_tan(1589),
aten_tan_(1590),
aten_tanh(1591),
aten_tanh_(1592),
aten_tanh_backward(1593),
aten_tensor_split(1594),
aten_tensordot(1595),
aten_thnn_conv2d(1596),
aten_threshold(1597),
aten_threshold_(1598),
aten_threshold_backward(1599),
aten_tile(1600),
aten_to(1601),
aten_to_dense(1602),
aten_to_dense_backward(1603),
aten_to_mkldnn(1604),
aten_to_mkldnn_backward(1605),
aten_to_padded_tensor(1606),
aten_to_sparse(1607),
aten_to_sparse_bsc(1608),
aten_to_sparse_bsr(1609),
aten_to_sparse_csc(1610),
aten_to_sparse_csr(1611),
aten_topk(1612),
aten_trace(1613),
aten_trace_backward(1614),
aten_transpose(1615),
aten_transpose_(1616),
aten_transpose_copy(1617),
aten_trapezoid(1618),
aten_trapz(1619),
aten_triangular_solve(1620),
aten_tril(1621),
aten_tril_(1622),
aten_tril_indices(1623),
aten_triplet_margin_loss(1624),
aten_triu(1625),
aten_triu_(1626),
aten_triu_indices(1627),
aten_true_divide(1628),
aten_true_divide_(1629),
aten_trunc(1630),
aten_trunc_(1631),
aten_type_as(1632),
aten_unbind(1633),
aten_unbind_copy(1634),
aten_unflatten(1635),
aten_unflatten_dense_tensors(1636),
aten_unfold(1637),
aten_unfold_backward(1638),
aten_unfold_copy(1639),
aten_uniform(1640),
aten_uniform_(1641),
aten_unique_consecutive(1642),
aten_unique_dim(1643),
aten_unique_dim_consecutive(1644),
aten_unsafe_chunk(1645),
aten_unsafe_split(1646),
aten_unsafe_split_with_sizes(1647),
aten_unsqueeze(1648),
aten_unsqueeze_(1649),
aten_unsqueeze_copy(1650),
aten_upsample_bicubic2d(1651),
aten_upsample_bicubic2d_backward(1652),
aten_upsample_bilinear2d(1653),
aten_upsample_bilinear2d_backward(1654),
aten_upsample_linear1d(1655),
aten_upsample_linear1d_backward(1656),
aten_upsample_nearest1d(1657),
aten_upsample_nearest1d_backward(1658),
aten_upsample_nearest2d(1659),
aten_upsample_nearest2d_backward(1660),
aten_upsample_nearest3d(1661),
aten_upsample_nearest3d_backward(1662),
aten_upsample_trilinear3d(1663),
aten_upsample_trilinear3d_backward(1664),
aten_value_selecting_reduction_backward(1665),
aten_values(1666),
aten_values_copy(1667),
aten_vander(1668),
aten_var(1669),
aten_var_mean(1670),
aten_vdot(1671),
aten_view(1672),
aten_view_as(1673),
aten_view_as_complex(1674),
aten_view_as_complex_copy(1675),
aten_view_as_real(1676),
aten_view_as_real_copy(1677),
aten_view_copy(1678),
aten_vsplit(1679),
aten_vstack(1680),
aten_where(1681),
aten_xlogy(1682),
aten_xlogy_(1683),
aten_zero(1684),
aten_zero_(1685),
aten_zeros(1686),
aten_zeros_like(1687),
  onnx_Add(1688),
  onnx_Concat(1689),
  onnx_Constant(1690),
  onnx_ConstantFill(1691),
  onnx_Div(1692),
  onnx_GRU(1693),
  onnx_Gather(1694),
  onnx_Gemm(1695),
  onnx_LSTM(1696),
  onnx_MatMul(1697),
  onnx_Min(1698),
  onnx_Max(1699),
  onnx_Mul(1700),
  onnx_Pow(1701),
  onnx_RNN(1702),
  onnx_Shape(1703),
  onnx_Size(1704),
  onnx_Slice(1705),
  onnx_Softmax(1706),
  onnx_Squeeze(1707),
  onnx_Sub(1708),
  onnx_Transpose(1709),
  onnx_Unsqueeze(1710),
  onnx_Loop(1711),
  onnx_If(1712),
  onnx_Reshape(1713),
  onnx_Expand(1714),
  onnx_Equal(1715),
  onnx_Greater(1716),
  onnx_GreaterOrEqual(1717),
  onnx_Less(1718),
  onnx_LessOrEqual(1719),
  onnx_Not(1720),
  aten_ATen(1721),
  onnx_Split(1722),
  onnx_ConstantOfShape(1723),
  onnx_Cast(1724),
  onnx_Mod(1725),
  onnx_Sqrt(1726),
  onnx_SplitToSequence(1727),
  onnx_SequenceAt(1728),
  onnx_SequenceConstruct(1729),
  onnx_SequenceEmpty(1730),
  onnx_SequenceInsert(1731),
  onnx_SequenceErase(1732),
  onnx_ConcatFromSequence(1733),
  onnx_Identity(1734),
  onnx_SoftmaxCrossEntropyLoss(1735),
  onnx_NegativeLogLikelihoodLoss(1736),
  onnx_LogSoftmax(1737),
  onnx_ReduceL1(1738),
  onnx_ReduceL2(1739),
  onnx_Conv(1740),
  onnx_BatchNormalization(1741),
  onnx_ReduceMean(1742),
  onnx_ReduceProd(1743),
  onnx_Relu(1744),
  onnx_Neg(1745),
  onnx_NonZero(1746),
  onnx_Range(1747),
  onnx_Tile(1748),
  onnx_Where(1749),
  onnx_Optional(1750),
  onnx_OptionalGetElement(1751),
  onnx_OptionalHasElement(1752),
  attr_A(1753),
attr_B(1754),
attr_C(1755),
attr_H(1756),
attr_HxW(1757),
attr_K(1758),
attr_L(1759),
attr_LD(1760),
attr_LU(1761),
attr_LU_data(1762),
attr_LU_pivots(1763),
attr_M(1764),
attr_N(1765),
attr_P(1766),
attr_Q(1767),
attr_R(1768),
attr_S(1769),
attr_U(1770),
attr_UPLO(1771),
attr_V(1772),
attr_Vh(1773),
attr_W(1774),
attr_X(1775),
attr_a(1776),
attr_abs(1777),
attr_accumulate(1778),
attr_addends(1779),
attr_adjoint(1780),
attr_align_corners(1781),
attr_allow_tf32(1782),
attr_alpha(1783),
attr_amsgrad(1784),
attr_anchor(1785),
attr_angle(1786),
attr_api_name(1787),
attr_append(1788),
attr_approximate(1789),
attr_arg1(1790),
attr_arg2(1791),
attr_arg3(1792),
attr_arg_out(1793),
attr_assume_unique(1794),
attr_atol(1795),
attr_attn_mask(1796),
attr_average_attn_weights(1797),
attr_averaging_const(1798),
attr_aweights(1799),
attr_axis(1800),
attr_axis0(1801),
attr_axis1(1802),
attr_b(1803),
attr_b_hh(1804),
attr_b_ih(1805),
attr_bag_size(1806),
attr_base(1807),
attr_batch1(1808),
attr_batch2(1809),
attr_batch_dim(1810),
attr_batch_first(1811),
attr_batch_size(1812),
attr_batch_sizes(1813),
attr_benchmark(1814),
attr_beta(1815),
attr_beta1(1816),
attr_beta2(1817),
attr_bias(1818),
attr_bias_defined(1819),
attr_bias_g(1820),
attr_bias_sizes(1821),
attr_bidirectional(1822),
attr_bin_edges(1823),
attr_bins(1824),
attr_bit_width(1825),
attr_blank(1826),
attr_blocksize(1827),
attr_boundaries(1828),
attr_buffer(1829),
attr_causal(1830),
attr_ccol_indices(1831),
attr_cdim(1832),
attr_cdist(1833),
attr_ceil_mode(1834),
attr_cell_state_fwd(1835),
attr_center(1836),
attr_ch_axis(1837),
attr_check_errors(1838),
attr_chunk_grad_outputs(1839),
attr_chunks(1840),
attr_coalesced(1841),
attr_coefficients(1842),
attr_col(1843),
attr_col_indices(1844),
attr_col_offsets(1845),
attr_col_offsets_hh(1846),
attr_col_offsets_ih(1847),
attr_compressed_idx(1848),
attr_compressed_indices(1849),
attr_compressed_indices_dtype(1850),
attr_compute_log_sumexp(1851),
attr_compute_mode(1852),
attr_compute_uv(1853),
attr_compute_v(1854),
attr_condition(1855),
attr_copy(1856),
attr_correction(1857),
attr_count(1858),
attr_count_include_pad(1859),
attr_counts(1860),
attr_cpu_dtype(1861),
attr_cpu_enabled(1862),
attr_cpu_nested_shape_example(1863),
attr_create_graph(1864),
attr_crow_indices(1865),
attr_cu_seqlens_k(1866),
attr_cu_seqlens_q(1867),
attr_cuda_dtype(1868),
attr_cuda_enabled(1869),
attr_cudnn_enable(1870),
attr_cudnn_enabled(1871),
attr_cum_seq_k(1872),
attr_cum_seq_q(1873),
attr_cx(1874),
attr_cx_(1875),
attr_cx_tmp(1876),
attr_cy(1877),
attr_cy_(1878),
attr_d(1879),
attr_data(1880),
attr_decimals(1881),
attr_delta(1882),
attr_dense(1883),
attr_dense_dim(1884),
attr_density(1885),
attr_descending(1886),
attr_destination(1887),
attr_deterministic(1888),
attr_device(1889),
attr_device_index(1890),
attr_dgrad_glu(1891),
attr_diagonal(1892),
attr_diagonals(1893),
attr_dilation(1894),
attr_dim(1895),
attr_dim0(1896),
attr_dim1(1897),
attr_dim2(1898),
attr_dimension(1899),
attr_dims(1900),
attr_dims_other(1901),
attr_dims_self(1902),
attr_divisor_override(1903),
attr_downscale_factor(1904),
attr_driver(1905),
attr_dropout(1906),
attr_dropout_mask(1907),
attr_dropout_p(1908),
attr_dropout_seed(1909),
attr_dropout_state(1910),
attr_dst(1911),
attr_dtype(1912),
attr_dual(1913),
attr_dummy(1914),
attr_dx(1915),
attr_edge_order(1916),
attr_eigenvalues(1917),
attr_eigenvectors(1918),
attr_eigvals(1919),
attr_eigvecs(1920),
attr_element(1921),
attr_elements(1922),
attr_ellipsis_idx(1923),
attr_embed_dim(1924),
attr_end(1925),
attr_end_dim(1926),
attr_eps(1927),
attr_epsilon(1928),
attr_equal_nan(1929),
attr_equation(1930),
attr_exp_avg_sqs(1931),
attr_exp_avgs(1932),
attr_expand1(1933),
attr_expand2(1934),
attr_expand3(1935),
attr_exponent(1936),
attr_exponential_average_factor(1937),
attr_fake_quant_enabled(1938),
attr_fake_quant_on(1939),
attr_ffn_bias_1(1940),
attr_ffn_bias_2(1941),
attr_ffn_weight_1(1942),
attr_ffn_weight_2(1943),
attr_filename(1944),
attr_fill_value(1945),
attr_flat(1946),
attr_forward(1947),
attr_found_inf(1948),
attr_from(1949),
attr_full(1950),
attr_full_matrices(1951),
attr_fuse_transform_0213(1952),
attr_fweights(1953),
attr_g(1954),
attr_gO(1955),
attr_generator(1956),
attr_ggI(1957),
attr_ggW(1958),
attr_ggb(1959),
attr_glu(1960),
attr_grad(1961),
attr_grad_bias(1962),
attr_grad_cy(1963),
attr_grad_factor(1964),
attr_grad_glu(1965),
attr_grad_hy(1966),
attr_grad_in(1967),
attr_grad_input(1968),
attr_grad_out(1969),
attr_grad_out_(1970),
attr_grad_output(1971),
attr_grad_scale(1972),
attr_grad_w(1973),
attr_grad_weight(1974),
attr_grad_x(1975),
attr_grad_y(1976),
attr_gradient(1977),
attr_grads(1978),
attr_grid(1979),
attr_group(1980),
attr_groups(1981),
attr_growth_interval(1982),
attr_growth_tracker(1983),
attr_half_to_float(1984),
attr_has_bias(1985),
attr_has_biases(1986),
attr_hermitian(1987),
attr_hidden_bias(1988),
attr_hidden_gates(1989),
attr_hidden_size(1990),
attr_high(1991),
attr_hist(1992),
attr_hop_length(1993),
attr_hx(1994),
attr_hx_(1995),
attr_hy_(1996),
attr_i1(1997),
attr_i2(1998),
attr_i3(1999),
attr_ignore_index(2000),
attr_imag(2001),
attr_impl_index(2002),
attr_implicit(2003),
attr_include_last_offset(2004),
attr_include_self(2005),
attr_incr_key(2006),
attr_incr_value(2007),
attr_increasing(2008),
attr_ind(2009),
attr_index(2010),
attr_indexing(2011),
attr_indices(2012),
attr_info(2013),
attr_initial(2014),
attr_input(2015),
attr_input1(2016),
attr_input2(2017),
attr_input3(2018),
attr_input_bias(2019),
attr_input_dtype(2020),
attr_input_g(2021),
attr_input_gates(2022),
attr_input_lengths(2023),
attr_input_scale(2024),
attr_input_size(2025),
attr_input_sizes(2026),
attr_inputs(2027),
attr_interpolation(2028),
attr_interpolation_mode(2029),
attr_inv_scale(2030),
attr_inverse(2031),
attr_invert(2032),
attr_invstd(2033),
attr_is_causal(2034),
attr_is_crow(2035),
attr_is_matrix(2036),
attr_is_result(2037),
attr_is_target(2038),
attr_k(2039),
attr_keepdim(2040),
attr_kernel_size(2041),
attr_key(2042),
attr_label_smoothing(2043),
attr_lambd(2044),
attr_largest(2045),
attr_last_dim_size(2046),
attr_layersOutputs(2047),
attr_layout(2048),
attr_left(2049),
attr_length(2050),
attr_lengths(2051),
attr_level(2052),
attr_like(2053),
attr_list(2054),
attr_log_alpha(2055),
attr_log_input(2056),
attr_log_probs(2057),
attr_log_target(2058),
attr_logabsdet(2059),
attr_logsumexp(2060),
attr_low(2061),
attr_lower(2062),
attr_lr(2063),
attr_ltm(2064),
attr_m(2065),
attr_mantissa(2066),
attr_margin(2067),
attr_mask(2068),
attr_mask_check(2069),
attr_mask_type(2070),
attr_mat(2071),
attr_mat1(2072),
attr_mat2(2073),
attr_matrices(2074),
attr_max(2075),
attr_max_exp_avg_sqs(2076),
attr_max_k(2077),
attr_max_norm(2078),
attr_max_q(2079),
attr_max_seqlen_q(2080),
attr_max_size(2081),
attr_max_val(2082),
attr_max_values(2083),
attr_maximize(2084),
attr_maximum_indices(2085),
attr_maxnorm(2086),
attr_mean(2087),
attr_mean_dy(2088),
attr_mean_dy_xmu(2089),
attr_median(2090),
attr_memory_format(2091),
attr_min(2092),
attr_min_indices(2093),
attr_min_val(2094),
attr_minlength(2095),
attr_mode(2096),
attr_momentum(2097),
attr_n(2098),
attr_n_bins(2099),
attr_n_fft(2100),
attr_names(2101),
attr_nan(2102),
attr_need_attn_weights(2103),
attr_need_weights(2104),
attr_neg_log_likelihood(2105),
attr_negative(2106),
attr_negative_slope(2107),
attr_neginf(2108),
attr_nested_size(2109),
attr_nested_strides(2110),
attr_new_data(2111),
attr_nnz(2112),
attr_noise(2113),
attr_non_blocking(2114),
attr_norm(2115),
attr_norm_bias_1(2116),
attr_norm_bias_2(2117),
attr_norm_first(2118),
attr_norm_type(2119),
attr_norm_weight_1(2120),
attr_norm_weight_2(2121),
attr_normalization(2122),
attr_normalized(2123),
attr_normalized_shape(2124),
attr_nt_example(2125),
attr_num_classes(2126),
attr_num_generated(2127),
attr_num_groups(2128),
attr_num_head(2129),
attr_num_heads(2130),
attr_num_layers(2131),
attr_num_samples(2132),
attr_num_weights(2133),
attr_numel(2134),
attr_observer_on(2135),
attr_offset(2136),
attr_offset2bag(2137),
attr_offsets(2138),
attr_onesided(2139),
attr_ord(2140),
attr_order(2141),
attr_other(2142),
attr_out(2143),
attr_out0(2144),
attr_out1(2145),
attr_out2(2146),
attr_out3(2147),
attr_out4(2148),
attr_out5(2149),
attr_out6(2150),
attr_out_dim(2151),
attr_out_int32(2152),
attr_outdim(2153),
attr_output(2154),
attr_output_mask(2155),
attr_output_padding(2156),
attr_output_scale(2157),
attr_output_size(2158),
attr_output_zero_point(2159),
attr_p(2160),
attr_packed(2161),
attr_packed_hh(2162),
attr_packed_ih(2163),
attr_packed_weight(2164),
attr_pad(2165),
attr_pad_mode(2166),
attr_padded(2167),
attr_padding(2168),
attr_padding_idx(2169),
attr_padding_mode(2170),
attr_padding_value(2171),
attr_params(2172),
attr_path(2173),
attr_pdist(2174),
attr_per_row_fake_quant(2175),
attr_per_sample_weights(2176),
attr_periodic(2177),
attr_philox_offset(2178),
attr_philox_seed(2179),
attr_pin_memory(2180),
attr_pivot(2181),
attr_pivots(2182),
attr_plain_idx(2183),
attr_plain_indices(2184),
attr_pos_weight(2185),
attr_posinf(2186),
attr_positive(2187),
attr_pow(2188),
attr_prepend(2189),
attr_primal(2190),
attr_prob(2191),
attr_proj_bias(2192),
attr_proj_size(2193),
attr_proj_weight(2194),
attr_q(2195),
attr_qkv(2196),
attr_qkv_bias(2197),
attr_qkv_weight(2198),
attr_qtensor(2199),
attr_quant_max(2200),
attr_quant_min(2201),
attr_quasi(2202),
attr_query(2203),
attr_r(2204),
attr_random_samples(2205),
attr_range(2206),
attr_rank(2207),
attr_ratio(2208),
attr_rcond(2209),
attr_real(2210),
attr_reduce(2211),
attr_reduce_range(2212),
attr_reduction(2213),
attr_repeats(2214),
attr_replacement(2215),
attr_requires_grad(2216),
attr_reserve(2217),
attr_reserveSpace(2218),
attr_reservedSpace(2219),
attr_residuals(2220),
attr_result(2221),
attr_retain_graph(2222),
attr_return_complex(2223),
attr_return_counts(2224),
attr_return_debug_mask(2225),
attr_return_inverse(2226),
attr_reverse(2227),
attr_right(2228),
attr_rounding_mode(2229),
attr_row(2230),
attr_row_indices(2231),
attr_rstd(2232),
attr_rtol(2233),
attr_running_max(2234),
attr_running_mean(2235),
attr_running_min(2236),
attr_running_var(2237),
attr_s(2238),
attr_save_invstd(2239),
attr_save_mean(2240),
attr_save_var(2241),
attr_save_var_transform(2242),
attr_saved_g(2243),
attr_saved_norms(2244),
attr_saved_v(2245),
attr_scalar(2246),
attr_scalar1(2247),
attr_scalar2(2248),
attr_scalars(2249),
attr_scale(2250),
attr_scale_backoff_factor(2251),
attr_scale_factors(2252),
attr_scale_grad_by_freq(2253),
attr_scale_growth_factor(2254),
attr_scale_hh(2255),
attr_scale_ih(2256),
attr_scales(2257),
attr_scales_d(2258),
attr_scales_h(2259),
attr_scales_w(2260),
attr_sections(2261),
attr_self(2262),
attr_self_is_result(2263),
attr_self_num_batch_dims(2264),
attr_self_or_result(2265),
attr_self_sizes(2266),
attr_sequences(2267),
attr_shape(2268),
attr_shared(2269),
attr_shifts(2270),
attr_side(2271),
attr_sigma(2272),
attr_sign(2273),
attr_singular_values(2274),
attr_size(2275),
attr_sizes(2276),
attr_sobolstate(2277),
attr_solution(2278),
attr_some(2279),
attr_sorted(2280),
attr_sorted_sequence(2281),
attr_sorter(2282),
attr_source(2283),
attr_spacing(2284),
attr_sparse(2285),
attr_sparse_dim(2286),
attr_sparse_grad(2287),
attr_split_size(2288),
attr_split_sizes(2289),
attr_src(2290),
attr_stable(2291),
attr_start(2292),
attr_start_dim(2293),
attr_state_steps(2294),
attr_std(2295),
attr_step(2296),
attr_steps(2297),
attr_storage_offset(2298),
attr_stride(2299),
attr_sumdim(2300),
attr_swap(2301),
attr_symmetric_quant(2302),
attr_t(2303),
attr_tangent(2304),
attr_target(2305),
attr_target_lengths(2306),
attr_targets(2307),
attr_tau(2308),
attr_tensor(2309),
attr_tensor1(2310),
attr_tensor2(2311),
attr_tensor_indices_or_sections(2312),
attr_tensors(2313),
attr_tensors1(2314),
attr_test_element(2315),
attr_test_elements(2316),
attr_the_template(2317),
attr_theta(2318),
attr_threshold(2319),
attr_to(2320),
attr_tol(2321),
attr_total(2322),
attr_total_length(2323),
attr_total_weight(2324),
attr_train(2325),
attr_training(2326),
attr_transpose(2327),
attr_transposed(2328),
attr_type1(2329),
attr_type2(2330),
attr_unbiased(2331),
attr_unitriangular(2332),
attr_unpack_data(2333),
attr_unpack_pivots(2334),
attr_unroll_dim(2335),
attr_unsafe(2336),
attr_upper(2337),
attr_upscale_factor(2338),
attr_use_gelu(2339),
attr_use_input_stats(2340),
attr_v(2341),
attr_value(2342),
attr_values(2343),
attr_var(2344),
attr_vec(2345),
attr_vec1(2346),
attr_vec2(2347),
attr_w_hh(2348),
attr_w_ih(2349),
attr_weight(2350),
attr_weight0(2351),
attr_weight1(2352),
attr_weight2(2353),
attr_weight3(2354),
attr_weight4(2355),
attr_weight_arr(2356),
attr_weight_buf(2357),
attr_weight_decay(2358),
attr_weight_g(2359),
attr_weight_scale(2360),
attr_weight_stride0(2361),
attr_weight_zero_point(2362),
attr_weights(2363),
attr_win_length(2364),
attr_window(2365),
attr_window_length(2366),
attr_with_replacement(2367),
attr_workspace(2368),
attr_wrap(2369),
attr_x(2370),
attr_x1(2371),
attr_x2(2372),
attr_y(2373),
attr_z(2374),
attr_z_state(2375),
attr_zero_infinity(2376),
attr_zero_point(2377),
attr_zero_point_hh(2378),
attr_zero_point_ih(2379),
attr_zero_points(2380),
  attr_Subgraph(2381),
  attr_ReverseSubgraph(2382),
  attr_f_real_outputs(2383),
  attr_df_input_vjps(2384),
  attr_df_input_captured_inputs(2385),
  attr_df_input_captured_outputs(2386),
  attr_df_output_vjps(2387),
  attr_axes(2388),
  attr_symbolic_shape_inputs(2389),
  attr_allow_stack_outputs(2390),
  attr_striding_inputs_desc(2391),
  attr_striding_outputs_desc(2392),
  attr_broadcast(2393),
  attr_direction(2394),
  attr_ends(2395),
  attr_inplace(2396),
  attr_input_as_shape(2397),
  attr_is_zero(2398),
  attr_num_none(2399),
  attr_num_present(2400),
  attr_perm(2401),
  attr_starts(2402),
  attr_profiled_type(2403),
  attr_transA(2404),
  attr_transB(2405),
  attr_name(2406),
  attr_module(2407),
  attr_beg(2408),
  attr_idx(2409),
  attr_split(2410),
  attr_slot(2411),
  attr_kinds(2412),
  attr_types(2413),
  attr_scope(2414),
  attr_keepdims(2415),
  attr_cache_id(2416),
  attr_new_axis(2417),
  attr_warn_id(2418),
  attr_output_layouts(2419),
  attr_allowzero(2420),
  attr_seen_none(2421),
  attr_overload_name(2422),
    num_symbols(2423);

    public final int value;
    private _keys(int v) { this.value = v; }
    private _keys(_keys e) { this.value = e.value; }
    public _keys intern() { for (_keys e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

// #define DEFINE_SYMBOL(ns, s)
//   namespace ns { constexpr Symbol s(static_cast<unique_t>(_keys::ns##_##s)); }
@Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol prim(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol prims(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol nvprims(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol aten(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol cuda(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol onnx(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol attr(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol scope(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol user(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol _caffe2(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol dimname(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol namespaces(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Assign(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol BroadcastingChunk(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol BroadcastSizes(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ReductionSizes(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Constant(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ChunkSizes(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ConstantMKLDNNTensor(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol BroadcastMKLDNNTensors(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol MKLDNNGroup(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol MKLDNNHardSwish(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol MKLDNNHardSigmoid(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol MKLDNNHardTanh(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol MKLDNNClamp(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol StaticRuntimeCopyOuts(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Drop(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Eval(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Expand();  /* onnx */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol FusionGroup(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol CudaFusionGroup(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol CudaFusionGuard(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol oneDNNFusionGroup(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol oneDNNFusionGuard(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol FunctionalGraph(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol add_optional(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol view_copy(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol permute_copy(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol reshape_copy(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol squeeze_copy(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol t_copy(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol transpose_copy(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol unsqueeze_copy(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol flatten_copy(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol expand_copy(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol expand_as_copy(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol DifferentiableGraph(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TensorExprGroup(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TensorExprDynamicGroup(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol StaticSubgraph(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol If(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Jump();  /* debug */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol JumpNZ();  /* debug */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol JumpZ();  /* debug */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Load(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Loop(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Param(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol PackPadded();  /* onnx */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol PadPacked();  /* onnx */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Placeholder();  /* debug */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Print(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol EmptyListLiteral(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol LegacyTypedConstructor(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol PythonOp(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol IgnoredPythonOp(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Reverse(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Return(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ReturnStmt(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol BreakStmt(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ContinueStmt(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ComprehensionScope(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Store(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol AutogradZero(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol AutogradAnyNonZero(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol AutogradAllNonZero(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol AutogradAllZero(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Starred(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TupleConstruct(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TupleUnpack(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TupleIndex(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TupleSlice(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ListConstruct(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ListUnpack(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol DictConstruct(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ModuleContainerIndex(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol EnumName(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol EnumValue(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol StringIndex(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol NumToTensor(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Uninitialized(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol VarConcat(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol VarStack(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol With(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Enter(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Exit(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol IfThenElse(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol Bool(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol Int(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol FloatImplicit(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ComplexImplicit(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol IntImplicit(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ScalarImplicit(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol Float(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol Complex(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol str(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol Delete(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol device(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol dtype(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol layout(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol id(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol requires_grad(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol MakeTestTensor();  /* test */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol AutogradAdd(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol GradOf(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol grad(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol backward(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Guard(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol BailOut(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TypeCheck(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol RequiresGradCheck(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol FallbackGraph(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol FusedConcat(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ConstantChunk(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol MMTreeReduce(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol MMBatchSide(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol list(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol dict(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol min(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol max(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol abs(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol divmod(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol zip(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol enumerate(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol range(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol rangelist(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol isinstance(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol tolist(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol unchecked_cast(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _grad_sum_to_size(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _size_if_not_equal(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _ncf_unsqueeze(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol warn(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sorted(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol floordiv(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __range_length(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __derive_index(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __round_to_zero_floordiv(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_scripting(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _unwrap_optional(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol fork(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol awaitable(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol forkClosure(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol awaitableClosure(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol awaitable_nowait(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol awaitable_wait(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol RaiseException(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Closure(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol CreateObject(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol SetAttr(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol GetAttr(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol HasAttr(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol profile(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol profile_ivalue(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol AddStatValue(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TimePoint(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol CallFunction(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol CallMethod(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol LoopContinuation(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol annotate(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TracedModuleForward(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TracedFork(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TracedAttr(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol rpc_async(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol rpc_sync(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol rpc_remote(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol is_cuda(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol append(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol as_tensor(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_avg_pool2d_backward(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol dim(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol format(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol percentFormat(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __not__(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __is__(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __isnot__(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _ger(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __getitem__(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _set_item(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol manual_seed();  
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hash(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol len();   
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef @Name("wait") Symbol _wait(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol save(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol keys(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ord(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol chr(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hex(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol oct(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol clear(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol setdefault(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bin(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pop(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol insert(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tensor(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol unchecked_unwrap_optional(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __contains__(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol BailoutTemplate();  
  @Namespace("c10::cuda") @MemberGetter public static native @Const @ByRef Symbol _set_device(); 
  @Namespace("c10::cuda") @MemberGetter public static native @Const @ByRef Symbol set_stream(); 
  @Namespace("c10::cuda") @MemberGetter public static native @Const @ByRef Symbol _current_device(); 
  @Namespace("c10::cuda") @MemberGetter public static native @Const @ByRef Symbol synchronize(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol has_torch_function(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_autocast_enabled(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_autocast_cpu_enabled(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __and__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __iand__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __ilshift__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __ior__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __irshift__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __ixor__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __lshift__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __or__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __rshift__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __xor__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _adaptive_avg_pool2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _adaptive_avg_pool2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _adaptive_avg_pool3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _adaptive_avg_pool3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _add_batch_dim(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _add_relu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _add_relu_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _addmm_activation(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _aminmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _amp_foreach_non_finite_check_and_unscale(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _amp_foreach_non_finite_check_and_unscale_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _amp_update_scale(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _amp_update_scale_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _assert_async(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _assert_tensor_metadata(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _autocast_to_full_precision(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _autocast_to_reduced_precision(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _batch_norm_impl_index(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _batch_norm_impl_index_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Byte(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Char(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Double(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Float(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Half(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Int(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Long(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Short(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cdist_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cdist_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cholesky_solve_helper(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _choose_qparams_per_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _chunk_grad_outputs_efficient_attention(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _coalesce(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _coalesced(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _coalesced_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _compute_linear_combination(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _conj(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _conj_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _conj_physical(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _conv_depthwise2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _convert_indices_from_coo_to_csr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _convert_indices_from_csr_to_coo(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _convolution(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _convolution_double_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _convolution_mode(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _copy_from(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _copy_from_and_resize(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _ctc_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _ctc_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cudnn_ctc_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cudnn_init_dropout_state(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cudnn_rnn(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cudnn_rnn_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cudnn_rnn_flatten_weight(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cufft_clear_plan_cache(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cufft_get_plan_cache_max_size(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cufft_get_plan_cache_size(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cufft_set_plan_cache_max_size(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cummax_helper(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cummin_helper(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _debug_has_internal_overlap(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _dimI(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _dimV(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _dim_arange(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _dirichlet_grad(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _efficient_attention_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _efficient_attention_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _efficientzerotensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _embedding_bag(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _embedding_bag_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _embedding_bag_dense_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _embedding_bag_forward_only(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _embedding_bag_per_sample_weights_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _embedding_bag_sparse_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _empty_affine_quantized(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _empty_per_channel_affine_quantized(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _euclidean_dist(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fake_quantize_learnable_per_channel_affine(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fake_quantize_learnable_per_channel_affine_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fake_quantize_learnable_per_tensor_affine(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fake_quantize_learnable_per_tensor_affine_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fake_quantize_per_tensor_affine_cachemask_tensor_qparams(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fft_c2c(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fft_c2r(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fft_r2c(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _flash_attention_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _flash_attention_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foobar(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_abs(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_abs_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_acos(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_acos_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_add(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_add_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_addcdiv(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_addcdiv_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_addcmul(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_addcmul_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_asin(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_asin_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_atan(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_atan_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_ceil(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_ceil_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_clamp_max(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_clamp_max_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_clamp_min(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_clamp_min_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_cos(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_cos_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_cosh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_cosh_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_div(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_div_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_erf(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_erf_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_erfc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_erfc_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_exp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_exp_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_expm1(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_expm1_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_floor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_floor_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_frac(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_frac_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_lerp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_lerp_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_lgamma(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_lgamma_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_log(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_log10(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_log10_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_log1p(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_log1p_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_log2(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_log2_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_log_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_maximum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_maximum_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_minimum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_minimum_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_mul(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_mul_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_neg(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_neg_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_reciprocal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_reciprocal_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_round(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_round_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_sigmoid(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_sigmoid_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_sin(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_sin_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_sinh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_sinh_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_sqrt(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_sqrt_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_sub(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_sub_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_tan(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_tan_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_tanh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_tanh_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_trunc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_trunc_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_zero(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _foreach_zero_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fused_adam(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fused_adam_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fused_adamw(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fused_adamw_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fused_dropout(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fused_moving_avg_obs_fq_helper(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fused_moving_avg_obs_fq_helper_functional(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fused_sdp_choice(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fw_primal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fw_primal_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _gather_sparse_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _grid_sampler_2d_cpu_fallback(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _grid_sampler_2d_cpu_fallback_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _has_compatible_shallow_copy_type(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _has_same_storage_numel(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _histogramdd_bin_edges(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _histogramdd_from_bin_cts(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _histogramdd_from_bin_tensors(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _index_put_impl(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _index_put_impl_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _indices(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _indices_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _is_all_true(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _is_any_true(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _is_zerotensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _linalg_check_errors(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _linalg_det(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _linalg_eigh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _linalg_slogdet(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _linalg_solve_ex(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _linalg_svd(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _local_scalar_dense(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _log_softmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _log_softmax_backward_data(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _logcumsumexp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _lstm_mps(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _lu_with_info(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _make_dual(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _make_dual_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _make_per_channel_quantized_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _make_per_tensor_quantized_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _masked_scale(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _masked_softmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _masked_softmax_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _mkldnn_reshape(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _mkldnn_transpose(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _mkldnn_transpose_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _mps_convolution(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _mps_convolution_transpose(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _native_batch_norm_legit(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _native_batch_norm_legit_functional(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _native_decoder_only_multi_head_attention(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _native_multi_head_attention(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _neg_view(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _neg_view_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _nested_from_padded(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _nested_from_padded_and_nested_example(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _nested_select_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _nested_sum_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _nested_tensor_from_mask(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _nested_tensor_from_mask_left_aligned(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _nested_tensor_from_tensor_list(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _nested_tensor_offsets(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _nested_tensor_size(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _nested_tensor_softmax_with_shape(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _nested_tensor_strides(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _nested_view_from_buffer(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _nested_view_from_buffer_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _new_zeros_with_same_feature_meta(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _nnpack_available(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _nnpack_spatial_convolution(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _nnz(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _pack_padded_sequence(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _pack_padded_sequence_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _pad_circular(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _pad_enum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _pad_packed_sequence(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _pdist_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _pdist_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _pin_memory(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _prelu_kernel(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _prelu_kernel_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _remove_batch_dim(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _reshape_alias(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _reshape_alias_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _reshape_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _reshape_from_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _resize_output(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _resize_output_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _rowwise_prune(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sample_dirichlet(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _saturate_weight_to_fp16(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _scaled_dot_product_attention(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _scaled_dot_product_attention_math(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _scaled_dot_product_efficient_attention(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _scaled_dot_product_efficient_attention_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _scaled_dot_product_flash_attention(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _scaled_dot_product_flash_attention_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _segment_reduce_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _shape_as_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _slow_conv2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _slow_conv2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sobol_engine_draw(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sobol_engine_ff(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sobol_engine_ff_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sobol_engine_initialize_state(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sobol_engine_initialize_state_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sobol_engine_scramble(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sobol_engine_scramble_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _softmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _softmax_backward_data(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_addmm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_broadcast_to(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_broadcast_to_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_bsc_tensor_unsafe(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_bsr_tensor_unsafe(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_compressed_tensor_unsafe(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_coo_tensor_unsafe(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_coo_tensor_with_dims(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_coo_tensor_with_dims_and_tensors(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_csc_tensor_unsafe(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_csr_prod(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_csr_sum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_csr_tensor_unsafe(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_log_softmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_log_softmax_backward_data(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_mm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_mm_reduce_impl(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_mm_reduce_impl_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_softmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_softmax_backward_data(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_sparse_matmul(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_sum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_sum_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _spdiags(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _stack(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _standard_gamma(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _standard_gamma_grad(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _test_ambiguous_defaults(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _test_autograd_multiple_dispatch(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _test_autograd_multiple_dispatch_view(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _test_autograd_multiple_dispatch_view_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _test_check_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _test_optional_filled_intlist(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _test_optional_floatlist(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _test_optional_intlist(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _test_serialization_subcmul(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _test_string_default(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _test_warn_in_autograd(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _thnn_differentiable_gru_cell_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _thnn_differentiable_lstm_cell_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _thnn_fused_gru_cell(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _thnn_fused_gru_cell_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _thnn_fused_lstm_cell(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _thnn_fused_lstm_cell_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _thnn_fused_lstm_cell_backward_impl(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _to_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _to_cpu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _to_dense(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _transform_bias_rescale_qkv(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _transformer_decoder_only_layer_fwd(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _transformer_encoder_layer_fwd(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _trilinear(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _triton_multi_head_attention(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _triton_scaled_dot_attention(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _unique(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _unique2(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _unpack_dual(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _unsafe_view(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _upsample_bicubic2d_aa(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _upsample_bicubic2d_aa_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _upsample_bilinear2d_aa(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _upsample_bilinear2d_aa_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _upsample_nearest_exact1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _upsample_nearest_exact1d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _upsample_nearest_exact2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _upsample_nearest_exact2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _upsample_nearest_exact3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _upsample_nearest_exact3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _use_cudnn_ctc_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _use_cudnn_rnn_flatten_weight(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _validate_compressed_sparse_indices(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _validate_sparse_bsc_tensor_args(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _validate_sparse_bsr_tensor_args(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _validate_sparse_compressed_tensor_args(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _validate_sparse_coo_tensor_args(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _validate_sparse_csc_tensor_args(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _validate_sparse_csr_tensor_args(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _values(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _values_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _version(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _weight_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _weight_norm_differentiable_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _weight_norm_interface(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _weight_norm_interface_backward();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol abs_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol absolute(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol absolute_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol acos(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol acos_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol acosh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol acosh_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_avg_pool1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_avg_pool2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_avg_pool3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_avg_pool3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_max_pool1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_max_pool2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_max_pool2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_max_pool3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_max_pool3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol add(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol add_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addbmm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addbmm_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addcdiv(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addcdiv_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addcmul(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addcmul_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addmm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addmm_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addmv(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addmv_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addr_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adjoint(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol affine_grid_generator(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol affine_grid_generator_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol alias(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol alias_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol align_as(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol align_tensors(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol align_to(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol all(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol allclose(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol alpha_dropout(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol alpha_dropout_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol amax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol amin(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol aminmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol angle(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol any(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arange(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arccos(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arccos_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arccosh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arccosh_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arcsin(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arcsin_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arcsinh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arcsinh_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arctan(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arctan2(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arctan2_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arctan_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arctanh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arctanh_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol argmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol argmin(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol argsort(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol argwhere(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol as_strided(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol as_strided_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol as_strided_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol as_strided_scatter(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol asin(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol asin_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol asinh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol asinh_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atan(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atan2(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atan2_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atan_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atanh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atanh_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atleast_1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atleast_2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atleast_3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol avg_pool1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol avg_pool2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol avg_pool2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol avg_pool3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol avg_pool3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol baddbmm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol baddbmm_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bartlett_window(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol batch_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol batch_norm_backward_elemt(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol batch_norm_backward_reduce(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol batch_norm_elemt(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol batch_norm_gather_stats(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol batch_norm_gather_stats_with_counts(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol batch_norm_stats(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol batch_norm_update_stats(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bernoulli(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bernoulli_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bilinear(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol binary_cross_entropy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol binary_cross_entropy_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol binary_cross_entropy_with_logits(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bincount(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol binomial(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bitwise_and(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bitwise_and_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bitwise_left_shift(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bitwise_left_shift_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bitwise_not(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bitwise_not_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bitwise_or(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bitwise_or_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bitwise_right_shift(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bitwise_right_shift_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bitwise_xor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bitwise_xor_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol blackman_window(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol block_diag(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bmm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol broadcast_tensors(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol broadcast_to(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bucketize(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol can_cast(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cartesian_prod(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cat(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cauchy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cauchy_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ccol_indices(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ccol_indices_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cdist(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ceil(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ceil_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol celu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol celu_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol chain_matmul(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol chalf(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol channel_shuffle(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cholesky(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cholesky_inverse(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cholesky_solve(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol choose_qparams_optimized(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol chunk(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol clamp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol clamp_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol clamp_max(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol clamp_max_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol clamp_min(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol clamp_min_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol clip(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol clip_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef @Name("clone") Symbol _clone(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol coalesce(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol col2im(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol col_indices(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol col_indices_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol column_stack(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol combinations(); 
 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol concat(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol concatenate(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conj(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conj_physical(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conj_physical_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol constant_pad_nd(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol contiguous(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conv1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conv2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conv3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conv_depthwise3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conv_tbc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conv_tbc_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conv_transpose1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conv_transpose2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conv_transpose3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol convolution(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol convolution_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol convolution_backward_overrideable(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol convolution_overrideable(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol copy_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol copy_sparse_to_sparse(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol copy_sparse_to_sparse_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol copysign(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol copysign_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol corrcoef(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cos(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cos_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cosh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cosh_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cosine_embedding_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cosine_similarity(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol count_nonzero(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cov(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cross(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cross_entropy_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol crow_indices(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol crow_indices_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ctc_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_affine_grid_generator(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_affine_grid_generator_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_batch_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_batch_norm_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution_add_relu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution_relu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution_transpose(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_grid_sampler(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_grid_sampler_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_is_acceptable(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cummax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cummaxmin_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cummin(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cumprod(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cumprod_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cumprod_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cumsum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cumsum_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cumulative_trapezoid(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol data(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol deg2rad(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol deg2rad_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol dense_dim(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol dequantize(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol det(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol detach(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol detach_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol detach_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol diag(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol diag_embed(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol diagflat(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol diagonal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol diagonal_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol diagonal_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol diagonal_scatter(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol diff(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol digamma(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol digamma_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol dist(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol div(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol div_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol divide(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol divide_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol dot(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol dropout(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol dropout_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol dsplit(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol dstack(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol einsum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol elu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol elu_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol elu_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol embedding(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol embedding_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol embedding_bag(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol embedding_dense_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol embedding_renorm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol embedding_renorm_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol embedding_sparse_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol empty(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol empty_like(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol empty_quantized(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol empty_strided(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol eq(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol eq_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol equal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol erf(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol erf_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol erfc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol erfc_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol erfinv(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol erfinv_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol exp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol exp2(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol exp2_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol exp_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol expand(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol expand_as();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol expm1(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol expm1_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol exponential(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol exponential_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol eye(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fake_quantize_per_channel_affine(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fake_quantize_per_channel_affine_cachemask(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fake_quantize_per_channel_affine_cachemask_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fake_quantize_per_tensor_affine(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fake_quantize_per_tensor_affine_cachemask(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fake_quantize_per_tensor_affine_cachemask_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fbgemm_linear_fp16_weight(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fbgemm_linear_fp16_weight_fp32_activation(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fbgemm_linear_int8_weight(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fbgemm_linear_int8_weight_fp32_activation(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fbgemm_linear_quantize_weight(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fbgemm_pack_gemm_matrix_fp16(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fbgemm_pack_quantized_matrix(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol feature_alpha_dropout(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol feature_alpha_dropout_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol feature_dropout(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol feature_dropout_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft_fft(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft_fft2(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft_fftfreq(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft_fftn(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft_fftshift(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft_hfft(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft_hfft2(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft_hfftn(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft_ifft(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft_ifft2(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft_ifftn(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft_ifftshift(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft_ihfft(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft_ihfft2(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft_ihfftn(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft_irfft(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft_irfft2(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft_irfftn(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft_rfft(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft_rfft2(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft_rfftfreq(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft_rfftn(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef @Name("fill") Symbol _fill(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fill_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fill_diagonal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fill_diagonal_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fix(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fix_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol flatten(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol flatten_dense_tensors(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol flip(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fliplr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol flipud(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol float_power(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol float_power_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol floor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol floor_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol floor_divide(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol floor_divide_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fmin(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fmod(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fmod_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol frac(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol frac_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fractional_max_pool2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fractional_max_pool2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fractional_max_pool3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fractional_max_pool3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol frexp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol frobenius_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol from_file(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol full(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol full_like(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fused_moving_avg_obs_fake_quant(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gather(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gather_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gcd(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gcd_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ge(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ge_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gelu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gelu_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gelu_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol geometric(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol geometric_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol geqrf(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ger(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol glu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol glu_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol glu_backward_jvp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol glu_jvp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gradient(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol greater(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol greater_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol greater_equal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol greater_equal_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol grid_sampler(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol grid_sampler_2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol grid_sampler_2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol grid_sampler_3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol grid_sampler_3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol group_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gru(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gru_cell(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gt(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gt_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hamming_window(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hann_window(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardshrink(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardshrink_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardsigmoid(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardsigmoid_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardsigmoid_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardswish(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardswish_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardswish_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardtanh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardtanh_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardtanh_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol heaviside(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol heaviside_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hinge_embedding_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol histc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol histogram(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol histogramdd(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hsplit(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hspmm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hstack(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol huber_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol huber_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hypot(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hypot_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol i0(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol i0_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol igamma(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol igamma_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol igammac(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol igammac_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol im2col(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol imag(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_add(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_add_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_copy_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_fill(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_fill_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_put(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_put_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_reduce(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_reduce_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_select(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_select_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol indices(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol indices_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol infinitely_differentiable_gelu_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol inner(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol instance_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol int_repr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol inverse(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_coalesced(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_complex(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_conj(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_distributed(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_floating_point(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_inference(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_leaf(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_neg(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_nonzero(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_pinned(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_same_size(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_set_to(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_signed(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_vulkan_available(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol isclose(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol isfinite(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol isin(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol isinf(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol isnan(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol isneginf(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol isposinf(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol isreal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol istft(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol item(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol kaiser_window(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol kl_div(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol kron(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol kthvalue(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol l1_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol layer_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lcm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lcm_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ldexp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ldexp_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol le(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol le_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol leaky_relu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol leaky_relu_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol leaky_relu_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lerp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lerp_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol less(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol less_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol less_equal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol less_equal_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lgamma(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lgamma_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lift(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lift_fresh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lift_fresh_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_cholesky(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_cholesky_ex(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_cond(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_cross(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_det(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_diagonal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_eig(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_eigh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_eigvals(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_eigvalsh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_householder_product(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_inv(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_inv_ex(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_ldl_factor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_ldl_factor_ex(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_ldl_solve(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_lstsq(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_lu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_lu_factor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_lu_factor_ex(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_lu_solve(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_matmul(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_matrix_exp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_matrix_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_matrix_power(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_matrix_rank(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_multi_dot(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_pinv(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_qr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_slogdet(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_solve(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_solve_ex(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_solve_triangular(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_svd(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_svdvals(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_tensorinv(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_tensorsolve(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_vander(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_vecdot(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_vector_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linear(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linear_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linspace(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log10(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log10_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log1p(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log1p_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log2(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log2_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log_normal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log_normal_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log_sigmoid(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log_sigmoid_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log_sigmoid_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log_softmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logaddexp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logaddexp2(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logcumsumexp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logdet(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logical_and(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logical_and_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logical_not(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logical_not_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logical_or(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logical_or_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logical_xor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logical_xor_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logit(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logit_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logit_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logspace(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logsumexp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lshift(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lstm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lstm_cell(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lstm_mps_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lt(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lt_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lu_solve(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lu_unpack(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mH(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mT(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol margin_ranking_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol masked_fill(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol masked_fill_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol masked_scatter(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol masked_scatter_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol masked_select(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol masked_select_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol matmul(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol matmul_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol matrix_H(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol matrix_exp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol matrix_exp_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol matrix_power();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool1d_with_indices(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool2d_with_indices(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool2d_with_indices_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool3d_with_indices(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool3d_with_indices_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_unpool2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_unpool3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol maximum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mean(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol median(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol meshgrid();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol minimum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_batch_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_batch_norm_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_convolution(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_convolution_add_relu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_convolution_relu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_convolution_transpose(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_depthwise_convolution(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_rnn(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_rnn_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mish(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mish_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mish_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mkldnn_adaptive_avg_pool2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mkldnn_adaptive_avg_pool2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mkldnn_convolution(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mkldnn_linear(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mkldnn_linear_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mkldnn_linear_backward_input(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mkldnn_linear_backward_weights(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mkldnn_max_pool2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mkldnn_max_pool2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mkldnn_max_pool3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mkldnn_max_pool3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mkldnn_reorder_conv2d_weight(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mkldnn_reorder_conv3d_weight(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mkldnn_rnn_layer(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mkldnn_rnn_layer_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mode(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol moveaxis(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol movedim(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mps_convolution_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mps_convolution_transpose_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mse_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mse_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol msort(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mul(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mul_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multi_margin_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multi_margin_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multilabel_margin_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multilabel_margin_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multilabel_margin_loss_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multinomial(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multiply(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multiply_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mv(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mvlgamma(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mvlgamma_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nan_to_num(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nan_to_num_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nanmean(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nanmedian(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nanquantile(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nansum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol narrow(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol narrow_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_batch_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_batch_norm_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_channel_shuffle(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_dropout(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_dropout_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_group_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_group_norm_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_layer_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_layer_norm_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ne(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ne_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol neg(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol neg_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol negative(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol negative_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nested_to_padded_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol new_empty(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol new_empty_strided(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol new_full(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol new_ones(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol new_zeros(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nextafter(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nextafter_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nll_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nll_loss2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nll_loss2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nll_loss2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nll_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nll_loss_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nll_loss_nd(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nonzero(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nonzero_numpy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol norm_except_dim(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol normal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol normal_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol normal_functional(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol not_equal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol not_equal_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nuclear_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol numpy_T(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol one_hot(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ones(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ones_like(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol orgqr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ormqr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol outer(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol output_nr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pad(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pad_sequence(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pairwise_distance(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pdist(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol permute();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pin_memory(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pinverse(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pixel_shuffle(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pixel_unshuffle(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol poisson(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol poisson_nll_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol polar(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol polygamma(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol polygamma_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol positive(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pow(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pow_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol prelu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol prod(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol promote_types(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol put(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol put_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol q_per_channel_axis(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol q_per_channel_scales(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol q_per_channel_zero_points(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol q_scale(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol q_zero_point(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol qr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol qscheme(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol quantile(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol quantize_per_channel(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol quantize_per_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol quantize_per_tensor_dynamic(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol quantized_batch_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol quantized_gru_cell(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol quantized_lstm_cell(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol quantized_max_pool1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol quantized_max_pool2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol quantized_rnn_relu_cell(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol quantized_rnn_tanh_cell(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rad2deg(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rad2deg_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rand(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rand_like(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol randint(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol randint_like(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol randn(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol randn_like(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol random(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol random_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol randperm();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ravel(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol real(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reciprocal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reciprocal_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol record_stream(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol refine_names(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reflection_pad1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reflection_pad1d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reflection_pad2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reflection_pad2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reflection_pad3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reflection_pad3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol relu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol relu6(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol relu6_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol relu_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol remainder(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol remainder_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rename(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rename_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol renorm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol renorm_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol repeat(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol repeat_interleave(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad1d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad3d_backward();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol requires_grad_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reshape(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reshape_as(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol resize(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol resize_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol resize_as(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol resize_as_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol resize_as_sparse(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol resize_as_sparse_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol resolve_conj(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol resolve_neg(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol result_type(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol retain_grad(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol retains_grad(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rnn_relu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rnn_relu_cell(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rnn_tanh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rnn_tanh_cell(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol roll(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rot90(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol round(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol round_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol row_indices(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol row_indices_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol row_stack(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rrelu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rrelu_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rrelu_with_noise(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rrelu_with_noise_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rrelu_with_noise_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rshift(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rsqrt(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rsqrt_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rsub(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol scalar_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol scaled_dot_product_attention(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol scatter(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol scatter_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol scatter_add(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol scatter_add_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol scatter_reduce(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol scatter_reduce_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol searchsorted(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol segment_reduce(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol select(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol select_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol select_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol select_scatter(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol selu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol selu_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol set(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol set_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol set_data(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sgn(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sgn_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sigmoid(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sigmoid_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sigmoid_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sign(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sign_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol signbit(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol silu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol silu_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol silu_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sin(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sin_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sinc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sinc_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sinh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sinh_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol size(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slice(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slice_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slice_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slice_scatter(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slogdet(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv3d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv_dilated2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv_dilated3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv_transpose2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv_transpose3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol smm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol smooth_l1_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol smooth_l1_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol soft_margin_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol soft_margin_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol softmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol softplus(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol softplus_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol softshrink(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol softshrink_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sort(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sparse_bsc_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sparse_bsr_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sparse_compressed_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sparse_coo_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sparse_csc_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sparse_csr_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sparse_dim(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sparse_mask(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sparse_resize(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sparse_resize_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sparse_resize_and_clear(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sparse_resize_and_clear_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sparse_sampled_addmm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_airy_ai(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_bessel_j0(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_bessel_j1(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_bessel_y0(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_bessel_y1(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_chebyshev_polynomial_t(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_chebyshev_polynomial_u(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_chebyshev_polynomial_v(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_chebyshev_polynomial_w(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_digamma(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_entr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_erf(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_erfc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_erfcx(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_erfinv(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_exp2(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_expit(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_expm1(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_gammainc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_gammaincc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_gammaln(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_hermite_polynomial_h(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_hermite_polynomial_he(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_i0(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_i0e(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_i1(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_i1e(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_laguerre_polynomial_l(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_legendre_polynomial_p(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_log1p(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_log_ndtr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_log_softmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_logit(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_logsumexp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_modified_bessel_i0(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_modified_bessel_i1(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_modified_bessel_k0(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_modified_bessel_k1(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_multigammaln(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_ndtr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_ndtri(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_polygamma(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_psi(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_round(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_scaled_modified_bessel_k0(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_scaled_modified_bessel_k1(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_shifted_chebyshev_polynomial_t(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_shifted_chebyshev_polynomial_u(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_shifted_chebyshev_polynomial_v(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_shifted_chebyshev_polynomial_w(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_sinc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_softmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_spherical_bessel_j0(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_xlog1py(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_xlogy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol special_zeta(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol split(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol split_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol split_with_sizes(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol split_with_sizes_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sqrt(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sqrt_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol square(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol square_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol squeeze(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol squeeze_();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sspaddmm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol stack(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol std(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol std_mean(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol stft(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol stride(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sub(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sub_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol subtract(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol subtract_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sum_to_size(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol svd(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol swapaxes(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol swapaxes_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol swapdims(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol swapdims_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol t(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol t_();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol take(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol take_along_dim(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tan(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tan_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tanh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tanh_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tanh_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tensor_split(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tensordot(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol thnn_conv2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol threshold(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol threshold_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol threshold_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tile(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol to(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol to_dense(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol to_dense_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol to_mkldnn(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol to_mkldnn_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol to_padded_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol to_sparse(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol to_sparse_bsc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol to_sparse_bsr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol to_sparse_csc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol to_sparse_csr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol topk(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol trace(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol trace_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol transpose(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol transpose_();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol trapezoid(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol trapz(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol triangular_solve(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tril(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tril_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tril_indices(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol triplet_margin_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol triu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol triu_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol triu_indices(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol true_divide(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol true_divide_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol trunc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol trunc_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol type_as(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unbind(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unbind_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unflatten(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unflatten_dense_tensors(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unfold(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unfold_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unfold_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol uniform(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol uniform_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unique_consecutive(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unique_dim(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unique_dim_consecutive(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unsafe_chunk(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unsafe_split(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unsafe_split_with_sizes(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unsqueeze(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unsqueeze_();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_bicubic2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_bicubic2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_bilinear2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_bilinear2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_linear1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_linear1d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest1d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_trilinear3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_trilinear3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol value_selecting_reduction_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol values(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol values_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol vander(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol var(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol var_mean(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol vdot(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol view(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol view_as(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol view_as_complex(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol view_as_complex_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol view_as_real(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol view_as_real_copy();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol vsplit(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol vstack(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol where(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol xlogy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol xlogy_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef @Name("zero") Symbol _zero(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol zero_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol zeros(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol zeros_like(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Add(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Concat();  
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol ConstantFill(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Div(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol GRU(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Gather(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Gemm(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol LSTM(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol MatMul(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Min(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Max(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Mul(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Pow(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol RNN(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Shape(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Size(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Slice(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Softmax(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Squeeze(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Sub(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Transpose(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Unsqueeze();   
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Reshape();  
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Equal(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Greater(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol GreaterOrEqual(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Less(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol LessOrEqual(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Not(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ATen(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Split(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol ConstantOfShape(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Cast(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Mod(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Sqrt(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol SplitToSequence(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol SequenceAt(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol SequenceConstruct(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol SequenceEmpty(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol SequenceInsert(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol SequenceErase(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol ConcatFromSequence(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Identity(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol SoftmaxCrossEntropyLoss(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol NegativeLogLikelihoodLoss(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol LogSoftmax(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol ReduceL1(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol ReduceL2(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Conv(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol BatchNormalization(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol ReduceMean(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol ReduceProd(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Relu(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Neg(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol NonZero(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Range(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Tile(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Where(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Optional(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol OptionalGetElement(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol OptionalHasElement(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol A(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol B(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol C(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol H(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol HxW(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol K(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol L(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol LD(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol LU(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol LU_data(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol LU_pivots(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol M(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol N(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol P(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol Q(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol R(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol S(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol U(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol UPLO(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol V(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol Vh(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol W(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol X(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol a();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol accumulate(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol addends();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol align_corners(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol allow_tf32(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol alpha(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol amsgrad(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol anchor();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol api_name();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol approximate(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol arg1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol arg2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol arg3(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol arg_out(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol assume_unique(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol atol(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol attn_mask(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol average_attn_weights(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol averaging_const(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol aweights(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol axis(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol axis0(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol axis1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol b(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol b_hh(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol b_ih(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol bag_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol base(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol batch1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol batch2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol batch_dim(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol batch_first(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol batch_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol batch_sizes(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol benchmark(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol beta(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol beta1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol beta2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol bias(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol bias_defined(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol bias_g(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol bias_sizes(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol bidirectional(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol bin_edges(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol bins(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol bit_width(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol blank(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol blocksize(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol boundaries(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol buffer(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol causal();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cdim();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ceil_mode(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cell_state_fwd(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol center(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ch_axis(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol check_errors(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol chunk_grad_outputs(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol chunks(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol coalesced(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol coefficients(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol col();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol col_offsets(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol col_offsets_hh(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol col_offsets_ih(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol compressed_idx(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol compressed_indices(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol compressed_indices_dtype(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol compute_log_sumexp(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol compute_mode(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol compute_uv(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol compute_v(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol condition();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol correction(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol count(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol count_include_pad(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol counts(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cpu_dtype(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cpu_enabled(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cpu_nested_shape_example(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol create_graph();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cu_seqlens_k(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cu_seqlens_q(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cuda_dtype(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cuda_enabled(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cudnn_enable(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cudnn_enabled(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cum_seq_k(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cum_seq_q(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cx(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cx_(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cx_tmp(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cy(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cy_(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol d();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol decimals(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol delta(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dense();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol density(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol descending(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol destination(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol deterministic();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol device_index(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dgrad_glu();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol diagonals(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dilation();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dim0(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dim1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dim2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dimension(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dims(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dims_other(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dims_self(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol divisor_override(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol downscale_factor(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol driver();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dropout_mask(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dropout_p(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dropout_seed(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dropout_state(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dst();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dual(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dummy(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dx(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol edge_order(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol eigenvalues(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol eigenvectors(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol eigvals(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol eigvecs(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol element(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol elements(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ellipsis_idx(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol embed_dim(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol end(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol end_dim(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol eps(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol epsilon(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol equal_nan(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol equation(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol exp_avg_sqs(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol exp_avgs(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol expand1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol expand2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol expand3(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol exponent(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol exponential_average_factor(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol fake_quant_enabled(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol fake_quant_on(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ffn_bias_1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ffn_bias_2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ffn_weight_1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ffn_weight_2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol filename(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol fill_value(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol flat(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol forward(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol found_inf(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol from();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol full_matrices(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol fuse_transform_0213(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol fweights(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol g(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol gO(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol generator(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ggI(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ggW(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ggb();   
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_bias(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_cy(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_factor(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_glu(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_hy(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_in(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_input(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_out(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_out_(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_output(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_scale(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_w(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_weight(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_x(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_y();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grads(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grid(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol group(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol groups(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol growth_interval(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol growth_tracker(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol half_to_float(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol has_bias(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol has_biases(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol hermitian(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol hidden_bias(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol hidden_gates(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol hidden_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol high(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol hist(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol hop_length(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol hx(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol hx_(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol hy_(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol i1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol i2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol i3(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ignore_index();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol impl_index(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol implicit(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol include_last_offset(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol include_self(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol incr_key(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol incr_value(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol increasing(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ind();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol indexing();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol info(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol initial(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input3(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input_bias(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input_dtype(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input_g(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input_gates(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input_lengths(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input_scale(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input_sizes(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol inputs(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol interpolation(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol interpolation_mode(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol inv_scale();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol invert(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol invstd(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol is_causal(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol is_crow(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol is_matrix(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol is_result(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol is_target(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol k(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol keepdim(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol kernel_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol key(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol label_smoothing(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol lambd(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol largest(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol last_dim_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol layersOutputs();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol left(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol length(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol lengths(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol level(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol like();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol log_alpha(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol log_input(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol log_probs(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol log_target(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol logabsdet();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol low(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol lower(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol lr(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ltm(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol m(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol mantissa(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol margin(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol mask(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol mask_check(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol mask_type(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol mat(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol mat1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol mat2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol matrices();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol max_exp_avg_sqs(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol max_k(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol max_norm(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol max_q(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol max_seqlen_q(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol max_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol max_val(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol max_values(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol maximize(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol maximum_indices(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol maxnorm();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol mean_dy(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol mean_dy_xmu();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol memory_format();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol min_indices(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol min_val(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol minlength();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol momentum(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol n(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol n_bins(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol n_fft(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol names(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol nan(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol need_attn_weights(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol need_weights(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol neg_log_likelihood();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol negative_slope(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol neginf(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol nested_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol nested_strides(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol new_data(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol nnz(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol noise(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol non_blocking();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol norm_bias_1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol norm_bias_2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol norm_first(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol norm_type(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol norm_weight_1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol norm_weight_2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol normalization(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol normalized(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol normalized_shape(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol nt_example(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol num_classes(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol num_generated(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol num_groups(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol num_head(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol num_heads(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol num_layers(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol num_samples(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol num_weights(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol numel(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol observer_on(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol offset(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol offset2bag(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol offsets(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol onesided();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol order(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol other(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol out(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol out0(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol out1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol out2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol out3(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol out4(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol out5(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol out6(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol out_dim(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol out_int32(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol outdim(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol output(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol output_mask(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol output_padding(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol output_scale(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol output_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol output_zero_point(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol p(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol packed(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol packed_hh(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol packed_ih(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol packed_weight();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol pad_mode(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol padded(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol padding(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol padding_idx(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol padding_mode(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol padding_value(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol params(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol path();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol per_row_fake_quant(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol per_sample_weights(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol periodic(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol philox_offset(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol philox_seed();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol pivot(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol pivots(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol plain_idx(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol plain_indices(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol pos_weight(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol posinf();   
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol prepend(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol primal(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol prob(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol proj_bias(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol proj_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol proj_weight(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol q(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol qkv(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol qkv_bias(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol qkv_weight(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol qtensor(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol quant_max(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol quant_min(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol quasi(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol query(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol r(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol random_samples();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol rank(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ratio(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol rcond();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol reduce(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol reduce_range(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol reduction(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol repeats(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol replacement();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol reserve(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol reserveSpace(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol reservedSpace(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol residuals(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol result(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol retain_graph(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol return_complex(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol return_counts(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol return_debug_mask(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol return_inverse(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol reverse(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol right(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol rounding_mode(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol row();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol rstd(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol rtol(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol running_max(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol running_mean(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol running_min(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol running_var(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol s(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol save_invstd(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol save_mean(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol save_var(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol save_var_transform(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol saved_g(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol saved_norms(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol saved_v(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol scalar(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol scalar1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol scalar2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol scalars(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol scale(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol scale_backoff_factor(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol scale_factors(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol scale_grad_by_freq(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol scale_growth_factor(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol scale_hh(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol scale_ih(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol scales(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol scales_d(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol scales_h(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol scales_w(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol sections(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol self(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol self_is_result(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol self_num_batch_dims(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol self_or_result(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol self_sizes(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol sequences(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol shape(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol shared(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol shifts(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol side(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol sigma();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol singular_values();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol sizes(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol sobolstate(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol solution(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol some();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol sorted_sequence(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol sorter(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol source(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol spacing(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol sparse();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol sparse_grad(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol split_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol split_sizes(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol src(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol stable(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol start(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol start_dim(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol state_steps();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol step(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol steps(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol storage_offset();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol sumdim(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol swap(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol symmetric_quant();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol tangent(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol target(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol target_lengths(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol targets(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol tau();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol tensor1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol tensor2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol tensor_indices_or_sections(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol tensors(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol tensors1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol test_element(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol test_elements(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol the_template(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol theta();   
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol tol(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol total(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol total_length(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol total_weight(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol train(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol training();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol transposed(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol type1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol type2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol unbiased(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol unitriangular(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol unpack_data(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol unpack_pivots(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol unroll_dim(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol unsafe(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol upper(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol upscale_factor(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol use_gelu(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol use_input_stats(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol v(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol value();   
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol vec(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol vec1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol vec2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol w_hh(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol w_ih(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weight(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weight0(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weight1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weight2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weight3(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weight4(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weight_arr(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weight_buf(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weight_decay(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weight_g(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weight_scale(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weight_stride0(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weight_zero_point(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weights(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol win_length(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol window(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol window_length(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol with_replacement(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol workspace(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol wrap(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol x(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol x1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol x2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol y(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol z(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol z_state(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol zero_infinity(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol zero_point(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol zero_point_hh(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol zero_point_ih(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol zero_points(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol Subgraph(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ReverseSubgraph(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol f_real_outputs(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol df_input_vjps(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol df_input_captured_inputs(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol df_input_captured_outputs(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol df_output_vjps(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol axes(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol symbolic_shape_inputs(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol allow_stack_outputs(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol striding_inputs_desc(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol striding_outputs_desc(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol broadcast(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol direction(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ends(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol inplace(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input_as_shape(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol is_zero(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol num_none(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol num_present(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol perm(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol starts(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol profiled_type(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol transA(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol transB(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol name(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol module(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol beg(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol idx();  
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol slot(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol kinds(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol types();  
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol keepdims(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cache_id(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol new_axis(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol warn_id(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol output_layouts(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol allowzero(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol seen_none(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol overload_name(); 
// #undef DEFINE_SYMBOL

 // namespace c10


// Parsed from ATen/core/grad_mode.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/core/GradMode.h>



// Parsed from ATen/core/ATenGeneral.h

// #pragma once

// #include <c10/macros/Macros.h>


// Parsed from ATen/core/Dimname.h

// #pragma once

// #include <ATen/core/symbol.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Optional.h>
// #include <ostream>

@Namespace("at") public enum NameType { BASIC((byte)(0)), WILDCARD((byte)(1));

    public final byte value;
    private NameType(byte v) { this.value = v; }
    private NameType(NameType e) { this.value = e.value; }
    public NameType intern() { for (NameType e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../Dimname.java



@Namespace("at") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Dimname dimname);

@Namespace("at") public static native @Cast("bool") @Name("operator ==") boolean equals(@Const @ByRef Dimname lhs, @Const @ByRef Dimname rhs);

@Namespace("at") public static native @Cast("bool") @Name("operator !=") boolean notEquals(@Const @ByRef Dimname lhs, @Const @ByRef Dimname rhs);

 // namespace at


// Parsed from ATen/core/DimVector.h

// #pragma once
// #include <c10/util/DimVector.h>

// Re-declaring 'DimVector' type and size inside 'at' namespace.
// This is done to avoid modifying every use into their 'c10'
// equivalent.

 // namespace at


// Parsed from ATen/core/Generator.h

// #pragma once

// #include <stdint.h>
// #include <mutex>
// #include <deque>
// #include <atomic>
// #include <typeinfo>
// #include <utility>
// #include <cstddef>

// #include <c10/util/Exception.h>
// #include <c10/util/C++17.h>
// #include <c10/util/intrusive_ptr.h>
// #include <c10/core/Device.h>
// #include <c10/core/DispatchKeySet.h>

// For the record I don't think this is a correct pimpl idiom.
// Including Impl header in interface header defeats the purpose
// because you can't change Impl private members without forcing
// everything that included the interface to rebuild.
// Impl should be forward-declared in the interface header instead.
// #include <c10/core/GeneratorImpl.h>
// Targeting ../Generator.java



/**
 * Utility function to static cast input Generator* to
 * the backend generator type (CPU/CUDAGeneratorImpl etc.)
 */

/**
 * Utility function used in tensor implementations, which
 * supplies the default generator to tensors, if an input generator
 * is not supplied. The input Generator* is also static casted to
 * the backend generator type (CPU/CUDAGeneratorImpl etc.)
 */

/**
 * Helper function for checking the validity of new random generator
 * state. Right now following conditions are checked:
 *
 * - The new state tensor must be a torch.ByteTensor
 * - Data of the new state tensor must be contiguous
 */
@Namespace("at::detail") public static native void check_rng_state(@Const @ByRef TensorImpl new_state);

 // namespace detail

 // namespace at


// Parsed from ATen/core/Dict.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/macros/Export.h>
// #include <c10/util/TypeTraits.h>
// #include <c10/util/TypeList.h>
// #include <c10/util/intrusive_ptr.h>
// #include <c10/util/order_preserving_flat_hash_map.h>
// #include <c10/util/Optional.h>
// #include <ATen/core/TensorBody.h>
// #include <ATen/core/jit_type_base.h>

// Targeting ../DictKeyHash.java


// Targeting ../DictKeyEqualTo.java




// Targeting ../GenericDictEntryRef.java


// Targeting ../GenericDictIterator.java



// Targeting ../StringGenericListDict.java


// Targeting ../GenericDict.java


// GenericDict is how IValue stores dicts. It is, however, not part of the
// public API. Kernels should use Dicts with concrete Key, Value types instead
// (maybe except for some internal prim ops).





// #include <ATen/core/Dict_inl.h>  // IWYU pragma: keep


// Parsed from ATen/core/List.h

// #pragma once

// #include <ATen/core/ivalue_to.h>
// #include <ATen/core/jit_type_base.h>
// #include <c10/macros/Macros.h>
// #include <c10/macros/Export.h>
// #include <c10/util/TypeTraits.h>
// #include <c10/util/TypeList.h>
// #include <c10/util/intrusive_ptr.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Optional.h>
// #include <vector>

// Targeting ../ListImpl.java







// Targeting ../ListElementConstReferenceTraits.java



// this wraps vector::iterator to make sure user code can't rely
// on it being the type of the underlying vector.
  

// Parsed from ATen/core/NamedTensor.h

// #pragma once

// #include <ATen/core/Dimname.h>
// #include <c10/core/TensorImpl.h>
// #include <c10/util/C++17.h>
// Targeting ../NamedTensorMeta.java


// Targeting ../NamesMode.java


// Targeting ../NoNamesGuard.java






// Sets the names of `tensor` to be `names`.
@Namespace("at") public static native @Const @ByRef TensorBase internal_set_names_inplace(@Const @ByRef TensorBase tensor, @ByVal DimnameListOptional names);
@Namespace("at") public static native @Const @ByRef TensorBase internal_set_names_inplace(@Const @ByRef TensorBase tensor, @StdMove DimnameVector names, @Cast("bool") boolean validate_names);

@Namespace("at") @MemberGetter public static native @Cast("const size_t") long kMaxNamedTensorDim();



// Some helper functions on TensorImpl. Useful for working with names in TH.
// XXX: Ideally these would exist as methods on TensorImpl
@Namespace("at::impl") public static native void internal_set_names_inplace(TensorImpl impl, @ByVal DimnameListOptional names, @Cast("bool") boolean validate_names);
@Namespace("at::impl") public static native void internal_set_names_inplace(TensorImpl impl, @StdMove DimnameVector names, @Cast("bool") boolean validate_names);



// Returns true if the tensor's names exist and are not all 'None'.
// Returns false if the tensor's names don't exist (were not allocated),
// or if all names are 'None'.
// We treat not-allocated-names the same as allocated names that are all 'None'.
@Namespace("at::impl") public static native @Cast("bool") boolean has_names(@Const TensorImpl impl);

// Returns the names of the tensor's dimensions.
// Unnamed tensors are treated as having 'None' in all dimension; this method
// would return a DimnameList of all 'None's for an unnamed tensor.
@Namespace("at::impl") public static native @ByVal DimnameArrayRef get_names(@Const TensorImpl impl);

// This is more of an implementation detail; one should use impl::get_names /
// Tensor::names() whenever possible because it provides a cleaner API.
// Returns the names of the tensor if they have been allocated; returns nullopt
// instead if the haven't been. The names of a tensor are not allocated if a
// tensor is constructed with names=None.
@Namespace("at::impl") public static native @ByVal DimnameListOptional get_opt_names(@Const TensorImpl impl);

 // namespace impl

 // namespace at


// Parsed from ATen/core/Reduction.h

// #pragma once

// NB: Keep this in sync with Reduction class in torch/nn/_reduction.py
// These constants control the reduction behavior of loss functions.
// Ideally, this would be a scoped enum, but jit doesn't support that
@Namespace("at::Reduction") public enum Reduction {
  None(0),             // Do not reduce
  Mean(1),             // (Possibly weighted) mean of losses
  Sum(2),              // Sum losses
  END(3);

    public final int value;
    private Reduction(int v) { this.value = v; }
    private Reduction(Reduction e) { this.value = e.value; }
    public Reduction intern() { for (Reduction e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
 // namespace Reduction
 // namespace at


// Parsed from ATen/core/Scalar.h

// #include <c10/core/Scalar.h>


// Parsed from ATen/core/TensorAccessor.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Exception.h>
// #include <c10/util/irange.h>
// #include <stdint.h>
// #include <cstddef>

// The PtrTraits argument to the TensorAccessor/GenericPackedTensorAccessor
// is used to enable the __restrict__ keyword/modifier for the data
// passed to cuda.

// #if defined(__CUDACC__) || defined(__HIPCC__)
// #endif

// TensorAccessorBase and TensorAccessor are used for both CPU and CUDA tensors.
// For CUDA tensors it is used in device code (only). This means that we restrict ourselves
// to functions and types available there (e.g. IntArrayRef isn't).

// The PtrTraits argument is only relevant to cuda to support `__restrict__` pointers.

// The `TensorAccessor` is typically instantiated for CPU `Tensor`s using
// `Tensor.accessor<T, N>()`.
// For CUDA `Tensor`s, `GenericPackedTensorAccessor` is used on the host and only
// indexing on the device uses `TensorAccessor`s.


// GenericPackedTensorAccessorBase and GenericPackedTensorAccessor are used on for CUDA `Tensor`s on the host
// and as
// In contrast to `TensorAccessor`s, they copy the strides and sizes on instantiation (on the host)
// in order to transfer them on the device when calling kernels.
// On the device, indexing of multidimensional tensors gives to `TensorAccessor`s.
// Use RestrictPtrTraits as PtrTraits if you want the tensor's data pointer to be marked as __restrict__.
// Instantiation from data, sizes, strides is only needed on the host and std::copy isn't available
// on the device, so those functions are host only.


// Can't put this directly into the macro function args because of commas
// #define AT_X GenericPackedTensorAccessor<T, N, PtrTraits, index_t>

// Old name for `GenericPackedTensorAccessor`
// #undef AT_X
 // namespace at


// Parsed from ATen/core/TensorBase.h

// #pragma once

// #include <c10/core/Device.h>
// #include <c10/core/Layout.h>
// #include <c10/core/MemoryFormat.h>
// #include <c10/core/ScalarType.h>
// #include <c10/core/ScalarTypeToTypeMeta.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorImpl.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/core/UndefinedTensorImpl.h>
// #include <c10/core/WrapDimMinimal.h>
// #include <c10/util/Exception.h>
// #include <c10/util/ExclusivelyOwnedTensorTraits.h>
// #include <c10/util/MaybeOwned.h>
// #include <c10/util/Optional.h>
// #include <c10/util/intrusive_ptr.h>

// #include <ATen/core/NamedTensor.h>
// #include <ATen/core/QuantizerBase.h>
// #include <c10/core/SymIntArrayRef.h>
// #include <ATen/core/TensorAccessor.h>


 // namespace torch::autograd

// Convert Tensor to TensorBase without any need to include Tensor.h
@Namespace("at") public static native @Const @ByRef TensorBase get_tensor_base(@Const @ByRef Tensor t);
@Namespace("at::impl") public static native @Cast("bool") boolean variable_excluded_from_dispatch();


// Targeting ../TensorBase.java








// Helper creator for Tensor class which doesn't requires the users to pass
// in an intrusive_ptr instead it just converts the argument passed to
// requested intrusive_ptr type.

 // namespace detail

@Namespace("at") public static native DispatchKey legacyExtractDispatchKey(@Const @ByRef TensorBase t);


// Targeting ../MaybeOwnedTraits.java


 // namespace c10





 // namespace symint

 // namespace at


// Parsed from ATen/core/TensorBody.h

// #pragma once

// #ifdef TORCH_ASSERT_NO_OPERATORS
// #error This change adds a dependency on native_functions.yaml,
//   meaning the file will need to be re-compiled every time an operator
//   is changed or added. Consider if your change would be better placed in
//   another file, or if a more specific header might achieve the same goal.
//   See NOTE: [Tensor vs. TensorBase]
// #endif

// #include <c10/core/Device.h>
// #include <c10/core/Layout.h>
// #include <c10/core/MemoryFormat.h>
// #include <c10/core/QScheme.h>
// #include <c10/core/Stream.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/ScalarType.h>
// #include <c10/core/ScalarTypeToTypeMeta.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorImpl.h>
// #include <c10/core/UndefinedTensorImpl.h>
// #include <c10/core/WrapDimMinimal.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/MaybeOwned.h>
// #include <c10/util/Optional.h>
// #include <c10/util/OptionalArrayRef.h>
// #include <c10/util/intrusive_ptr.h>
// #include <c10/macros/Export.h>
// #include <ATen/core/CheckMemoryFormat.h>
// #include <ATen/core/DeprecatedTypePropertiesRegistry.h>
// #include <ATen/core/DeprecatedTypeProperties.h>
// #include <ATen/core/NamedTensor.h>
// #include <ATen/core/QuantizerBase.h>
// #include <c10/core/SymInt.h>
// #include <ATen/core/TensorAccessor.h>
// #include <ATen/core/TensorBase.h>


// #include <ATen/MethodOperators.h>

// Targeting ../DeprecatedTypeProperties.java


 // namespace at
 // namespace indexing
 // namespace at

 // namespace torch::autograd
// Targeting ../Tensor.java


// Helper creator for Tensor class which doesn't requires the users to pass
// in an intrusive_ptr instead it just converts the argument passed to
// requested intrusive_ptr type.

 // namespace detail

 // namespace at

// aten::_backward(Tensor self, Tensor[] inputs, Tensor? gradient=None, bool? retain_graph=None, bool create_graph=False) -> ()


// aten::set_data(Tensor(a!) self, Tensor new_data) -> ()


// aten::data(Tensor self) -> Tensor


// aten::is_leaf(Tensor self) -> bool


// aten::output_nr(Tensor self) -> int


// aten::_version(Tensor self) -> int


// aten::requires_grad_(Tensor(a!) self, bool requires_grad=True) -> Tensor(a!)


// aten::retain_grad(Tensor(a!) self) -> ()


// aten::retains_grad(Tensor self) -> bool


// aten::_fw_primal(Tensor(a) self, int level) -> Tensor(a)


// aten::rename_(Tensor(a!) self, Dimname[]? names) -> Tensor(a!)


// aten::rename(Tensor(a) self, Dimname[]? names) -> Tensor(a)


// aten::align_to(Tensor(a) self, Dimname[] names) -> Tensor(a)


// aten::align_to.ellipsis_idx(Tensor(a) self, Dimname[] order, int ellipsis_idx) -> Tensor(a)


// aten::align_as(Tensor self, Tensor other) -> Tensor


// aten::refine_names(Tensor(a) self, Dimname[] names) -> Tensor(a)


// aten::abs(Tensor self) -> Tensor


// aten::abs_(Tensor(a!) self) -> Tensor(a!)


// aten::absolute(Tensor self) -> Tensor


// aten::absolute_(Tensor(a!) self) -> Tensor(a!)


// aten::angle(Tensor self) -> Tensor


// aten::sgn(Tensor self) -> Tensor


// aten::sgn_(Tensor(a!) self) -> Tensor(a!)


// aten::chalf(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor


// aten::_conj(Tensor(a) self) -> Tensor(a)


// aten::conj(Tensor(a) self) -> Tensor(a)


// aten::_conj_physical(Tensor self) -> Tensor


// aten::conj_physical(Tensor self) -> Tensor


// aten::conj_physical_(Tensor(a!) self) -> Tensor(a!)


// aten::resolve_conj(Tensor(a) self) -> Tensor(a)


// aten::resolve_neg(Tensor(a) self) -> Tensor(a)


// aten::_neg_view(Tensor(a) self) -> Tensor(a)


// aten::acos(Tensor self) -> Tensor


// aten::acos_(Tensor(a!) self) -> Tensor(a!)


// aten::arccos(Tensor self) -> Tensor


// aten::arccos_(Tensor(a!) self) -> Tensor(a!)


// aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor


// aten::add_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)


// aten::add.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor


// aten::add_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)


// aten::addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor


// aten::addmv_(Tensor(a!) self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)


// aten::addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor


// aten::addr_(Tensor(a!) self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)


// aten::_is_all_true(Tensor self) -> Tensor


// aten::_is_any_true(Tensor self) -> Tensor


// aten::all.dim(Tensor self, int dim, bool keepdim=False) -> Tensor


// aten::all.dimname(Tensor self, Dimname dim, bool keepdim=False) -> Tensor


// aten::allclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> bool


// aten::any.dim(Tensor self, int dim, bool keepdim=False) -> Tensor


// aten::any.dimname(Tensor self, Dimname dim, bool keepdim=False) -> Tensor


// aten::argmax(Tensor self, int? dim=None, bool keepdim=False) -> Tensor


// aten::argmin(Tensor self, int? dim=None, bool keepdim=False) -> Tensor


// aten::acosh(Tensor self) -> Tensor


// aten::acosh_(Tensor(a!) self) -> Tensor(a!)


// aten::arccosh(Tensor self) -> Tensor


// aten::arccosh_(Tensor(a!) self) -> Tensor(a!)


// aten::asinh(Tensor self) -> Tensor


// aten::asinh_(Tensor(a!) self) -> Tensor(a!)


// aten::arcsinh(Tensor self) -> Tensor


// aten::arcsinh_(Tensor(a!) self) -> Tensor(a!)


// aten::atanh(Tensor self) -> Tensor


// aten::atanh_(Tensor(a!) self) -> Tensor(a!)


// aten::arctanh(Tensor self) -> Tensor


// aten::arctanh_(Tensor(a!) self) -> Tensor(a!)


// aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)


// aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)


// aten::as_strided_(Tensor(a!) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a!)


// aten::as_strided_(Tensor(a!) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a!)


// aten::asin(Tensor self) -> Tensor


// aten::asin_(Tensor(a!) self) -> Tensor(a!)


// aten::arcsin(Tensor self) -> Tensor


// aten::arcsin_(Tensor(a!) self) -> Tensor(a!)


// aten::atan(Tensor self) -> Tensor


// aten::atan_(Tensor(a!) self) -> Tensor(a!)


// aten::arctan(Tensor self) -> Tensor


// aten::arctan_(Tensor(a!) self) -> Tensor(a!)


// aten::baddbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor


// aten::baddbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)


// aten::bernoulli(Tensor self, *, Generator? generator=None) -> Tensor


// aten::bernoulli_.Tensor(Tensor(a!) self, Tensor p, *, Generator? generator=None) -> Tensor(a!)


// aten::bernoulli_.float(Tensor(a!) self, float p=0.5, *, Generator? generator=None) -> Tensor(a!)


// aten::bernoulli.p(Tensor self, float p, *, Generator? generator=None) -> Tensor


// aten::bincount(Tensor self, Tensor? weights=None, int minlength=0) -> Tensor


// aten::bitwise_not(Tensor self) -> Tensor


// aten::bitwise_not_(Tensor(a!) self) -> Tensor(a!)


// aten::copysign.Tensor(Tensor self, Tensor other) -> Tensor


// aten::copysign_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::copysign.Scalar(Tensor self, Scalar other) -> Tensor


// aten::copysign_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::logical_not(Tensor self) -> Tensor


// aten::logical_not_(Tensor(a!) self) -> Tensor(a!)


// aten::logical_xor(Tensor self, Tensor other) -> Tensor


// aten::logical_xor_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::logical_and(Tensor self, Tensor other) -> Tensor


// aten::logical_and_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::logical_or(Tensor self, Tensor other) -> Tensor


// aten::logical_or_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bmm(Tensor self, Tensor mat2) -> Tensor


// aten::broadcast_to(Tensor(a) self, SymInt[] size) -> Tensor(a)


// aten::broadcast_to(Tensor(a) self, SymInt[] size) -> Tensor(a)


// aten::ceil(Tensor self) -> Tensor


// aten::ceil_(Tensor(a!) self) -> Tensor(a!)


// aten::unsafe_chunk(Tensor self, int chunks, int dim=0) -> Tensor[]


// aten::chunk(Tensor(a -> *) self, int chunks, int dim=0) -> Tensor(a)[]


// aten::tensor_split.sections(Tensor(a -> *) self, SymInt sections, int dim=0) -> Tensor(a)[]


// aten::tensor_split.sections(Tensor(a -> *) self, SymInt sections, int dim=0) -> Tensor(a)[]


// aten::tensor_split.indices(Tensor(a -> *) self, SymInt[] indices, int dim=0) -> Tensor(a)[]


// aten::tensor_split.indices(Tensor(a -> *) self, SymInt[] indices, int dim=0) -> Tensor(a)[]


// aten::tensor_split.tensor_indices_or_sections(Tensor(a -> *) self, Tensor tensor_indices_or_sections, int dim=0) -> Tensor(a)[]


// aten::clamp(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor


// aten::clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor


// aten::clamp_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)


// aten::clamp_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)


// aten::clamp_max(Tensor self, Scalar max) -> Tensor


// aten::clamp_max.Tensor(Tensor self, Tensor max) -> Tensor


// aten::clamp_max_(Tensor(a!) self, Scalar max) -> Tensor(a!)


// aten::clamp_max_.Tensor(Tensor(a!) self, Tensor max) -> Tensor(a!)


// aten::clamp_min(Tensor self, Scalar min) -> Tensor


// aten::clamp_min.Tensor(Tensor self, Tensor min) -> Tensor


// aten::clamp_min_(Tensor(a!) self, Scalar min) -> Tensor(a!)


// aten::clamp_min_.Tensor(Tensor(a!) self, Tensor min) -> Tensor(a!)


// aten::clip(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor


// aten::clip.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor


// aten::clip_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)


// aten::clip_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)


// aten::contiguous(Tensor(a) self, *, MemoryFormat memory_format=contiguous_format) -> Tensor(a)


// aten::copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)


// aten::cos(Tensor self) -> Tensor


// aten::cos_(Tensor(a!) self) -> Tensor(a!)


// aten::cosh(Tensor self) -> Tensor


// aten::cosh_(Tensor(a!) self) -> Tensor(a!)


// aten::count_nonzero.dim_IntList(Tensor self, int[] dim) -> Tensor


// aten::count_nonzero(Tensor self, int? dim=None) -> Tensor


// aten::cov(Tensor self, *, int correction=1, Tensor? fweights=None, Tensor? aweights=None) -> Tensor


// aten::corrcoef(Tensor self) -> Tensor


// aten::cummax(Tensor self, int dim) -> (Tensor values, Tensor indices)


// aten::cummax.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)


// aten::cummin(Tensor self, int dim) -> (Tensor values, Tensor indices)


// aten::cummin.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)


// aten::cumprod(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor


// aten::cumprod_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -> Tensor(a!)


// aten::cumprod.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor


// aten::cumprod_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -> Tensor(a!)


// aten::cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor


// aten::cumsum_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -> Tensor(a!)


// aten::cumsum.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor


// aten::cumsum_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -> Tensor(a!)


// aten::diag_embed(Tensor self, int offset=0, int dim1=-2, int dim2=-1) -> Tensor


// aten::diagflat(Tensor self, int offset=0) -> Tensor


// aten::diagonal(Tensor(a) self, int offset=0, int dim1=0, int dim2=1) -> Tensor(a)


// aten::diagonal.Dimname(Tensor(a) self, *, Dimname outdim, Dimname dim1, Dimname dim2, int offset=0) -> Tensor(a)


// aten::fill_diagonal_(Tensor(a!) self, Scalar fill_value, bool wrap=False) -> Tensor(a!)


// aten::diff(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None) -> Tensor


// aten::div.Tensor(Tensor self, Tensor other) -> Tensor


// aten::div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::div.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor


// aten::div_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)


// aten::div.Scalar(Tensor self, Scalar other) -> Tensor


// aten::div_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::div.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor


// aten::div_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)


// aten::divide.Tensor(Tensor self, Tensor other) -> Tensor


// aten::divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::divide.Scalar(Tensor self, Scalar other) -> Tensor


// aten::divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::divide.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor


// aten::divide_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)


// aten::divide.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor


// aten::divide_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)


// aten::true_divide.Tensor(Tensor self, Tensor other) -> Tensor


// aten::true_divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::true_divide.Scalar(Tensor self, Scalar other) -> Tensor


// aten::true_divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::dot(Tensor self, Tensor tensor) -> Tensor


// aten::vdot(Tensor self, Tensor other) -> Tensor


// aten::new_empty(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_empty(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_empty(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_empty(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_empty_strided(Tensor self, SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_empty_strided(Tensor self, SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_empty_strided(Tensor self, SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_empty_strided(Tensor self, SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_full(Tensor self, SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_full(Tensor self, SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_full(Tensor self, SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_full(Tensor self, SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_zeros(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_zeros(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_zeros(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_zeros(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_ones(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_ones(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_ones(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::new_ones(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor


// aten::resize_(Tensor(a!) self, SymInt[] size, *, MemoryFormat? memory_format=None) -> Tensor(a!)


// aten::resize_(Tensor(a!) self, SymInt[] size, *, MemoryFormat? memory_format=None) -> Tensor(a!)


// aten::erf(Tensor self) -> Tensor


// aten::erf_(Tensor(a!) self) -> Tensor(a!)


// aten::erfc(Tensor self) -> Tensor


// aten::erfc_(Tensor(a!) self) -> Tensor(a!)


// aten::exp(Tensor self) -> Tensor


// aten::exp_(Tensor(a!) self) -> Tensor(a!)


// aten::exp2(Tensor self) -> Tensor


// aten::exp2_(Tensor(a!) self) -> Tensor(a!)


// aten::expm1(Tensor self) -> Tensor


// aten::expm1_(Tensor(a!) self) -> Tensor(a!)


// aten::expand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -> Tensor(a)


// aten::expand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -> Tensor(a)


// aten::expand_as(Tensor(a) self, Tensor other) -> Tensor(a)


// aten::flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -> Tensor(a)


// aten::flatten.named_out_dim(Tensor(a) self, int start_dim, int end_dim, Dimname out_dim) -> Tensor(a)


// aten::flatten.using_names(Tensor(a) self, Dimname start_dim, Dimname end_dim, Dimname out_dim) -> Tensor(a)


// aten::flatten.DimnameList(Tensor(a) self, Dimname[] dims, Dimname out_dim) -> Tensor(a)


// aten::unflatten.int(Tensor(a) self, int dim, int[] sizes) -> Tensor(a)


// aten::unflatten.Dimname(Tensor(a) self, Dimname dim, int[] sizes, Dimname[] names) -> Tensor(a)


// aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)


// aten::fill_.Tensor(Tensor(a!) self, Tensor value) -> Tensor(a!)


// aten::floor(Tensor self) -> Tensor


// aten::floor_(Tensor(a!) self) -> Tensor(a!)


// aten::floor_divide(Tensor self, Tensor other) -> Tensor


// aten::floor_divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::floor_divide.Scalar(Tensor self, Scalar other) -> Tensor


// aten::floor_divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::frac(Tensor self) -> Tensor


// aten::frac_(Tensor(a!) self) -> Tensor(a!)


// aten::gcd(Tensor self, Tensor other) -> Tensor


// aten::gcd_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::lcm(Tensor self, Tensor other) -> Tensor


// aten::lcm_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor


// aten::index_copy_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)


// aten::index_copy(Tensor self, int dim, Tensor index, Tensor source) -> Tensor


// aten::index_copy_.dimname(Tensor(a!) self, Dimname dim, Tensor index, Tensor source) -> Tensor(a!)


// aten::index_copy.dimname(Tensor self, Dimname dim, Tensor index, Tensor source) -> Tensor


// aten::index_put_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor(a!)


// aten::index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor


// aten::isclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> Tensor


// aten::isnan(Tensor self) -> Tensor


// aten::is_distributed(Tensor self) -> bool


// aten::is_floating_point(Tensor self) -> bool


// aten::is_complex(Tensor self) -> bool


// aten::is_conj(Tensor self) -> bool


// aten::_is_zerotensor(Tensor self) -> bool


// aten::is_neg(Tensor self) -> bool


// aten::isreal(Tensor self) -> Tensor


// aten::is_nonzero(Tensor self) -> bool


// aten::is_same_size(Tensor self, Tensor other) -> bool


// aten::is_signed(Tensor self) -> bool


// aten::is_inference(Tensor self) -> bool


// aten::kron(Tensor self, Tensor other) -> Tensor


// aten::kthvalue(Tensor self, int k, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::kthvalue.dimname(Tensor self, int k, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::nan_to_num(Tensor self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor


// aten::nan_to_num_(Tensor(a!) self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor(a!)


// aten::ldexp.Tensor(Tensor self, Tensor other) -> Tensor


// aten::ldexp_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::log(Tensor self) -> Tensor


// aten::log_(Tensor(a!) self) -> Tensor(a!)


// aten::log10(Tensor self) -> Tensor


// aten::log10_(Tensor(a!) self) -> Tensor(a!)


// aten::log1p(Tensor self) -> Tensor


// aten::log1p_(Tensor(a!) self) -> Tensor(a!)


// aten::log2(Tensor self) -> Tensor


// aten::log2_(Tensor(a!) self) -> Tensor(a!)


// aten::logaddexp(Tensor self, Tensor other) -> Tensor


// aten::logaddexp2(Tensor self, Tensor other) -> Tensor


// aten::xlogy.Tensor(Tensor self, Tensor other) -> Tensor


// aten::xlogy.Scalar_Other(Tensor self, Scalar other) -> Tensor


// aten::xlogy_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::xlogy_.Scalar_Other(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor


// aten::log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor


// aten::logcumsumexp(Tensor self, int dim) -> Tensor


// aten::logcumsumexp.dimname(Tensor self, Dimname dim) -> Tensor


// aten::logsumexp(Tensor self, int[1] dim, bool keepdim=False) -> Tensor


// aten::logsumexp.names(Tensor self, Dimname[1] dim, bool keepdim=False) -> Tensor


// aten::matmul(Tensor self, Tensor other) -> Tensor


// aten::matrix_power(Tensor self, int n) -> Tensor


// aten::matrix_exp(Tensor self) -> Tensor


// aten::aminmax(Tensor self, *, int? dim=None, bool keepdim=False) -> (Tensor min, Tensor max)


// aten::max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::amax(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor


// aten::mean(Tensor self, *, ScalarType? dtype=None) -> Tensor


// aten::mean.dim(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::mean.names_dim(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::nanmean(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::median(Tensor self) -> Tensor


// aten::median.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::median.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::nanmedian(Tensor self) -> Tensor


// aten::nanmedian.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::nanmedian.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::amin(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor


// aten::mm(Tensor self, Tensor mat2) -> Tensor


// aten::mode(Tensor self, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::mode.dimname(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)


// aten::mul.Tensor(Tensor self, Tensor other) -> Tensor


// aten::mul_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::mul.Scalar(Tensor self, Scalar other) -> Tensor


// aten::mul_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::multiply.Tensor(Tensor self, Tensor other) -> Tensor


// aten::multiply_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::multiply.Scalar(Tensor self, Scalar other) -> Tensor


// aten::multiply_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::mv(Tensor self, Tensor vec) -> Tensor


// aten::mvlgamma(Tensor self, int p) -> Tensor


// aten::mvlgamma_(Tensor(a!) self, int p) -> Tensor(a!)


// aten::narrow_copy(Tensor self, int dim, SymInt start, SymInt length) -> Tensor


// aten::narrow_copy(Tensor self, int dim, SymInt start, SymInt length) -> Tensor


// aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -> Tensor(a)


// aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -> Tensor(a)


// aten::narrow.Tensor(Tensor(a) self, int dim, Tensor start, SymInt length) -> Tensor(a)


// aten::narrow.Tensor(Tensor(a) self, int dim, Tensor start, SymInt length) -> Tensor(a)


// aten::permute(Tensor(a) self, int[] dims) -> Tensor(a)


// aten::movedim.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)


// aten::movedim.int(Tensor(a) self, int source, int destination) -> Tensor(a)


// aten::moveaxis.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)


// aten::moveaxis.int(Tensor(a) self, int source, int destination) -> Tensor(a)


// aten::numpy_T(Tensor(a) self) -> Tensor(a)


// aten::matrix_H(Tensor(a) self) -> Tensor(a)


// aten::mT(Tensor(a) self) -> Tensor(a)


// aten::mH(Tensor(a) self) -> Tensor(a)


// aten::adjoint(Tensor(a) self) -> Tensor(a)


// aten::is_pinned(Tensor self, Device? device=None) -> bool


// aten::pin_memory(Tensor(a) self, Device? device=None) -> Tensor(a)


// aten::pinverse(Tensor self, float rcond=1e-15) -> Tensor


// aten::rad2deg(Tensor self) -> Tensor


// aten::rad2deg_(Tensor(a!) self) -> Tensor(a!)


// aten::deg2rad(Tensor self) -> Tensor


// aten::deg2rad_(Tensor(a!) self) -> Tensor(a!)


// aten::ravel(Tensor(a) self) -> Tensor(a)


// aten::reciprocal(Tensor self) -> Tensor


// aten::reciprocal_(Tensor(a!) self) -> Tensor(a!)


// aten::neg(Tensor self) -> Tensor


// aten::neg_(Tensor(a!) self) -> Tensor(a!)


// aten::negative(Tensor self) -> Tensor


// aten::negative_(Tensor(a!) self) -> Tensor(a!)


// aten::repeat(Tensor self, SymInt[] repeats) -> Tensor


// aten::repeat(Tensor self, SymInt[] repeats) -> Tensor


// aten::repeat_interleave.self_Tensor(Tensor self, Tensor repeats, int? dim=None, *, int? output_size=None) -> Tensor


// aten::repeat_interleave.self_int(Tensor self, SymInt repeats, int? dim=None, *, int? output_size=None) -> Tensor


// aten::repeat_interleave.self_int(Tensor self, SymInt repeats, int? dim=None, *, int? output_size=None) -> Tensor


// aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)


// aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)


// aten::_reshape_alias(Tensor(a) self, SymInt[] size, SymInt[] stride) -> Tensor(a)


// aten::_reshape_alias(Tensor(a) self, SymInt[] size, SymInt[] stride) -> Tensor(a)


// aten::reshape_as(Tensor(a) self, Tensor other) -> Tensor(a)


// aten::round(Tensor self) -> Tensor


// aten::round_(Tensor(a!) self) -> Tensor(a!)


// aten::round.decimals(Tensor self, *, int decimals) -> Tensor


// aten::round_.decimals(Tensor(a!) self, *, int decimals) -> Tensor(a!)


// aten::relu(Tensor self) -> Tensor


// aten::relu_(Tensor(a!) self) -> Tensor(a!)


// aten::prelu(Tensor self, Tensor weight) -> Tensor


// aten::hardshrink(Tensor self, Scalar lambd=0.5) -> Tensor


// aten::hardshrink_backward(Tensor grad_out, Tensor self, Scalar lambd) -> Tensor


// aten::rsqrt(Tensor self) -> Tensor


// aten::rsqrt_(Tensor(a!) self) -> Tensor(a!)


// aten::select.Dimname(Tensor(a) self, Dimname dim, int index) -> Tensor(a)


// aten::select.int(Tensor(a) self, int dim, SymInt index) -> Tensor(a)


// aten::select.int(Tensor(a) self, int dim, SymInt index) -> Tensor(a)


// aten::sigmoid(Tensor self) -> Tensor


// aten::sigmoid_(Tensor(a!) self) -> Tensor(a!)


// aten::logit(Tensor self, float? eps=None) -> Tensor


// aten::logit_(Tensor(a!) self, float? eps=None) -> Tensor(a!)


// aten::sin(Tensor self) -> Tensor


// aten::sin_(Tensor(a!) self) -> Tensor(a!)


// aten::sinc(Tensor self) -> Tensor


// aten::sinc_(Tensor(a!) self) -> Tensor(a!)


// aten::sinh(Tensor self) -> Tensor


// aten::sinh_(Tensor(a!) self) -> Tensor(a!)


// aten::detach(Tensor(a) self) -> Tensor(a)


// aten::detach_(Tensor(a!) self) -> Tensor(a!)


// aten::size.Dimname(Tensor self, Dimname dim) -> int


// aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)


// aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)


// aten::slice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor


// aten::slice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor


// aten::select_scatter(Tensor self, Tensor src, int dim, SymInt index) -> Tensor


// aten::select_scatter(Tensor self, Tensor src, int dim, SymInt index) -> Tensor


// aten::diagonal_scatter(Tensor self, Tensor src, int offset=0, int dim1=0, int dim2=1) -> Tensor


// aten::as_strided_scatter(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor


// aten::as_strided_scatter(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor


// aten::smm(Tensor self, Tensor mat2) -> Tensor


// aten::softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor


// aten::softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor


// aten::unsafe_split.Tensor(Tensor self, SymInt split_size, int dim=0) -> Tensor[]


// aten::unsafe_split.Tensor(Tensor self, SymInt split_size, int dim=0) -> Tensor[]


// aten::split.Tensor(Tensor(a -> *) self, SymInt split_size, int dim=0) -> Tensor(a)[]


// aten::split.Tensor(Tensor(a -> *) self, SymInt split_size, int dim=0) -> Tensor(a)[]


// aten::split.sizes(Tensor(a -> *) self, SymInt[] split_size, int dim=0) -> Tensor(a)[]


// aten::split.sizes(Tensor(a -> *) self, SymInt[] split_size, int dim=0) -> Tensor(a)[]


// aten::unsafe_split_with_sizes(Tensor self, SymInt[] split_sizes, int dim=0) -> Tensor[]


// aten::unsafe_split_with_sizes(Tensor self, SymInt[] split_sizes, int dim=0) -> Tensor[]


// aten::split_with_sizes(Tensor(a -> *) self, SymInt[] split_sizes, int dim=0) -> Tensor(a)[]


// aten::split_with_sizes(Tensor(a -> *) self, SymInt[] split_sizes, int dim=0) -> Tensor(a)[]


// aten::hsplit.int(Tensor(a -> *) self, int sections) -> Tensor(a)[]


// aten::hsplit.array(Tensor(a -> *) self, int[] indices) -> Tensor(a)[]


// aten::vsplit.int(Tensor(a -> *) self, int sections) -> Tensor(a)[]


// aten::vsplit.array(Tensor(a -> *) self, int[] indices) -> Tensor(a)[]


// aten::dsplit.int(Tensor(a -> *) self, int sections) -> Tensor(a)[]


// aten::dsplit.array(Tensor(a -> *) self, int[] indices) -> Tensor(a)[]


// aten::squeeze(Tensor(a) self) -> Tensor(a)


// aten::squeeze.dim(Tensor(a) self, int dim) -> Tensor(a)


// aten::squeeze.dimname(Tensor(a) self, Dimname dim) -> Tensor(a)


// aten::squeeze.dims(Tensor(a) self, int[] dim) -> Tensor(a)


// aten::squeeze_(Tensor(a!) self) -> Tensor(a!)


// aten::squeeze_.dim(Tensor(a!) self, int dim) -> Tensor(a!)


// aten::squeeze_.dims(Tensor(a!) self, int[] dim) -> Tensor(a!)


// aten::squeeze_.dimname(Tensor(a!) self, Dimname dim) -> Tensor(a!)


// aten::sspaddmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor


// aten::stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor


// aten::stft.center(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, str pad_mode="reflect", bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor


// aten::istft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, bool normalized=False, bool? onesided=None, int? length=None, bool return_complex=False) -> Tensor


// aten::stride.Dimname(Tensor self, Dimname dim) -> int


// aten::sum(Tensor self, *, ScalarType? dtype=None) -> Tensor


// aten::sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::sum.dim_DimnameList(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::nansum(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::sum_to_size(Tensor self, int[] size) -> Tensor


// aten::sqrt(Tensor self) -> Tensor


// aten::sqrt_(Tensor(a!) self) -> Tensor(a!)


// aten::square(Tensor self) -> Tensor


// aten::square_(Tensor(a!) self) -> Tensor(a!)


// aten::std(Tensor self, bool unbiased=True) -> Tensor


// aten::std.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> Tensor


// aten::std.correction(Tensor self, int[1]? dim=None, *, int? correction=None, bool keepdim=False) -> Tensor


// aten::std.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor


// aten::std.correction_names(Tensor self, Dimname[1] dim, *, int? correction=None, bool keepdim=False) -> Tensor


// aten::prod(Tensor self, *, ScalarType? dtype=None) -> Tensor


// aten::prod.dim_int(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::prod.dim_Dimname(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor


// aten::t(Tensor(a) self) -> Tensor(a)


// aten::t_(Tensor(a!) self) -> Tensor(a!)


// aten::tan(Tensor self) -> Tensor


// aten::tan_(Tensor(a!) self) -> Tensor(a!)


// aten::tanh(Tensor self) -> Tensor


// aten::tanh_(Tensor(a!) self) -> Tensor(a!)


// aten::tile(Tensor self, int[] dims) -> Tensor


// aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)


// aten::transpose.Dimname(Tensor(a) self, Dimname dim0, Dimname dim1) -> Tensor(a)


// aten::transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)


// aten::flip(Tensor self, int[] dims) -> Tensor


// aten::fliplr(Tensor self) -> Tensor


// aten::flipud(Tensor self) -> Tensor


// aten::roll(Tensor self, int[1] shifts, int[1] dims=[]) -> Tensor


// aten::rot90(Tensor self, int k=1, int[] dims=[0,1]) -> Tensor


// aten::_nested_tensor_size(Tensor self) -> Tensor


// aten::_nested_tensor_strides(Tensor self) -> Tensor


// aten::_nested_tensor_offsets(Tensor self) -> int[]


// aten::trunc(Tensor self) -> Tensor


// aten::trunc_(Tensor(a!) self) -> Tensor(a!)


// aten::fix(Tensor self) -> Tensor


// aten::fix_(Tensor(a!) self) -> Tensor(a!)


// aten::type_as(Tensor self, Tensor other) -> Tensor


// aten::unsqueeze(Tensor(a) self, int dim) -> Tensor(a)


// aten::unsqueeze_(Tensor(a!) self, int dim) -> Tensor(a!)


// aten::var(Tensor self, bool unbiased=True) -> Tensor


// aten::var.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> Tensor


// aten::var.correction(Tensor self, int[1]? dim=None, *, int? correction=None, bool keepdim=False) -> Tensor


// aten::var.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor


// aten::var.correction_names(Tensor self, Dimname[1] dim, *, int? correction=None, bool keepdim=False) -> Tensor


// aten::view_as(Tensor(a) self, Tensor other) -> Tensor(a)


// aten::where.self(Tensor condition, Tensor self, Tensor other) -> Tensor


// aten::where.ScalarOther(Tensor condition, Tensor self, Scalar other) -> Tensor


// aten::norm.ScalarOpt_dtype(Tensor self, Scalar? p, *, ScalarType dtype) -> Tensor


// aten::norm.Scalar(Tensor self, Scalar p=2) -> Tensor


// aten::norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor


// aten::norm.ScalarOpt_dim(Tensor self, Scalar? p, int[1] dim, bool keepdim=False) -> Tensor


// aten::norm.names_ScalarOpt_dim_dtype(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor


// aten::norm.names_ScalarOpt_dim(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False) -> Tensor


// aten::frexp.Tensor(Tensor self) -> (Tensor mantissa, Tensor exponent)


// aten::clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor


// aten::positive(Tensor(a) self) -> Tensor(a)


// aten::resize_as_(Tensor(a!) self, Tensor the_template, *, MemoryFormat? memory_format=None) -> Tensor(a!)


// aten::resize_as_sparse_(Tensor(a!) self, Tensor the_template) -> Tensor(a!)


// aten::zero_(Tensor(a!) self) -> Tensor(a!)


// aten::sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor


// aten::sub_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)


// aten::sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor


// aten::sub_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)


// aten::subtract.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor


// aten::subtract_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)


// aten::subtract.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor


// aten::subtract_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)


// aten::heaviside(Tensor self, Tensor values) -> Tensor


// aten::heaviside_(Tensor(a!) self, Tensor values) -> Tensor(a!)


// aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor


// aten::addmm_(Tensor(a!) self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)


// aten::_addmm_activation(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, bool use_gelu=False) -> Tensor


// aten::sparse_resize_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -> Tensor(a!)


// aten::sparse_resize_and_clear_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -> Tensor(a!)


// aten::sparse_mask(Tensor self, Tensor mask) -> Tensor


// aten::to_dense(Tensor self, ScalarType? dtype=None) -> Tensor


// aten::_to_dense(Tensor self, ScalarType? dtype=None) -> Tensor


// aten::sparse_dim(Tensor self) -> int


// aten::_dimI(Tensor self) -> int


// aten::dense_dim(Tensor self) -> int


// aten::_dimV(Tensor self) -> int


// aten::_nnz(Tensor self) -> int


// aten::coalesce(Tensor(a) self) -> Tensor(a)


// aten::is_coalesced(Tensor self) -> bool


// aten::_indices(Tensor(a) self) -> Tensor(a)


// aten::_values(Tensor(a) self) -> Tensor(a)


// aten::_coalesced_(Tensor(a!) self, bool coalesced) -> Tensor(a!)


// aten::indices(Tensor(a) self) -> Tensor(a)


// aten::values(Tensor(a) self) -> Tensor(a)


// aten::crow_indices(Tensor(a) self) -> Tensor(a)


// aten::col_indices(Tensor(a) self) -> Tensor(a)


// aten::ccol_indices(Tensor(a) self) -> Tensor(a)


// aten::row_indices(Tensor(a) self) -> Tensor(a)


// aten::unbind.int(Tensor(a -> *) self, int dim=0) -> Tensor(a)[]


// aten::unbind.Dimname(Tensor(a -> *) self, Dimname dim) -> Tensor(a)[]


// aten::to_sparse.sparse_dim(Tensor self, int sparse_dim) -> Tensor


// aten::to_sparse(Tensor self, *, Layout? layout=None, int[2]? blocksize=None, int? dense_dim=None) -> Tensor


// aten::to_sparse_csr(Tensor self, int? dense_dim=None) -> Tensor


// aten::to_sparse_csc(Tensor self, int? dense_dim=None) -> Tensor


// aten::to_sparse_bsr(Tensor self, int[2] blocksize, int? dense_dim=None) -> Tensor


// aten::to_sparse_bsc(Tensor self, int[2] blocksize, int? dense_dim=None) -> Tensor


// aten::to_mkldnn(Tensor self, ScalarType? dtype=None) -> Tensor


// aten::dequantize.self(Tensor self) -> Tensor


// aten::q_scale(Tensor self) -> float


// aten::q_zero_point(Tensor self) -> int


// aten::q_per_channel_scales(Tensor self) -> Tensor


// aten::q_per_channel_zero_points(Tensor self) -> Tensor


// aten::q_per_channel_axis(Tensor self) -> int


// aten::int_repr(Tensor self) -> Tensor


// aten::qscheme(Tensor self) -> QScheme


// aten::_autocast_to_reduced_precision(Tensor(a) self, bool cuda_enabled, bool cpu_enabled, ScalarType cuda_dtype, ScalarType cpu_dtype) -> Tensor(a)


// aten::_autocast_to_full_precision(Tensor(a) self, bool cuda_enabled, bool cpu_enabled) -> Tensor(a)


// aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)


// aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)


// aten::to.device(Tensor(a) self, Device device, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)


// aten::to.dtype(Tensor(a) self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)


// aten::to.other(Tensor(a) self, Tensor other, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)


// aten::item(Tensor self) -> Scalar


// aten::set_.source_Storage(Tensor(a!) self, Storage source) -> Tensor(a!)


// aten::set_.source_Storage_storage_offset(Tensor(a!) self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -> Tensor(a!)


// aten::set_.source_Storage_storage_offset(Tensor(a!) self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -> Tensor(a!)


// aten::set_.source_Tensor_storage_offset(Tensor(a!) self, Tensor source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -> Tensor(a!)


// aten::set_.source_Tensor_storage_offset(Tensor(a!) self, Tensor source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -> Tensor(a!)


// aten::set_.source_Tensor(Tensor(a!) self, Tensor source) -> Tensor(a!)


// aten::set_(Tensor(a!) self) -> Tensor(a!)


// aten::is_set_to(Tensor self, Tensor tensor) -> bool


// aten::masked_fill_.Scalar(Tensor(a!) self, Tensor mask, Scalar value) -> Tensor(a!)


// aten::masked_fill.Scalar(Tensor self, Tensor mask, Scalar value) -> Tensor


// aten::masked_fill_.Tensor(Tensor(a!) self, Tensor mask, Tensor value) -> Tensor(a!)


// aten::masked_fill.Tensor(Tensor self, Tensor mask, Tensor value) -> Tensor


// aten::masked_scatter_(Tensor(a!) self, Tensor mask, Tensor source) -> Tensor(a!)


// aten::masked_scatter(Tensor self, Tensor mask, Tensor source) -> Tensor


// aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)


// aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)


// aten::view.dtype(Tensor(a) self, ScalarType dtype) -> Tensor(a)


// aten::put_(Tensor(a!) self, Tensor index, Tensor source, bool accumulate=False) -> Tensor(a!)


// aten::put(Tensor self, Tensor index, Tensor source, bool accumulate=False) -> Tensor


// aten::index_add_(Tensor(a!) self, int dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor(a!)


// aten::index_add(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor


// aten::index_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor


// aten::index_reduce_(Tensor(a!) self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -> Tensor(a!)


// aten::index_reduce(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -> Tensor


// aten::index_fill_.int_Scalar(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)


// aten::index_fill.int_Scalar(Tensor self, int dim, Tensor index, Scalar value) -> Tensor


// aten::index_fill_.int_Tensor(Tensor(a!) self, int dim, Tensor index, Tensor value) -> Tensor(a!)


// aten::index_fill.int_Tensor(Tensor self, int dim, Tensor index, Tensor value) -> Tensor


// aten::index_fill_.Dimname_Scalar(Tensor(a!) self, Dimname dim, Tensor index, Scalar value) -> Tensor(a!)


// aten::index_fill_.Dimname_Tensor(Tensor(a!) self, Dimname dim, Tensor index, Tensor value) -> Tensor(a!)


// aten::index_fill.Dimname_Scalar(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor


// aten::index_fill.Dimname_Tensor(Tensor self, Dimname dim, Tensor index, Tensor value) -> Tensor


// aten::scatter.src(Tensor self, int dim, Tensor index, Tensor src) -> Tensor


// aten::scatter_.src(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)


// aten::scatter.value(Tensor self, int dim, Tensor index, Scalar value) -> Tensor


// aten::scatter_.value(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)


// aten::scatter.reduce(Tensor self, int dim, Tensor index, Tensor src, *, str reduce) -> Tensor


// aten::scatter_.reduce(Tensor(a!) self, int dim, Tensor index, Tensor src, *, str reduce) -> Tensor(a!)


// aten::scatter.value_reduce(Tensor self, int dim, Tensor index, Scalar value, *, str reduce) -> Tensor


// aten::scatter_.value_reduce(Tensor(a!) self, int dim, Tensor index, Scalar value, *, str reduce) -> Tensor(a!)


// aten::scatter.dimname_src(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor


// aten::scatter.dimname_value(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor


// aten::scatter_add(Tensor self, int dim, Tensor index, Tensor src) -> Tensor


// aten::scatter_add_(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)


// aten::scatter_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor


// aten::scatter_reduce.two(Tensor self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True) -> Tensor


// aten::scatter_reduce_.two(Tensor(a!) self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True) -> Tensor(a!)


// aten::eq_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::eq_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bitwise_and.Scalar(Tensor self, Scalar other) -> Tensor


// aten::bitwise_and.Tensor(Tensor self, Tensor other) -> Tensor


// aten::bitwise_and_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::bitwise_and_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::__and__.Scalar(Tensor self, Scalar other) -> Tensor


// aten::__and__.Tensor(Tensor self, Tensor other) -> Tensor


// aten::__iand__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::__iand__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bitwise_or.Scalar(Tensor self, Scalar other) -> Tensor


// aten::bitwise_or.Tensor(Tensor self, Tensor other) -> Tensor


// aten::bitwise_or_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::bitwise_or_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::__or__.Scalar(Tensor self, Scalar other) -> Tensor


// aten::__or__.Tensor(Tensor self, Tensor other) -> Tensor


// aten::__ior__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::__ior__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bitwise_xor.Scalar(Tensor self, Scalar other) -> Tensor


// aten::bitwise_xor.Tensor(Tensor self, Tensor other) -> Tensor


// aten::bitwise_xor_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::bitwise_xor_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::__xor__.Scalar(Tensor self, Scalar other) -> Tensor


// aten::__xor__.Tensor(Tensor self, Tensor other) -> Tensor


// aten::__ixor__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::__ixor__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::__lshift__.Scalar(Tensor self, Scalar other) -> Tensor


// aten::__lshift__.Tensor(Tensor self, Tensor other) -> Tensor


// aten::__ilshift__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::__ilshift__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bitwise_left_shift.Tensor(Tensor self, Tensor other) -> Tensor


// aten::bitwise_left_shift_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bitwise_left_shift.Tensor_Scalar(Tensor self, Scalar other) -> Tensor


// aten::bitwise_left_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::__rshift__.Scalar(Tensor self, Scalar other) -> Tensor


// aten::__rshift__.Tensor(Tensor self, Tensor other) -> Tensor


// aten::__irshift__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::__irshift__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bitwise_right_shift.Tensor(Tensor self, Tensor other) -> Tensor


// aten::bitwise_right_shift_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::bitwise_right_shift.Tensor_Scalar(Tensor self, Scalar other) -> Tensor


// aten::bitwise_right_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::tril_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)


// aten::triu_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)


// aten::digamma_(Tensor(a!) self) -> Tensor(a!)


// aten::lerp_.Scalar(Tensor(a!) self, Tensor end, Scalar weight) -> Tensor(a!)


// aten::lerp_.Tensor(Tensor(a!) self, Tensor end, Tensor weight) -> Tensor(a!)


// aten::addbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)


// aten::addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor


// aten::random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -> Tensor(a!)


// aten::random_.to(Tensor(a!) self, int to, *, Generator? generator=None) -> Tensor(a!)


// aten::random_(Tensor(a!) self, *, Generator? generator=None) -> Tensor(a!)


// aten::uniform_(Tensor(a!) self, float from=0, float to=1, *, Generator? generator=None) -> Tensor(a!)


// aten::cauchy_(Tensor(a!) self, float median=0, float sigma=1, *, Generator? generator=None) -> Tensor(a!)


// aten::log_normal_(Tensor(a!) self, float mean=1, float std=2, *, Generator? generator=None) -> Tensor(a!)


// aten::exponential_(Tensor(a!) self, float lambd=1, *, Generator? generator=None) -> Tensor(a!)


// aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -> Tensor(a!)


// aten::diag(Tensor self, int diagonal=0) -> Tensor


// aten::cross(Tensor self, Tensor other, int? dim=None) -> Tensor


// aten::triu(Tensor self, int diagonal=0) -> Tensor


// aten::tril(Tensor self, int diagonal=0) -> Tensor


// aten::trace(Tensor self) -> Tensor


// aten::ne.Scalar(Tensor self, Scalar other) -> Tensor


// aten::ne.Tensor(Tensor self, Tensor other) -> Tensor


// aten::ne_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::ne_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::not_equal.Scalar(Tensor self, Scalar other) -> Tensor


// aten::not_equal.Tensor(Tensor self, Tensor other) -> Tensor


// aten::not_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::not_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::eq.Scalar(Tensor self, Scalar other) -> Tensor


// aten::eq.Tensor(Tensor self, Tensor other) -> Tensor


// aten::ge.Scalar(Tensor self, Scalar other) -> Tensor


// aten::ge.Tensor(Tensor self, Tensor other) -> Tensor


// aten::ge_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::ge_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::greater_equal.Scalar(Tensor self, Scalar other) -> Tensor


// aten::greater_equal.Tensor(Tensor self, Tensor other) -> Tensor


// aten::greater_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::greater_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::le.Scalar(Tensor self, Scalar other) -> Tensor


// aten::le.Tensor(Tensor self, Tensor other) -> Tensor


// aten::le_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::le_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::less_equal.Scalar(Tensor self, Scalar other) -> Tensor


// aten::less_equal.Tensor(Tensor self, Tensor other) -> Tensor


// aten::less_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::less_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::gt.Scalar(Tensor self, Scalar other) -> Tensor


// aten::gt.Tensor(Tensor self, Tensor other) -> Tensor


// aten::gt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::gt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::greater.Scalar(Tensor self, Scalar other) -> Tensor


// aten::greater.Tensor(Tensor self, Tensor other) -> Tensor


// aten::greater_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::greater_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::lt.Scalar(Tensor self, Scalar other) -> Tensor


// aten::lt.Tensor(Tensor self, Tensor other) -> Tensor


// aten::lt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::lt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::less.Scalar(Tensor self, Scalar other) -> Tensor


// aten::less.Tensor(Tensor self, Tensor other) -> Tensor


// aten::less_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::less_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::take(Tensor self, Tensor index) -> Tensor


// aten::take_along_dim(Tensor self, Tensor indices, int? dim=None) -> Tensor


// aten::index_select(Tensor self, int dim, Tensor index) -> Tensor


// aten::index_select.dimname(Tensor self, Dimname dim, Tensor index) -> Tensor


// aten::masked_select(Tensor self, Tensor mask) -> Tensor


// aten::nonzero(Tensor self) -> Tensor


// aten::nonzero_numpy(Tensor self) -> Tensor[]


// aten::argwhere(Tensor self) -> Tensor


// aten::gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor


// aten::gather.dimname(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False) -> Tensor


// aten::addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor


// aten::addcmul_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)


// aten::addcdiv(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor


// aten::addcdiv_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)


// aten::triangular_solve(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False) -> (Tensor solution, Tensor cloned_coefficient)


// aten::svd(Tensor self, bool some=True, bool compute_uv=True) -> (Tensor U, Tensor S, Tensor V)


// aten::swapaxes(Tensor(a) self, int axis0, int axis1) -> Tensor(a)


// aten::swapaxes_(Tensor(a!) self, int axis0, int axis1) -> Tensor(a!)


// aten::swapdims(Tensor(a) self, int dim0, int dim1) -> Tensor(a)


// aten::swapdims_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)


// aten::cholesky(Tensor self, bool upper=False) -> Tensor


// aten::cholesky_solve(Tensor self, Tensor input2, bool upper=False) -> Tensor


// aten::cholesky_inverse(Tensor self, bool upper=False) -> Tensor


// aten::qr(Tensor self, bool some=True) -> (Tensor Q, Tensor R)


// aten::geqrf(Tensor self) -> (Tensor a, Tensor tau)


// aten::orgqr(Tensor self, Tensor input2) -> Tensor


// aten::ormqr(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False) -> Tensor


// aten::lu_solve(Tensor self, Tensor LU_data, Tensor LU_pivots) -> Tensor


// aten::multinomial(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None) -> Tensor


// aten::lgamma_(Tensor(a!) self) -> Tensor(a!)


// aten::lgamma(Tensor self) -> Tensor


// aten::digamma(Tensor self) -> Tensor


// aten::polygamma(int n, Tensor self) -> Tensor


// aten::polygamma_(Tensor(a!) self, int n) -> Tensor(a!)


// aten::erfinv(Tensor self) -> Tensor


// aten::erfinv_(Tensor(a!) self) -> Tensor(a!)


// aten::i0(Tensor self) -> Tensor


// aten::i0_(Tensor(a!) self) -> Tensor(a!)


// aten::sign(Tensor self) -> Tensor


// aten::sign_(Tensor(a!) self) -> Tensor(a!)


// aten::signbit(Tensor self) -> Tensor


// aten::dist(Tensor self, Tensor other, Scalar p=2) -> Tensor


// aten::atan2_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::atan2(Tensor self, Tensor other) -> Tensor


// aten::arctan2(Tensor self, Tensor other) -> Tensor


// aten::arctan2_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::lerp.Scalar(Tensor self, Tensor end, Scalar weight) -> Tensor


// aten::lerp.Tensor(Tensor self, Tensor end, Tensor weight) -> Tensor


// aten::histc(Tensor self, int bins=100, Scalar min=0, Scalar max=0) -> Tensor


// aten::histogram.bins_tensor(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor bin_edges)


// aten::histogram.bin_ct(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor bin_edges)


// aten::fmod.Scalar(Tensor self, Scalar other) -> Tensor


// aten::fmod_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::fmod.Tensor(Tensor self, Tensor other) -> Tensor


// aten::fmod_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::hypot(Tensor self, Tensor other) -> Tensor


// aten::hypot_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::igamma(Tensor self, Tensor other) -> Tensor


// aten::igamma_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::igammac(Tensor self, Tensor other) -> Tensor


// aten::igammac_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::nextafter(Tensor self, Tensor other) -> Tensor


// aten::nextafter_(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::remainder.Scalar(Tensor self, Scalar other) -> Tensor


// aten::remainder_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)


// aten::remainder.Tensor(Tensor self, Tensor other) -> Tensor


// aten::remainder_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)


// aten::min(Tensor self) -> Tensor


// aten::fmin(Tensor self, Tensor other) -> Tensor


// aten::max(Tensor self) -> Tensor


// aten::fmax(Tensor self, Tensor other) -> Tensor


// aten::maximum(Tensor self, Tensor other) -> Tensor


// aten::max.other(Tensor self, Tensor other) -> Tensor


// aten::minimum(Tensor self, Tensor other) -> Tensor


// aten::min.other(Tensor self, Tensor other) -> Tensor


// aten::quantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor


// aten::quantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor


// aten::nanquantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor


// aten::nanquantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor


// aten::sort(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)


// aten::sort.stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)


// aten::sort.dimname(Tensor self, Dimname dim, bool descending=False) -> (Tensor values, Tensor indices)


// aten::sort.dimname_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False) -> (Tensor values, Tensor indices)


// aten::msort(Tensor self) -> Tensor


// aten::argsort(Tensor self, int dim=-1, bool descending=False) -> Tensor


// aten::argsort.stable(Tensor self, *, bool stable, int dim=-1, bool descending=False) -> Tensor


// aten::argsort.dimname(Tensor self, Dimname dim, bool descending=False) -> Tensor


// aten::topk(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)


// aten::all(Tensor self) -> Tensor


// aten::any(Tensor self) -> Tensor


// aten::renorm(Tensor self, Scalar p, int dim, Scalar maxnorm) -> Tensor


// aten::renorm_(Tensor(a!) self, Scalar p, int dim, Scalar maxnorm) -> Tensor(a!)


// aten::unfold(Tensor(a) self, int dimension, int size, int step) -> Tensor(a)


// aten::equal(Tensor self, Tensor other) -> bool


// aten::pow.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor


// aten::pow.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor


// aten::pow_.Scalar(Tensor(a!) self, Scalar exponent) -> Tensor(a!)


// aten::pow_.Tensor(Tensor(a!) self, Tensor exponent) -> Tensor(a!)


// aten::float_power.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor


// aten::float_power.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor


// aten::float_power_.Scalar(Tensor(a!) self, Scalar exponent) -> Tensor(a!)


// aten::float_power_.Tensor(Tensor(a!) self, Tensor exponent) -> Tensor(a!)


// aten::normal_(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor(a!)


// aten::alias(Tensor(a) self) -> Tensor(a)


// aten::isfinite(Tensor self) -> Tensor


// aten::isinf(Tensor self) -> Tensor


// aten::record_stream(Tensor(a!) self, Stream s) -> ()


// aten::isposinf(Tensor self) -> Tensor


// aten::isneginf(Tensor self) -> Tensor


// aten::det(Tensor self) -> Tensor


// aten::slogdet(Tensor self) -> (Tensor sign, Tensor logabsdet)


// aten::logdet(Tensor self) -> Tensor


// aten::inverse(Tensor self) -> Tensor


// aten::inner(Tensor self, Tensor other) -> Tensor


// aten::outer(Tensor self, Tensor vec2) -> Tensor


// aten::ger(Tensor self, Tensor vec2) -> Tensor


// aten::to_padded_tensor(Tensor self, float padding, SymInt[]? output_size=None) -> Tensor


// aten::to_padded_tensor(Tensor self, float padding, SymInt[]? output_size=None) -> Tensor

 // namespace at
 // namespace c10




 // namespace at


// Parsed from ATen/core/Tensor.h

// #pragma once

// #include <ATen/core/TensorBody.h>
// #include <c10/util/Exception.h>
// Targeting ../OptionalTensorRef.java







 // namespace at


// Parsed from ATen/core/Formatting.h

// #pragma once

// #include <ostream>
// #include <string>

// #include <c10/core/Scalar.h>
// #include <ATen/core/Tensor.h>


@Namespace("c10") public static native @StdString BytePointer toString(@Const @ByRef Scalar s);


@Namespace("at") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef DeprecatedTypeProperties t);
@Namespace("at") public static native @Cast("std::ostream*") @ByRef Pointer print(
    @Cast("std::ostream*") @ByRef Pointer stream,
    @Const @ByRef Tensor tensor,
    @Cast("int64_t") long linesize);
@Namespace("at") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Tensor t);
@Namespace("at") public static native void print(@Const @ByRef Tensor t, @Cast("int64_t") long linesize/*=80*/);
@Namespace("at") public static native void print(@Const @ByRef Tensor t);



// Parsed from ATen/core/UnsafeFromTH.h

// #pragma once
// #include <ATen/core/Tensor.h>

@Namespace("at") public static native @ByVal Tensor unsafeTensorFromTH(Pointer th_pointer, @Cast("bool") boolean retain);

@Namespace("at") public static native @Cast({"", "c10::Storage&&"}) @StdMove Storage unsafeStorageFromTH(Pointer th_pointer, @Cast("bool") boolean retain);




// Parsed from ATen/core/Variadic.h

// #pragma once

// #include <cstdint>
// #include <tuple>
// #include <type_traits>
// #include <utility>

// #include <c10/util/ArrayRef.h>
// #include <ATen/core/List.h>

// This class allows you to write variadic functions which
// call a (possibly overloaded) function on each argument,
// in order.  This is most commonly used in autogenerated code,
// where it is convenient to have a function that can uniformly
// take arguments of different types.  If your arguments
// are homogenous consider using a std::initializer_list instead.
//
// For examples of this in use, see torch/csrc/utils/variadic.h

 // namespace torch


// Parsed from ATen/core/blob.h

// #pragma once

// #include <cstddef>
// #include <sstream>
// #include <type_traits>
// #include <typeinfo>
// #include <vector>

// #include <c10/util/intrusive_ptr.h>
// #include <c10/util/typeid.h>
// #include <c10/macros/Macros.h>
// Targeting ../Blob.java



@Namespace("caffe2") public static native void swap(@ByRef Blob lhs, @ByRef Blob rhs);

@Namespace("caffe2") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Blob v);

 // namespace caffe2


// Parsed from ATen/core/class_type.h

// #pragma once

// #include <memory>

// #include <ATen/core/ivalue.h>
// #include <ATen/core/jit_type_base.h>
// #include <c10/util/Optional.h>
 // namespace jit
 // namespace torch

// This enumerator represents the 'kind' of an attribute - a buffer, a parameter, or neither.
// This state is mutually exclusive. Buffers and Parameters can only appear on modules.
@Namespace("c10") public enum AttributeKind {
  BUFFER(0),
  PARAMETER(1),
  REGULAR_ATTRIBUTE(2);

    public final int value;
    private AttributeKind(int v) { this.value = v; }
    private AttributeKind(AttributeKind e) { this.value = e.value; }
    public AttributeKind intern() { for (AttributeKind e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../ClassAttribute.java



/**
 * User Defined Types
 */
// Targeting ../ClassType.java






// Parsed from ATen/core/enum_tag.h

// #pragma once

// @generated by torchgen/gen.py from enum_tag.h
    // Enum of valid tags obtained from the entries in tags.yaml
    @Namespace("at") public enum Tag {
        core(0),
        data_dependent_output(1),
        dynamic_output_shape(2),
        generated(3),
        inplace_view(4),
        nondeterministic_bitwise(5),
        nondeterministic_seeded(6),
        pointwise(7),
        view_copy(8);

        public final int value;
        private Tag(int v) { this.value = v; }
        private Tag(Tag e) { this.value = e.value; }
        public Tag intern() { for (Tag e : values()) if (e.value == value) return e; return this; }
        @Override public String toString() { return intern().name(); }
    }



// Parsed from ATen/core/enum_type.h

// #pragma once

// #include <ATen/core/ivalue.h>

// #include <utility>
// Targeting ../EnumType.java



 // namespace c10


// Parsed from ATen/core/type_ptr.h

// #pragma once

// #include <memory>
// #include <type_traits>

// #include <c10/util/Exception.h>
// #include <c10/util/MaybeOwned.h>
// Targeting ../SingletonTypePtr.java


// Targeting ../AnyTypePtr.java


// Targeting ../AnyEnumTypePtr.java


// Targeting ../NumberTypePtr.java


// Targeting ../FloatTypePtr.java


// Targeting ../ComplexTypePtr.java


// Targeting ../IntTypePtr.java


// Targeting ../BoolTypePtr.java


// Targeting ../StringTypePtr.java


// Targeting ../StorageTypePtr.java


// Targeting ../NoneTypePtr.java


// Targeting ../GeneratorTypePtr.java


// Targeting ../QuantizerTypePtr.java


// Targeting ../QSchemeTypePtr.java


// Targeting ../DeviceObjTypePtr.java


// Targeting ../StreamObjTypePtr.java


// Targeting ../CapsuleTypePtr.java


// Targeting ../PyObjectTypePtr.java


// Targeting ../LayoutTypePtr.java


// Targeting ../ScalarTypeTypePtr.java


// Targeting ../AnyListTypePtr.java


// Targeting ../AnyTupleTypePtr.java


// Targeting ../AnyClassTypePtr.java







 // namespace c10


// Parsed from ATen/core/functional.h

// #pragma once

// #include <vector>
// #include <c10/util/ArrayRef.h>

// The passed in function must take T by value (T), or by
// const reference (const T&); taking T by non-const reference
// will result in an error like:
//
//    error: no type named 'type' in 'class std::result_of<foobar::__lambda(T)>'
//
// No explicit template parameters are required.

// Overload for explicit function and ArrayRef

// C++ forbids taking an address of a constructor, so here's a workaround...
// Overload for constructor (R) application

 // namespace c10


// Parsed from ATen/core/ivalue.h

// #pragma once

// #include <ATen/core/DimVector.h>
// #include <ATen/core/TensorBody.h>
// #include <ATen/core/blob.h>
// #include <ATen/core/custom_class.h>
// #include <ATen/core/ivalue_to.h>
// #include <ATen/core/jit_type_base.h>
// #include <ATen/core/type_factory.h>
// #include <c10/core/SymFloat.h>
// #include <c10/macros/Export.h>
// #include <c10/util/C++17.h>
// #include <c10/util/MaybeOwned.h>
// #include <c10/util/intrusive_ptr.h>
// #include <typeindex>
// #include <utility>
// Targeting ../CustomClassHolder.java


 // namespace jit
 // namespace torch
// Targeting ../RRefInterface.java



@Namespace("c10") public static native @Cast("bool") boolean _fastEqualsForContainer(@Const @ByRef IValue lhs, @Const @ByRef IValue rhs);

@Namespace("c10") public static native Function checkObjectSortSchema(
    @Const @SharedPtr @ByRef ClassType t,
    @Cast("std::stringstream*") @ByRef Pointer why_not);

// A comparator that checks ordering of two IValues of same type.

@Namespace("c10") public static native @ByVal @Cast("c10::IValueComparator*") Pointer getLessThanComparator(@Const @ByRef IValue v);
@Namespace("c10") public static native @ByVal @Cast("c10::IValueComparator*") Pointer getGreaterThanComparator(@Const @ByRef IValue v);
// Targeting ../Tuple.java


// Targeting ../Future.java


// Targeting ../Await.java


// Targeting ../ConstantString.java


// Targeting ../Object.java


// Targeting ../PyObjectHolder.java


// Targeting ../EnumHolder.java


// Targeting ../ComplexHolder.java


// Targeting ../StreamData3Holder.java



 // namespace ivalue

// This is an owning wrapper for a c10::optional<std::vector<T>>
// that can be implicitly converted to a (non-owning) optional<ArrayRef<T>>.
// Its purpose is to be used in generated code to keep the vector alive
// either until the end of a statement (as a temporary), or as a saved arg
// in autograd.
// Targeting ../Capsule.java



// IValue is the generic tagged union used by the interpreter to hold
// all value types.
// It is a 16-byte object with an 8-byte payload and an 8-byte tag.
// The tag is currently 4 bytes to determine the type, and 1 byte
// to mark whether that type is a subtype of c10::intrusive_ptr_target and needs
// retain/release calls.


///
///
///
///
///
// #define TORCH_FORALL_TAGS(_)
//   _(None)
//   _(Tensor)
//   _(Storage)
//   _(Double)
//   _(ComplexDouble)
//   _(Int)
//   _(SymInt)
//   _(SymFloat)
//   _(Bool)
//   _(Tuple)
//   _(String)
//   _(Blob)
//   _(GenericList)
//   _(GenericDict)
//   _(Future)
//   _(Await)
//   _(Device)
//   _(Stream)
//   _(Object)
//   _(PyObject)
//   _(Uninitialized)
//   _(Capsule)
//   _(RRef)
//   _(Quantizer)
//   _(Generator)
//   _(Enum)
// Targeting ../IValue.java


// Targeting ../WeakIValue.java


// Targeting ../StrongTypePtr.java


// Targeting ../WeakTypePtr.java


// Targeting ../WeakOrStrongCompilationUnit.java


// Targeting ../WeakOrStrongTypePtr.java




 // namespace c10

// #include <ATen/core/ivalue_inl.h>  // IWYU pragma: keep


// Parsed from ATen/core/ivalue_to.h

// #pragma once

// #include <string>
 // namespace at
// Targeting ../ivalue_to_const_ref_overload_return.java



 // namespace detail
 // namespace c10


// Parsed from ATen/core/operator_name.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Optional.h>
// #include <c10/util/string_view.h>
// #include <string>
// #include <utility>
// #include <ostream>
// Targeting ../OperatorName.java


// Targeting ../OperatorNameView.java







@Namespace("c10") public static native @StdString BytePointer toString(@Const @ByRef OperatorName opName);


 // namespace c10



// Parsed from ATen/core/qualified_name.h

// #pragma once

// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>
// #include <c10/util/StringUtil.h>
// #include <c10/util/irange.h>
// #include <string>
// Targeting ../QualifiedName.java


 // namespace c10
 // namespace std


// Parsed from ATen/core/stack.h

// #pragma once

// #include <type_traits>

// #include <ATen/core/ivalue.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/irange.h>

// TODO move this to c10 namespace
// Targeting ../Operation.java



// An operation with N inputs and M outputs pops the last N inputs off
// the stack and pushes its M inputs onto the stack
// before: <other stack items> I0, I1, ... IN <- stack.back()
// after: <other stack items> O0, O1, ... OM
// operations are defined this way so that ownership of inputs can be
// transferred to the operation and it can incrementally drop ownership of
// tensors when they become unneeded. For large operations, like 'run an entire
// subgraph', this functionality is very important for minimizing gpu memory
// usage return value is the relative 'offset' to jump to for the next
// operation:
//   pc += 1 + offset
// so a return value of 0 goes to the next instruction

// treat the last N elements of the stack as a list, looking up
// element i
@Namespace("torch::jit") public static native @ByRef IValue peek(@ByRef IValueVector stack, @Cast("size_t") long i, @Cast("size_t") long N);
// treat the last N elements of the stack as a list, looking up the
// slice starting at index i and having length len
@Namespace("torch::jit") public static native @ByVal @Cast("at::ArrayRef<c10::IValue>*") IValueArrayRef peekSlice(
    @Const @ByRef IValueVector stack,
    @Cast("size_t") long i,
    @Cast("size_t") long len,
    @Cast("size_t") long N);
@Namespace("torch::jit") public static native @ByVal @Cast("at::ArrayRef<c10::IValue>*") IValueArrayRef last(@Const @ByRef IValueVector stack, @Cast("size_t") long N);
@Namespace("torch::jit") public static native void drop(@ByRef IValueVector stack, @Cast("size_t") long n);
@Namespace("torch::jit") public static native @ByVal IValue pop(@ByRef IValueVector stack);
@Namespace("torch::jit") public static native @ByVal IValueVector pop(@ByRef IValueVector stack, @Cast("size_t") long n);

// variadic pop:
// int64_t a; at::Tensor b;
// pop(stack, a, b);
// equivalent to:
// b = pop(stack).toTensor();
// a = pop(stack).toInt();

@Namespace("torch::jit") public static native void push_one(@ByRef IValueVector stack, @ByVal TensorOptions options);

// The packer here is carefully written not to make any unnecessary
// copies.

// pack takes the return values of aten functions pushes them onto the stack

 // namespace jit
 // namespace torch


// Parsed from ATen/core/alias_info.h

// #pragma once
// #include <unordered_set>
// #include <vector>
// #include <ATen/core/symbol.h>
// #include <c10/util/Exception.h>
// #include <c10/util/hash.h>
// Targeting ../AliasInfo.java





// this does match the way things are represented in the schema

 // namespace c10



// Parsed from ATen/core/jit_type_base.h

// #pragma once

// #include <functional>
// #include <memory>
// #include <string>
// #include <utility>

// #include <ATen/core/qualified_name.h>
// #include <ATen/core/type_ptr.h>
// #include <c10/core/SymInt.h>
// #include <c10/core/SymFloat.h>
// #include <c10/core/SymIntArrayRef.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Optional.h>

// #define C10_FORALL_TYPES(_)
//   _(AnyType)
//   _(EnumType)
//   _(AnyEnumType)
//   _(TensorType)
//   _(StorageType)
//   _(TupleType)
//   _(ListType)
//   _(DictType)
//   _(NumberType)
//   _(FloatType)
//   _(ComplexType)
//   _(FutureType)
//   _(AwaitType)
//   _(RRefType)
//   _(IntType)
//   _(NoneType)
//   _(StringType)
//   _(GeneratorType)
//   _(QuantizerType)
//   _(BoolType)
//   _(OptionalType)
//   _(VarType)
//   _(DeviceObjType)
//   _(StreamObjType)
//   _(FunctionType)
//   _(ClassType)
//   _(PyObjectType)
//   _(CapsuleType)
//   _(InterfaceType)
//   _(QSchemeType)
//   _(ScalarTypeType)
//   _(LayoutType)
//   _(MemoryFormatType)
//   _(AnyListType)
//   _(AnyTupleType)
//   _(AnyClassType)
//   _(SymIntType)
//   _(SymFloatType)
//   _(UnionType)
//   _(DynamicType)

@Namespace("c10") public enum TypeKind {
  AnyType(0),
  EnumType(1),
  AnyEnumType(2),
  TensorType(3),
  StorageType(4),
  TupleType(5),
  ListType(6),
  DictType(7),
  NumberType(8),
  FloatType(9),
  ComplexType(10),
  FutureType(11),
  AwaitType(12),
  RRefType(13),
  IntType(14),
  NoneType(15),
  StringType(16),
  GeneratorType(17),
  QuantizerType(18),
  BoolType(19),
  OptionalType(20),
  VarType(21),
  DeviceObjType(22),
  StreamObjType(23),
  FunctionType(24),
  ClassType(25),
  PyObjectType(26),
  CapsuleType(27),
  InterfaceType(28),
  QSchemeType(29),
  ScalarTypeType(30),
  LayoutType(31),
  MemoryFormatType(32),
  AnyListType(33),
  AnyTupleType(34),
  AnyClassType(35),
  SymIntType(36),
  SymFloatType(37),
  UnionType(38),
  DynamicType(39);

    public final int value;
    private TypeKind(int v) { this.value = v; }
    private TypeKind(TypeKind e) { this.value = e.value; }
    public TypeKind intern() { for (TypeKind e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") public static native @Cast("const char*") BytePointer typeKindToString(TypeKind kind);
@Namespace("c10") public static native String typeKindToString(@Cast("c10::TypeKind") int kind);

// Use this to customize how a Type is printed using `annotation_str()`. If
// c10::nullopt is returned, `annotation_str()` falls through to its default
// implementation.
 // namespace detail
// #define TORCH_DECLARE_SINGLETON(Type)
//   struct Type;
//   namespace detail {
//   template <> struct IsSingletonType<Type> : public std::integral_constant<bool, true> {};
//   }
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

// Targeting ../Type.java
























// Explicitly enable MaybeOwned<shared_ptr<T>>, rather than allowing
// MaybeOwned to be used for any type right away.
// Targeting ../SharedType.java







// Targeting ../NamedType.java



 // namespace c10
 // namespace std


// Parsed from ATen/core/jit_type.h

// #pragma once

// #include <ATen/core/custom_class.h>
// #include <ATen/core/jit_type_base.h>
// #include <ATen/core/TensorBody.h>
// #include <ATen/core/functional.h>
// #include <ATen/core/symbol.h>
// #include <ATen/core/type_factory.h>
// #include <ATen/core/qualified_name.h>
// #include <c10/util/TypeList.h>
// #include <c10/util/Optional.h>
// #include <c10/core/SymFloat.h>

// #include <array>
// #include <memory>
// #include <ostream>
// #include <sstream>
// #include <type_traits>
// #include <utility>
 // namespace jit
 // namespace torch




@Namespace("c10") public static native @Cast("bool") boolean is_contiguous_strides(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides);
@Namespace("c10") public static native @Cast("bool") boolean is_contiguous_strides(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... strides);
// Targeting ../AnyType.java



@Namespace("c10") public static native @StdString BytePointer toString(@Const @ByRef Type type);

// Shim for compatibility with code that uses TypePtr.
@Namespace("c10") public static native @StdString BytePointer toString(@Const @ByRef Type.TypePtr typePtr);


// Targeting ../AwaitSingleElementType.java


// Targeting ../ListSingleElementType.java


// Targeting ../RRefSingleElementType.java


// Targeting ../FutureSingleElementType.java


// Targeting ../OptionalSingleElementType.java


// Targeting ../UnionType.java


// Targeting ../OptionalType.java


// Targeting ../Stride.java



@Namespace("c10") public static native @ByVal StrideOptional merge_primitive(
    @Const @ByRef StrideOptional a,
    @Const @ByRef StrideOptional b);
// Targeting ../ShapeSymbol.java



@Namespace("c10") public static native @ByVal ShapeSymbol merge_primitive(
    @Const @ByRef ShapeSymbol a,
    @Const @ByRef ShapeSymbol b);
// Targeting ../SymbolicShape.java


@Namespace("c10::detail") public static native @Cast("bool") boolean isComplete(@Const @ByRef Stride s);

// Targeting ../LongVaryingShape.java


// Targeting ../StrideVaryingShape.java


// TODO: investigate making this SingletonOrSharedTypePtr<TensorType>
// Targeting ../TensorType.java


// Targeting ../ListType.java


// Targeting ../DictType.java


// Targeting ../FutureType.java


// Targeting ../AwaitType.java


// Targeting ../RRefType.java



// Any should never appear in a named type like a class, namedtuple or
// interface. If it does, then dynamic type information will be lost in the
// Pickler, leading to hard-to-track-down bugs that will only occur
// after saving or loading a model. This is because we rely on the
// static types in named types to reconstruct type tags of loaded
// values. Lifting this restriction requires solving the serialization
// problem first.
@Namespace("c10") public static native void checkNoAny(
    @Const @ByRef Type base,
    @Cast("const char*") BytePointer what,
    @StdString BytePointer attrname,
    @Const @ByRef Type.TypePtr attrtype);
@Namespace("c10") public static native void checkNoAny(
    @Const @ByRef Type base,
    String what,
    @StdString String attrname,
    @Const @ByRef Type.TypePtr attrtype);
// Targeting ../TupleType.java



// the common supertype of all Enums, only used in operator registraion.
// EnumType <: AnyEnumType for all Enums
// Targeting ../AnyEnumType.java


// Targeting ../NumberType.java


// Targeting ../FloatType.java


// Targeting ../ComplexType.java



// We need to introduce `SymIntType` to represent the `SymInt` type
// used in function schemas e.g. `aten::narrow_copy(... SymInt length)
// `SymInt` will be used to enable tracing arithmetic operations on
// dimension values. Please see [SymInt.h] for more information
// Targeting ../SymIntType.java


// Targeting ../SymFloatType.java


// Targeting ../IntType.java


// Targeting ../BoolType.java


// Targeting ../StringType.java


// Targeting ../StorageType.java


// Targeting ../FunctionType.java


// Targeting ../NoneType.java


// Targeting ../GeneratorType.java


// Targeting ../QuantizerType.java


// Targeting ../QSchemeType.java


// Targeting ../DeviceObjType.java


// Targeting ../StreamObjType.java


// Targeting ../VarType.java


// Targeting ../CapsuleType.java


// Targeting ../PyObjectType.java



@Namespace("c10") public enum TypeVerbosity {
  None(0),
  Type(1),
  TypeAndStride(2),
  Full(3),
  Symbolic(4),
  Default(Full.value);

    public final int value;
    private TypeVerbosity(int v) { this.value = v; }
    private TypeVerbosity(TypeVerbosity e) { this.value = e.value; }
    public TypeVerbosity intern() { for (TypeVerbosity e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") public static native TypeVerbosity type_verbosity();






// what is the type, ignoring extra size/shape information?
// e.g. Tensor(2x3) -> Dynamic, and Tuple(Tensor(2x3),...) -> Tuple(Dynamic,...)

// `unshapedType` is used to remove Tensor subtypes. We treat all Tensor
// subtypes as simply "Tensor"; we also create a new version of any
// container types in which internal Tensors have undergone the same
// operation. This is used for type comparisons between two Tensor types
// (`unshapedType` means that we don't falsely return `false` for e.g.
// Tensors of different dimensions). It's also used in the alias
// analysis pass.
// Be careful with calls because this can be very slow. If calling this
// on a graph, use `EraseShapeInformation` in shape_analysis.h
@Namespace("c10") public static native @ByVal Type.TypePtr unshapedType(@Const @ByRef Type.TypePtr type);




@Namespace("c10") public static native @ByVal ScalarTypeOptional tryScalarTypeFromJitType(@Const @ByRef Type type);

@Namespace("c10") public static native ScalarType scalarTypeFromJitType(@Const @ByRef Type type);

// Attempt to find the correct supertype of the two types `t1` and `t2`.
// If no supertype is found, then nullopt will be returned if
// `default_to_union` is false, and `Union[t1, t2]` will be returned
// if it is true. If `t1 == t2`, or `t1` is a type refinement of `t2`,
// then `t2` will be returned (and vice versa).
//
// Two different tensortypes will return dynamic.
//
// Currently we chose not to support returning a NumberType for
// two types from the set of {FloatType, IntType, ComplexType}, because
// there is a lack of operator support for NumberType.
//
// If `type_hint` is an `InterfaceType`, then we can use that as a
// potential supertype for `ClassType`s in the list. Otherwise, we have
// no way to find and use some common interface type
@Namespace("c10") public static native @ByVal TypePtrOptional unifyTypes(
    @Const @ByRef Type.TypePtr t1,
    @Const @ByRef Type.TypePtr t2,
    @Cast("bool") boolean default_to_union/*=false*/,
    @ByVal(nullValue = "c10::TypePtr(nullptr)") Type.TypePtr type_hint);
@Namespace("c10") public static native @ByVal TypePtrOptional unifyTypes(
    @Const @ByRef Type.TypePtr t1,
    @Const @ByRef Type.TypePtr t2);

@Namespace("c10") public static native @ByVal TypePtrOptional unifyTypeList(
    @ByVal TypeArrayRef elements,
    @Cast("std::ostream*") @ByRef Pointer why_not,
    @Cast("bool") boolean default_to_union/*=false*/,
    @ByVal(nullValue = "c10::TypePtr(nullptr)") Type.TypePtr type_hint);
@Namespace("c10") public static native @ByVal TypePtrOptional unifyTypeList(
    @ByVal TypeArrayRef elements,
    @Cast("std::ostream*") @ByRef Pointer why_not);
// Targeting ../getTypePtr_.java


 // namespace detail
// Targeting ../MatchTypeReturn.java



// attempt to match the type variables in formal to actual, adding them to type_env.
// If no match is possible this returns a MatchTypeReturn with r.success() == false
// and a r.reason() that describes why it could not match.
// note: It is possible to successfully match a formal, but for type variables
// in the formal to still not be defined. In particular, None matches Optional[T]
// but does not define the value of T.
@Namespace("c10") public static native @ByVal MatchTypeReturn matchTypeVariables(@Const @ByRef Type.TypePtr formal, @Const @ByRef Type.TypePtr actual, @ByRef TypeEnv type_env);

// replace type variables appearing in `type` with the values in
// `type_env`. Returns nullptr if a variable used in `type`
// does not appear in `type_env`
@Namespace("c10") public static native @ByVal Type.TypePtr tryEvalTypeVariables(@Const @ByRef Type.TypePtr type, @ByRef TypeEnv type_env);

@Namespace("c10") public static native @Cast("bool") boolean elementTypeCanBeInferredFromMembers(@Const @ByRef Type.TypePtr elem_type);
// Targeting ../InterfaceType.java


// Targeting ../LayoutEnumerationType.java


// Targeting ../ScalarTypeEnumerationType.java


// Targeting ../MemoryFormattEnumerationType.java



// WARNING: These enumeration types below DO NOT actually get parsed out
// from the logical schema strings, instead they are mapped as ints.  To
// observe these types, use real_type() instead of type() on Argument
// Targeting ../ScalarTypeType.java


// Targeting ../MemoryFormatType.java


// Targeting ../LayoutType.java


 // namespace detail

// the common supertype of all lists,
// List[T] <: AnyList for all T
// Targeting ../AnyListType.java



// the common supertype of all tuples,
// Tuple[T...] <: AnyTuple for all T
// Targeting ../AnyTupleType.java



// the common supertype of all classes,
// ClassType <: AnyClassType for all classes
// Targeting ../AnyClassType.java








// Targeting ../InferredType.java



@Namespace("c10") public static native @Cast("bool") boolean containsAnyType(@Const @ByRef Type.TypePtr type);

 // namespace c10


// Parsed from ATen/core/function_schema.h

// #pragma once

// #include <c10/util/StringUtil.h>
// #include <c10/util/string_view.h>
// #include <c10/util/irange.h>
// #include <ATen/core/jit_type.h>
// #include <ATen/core/symbol.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/alias_info.h>
// #include <ATen/core/operator_name.h>
// #include <ATen/core/dispatch/OperatorOptions.h>
// #include <unordered_map>

// schema as used in the compiler for resolving function calls and reporting
// errors. These objects should be constructed from C10 schema once those
// are available.


// Targeting ../Argument.java







@Namespace("c10") public enum SchemaArgType { input(0), output(1);

    public final int value;
    private SchemaArgType(int v) { this.value = v; }
    private SchemaArgType(SchemaArgType e) { this.value = e.value; }
    public SchemaArgType intern() { for (SchemaArgType e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../SchemaArgument.java




// Targeting ../FunctionSchema.java







// print out Argument, which is compatible with FunctionSchema parser
// full format: Type(alias)? name=default_value




@Namespace("c10") public static native @StdString BytePointer toString(@Const @ByRef FunctionSchema schema);

 // namespace c10
 // namespace std


// #include <ATen/core/function_schema_inl.h>  // IWYU pragma: keep


// Parsed from ATen/core/function.h

// #pragma once

// #include <ATen/core/function_schema.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/qualified_name.h>
// #include <c10/util/Exception.h>
// #include <c10/util/FunctionRef.h>

@Namespace("at") public static native void launch(@ByVal Func func);


// Targeting ../RecursiveMethodCallError.java



@Namespace("torch::jit") public static native void preoptimizeGraph(@SharedPtr @ByRef Graph graph, @Cast("bool") boolean disable_autocast/*=false*/);
@Namespace("torch::jit") public static native void preoptimizeGraph(@SharedPtr @ByRef Graph graph);
// Targeting ../Function.java


 // namespace jit
 // namespace torch


// Parsed from ATen/core/boxing/KernelFunction.h

// #pragma once

// #include <ATen/core/ATen_fwd.h>
// #include <ATen/core/boxing/BoxedKernel.h>
// #include <ATen/core/stack.h>
// #include <c10/core/DispatchKeySet.h>
// #include <c10/util/intrusive_ptr.h>
// #include <c10/util/TypeList.h>
// Targeting ../OperatorKernel.java


// Targeting ../KernelFunction.java





// #include <ATen/core/boxing/KernelFunction_impl.h>


// Parsed from ATen/core/dispatch/CppSignature.h

// #pragma once

// #include <typeindex>
// #include <c10/core/DispatchKeySet.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/Metaprogramming.h>
// #include <c10/util/Type.h>
// Targeting ../CppSignature.java









// Parsed from ATen/core/dispatch/DispatchKeyExtractor.h

// #pragma once

// #include <cstdint>
// #include <ATen/core/function_schema.h>
// #include <ATen/core/jit_type.h>
// #include <c10/util/Bitset.h>
// #include <c10/core/DispatchKeySet.h>
// #include <c10/util/irange.h>
// #include <ATen/core/Variadic.h>
// #include <ATen/core/stack.h>

// Take a DispatchKeySet for a Tensor and determine what the actual dispatch
// DispatchKey should be, taking into account TLS, and skipping backends which
// fall through.
//
// Unlike Tensor::key_set(), the value of this on a tensor can change depending
// on TLS.
//
// NB: If there is no valid dispatch key, this will return Undefined
@Namespace("c10::impl") public static native @ByVal DispatchKeySet computeDispatchKeySet(
    @ByVal DispatchKeySet ks,
    @ByVal DispatchKeySet key_mask
);


  // A small gadget to extract the DispatchKeySet from types which are known
  // to have it.  Used to extract dispatch keys from unboxed calls.

  // NB: take by const reference (Don't do universal forwarding here! You
  // don't want to move into this function!)

// Targeting ../DispatchKeyExtractor.java






// Parsed from ATen/core/dispatch/RegistrationHandleRAII.h

// #pragma once

// #include <functional>
// Targeting ../RegistrationHandleRAII.java






// Parsed from ATen/core/dispatch/OperatorOptions.h

// #pragma once

// #include <cstdint>

@Namespace("c10") public enum AliasAnalysisKind {
  INTERNAL_SPECIAL_CASE((byte)(0)),
  CONSERVATIVE((byte)(1)), // The most conservative alias analysis type, assumes
                // side-effects. This is the default analysis.
  FROM_SCHEMA((byte)(2)),
  PURE_FUNCTION((byte)(3));

    public final byte value;
    private AliasAnalysisKind(byte v) { this.value = v; }
    private AliasAnalysisKind(AliasAnalysisKind e) { this.value = e.value; }
    public AliasAnalysisKind intern() { for (AliasAnalysisKind e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

// #if !defined(_MSC_VER)
@Namespace("c10") public static native @Cast("const char*") BytePointer toString(AliasAnalysisKind aliasAnalysisKind);

 // namespace c10


// Parsed from ATen/core/dispatch/OperatorEntry.h

// #pragma once

// #include <ATen/core/function_schema.h>
// #include <c10/util/Metaprogramming.h>
// #include <c10/util/flat_hash_map.h>
// #include <c10/util/either.h>
// #include <c10/util/Optional.h>
// #include <c10/core/DispatchKey.h>
// #include <c10/core/PyHandleCache.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/boxing/KernelFunction.h>
// #include <ATen/core/dispatch/DispatchKeyExtractor.h>

// #include <ATen/core/dispatch/OperatorOptions.h>
// #include <ATen/core/dispatch/CppSignature.h>
// #include <ATen/core/dispatch/RegistrationHandleRAII.h>
// #include <ATen/core/enum_tag.h>

// #include <list>
// #include <array>

// #ifdef C10_MOBILE
// #endif

// This data structure represents a kernel that was registered to us from a
// user.  Unlike KernelFunction, AnnotatedKernel contains some extra metadata
// about the kernel that isn't necessary for actual dispatching (this is why
// we don't put AnnotatedKernel in the actual DispatchTable), but is useful for
// giving good error messages.
// Targeting ../AnnotatedSchema.java



// Internal data structure that records information about a specific operator.
// It's not part of the public API; typically, users will interact with
// OperatorHandle instead.
//
// Concurrent writes to OperatorEntry are protected by the GLOBAL Dispatcher
// lock (this is important because some methods in OperatorEntry access
// dispatcher state)

 // namespace impl
 // namespace c10


// Parsed from ATen/core/dispatch/Dispatcher.h

// #pragma once

// #include <ATen/SequenceNumber.h>
// #include <ATen/core/boxing/KernelFunction.h>
// #include <ATen/core/boxing/impl/boxing.h>
// #include <ATen/core/dispatch/OperatorEntry.h>
// #include <ATen/core/dispatch/CppSignature.h>
// #include <ATen/core/dispatch/RegistrationHandleRAII.h>
// #include <ATen/record_function.h>
// #include <c10/util/Exception.h>
// #include <c10/util/LeftRight.h>
// #include <list>
// #include <mutex>
// #include <condition_variable>
// #include <type_traits>

// #include <ATen/core/grad_mode.h>
// #include <ATen/core/enum_tag.h>

@Namespace("c10") public static native @Cast("bool") boolean show_dispatch_trace();
@Namespace("c10") public static native void dispatch_trace_nesting_incr();
@Namespace("c10") public static native void dispatch_trace_nesting_decr();
@Namespace("c10") public static native @Cast("int64_t") long dispatch_trace_nesting_value();
// Targeting ../DispatchTraceNestingGuard.java


// Targeting ../OpRegistrationListener.java


// Targeting ../RegistrationListenerList.java



// Targeting ../SchemaRegistrationHandleRAII.java


// Targeting ../Dispatcher.java


// Targeting ../OperatorHandle.java



/**
 * This is a handle to an operator schema registered with the dispatcher.
 * It holds the same information as an OperatorHandle, but it is templated
 * on the operator arguments and allows calling the operator in an
 * unboxed way.
 */

// CaptureKernelCall is intended to capture return values from Dispatcher
// unboxed kernel calls. A record function may request to get outputs from the
// kernel calls. For boxed kernels, it's straightforward, the returned values
// are in the stack object. The stack can be passed to record functions. For
// unboxed kernels, we need to handle different kinds of return values, cache
// them temporarily, then release the values for the actual function call
// return.

// Handle the lvalue reference differently since it should not be moved.


// Handle case where the kernel returns void.

 // namespace detail

// See [Note: Argument forwarding in the dispatcher] for why Args doesn't use &&


// See [Note: Argument forwarding in the dispatcher] for why Args doesn't use &&


// See [Note: Argument forwarding in the dispatcher] for why Args doesn't use &&




// NB: this doesn't count as a "true" dispatcher jump, so no instrumentation




 // namespace c10

 // namespace std


// Parsed from ATen/core/op_registration/op_allowlist.h

// #pragma once

// TODO: unify to C10_MOBILE. In theory this header could be used in OSS.
// #ifdef TEMPLATE_SELECTIVE_BUILD
// #include <ATen/selected_mobile_ops.h>
// #endif

/**
 * This header implements functionality to build PyTorch with only a certain
 * set of operators (+ dependencies) included.
 *
 * - Build with -DTORCH_OPERATOR_WHITELIST="aten::add;aten::sub" and only these
 *   two ops will be included in your build.  The allowlist records operators
 *   only, no overloads; if you include aten::add, all overloads of aten::add
 *   will be included.
 *
 * Internally, this is done by removing the operator registration calls
 * using compile time programming, and the linker will then prune all
 * operator functions that weren't registered.
 * See Note [Selective build] for more details
 *
 * WARNING: The allowlist mechanism doesn't work for all ways you could go about
 * registering an operator.  If the dispatch key / operator name is not
 * sufficiently obvious at compile time, then the allowlisting mechanism
 * will fail (and the operator will be included in the binary anyway).
 */

// #include <c10/util/string_view.h>
// #include <c10/core/DispatchKey.h>
// #include <c10/macros/Macros.h>


// #if defined(ENABLE_RECORD_KERNEL_FUNCTION_DTYPE)
// #include <ATen/record_function.h>
// #endif

@Namespace("c10::impl") public static native @Cast("const bool") boolean allowlist_contains(@ByVal @Cast("c10::string_view*") Pointer allowlist, @ByVal @Cast("c10::string_view*") Pointer item);  // Forward Declare

/**
 * In selective build mode returns true/false depending on whether a build
 * feature is available or not.
 *
 * In instrumenting mode (tracing mode), always returns true, and doesn't
 * trigger any side effects.
 */
@Namespace("c10::impl") public static native @Cast("const bool") boolean is_build_feature_available(@Cast("const char*") BytePointer name);
@Namespace("c10::impl") public static native @Cast("const bool") boolean is_build_feature_available(String name);



/**
 * Use BUILD_FEATURE_REQUIRED macro in user-code.
 *
 * In selective build mode becomes a no-op if the build feature passed
 * in is available. If not available, throws an exception (c10::Error).
 * The compiler is able to perform dead code elimination for code
 * following this method if the build feature is not available.
 *
 * In instrumenting mode (tracing mode), registers (as a side effect)
 * the presence of this specific build feature being triggered.
 */
// #if !defined(ENABLE_RECORD_KERNEL_FUNCTION_DTYPE)  // selective build mode

// #if defined(TORCH_BUILD_FEATURE_ALLOWLIST)
// #define BUILD_FEATURE_REQUIRED(NAME)
//   if (!c10::impl::is_build_feature_available(NAME)) {
//     ::c10::impl::build_feature_required_feature_not_available(NAME);
//   }
// #else  // Everything trivially selected
// #define BUILD_FEATURE_REQUIRED(NAME)

// #endif

// #else  // trace mode
// #define BUILD_FEATURE_REQUIRED(NAME)
//   RECORD_FUNCTION_WITH_SCOPE(
//       at::RecordScope::BUILD_FEATURE,
//       std::string(NAME),
//       {});
// #endif

// Use this macro, and not is_build_feature_available
// #define BUILD_FEATURE_AVAILABLE(NAME) ::c10::impl::is_build_feature_available(NAME)

// returns true iff allowlist contains item
// allowlist_contains("a;bc;d", "bc") == true

// Returns true iff the given op name is on the allowlist
// and should be registered
@Namespace("c10::impl") public static native @Cast("const bool") boolean op_allowlist_check(@ByVal @Cast("c10::string_view*") Pointer op_name);

// Returns true iff the given schema string is on the allowlist
// and should be registered
@Namespace("c10::impl") public static native @Cast("const bool") boolean schema_allowlist_check(@ByVal @Cast("c10::string_view*") Pointer schema);

// Returns true iff the given custom class name is on the allowlist
// and should be registered
@Namespace("c10::impl") public static native @Cast("const bool") boolean custom_class_allowlist_check(@ByVal @Cast("c10::string_view*") Pointer custom_class_name);

// schema_allowlist_check() implicitly depends on a macro, TORCH_OPERATOR_WHITELIST.
// Add this API to pass arbitrary allowlist.
@Namespace("c10::impl") public static native @Cast("const bool") boolean op_allowlist_contains_name_in_schema(@ByVal @Cast("c10::string_view*") Pointer allowlist, @ByVal @Cast("c10::string_view*") Pointer schema);

// Returns true iff the given dispatch key is on the allowlist
// and should be registered.  When we turn this on, the list of valid
// mobile dispatch keys is hard coded (but you need to make sure
// that you have the correct set of dispatch keys for this).
@Namespace("c10::impl") public static native @Cast("const bool") boolean dispatch_key_allowlist_check(DispatchKey arg0);
@Namespace("c10::impl") public static native @Cast("const bool") boolean dispatch_key_allowlist_check(@Cast("c10::DispatchKey") short arg0);

 // namespace impl
 // namespace c10


// Parsed from ATen/record_function.h

// #pragma once

// #include <ATen/core/ivalue.h>
// #include <ATen/core/operator_name.h>
// #include <c10/macros/Export.h>
// #include <c10/util/Optional.h>
// #include <c10/util/SmallVector.h>
// #include <c10/util/variant.h>

// #include <array>
// #include <atomic>
// #include <functional>
// #include <memory>


// Kind of record function scope;
@Namespace("at") public enum RecordScope {
  // c10/ATen ops, autograd nodes
  FUNCTION((byte)(0)),
  // Functions/nodes called from the autograd
  BACKWARD_FUNCTION((byte)(1)),
  // TorchScript functions, methods
  TORCHSCRIPT_FUNCTION((byte)(2)),
  // Kernel Function dtype Tag
  KERNEL_FUNCTION_DTYPE((byte)(3)),
  // Torchbind custom class,
  CUSTOM_CLASS((byte)(4)),
  // Generic Build Feature
  BUILD_FEATURE((byte)(5)),
  // Kernel Function dtype Tag
  LITE_INTERPRETER((byte)(6)),
  // User defined scope (e.g. with record_function())
  USER_SCOPE((byte)(7)),
  // Scopes for static runtime, a specialized TorchScript interpreter
  STATIC_RUNTIME_OP((byte)(8)),
  STATIC_RUNTIME_MODEL((byte)(9)),
  NUM_SCOPES((byte)(10));// must be the last in the list

    public final byte value;
    private RecordScope(byte v) { this.value = v; }
    private RecordScope(RecordScope e) { this.value = e.value; }
    public RecordScope intern() { for (RecordScope e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

 // namespace at

// Targeting ../StringView.java



// Soft limit on the number of callbacks to use;
@Namespace("at") @MemberGetter public static native @Cast("const std::size_t") long kSoftLimitCallbacks();
// Targeting ../ObserverContext.java



//
// PyTorch callbacks/observers API:
//

/**
 * RecordFunctionCallback represents a pair of callbacks to be used with
 * RecordFunction, members:
 *   start, end - the callbacks to run when entering and exiting the scope;
 *     optionally, the start callback may return an ObserverContext which will
 *     be passed to the end callback, use appropriate constructor accordingly.
 *   needs_inputs - whether the callbacks need the inputs passed from the
 * observed function/range; NOTE: passing the inputs incurs an additional
 * overhead; sampling_probability - if not 1.0, then the callback is
 * probabilistically sampled to run; NOTE: start and end callbacks always run as
 * a pair and are sampled together; scopes - types of scopes to execute the
 * callbacks on (see RecordScope); passing empty set means the callbacks will be
 * executed for all possible scope types should_run - optional function that
 * returns whether this callback should run; overwrites the effect of setting
 * sampling_probability
 */

// Notes:
//  - two types of callbacks are provided: thread local and global
//     - thread local callbacks are added/removed only for the given thread
//       and are stored locally for each thread and separately from the list
//       of the global callbacks
//     - global callbacks are stored in a single per process list and are
//       invoked by every RecordFunction, in addition to the thread local
//       callbacks specific to the given thread
//  - we allow the added callbacks to be sampled, by specifying a sampling
//    probability for each callback pair, if the start callback is
//    not picked to run, the corresponding end callback won't be called
//  - a typical use case for the global callbacks is passive monitoring
//    in the background (e.g. fleet-wide monitoring), without focusing on
//    the specific piece of code
//  - in contrast, thread local callbacks are enabled locally, on demand,
//    for the specific piece of code (range) and are not sampled
//  - a typical use case for thread local callbacks is profiler and code
//    execution tracer
//  - note, thread local callbacks are automatically propagated with
//    ThreadLocalState across JIT continuations and async tasks (at::launch)

@Namespace("at") @MemberGetter public static native @Cast("const at::CallbackHandle") long INVALID_CALLBACK_HANDLE();
// Targeting ../RecordFunctionCallbacksEntry.java



// Holds pairs (callbacks, unique_id)
// Targeting ../RecordFunction.java



@Namespace("at") public static native @ByVal @Cast("at::StepCallbacks*") Pointer getStepCallbacks(RecordScope scope);
@Namespace("at") public static native @ByVal @Cast("at::StepCallbacks*") Pointer getStepCallbacks(@Cast("at::RecordScope") byte scope);

@Namespace("at") public static native @ByVal @Cast("c10::optional<at::StepCallbacks>*") Pointer getStepCallbacksUnlessEmpty(
    RecordScope scope);
@Namespace("at") public static native @ByVal @Cast("c10::optional<at::StepCallbacks>*") Pointer getStepCallbacksUnlessEmpty(
    @Cast("at::RecordScope") byte scope);

 // namespace detail

// optional argument - function's seq_no
// #define RECORD_FUNCTION_WITH_SCOPE(scope, fn, inputs, ...)
//   at::RecordFunction guard(scope);
//   if (guard.isActive()) {
//     ::at::detail::record_function_with_scope(
//         guard, fn, inputs, ##__VA_ARGS__);
//   }

// #define RECORD_FUNCTION_WITH_SCOPE_INPUTS_OUTPUTS(
//     scope, fn, inputs, outputs, ...)
//   at::RecordFunction guard(scope);
//   if (guard.isActive()) {
//     if (guard.needsInputs()) {
//       guard.before(fn, inputs, ##__VA_ARGS__);
//     } else {
//       guard.before(fn, ##__VA_ARGS__);
//     }
//     if (guard.needsOutputs()) {
//       guard.setOutputs(outputs);
//     }
//   }

// #define RECORD_FUNCTION(fn, inputs, ...)
//   RECORD_FUNCTION_WITH_SCOPE(
//       at::RecordScope::FUNCTION, fn, inputs, ##__VA_ARGS__)

// #define RECORD_TORCHSCRIPT_FUNCTION(mn, inputs)
//   RECORD_FUNCTION_WITH_SCOPE(at::RecordScope::TORCHSCRIPT_FUNCTION, mn, inputs)

// #define RECORD_FUNCTION_WITH_INPUTS_OUTPUTS(fn, inputs, outputs, ...)
//   RECORD_FUNCTION_WITH_SCOPE_INPUTS_OUTPUTS(
//       at::RecordScope::FUNCTION, fn, inputs, outputs, ##__VA_ARGS__)

// Custom user scopes in C++; similar to Python's 'with record_function("..."):'
// #define RECORD_USER_SCOPE(fn)
//   RECORD_FUNCTION_WITH_SCOPE(
//       at::RecordScope::USER_SCOPE, fn, c10::ArrayRef<const c10::IValue>{})

// RECORD_USER_SCOPE with inputs
// #define RECORD_USER_SCOPE_WITH_INPUTS(fn, inputs)
//   RECORD_FUNCTION_WITH_SCOPE(at::RecordScope::USER_SCOPE, fn, inputs)

// Helper macro to pass in debug handle that is used to
// post process events
// #define RECORD_WITH_SCOPE_DEBUG_HANDLE_AND_INPUTS(
//     scope, fn, debug_handle, inputs, ...)
//   at::RecordFunction guard(scope);
//   if (guard.isActive()) {
//     ::at::detail::record_function_with_scope_and_debug_handle(
//         guard, fn, debug_handle, inputs, ##__VA_ARGS__);
//   }

// Helper macros to record LITE INTERPETER scope events with debug handles
// #define RECORD_EDGE_SCOPE_WITH_DEBUG_HANDLE_AND_INPUTS(
//     fn, debug_handle, inputs)
//   RECORD_WITH_SCOPE_DEBUG_HANDLE_AND_INPUTS(
//       at::RecordScope::LITE_INTERPRETER, fn, debug_handle, inputs)

// Bookend to the RECORD_FUNCTION macros.  Use this after the kernel
// launch to let the profiler bind the outputs to the op that produced
// them.  Note that guard is declared by RECORD_FUNCTION so this macro
// needs to be called from the same scope as RECORD_FUNCTION
// #define RECORD_OUTPUTS(outputs)
//   if (guard.needsOutputs()) {
//     guard.setOutputs(
//         std::vector<c10::IValue>(outputs.begin(), outputs.end()));
//   }

/**
 * addThreadLocalCallback adds a thread local callback to run with
 * RecordFunction, returns handle to use with removeThreadLocalCallback
 */
@Namespace("at") public static native @Cast("at::CallbackHandle") long addThreadLocalCallback(@ByVal @Cast("at::RecordFunctionCallback*") Pointer cb);

/**
 * hasThreadLocalCallbacks returns whether there're callbacks registered
 * with addThreadLocalCallback
 */
@Namespace("at") public static native @Cast("bool") boolean hasThreadLocalCallbacks();

/**
 * clearThreadLocalCallbacks removes all thread local callbacks
 */
@Namespace("at") public static native void clearThreadLocalCallbacks();

/**
 * addGlobalCallback adds a global callback to run with RecordFunction:
 *
 * only during the program initialization
 */
@Namespace("at") public static native @Cast("at::CallbackHandle") long addGlobalCallback(@ByVal @Cast("at::RecordFunctionCallback*") Pointer cb);

/**
 * removeCallback removes a callback given the handle returned by
 * addThreadLocalCallback or addGlobalCallback;
 *
 * no other code can run simultaneously
 */
@Namespace("at") public static native void removeCallback(@Cast("at::CallbackHandle") long handle);

/**
 * Prevent the given callback from executing. If handle is invalid,
 * does nothing.
 */
@Namespace("at") public static native void disableCallback(@Cast("at::CallbackHandle") long handle);

/**
 * Allow the given callback, previously disabled with disableCallback, to
 * execute again. If handle is invalid, does nothing.
 */
@Namespace("at") public static native void reenableCallback(@Cast("at::CallbackHandle") long handle);

/**
 * hasGlobalCallbacks returns whether there're global callbacks
 * registered with pushGlobalCallback
 */
@Namespace("at") public static native @Cast("bool") boolean hasGlobalCallbacks();

/**
 * clearGlobalCallbacks removes all global callbacks
 */
@Namespace("at") public static native void clearGlobalCallbacks();

// for both thread local and global callbacks
@Namespace("at") public static native @Cast("bool") boolean hasCallbacks();
@Namespace("at") public static native void clearCallbacks();

/**
 * enableRecordFunction enables RecordFunction thread locally
 */
@Namespace("at") public static native void enableRecordFunction(@Cast("bool") boolean enable/*=true*/);
@Namespace("at") public static native void enableRecordFunction();

/**
 * isRecordFunctionEnabled returns whether RecordFunction
 * is enabled thread locally
 */
@Namespace("at") public static native @Cast("bool") boolean isRecordFunctionEnabled();
// Targeting ../RecordFunctionGuard.java


// Targeting ../DisableRecordFunctionGuard.java


// Targeting ../RecordFunctionTLS.java



@Namespace("at") public static native @Const @ByRef RecordFunctionTLS get_record_function_tls_();

@Namespace("at") public static native void set_record_function_tls_(@Const @ByRef RecordFunctionTLS tls);

@Namespace("at") public static native void set_record_function_seed_for_testing(@Cast("uint32_t") int seed);

 // namespace at


// Parsed from ATen/ThreadLocalState.h

// #pragma once

// #include <stack>

// #include <c10/core/InferenceMode.h>
// #include <c10/core/impl/LocalDispatchKeySet.h>
// #include <c10/util/Exception.h>
// #include <c10/util/ThreadLocalDebugInfo.h>

// #include <ATen/FuncTorchTLS.h>
// #include <ATen/PythonTorchFunctionTLS.h>
// #include <ATen/SavedTensorHooks.h>
// #include <ATen/ThreadLocalPythonObjects.h>
// #include <ATen/record_function.h>
// #include <c10/core/impl/PythonDispatcherTLS.h>
// #include <c10/core/impl/TorchDispatchModeTLS.h>
// Targeting ../ThreadLocalState.java


// Targeting ../ThreadLocalStateGuard.java



 // namespace at


// Parsed from ATen/ATen.h

// #pragma once

// #if !defined(_MSC_VER) && __cplusplus < 201402L
// #error C++14 or later compatible compiler is required to use ATen.
// #endif

// #include <ATen/Context.h>
// #include <ATen/Device.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/DimVector.h>
// #include <ATen/Dispatch.h>
// #include <ATen/Formatting.h>
// #include <ATen/Functions.h>
// #include <ATen/NamedTensor.h>
// #include <ATen/ScalarOps.h>
// #include <ATen/Tensor.h>
// #include <ATen/TensorGeometry.h>
// #include <ATen/TensorIndexing.h>
// #include <ATen/TensorOperators.h>
// #include <ATen/Version.h>
// #include <ATen/core/ATenGeneral.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Scalar.h>
// #include <ATen/core/UnsafeFromTH.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/jit_type.h>
// #include <c10/core/Allocator.h>
// #include <c10/core/InferenceMode.h>
// #include <c10/core/Layout.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Exception.h>

// TODO: try to remove this
// There is some back story, see https://github.com/pytorch/pytorch/issues/48684
// #include <ATen/NativeFunctions.h>


// Parsed from ATen/Config.h

// #pragma once

// Test these using #if AT_MKL_ENABLED(), not #ifdef, so that it's
// obvious if you forgot to include Config.h
//    c.f. https://stackoverflow.com/questions/33759787/generating-an-error-if-checked-boolean-macro-is-not-defined
//
// DO NOT put the macros for CUDA libraries in this file; they belong in cuda/CUDAConfig.h

// #define AT_MKLDNN_ENABLED() 0
// #define AT_MKL_ENABLED() 1
// #define AT_MKL_SEQUENTIAL() 0
// #define AT_FFTW_ENABLED() 0
// #define AT_POCKETFFT_ENABLED() 0
// #define AT_NNPACK_ENABLED() 1
// #define CAFFE2_STATIC_LINK_CUDA() 0
// #define AT_BUILD_WITH_BLAS() 1
// #define AT_BUILD_WITH_LAPACK() 1
public static final int AT_PARALLEL_OPENMP = 1;
public static final int AT_PARALLEL_NATIVE = 0;
public static final int AT_PARALLEL_NATIVE_TBB = 0;
// #define AT_BLAS_F2C() 0
// #define AT_BLAS_USE_CBLAS_DOT() 0


// Parsed from ATen/Device.h

// #pragma once
// #include <c10/core/Device.h>


// Parsed from ATen/DeviceGuard.h

// #pragma once

// #include <ATen/core/IListRef.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/DeviceGuard.h>
// #include <c10/core/ScalarType.h> // TensorList whyyyyy

// Are you here because you're wondering why DeviceGuard(tensor) no
// longer works?  For code organization reasons, we have temporarily(?)
// removed this constructor from DeviceGuard.  The new way to
// spell it is:
//
//    OptionalDeviceGuard guard(device_of(tensor));

/** Return the Device of a Tensor, if the Tensor is defined. */
@Namespace("at") public static native @ByVal DeviceOptional device_of(@Const @ByRef Tensor t);

@Namespace("at") public static native @ByVal DeviceOptional device_of(@Const @ByRef TensorOptional t);

/** Return the Device of a TensorList, if the list is non-empty and
 *  the first Tensor is defined.  (This function implicitly assumes
 *  that all tensors in the list have the same device.) */
@Namespace("at") public static native @ByVal DeviceOptional device_of(@ByVal TensorArrayRef t);

 // namespace at


// Parsed from ATen/DimVector.h

// #pragma once
// #include <ATen/core/DimVector.h>


// Parsed from ATen/Dispatch.h

// #pragma once

// #include <ATen/core/DeprecatedTypeProperties.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Half.h>
// #include <c10/util/Metaprogramming.h>
// #include <c10/util/complex.h>
// #include <c10/util/string_view.h>

// #ifdef __CUDACC__
// #include <cuda.h> // For CUDA_VERSION
// #endif

// #ifdef TEMPLATE_SELECTIVE_BUILD
// #include <ATen/selected_mobile_ops.h>
// #else
/**
 * The method should_include_kernel_dtype() returns true/false
 * based on whether the switching code for a specific dtype should be
 * included based on build time constants generated from tracing model
 * execution. This method will be implmeneted via code-generation and
 * included in this file when code-gen is ready.
 */
@Namespace("at") public static native @Cast("const bool") boolean should_include_kernel_dtype(
    @Cast("const char*") BytePointer arg0,
    ScalarType arg1
);
@Namespace("at") public static native @Cast("const bool") boolean should_include_kernel_dtype(
    String arg0,
    ScalarType arg1
);
 // namespace at
// #endif

/**
 * In the Facebook internal build (using BUCK), this macro is enabled by
 * passing in -c pt.enable_record_kernel_dtype=1 when building the tracer
 * binary.
 */
// #if defined ENABLE_RECORD_KERNEL_FUNCTION_DTYPE
// #else
// #define RECORD_KERNEL_FUNCTION_DTYPE(NAME, enum_type)
// #endif

// Avoid if_constexpr if possble, as it's more expensive to compile
// #if defined __cpp_if_constexpr
// #define AT_PRIVATE_CHECK_SELECTIVE_BUILD(enum_type)
//   do {
//     if constexpr (!at::should_include_kernel_dtype(
//                       at_dispatch_name, enum_type)) {
//       AT_ERROR(
//           "dtype '",
//           toString(enum_type),
//           "' not selected for kernel tag ",
//           at_dispatch_name);
//     }
//   } while (0)
// #else // defined __cpp_if_constexpr
// #define AT_PRIVATE_CHECK_SELECTIVE_BUILD(enum_type)
//   at::guts::if_constexpr<!at::should_include_kernel_dtype(
//       at_dispatch_name, enum_type)>([&] {
//     AT_ERROR(
//         "dtype '",
//         toString(enum_type),
//         "' not selected for kernel tag ",
//         at_dispatch_name);
//   })
// #endif

// #define AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, HINT, ...)
//   case enum_type: {
//     AT_PRIVATE_CHECK_SELECTIVE_BUILD(enum_type);
//     using HINT C10_UNUSED = c10::impl::ScalarTypeToCPPTypeT<enum_type>;
//     return __VA_ARGS__();
//   }

// #define AT_DISPATCH_CASE(enum_type, ...)
//   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)

// #define AT_DISPATCH_CASE_QINT(enum_type, scalar_type, ...)
//   case enum_type: {
//     AT_PRIVATE_CHECK_SELECTIVE_BUILD(enum_type);
//     using scalar_t = scalar_type;
//     using underlying_t C10_UNUSED = typename scalar_t::underlying;
//     const auto& SCALAR_TYPE C10_UNUSED = enum_type;
//     const auto& UNDERLYING_TYPE C10_UNUSED = toUnderlying(enum_type);
//     return __VA_ARGS__();
//   }

// #define AT_QINT_SUB_BYTE_PRIVATE_CASE_TYPE(
//     enum_type, scalar_type, bitwidth, qmin, qmax, ...)
//   case enum_type: {
//     AT_PRIVATE_CHECK_SELECTIVE_BUILD(enum_type);
//     using scalar_t = scalar_type;
//     using underlying_t C10_UNUSED = typename scalar_t::underlying;
//     const auto& SCALAR_TYPE C10_UNUSED = enum_type;
//     const auto& UNDERLYING_TYPE C10_UNUSED = toUnderlying(enum_type);
//     C10_UNUSED int bit_width = bitwidth;
//     C10_UNUSED int64_t quant_min = qmin;
//     C10_UNUSED int64_t quant_max = qmax;
//     return __VA_ARGS__();
//   }

@Namespace("detail") public static native ScalarType scalar_type(ScalarType s);

@Namespace("detail") public static native @Deprecated ScalarType scalar_type(@Const @ByRef DeprecatedTypeProperties t);

@Namespace("detail") public static native @Deprecated void deprecated_AT_DISPATCH_ALL_TYPES_AND_HALF();

@Namespace("detail") public static native @Deprecated void deprecated_AT_DISPATCH_ALL_TYPES_AND_HALF_AND_COMPLEX();

 // namespace detail

// The AT_DISPATCH_* family of macros provides the ability to
// conveniently generate specializations of a kernel over all of the
// dtypes we care about in PyTorch.  We call it "dispatch" because
// we are "dispatching" to the correct, dtype-specific kernel.
//
// A standard usage looks like:
//
//      AT_DISPATCH_ALL_TYPES(self.scalar_type(), "op_name", [&] {
//          // Your code here, with 'scalar_t' now defined to
//          // be the dtype in question
//      });
//
// There are many variations of this macro, so it's important to
// understand exactly /which/ dtypes you want to get instantiated, as
// well as what the "default" set is.
//
// The default set of dtypes that are instantiated (e.g., by
// AT_DISPATCH_ALL_TYPES) are floating point types (float, double),
// and integral types (int32_t, int64_t, int16_t, int8_t, uint8_t),
// but NOT booleans (bool), half-precision floats (Half) or
// complex number (c10::complex<float>, c10::complex<double>).
// This "cut" is somewhat historical (the default types are the
// ones that TH historically supported), but it also reflects the
// fact that the non-default types are "poorly" behaved (booleans
// are NOT integers mod 2, half precision operations ~essentially
// don't exist on CPU, complex numbers are an experimental application).
//
// Here are the questions you should generally ask to decide which
// dispatch you want:
//
// 1. Is this an integral or floating point specific operation?
//    (If so, you'll want one of the FLOATING or INTEGRAL macros.)
//
// 2. Should half be supported?  (If you're on CPU, the answer is almost
//    definitely no.  If you do want support, use one of the AND_HALF
//    macros)
//
// Much rarer situations:
//
// 3. Should bool be supported?  (You often have to write your kernel
//    differently if arithmetic operations are involved.)  If so,
//    Use AT_DISPATCH_ALL_TYPES_AND along with ScalarType::Bool
//
// 4. Should complex be supported?  The answer is almost always no,
//    unless you are working on "generic" code that should work on
//    all dtypes.
//
// Parameters:
// -----------
//
// 1. The NAME argument is a "tag" that is used to trace and then
//    conditionally compile fragments of the case statements such
//    that the kernel functions are specialized only for the dtypes
//    that are needed. The NAME parameter *must* be a build time
//    const char* (can't be std::string, etc...)
//
// Please ensure that the NAME is unique for every implementation
// or you run the risk of over-including code for the kernel
// functions. There is no risk of missing out on any code, so
// it's mostly a risk of a Type-2 error, and not a Type-1 error.
//
// Switch-like syntax:
// -------------------
// There is also a switch-case like syntax which is useful if a kernel
// needs to be specialized for particular scalar types
//
//      AT_DISPATCH_SWITCH(self.scalar_type(), "op_name",
//          AT_DISPATCH_CASE_INTEGRAL_TYPES([&] {
//            op_integral<scalar_t>(iter);
//          })
//          AT_DISPATCH_CASE_FLOATING_TYPES([&] {
//            op_floating<scalar_t>(iter);
//          })
//          AT_DISPATCH_CASE(kBool, [&] {
//            op_bool(iter);
//          })
//      );
//
// For each AT_DISPATCH_FOO macro, there is a corresponding
// AT_DISPATCH_CASE_FOO macro which can be used inside of an
// AT_DISPATCH_SWITCH block.

// NB: the the_type variable is not used, but we have kept it for
// backwards compatibility.  It's probably not used by anyone though;
// but we're just being safe (and it doesn't hurt.)  Note we must
// use it to shut up warnings about unused store.

// #define AT_DISPATCH_SWITCH(TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     constexpr const char* at_dispatch_name = NAME;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(at_dispatch_name, _st);
//     switch (_st) {
//       __VA_ARGS__
//       default:
//         AT_ERROR(
//             '"',
//             at_dispatch_name,
//             "\" not implemented for '",
//             toString(_st),
//             "'");
//     }
//   }()

// #define AT_DISPATCH_CASE_FLOATING_TYPES(...)
//   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__)
//   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)

// #define AT_DISPATCH_FLOATING_TYPES(TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__))

// #define AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(...)
//   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__)
//   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)
//   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)

// #define AT_DISPATCH_FLOATING_TYPES_AND_HALF(TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))

// #define AT_DISPATCH_CASE_FLOATING_TYPES_AND(SCALARTYPE, ...)
//   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE, __VA_ARGS__)

// #define AT_DISPATCH_FLOATING_TYPES_AND(SCALARTYPE, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_FLOATING_TYPES_AND(SCALARTYPE, __VA_ARGS__))

// #define AT_DISPATCH_CASE_FLOATING_TYPES_AND2(SCALARTYPE1, SCALARTYPE2, ...)
//   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)

// #define AT_DISPATCH_FLOATING_TYPES_AND2(
//     SCALARTYPE1, SCALARTYPE2, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(
//           SCALARTYPE1, SCALARTYPE2, __VA_ARGS__))

// #define AT_DISPATCH_CASE_COMPLEX_TYPES(...)
//   AT_DISPATCH_CASE(at::ScalarType::ComplexDouble, __VA_ARGS__)
//   AT_DISPATCH_CASE(at::ScalarType::ComplexFloat, __VA_ARGS__)

// #define AT_DISPATCH_COMPLEX_TYPES(TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(TYPE, NAME, AT_DISPATCH_CASE_COMPLEX_TYPES(__VA_ARGS__))

// #define AT_DISPATCH_CASE_COMPLEX_TYPES_AND(SCALARTYPE, ...)
//   AT_DISPATCH_CASE_COMPLEX_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE, __VA_ARGS__)

// #define AT_DISPATCH_COMPLEX_TYPES_AND(SCALARTYPE, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE, NAME, AT_DISPATCH_CASE_COMPLEX_TYPES_AND(SCALARTYPE, __VA_ARGS__))

// #define AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES(...)
//   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE_COMPLEX_TYPES(__VA_ARGS__)

// #define AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES(TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES(__VA_ARGS__))

// #define AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES_AND1(SCALARTYPE, ...)
//   AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE, __VA_ARGS__)

// #define AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(
//     SCALARTYPE, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES_AND1(
//           SCALARTYPE, __VA_ARGS__))

// #define AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES_AND2(
//     SCALARTYPE1, SCALARTYPE2, ...)
//   AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)

// #define AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(
//     SCALARTYPE1, SCALARTYPE2, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES_AND2(
//           SCALARTYPE1, SCALARTYPE2, __VA_ARGS__))

// #define AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES_AND3(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, ...)
//   AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE3, __VA_ARGS__)

// #define AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND3(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_FLOATING_AND_COMPLEX_TYPES_AND3(
//           SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, __VA_ARGS__))

// #define AT_DISPATCH_CASE_INTEGRAL_TYPES(...)
//   AT_DISPATCH_CASE(at::ScalarType::Byte, __VA_ARGS__)
//   AT_DISPATCH_CASE(at::ScalarType::Char, __VA_ARGS__)
//   AT_DISPATCH_CASE(at::ScalarType::Int, __VA_ARGS__)
//   AT_DISPATCH_CASE(at::ScalarType::Long, __VA_ARGS__)
//   AT_DISPATCH_CASE(at::ScalarType::Short, __VA_ARGS__)

// #define AT_DISPATCH_INTEGRAL_TYPES(TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(TYPE, NAME, AT_DISPATCH_CASE_INTEGRAL_TYPES(__VA_ARGS__))

// #define AT_DISPATCH_CASE_INTEGRAL_TYPES_AND(SCALARTYPE, ...)
//   AT_DISPATCH_CASE_INTEGRAL_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE, __VA_ARGS__)

// #define AT_DISPATCH_INTEGRAL_TYPES_AND(SCALARTYPE, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_INTEGRAL_TYPES_AND(SCALARTYPE, __VA_ARGS__))

// #define AT_DISPATCH_CASE_ALL_TYPES(...)
//   AT_DISPATCH_CASE_INTEGRAL_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)

// #define AT_DISPATCH_ALL_TYPES(TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(TYPE, NAME, AT_DISPATCH_CASE_ALL_TYPES(__VA_ARGS__))

// #define AT_DISPATCH_CASE_QINT_TYPES(...)
//   AT_DISPATCH_CASE_QINT(at::kQInt8, at::qint8, __VA_ARGS__)
//   AT_DISPATCH_CASE_QINT(at::kQUInt8, at::quint8, __VA_ARGS__)
//   AT_DISPATCH_CASE_QINT(at::kQInt32, at::qint32, __VA_ARGS__)

// #define AT_DISPATCH_QINT_TYPES(TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(TYPE, NAME, AT_DISPATCH_CASE_QINT_TYPES(__VA_ARGS__))

// #define AT_DISPATCH_CASE_QINT_BYTE_TYPES(...)
//   AT_DISPATCH_CASE_QINT(at::kQInt8, at::qint8, __VA_ARGS__)
//   AT_DISPATCH_CASE_QINT(at::kQUInt8, at::quint8, __VA_ARGS__)

// #define AT_DISPATCH_QINT_BYTE_TYPES(TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(TYPE, NAME, AT_DISPATCH_CASE_QINT_BYTE_TYPES(__VA_ARGS__))

// #define AT_DISPATCH_CASE_QINT_AND_SUB_BYTE_TYPES(...)
//   AT_QINT_SUB_BYTE_PRIVATE_CASE_TYPE(
//       at::kQInt8, at::qint8, CHAR_BIT, SCHAR_MIN, SCHAR_MAX, __VA_ARGS__)
//   AT_QINT_SUB_BYTE_PRIVATE_CASE_TYPE(
//       at::kQUInt8, at::quint8, CHAR_BIT, 0, UCHAR_MAX, __VA_ARGS__)
//   AT_QINT_SUB_BYTE_PRIVATE_CASE_TYPE(
//       at::kQInt32,
//       at::qint32,
//       CHAR_BIT * sizeof(int),
//       INT_MIN,
//       INT_MAX,
//       __VA_ARGS__)
//   AT_QINT_SUB_BYTE_PRIVATE_CASE_TYPE(
//       at::kQUInt4x2, at::quint4x2, 4, 0, 15, __VA_ARGS__)
//   AT_QINT_SUB_BYTE_PRIVATE_CASE_TYPE(
//       at::kQUInt2x4, at::quint2x4, 2, 0, 3, __VA_ARGS__)

// #define AT_DISPATCH_QINT_AND_SUB_BYTE_TYPES(TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE, NAME, AT_DISPATCH_CASE_QINT_AND_SUB_BYTE_TYPES(__VA_ARGS__))

// #define AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX(...)
//   AT_DISPATCH_CASE_ALL_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE_COMPLEX_TYPES(__VA_ARGS__)

// #define AT_DISPATCH_ALL_TYPES_AND_COMPLEX(TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE, NAME, AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX(__VA_ARGS__))

// #define AT_DISPATCH_CASE_ALL_TYPES_AND(SCALARTYPE, ...)
//   AT_DISPATCH_CASE_ALL_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE, __VA_ARGS__)

// #define AT_DISPATCH_ALL_TYPES_AND(SCALARTYPE, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE, NAME, AT_DISPATCH_CASE_ALL_TYPES_AND(SCALARTYPE, __VA_ARGS__))

// #define AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX_AND(SCALARTYPE, ...)
//   AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE, __VA_ARGS__)

// #define AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND(SCALARTYPE, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX_AND(SCALARTYPE, __VA_ARGS__))

// #define AT_DISPATCH_CASE_ALL_TYPES_AND2(SCALARTYPE1, SCALARTYPE2, ...)
//   AT_DISPATCH_CASE_ALL_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)

// #define AT_DISPATCH_ALL_TYPES_AND2(SCALARTYPE1, SCALARTYPE2, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_ALL_TYPES_AND2(SCALARTYPE1, SCALARTYPE2, __VA_ARGS__))

// #define AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX_AND2(
//     SCALARTYPE1, SCALARTYPE2, ...)
//   AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)

// #define AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(
//     SCALARTYPE1, SCALARTYPE2, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX_AND2(
//           SCALARTYPE1, SCALARTYPE2, __VA_ARGS__))

// #define AT_DISPATCH_CASE_ALL_TYPES_AND3(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, ...)
//   AT_DISPATCH_CASE_ALL_TYPES(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE3, __VA_ARGS__)

// #define AT_DISPATCH_ALL_TYPES_AND3(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_ALL_TYPES_AND3(
//           SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, __VA_ARGS__))

// #define AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX_AND3(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, ...)
//   AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE3, __VA_ARGS__)

// #define AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX_AND3(
//           SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, __VA_ARGS__))

// #define AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX_AND4(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, SCALARTYPE4, ...)
//   AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX(__VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE3, __VA_ARGS__)
//   AT_DISPATCH_CASE(SCALARTYPE4, __VA_ARGS__)

// #define AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, SCALARTYPE4, TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_ALL_TYPES_AND_COMPLEX_AND4(
//           SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, SCALARTYPE4, __VA_ARGS__))

// #define AT_DISPATCH_INDEX_TYPES(TYPE, NAME, ...)
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_PRIVATE_CASE_TYPE_USING_HINT(
//           at::ScalarType::Int, index_t, __VA_ARGS__)
//           AT_PRIVATE_CASE_TYPE_USING_HINT(
//               at::ScalarType::Long, index_t, __VA_ARGS__))

// ----------------------------------------------------------------------------
// DEPRECATED MACROS, DON'T USE THESE
// ----------------------------------------------------------------------------

// #define AT_DISPATCH_ALL_TYPES_AND_HALF(TYPE, NAME, ...)
//   detail::deprecated_AT_DISPATCH_ALL_TYPES_AND_HALF();
//   AT_DISPATCH_SWITCH(
//       TYPE,
//       NAME,
//       AT_DISPATCH_CASE_ALL_TYPES_AND(at::ScalarType::Half, __VA_ARGS__))


// Parsed from ATen/EmptyTensor.h

// #pragma once
// #include <ATen/core/TensorBase.h>

@Namespace("at::detail") public static native @Cast("size_t") long computeStorageNbytesContiguous(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @Cast("size_t") long itemsize,
    @Cast("size_t") long storage_offset/*=0*/);
@Namespace("at::detail") public static native @Cast("size_t") long computeStorageNbytesContiguous(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @Cast("size_t") long itemsize);
@Namespace("at::detail") public static native @Cast("size_t") long computeStorageNbytesContiguous(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @Cast("size_t") long itemsize,
    @Cast("size_t") long storage_offset/*=0*/);
@Namespace("at::detail") public static native @Cast("size_t") long computeStorageNbytesContiguous(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @Cast("size_t") long itemsize);
@Namespace("at::detail") public static native @ByVal SymInt computeStorageNbytesContiguous(
    @ByVal SymIntRef sizes,
    @Const @ByRef SymInt itemsize,
    @Const @ByRef(nullValue = "c10::SymInt(0)") SymInt storage_offset);
@Namespace("at::detail") public static native @ByVal SymInt computeStorageNbytesContiguous(
    @ByVal SymIntRef sizes,
    @Const @ByRef SymInt itemsize);
@Namespace("at::detail") public static native @Cast("size_t") long computeStorageNbytes(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @Cast("size_t") long itemsize,
    @Cast("size_t") long storage_offset/*=0*/);
@Namespace("at::detail") public static native @Cast("size_t") long computeStorageNbytes(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @Cast("size_t") long itemsize);
@Namespace("at::detail") public static native @Cast("size_t") long computeStorageNbytes(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @Cast("size_t") long itemsize,
    @Cast("size_t") long storage_offset/*=0*/);
@Namespace("at::detail") public static native @Cast("size_t") long computeStorageNbytes(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @Cast("size_t") long itemsize);
@Namespace("at::detail") public static native @ByVal SymInt computeStorageNbytes(
    @ByVal SymIntRef sizes,
    @ByVal SymIntRef strides,
    @Const @ByRef SymInt itemsize,
    @Const @ByRef(nullValue = "c10::SymInt(0)") SymInt storage_offset);
@Namespace("at::detail") public static native @ByVal SymInt computeStorageNbytes(
    @ByVal SymIntRef sizes,
    @ByVal SymIntRef strides,
    @Const @ByRef SymInt itemsize);

@Namespace("at::detail") public static native @ByVal TensorBase empty_generic(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size,
    Allocator allocator,
    @ByVal DispatchKeySet ks,
    ScalarType scalar_type,
    @ByVal MemoryFormatOptional memory_format_opt);
@Namespace("at::detail") public static native @ByVal TensorBase empty_generic(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size,
    Allocator allocator,
    @ByVal DispatchKeySet ks,
    ScalarType scalar_type,
    @ByVal MemoryFormatOptional memory_format_opt);

@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_generic(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride,
    Allocator allocator,
    @ByVal DispatchKeySet ks,
    ScalarType scalar_type);
@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_generic(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride,
    Allocator allocator,
    @ByVal DispatchKeySet ks,
    ScalarType scalar_type);

@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_symint_generic(
    @ByVal SymIntRef size,
    @ByVal SymIntRef stride,
    Allocator allocator,
    @ByVal DispatchKeySet ks,
    ScalarType scalar_type);

@Namespace("at::detail") public static native @ByVal TensorBase empty_cpu(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size,
    ScalarType dtype,
    @Cast("bool") boolean pin_memory/*=false*/,
    @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format_opt);
@Namespace("at::detail") public static native @ByVal TensorBase empty_cpu(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size,
    ScalarType dtype);
@Namespace("at::detail") public static native @ByVal TensorBase empty_cpu(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size,
    ScalarType dtype,
    @Cast("bool") boolean pin_memory/*=false*/,
    @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format_opt);
@Namespace("at::detail") public static native @ByVal TensorBase empty_cpu(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size,
    ScalarType dtype);

@Namespace("at::detail") public static native @ByVal TensorBase empty_cpu(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size,
    @ByVal ScalarTypeOptional dtype_opt,
    @ByVal LayoutOptional layout_opt,
    @ByVal DeviceOptional device_opt,
    @ByVal BoolOptional pin_memory_opt,
    @ByVal MemoryFormatOptional memory_format_opt);
@Namespace("at::detail") public static native @ByVal TensorBase empty_cpu(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size,
    @ByVal ScalarTypeOptional dtype_opt,
    @ByVal LayoutOptional layout_opt,
    @ByVal DeviceOptional device_opt,
    @ByVal BoolOptional pin_memory_opt,
    @ByVal MemoryFormatOptional memory_format_opt);

@Namespace("at::detail") public static native @ByVal TensorBase empty_cpu(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef TensorOptions options);
@Namespace("at::detail") public static native @ByVal TensorBase empty_cpu(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef TensorOptions options);

@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_cpu(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride,
    ScalarType dtype,
    @Cast("bool") boolean pin_memory/*=false*/);
@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_cpu(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride,
    ScalarType dtype);
@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_cpu(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride,
    ScalarType dtype,
    @Cast("bool") boolean pin_memory/*=false*/);
@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_cpu(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride,
    ScalarType dtype);

@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_cpu(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride,
    @ByVal ScalarTypeOptional dtype_opt,
    @ByVal LayoutOptional layout_opt,
    @ByVal DeviceOptional device_opt,
    @ByVal BoolOptional pin_memory_opt);
@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_cpu(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride,
    @ByVal ScalarTypeOptional dtype_opt,
    @ByVal LayoutOptional layout_opt,
    @ByVal DeviceOptional device_opt,
    @ByVal BoolOptional pin_memory_opt);

@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_cpu(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride,
    @Const @ByRef TensorOptions options);
@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_cpu(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride,
    @Const @ByRef TensorOptions options);

@Namespace("at::detail") public static native @ByVal TensorBase empty_meta(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size,
    ScalarType dtype,
    @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format_opt);
@Namespace("at::detail") public static native @ByVal TensorBase empty_meta(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size,
    ScalarType dtype);
@Namespace("at::detail") public static native @ByVal TensorBase empty_meta(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size,
    ScalarType dtype,
    @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format_opt);
@Namespace("at::detail") public static native @ByVal TensorBase empty_meta(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size,
    ScalarType dtype);

@Namespace("at::detail") public static native @ByVal TensorBase empty_meta(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size,
    @ByVal ScalarTypeOptional dtype_opt,
    @ByVal LayoutOptional layout_opt,
    @ByVal DeviceOptional device_opt,
    @ByVal BoolOptional pin_memory_opt,
    @ByVal MemoryFormatOptional memory_format_opt);
@Namespace("at::detail") public static native @ByVal TensorBase empty_meta(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size,
    @ByVal ScalarTypeOptional dtype_opt,
    @ByVal LayoutOptional layout_opt,
    @ByVal DeviceOptional device_opt,
    @ByVal BoolOptional pin_memory_opt,
    @ByVal MemoryFormatOptional memory_format_opt);

@Namespace("at::detail") public static native @ByVal TensorBase empty_symint_meta(
    @ByVal SymIntRef size,
    @ByVal ScalarTypeOptional dtype_opt,
    @ByVal LayoutOptional layout_opt,
    @ByVal DeviceOptional device_opt,
    @ByVal BoolOptional pin_memory_opt,
    @ByVal MemoryFormatOptional memory_format_opt);

@Namespace("at::detail") public static native @ByVal TensorBase empty_meta(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef TensorOptions options);
@Namespace("at::detail") public static native @ByVal TensorBase empty_meta(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef TensorOptions options);

@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_meta(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, ScalarType dtype);
@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_meta(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, ScalarType dtype);

@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_meta(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride,
    @ByVal ScalarTypeOptional dtype_opt,
    @ByVal LayoutOptional layout_opt,
    @ByVal DeviceOptional device_opt,
    @ByVal BoolOptional pin_memory_opt);
@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_meta(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride,
    @ByVal ScalarTypeOptional dtype_opt,
    @ByVal LayoutOptional layout_opt,
    @ByVal DeviceOptional device_opt,
    @ByVal BoolOptional pin_memory_opt);

@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_meta(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride,
    @Const @ByRef TensorOptions options);
@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_meta(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride,
    @Const @ByRef TensorOptions options);

@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_symint_meta(
    @ByVal SymIntRef size,
    @ByVal SymIntRef stride,
    ScalarType dtype);

@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_symint_meta(
    @ByVal SymIntRef size,
    @ByVal SymIntRef stride,
    @ByVal ScalarTypeOptional dtype_opt,
    @ByVal LayoutOptional layout_opt,
    @ByVal DeviceOptional device_opt,
    @ByVal BoolOptional pin_memory_opt);

@Namespace("at::detail") public static native @ByVal TensorBase empty_strided_symint_meta(
    @ByVal SymIntRef size,
    @ByVal SymIntRef stride,
    @Const @ByRef TensorOptions options);

 // namespace detail
 // namespace at


// Parsed from ATen/LinalgBackend.h

// #pragma once

// #include <c10/util/Exception.h>

// #include <ostream>
// #include <string>

@Namespace("at") public enum LinalgBackend { Default((byte)(0)), Cusolver((byte)(1)), Magma((byte)(2));

    public final byte value;
    private LinalgBackend(byte v) { this.value = v; }
    private LinalgBackend(LinalgBackend e) { this.value = e.value; }
    public LinalgBackend intern() { for (LinalgBackend e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("at") public static native @StdString BytePointer LinalgBackendToString(LinalgBackend backend);
@Namespace("at") public static native @StdString String LinalgBackendToString(@Cast("at::LinalgBackend") byte backend);

@Namespace("at") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(
    @Cast("std::ostream*") @ByRef Pointer stream,
    LinalgBackend backend);
@Namespace("at") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(
    @Cast("std::ostream*") @ByRef Pointer stream,
    @Cast("at::LinalgBackend") byte backend);

 // namespace at


// Parsed from ATen/Formatting.h

// #include <ATen/core/Formatting.h>


// Parsed from ATen/Generator.h

// #pragma once
// #include <ATen/core/Generator.h>


// Parsed from ATen/PadNd.h

// #pragma once
// #include <c10/util/Exception.h>
// #include <c10/util/string_view.h>

@Namespace("at") public enum padding_mode {
  reflect(0),
  replicate(1),
  circular(2),
  constant(3);

    public final int value;
    private padding_mode(int v) { this.value = v; }
    private padding_mode(padding_mode e) { this.value = e.value; }
    public padding_mode intern() { for (padding_mode e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("at") public static native @ByVal @Cast("c10::string_view*") Pointer padding_mode_string(padding_mode m);
@Namespace("at") public static native @ByVal @Cast("c10::string_view*") Pointer padding_mode_string(@Cast("at::padding_mode") int m);

 // namespace at


// Parsed from ATen/Parallel.h

// #pragma once
// #include <ATen/Config.h>
// #include <c10/macros/Macros.h>
// #include <functional>
// #include <string>

@Namespace("at") public static native @Cast("int64_t") long divup(@Cast("int64_t") long x, @Cast("int64_t") long y);

// Called during new thread initialization
@Namespace("at") public static native void init_num_threads();

// Sets the number of threads to be used in parallel region
@Namespace("at") public static native void set_num_threads(int arg0);

// Returns the maximum number of threads that may be used in a parallel region
@Namespace("at") public static native int get_num_threads();

// Returns the current thread number (starting from 0)
// in the current parallel region, or 0 in the sequential region
@Namespace("at") public static native int get_thread_num();

// Checks whether the code runs in parallel region
@Namespace("at") public static native @Cast("bool") boolean in_parallel_region();

// Initialise num_threads lazily at first parallel call
@Namespace("at::internal") public static native void lazy_init_num_threads();

@Namespace("at::internal") public static native void set_thread_num(int arg0);
// Targeting ../ThreadIdGuard.java



 // namespace internal

/*
parallel_for

begin: index at which to start applying user function

end: index at which to stop applying user function

grain_size: number of elements per chunk. impacts the degree of parallelization

f: user function applied in parallel to the chunks, signature:
  void f(int64_t begin, int64_t end)

Warning: parallel_for does NOT copy thread local
states from the current thread to the worker threads.
This means for example that Tensor operations CANNOT be used in the
body of your function, only data pointers.
*/

/*
parallel_reduce

begin: index at which to start applying reduction

end: index at which to stop applying reduction

grain_size: number of elements per chunk. impacts number of elements in
intermediate results tensor and degree of parallelization.

ident: identity for binary combination function sf. sf(ident, x) needs to return
x.

f: function for reduction over a chunk. f needs to be of signature scalar_t
f(int64_t partial_begin, int64_t partial_end, scalar_t identifiy)

sf: function to combine two partial results. sf needs to be of signature
scalar_t sf(scalar_t x, scalar_t y)

For example, you might have a tensor of 10000 entires and want to sum together
all the elements. Parallel_reduce with a grain_size of 2500 will then allocate
an intermediate result tensor with 4 elements. Then it will execute the function
"f" you provide and pass the beginning and end index of these chunks, so
0-2499, 2500-4999, etc. and the combination identity. It will then write out
the result from each of these chunks into the intermediate result tensor. After
that it'll reduce the partial results from each chunk into a single number using
the combination function sf and the identity ident. For a total summation this
would be "+" and 0 respectively. This is similar to tbb's approach [1], where
you need to provide a function to accumulate a subrange, a function to combine
two partial results and an identity.

Warning: parallel_reduce does NOT copy thread local
states from the current thread to the worker threads.
This means for example that Tensor operations CANNOT be used in the
body of your function, only data pointers.

[1] https://software.intel.com/en-us/node/506154
*/

// Returns a detailed string describing parallelization settings
@Namespace("at") public static native @StdString BytePointer get_parallel_info();

// Sets number of threads used for inter-op parallelism
@Namespace("at") public static native void set_num_interop_threads(int arg0);

// Returns the number of threads used for inter-op parallelism
@Namespace("at") public static native int get_num_interop_threads();

// Launches inter-op parallel task

 // namespace internal

// Launches intra-op parallel task
@Namespace("at") public static native void intraop_launch(@ByVal Func func);

// Returns number of intra-op threads used by default
@Namespace("at") public static native int intraop_default_num_threads();

 // namespace at

// #if AT_PARALLEL_OPENMP
// #include <ATen/ParallelOpenMP.h> // IWYU pragma: keep
// #elif AT_PARALLEL_NATIVE
// #include <ATen/ParallelNative.h> // IWYU pragma: keep
// #elif AT_PARALLEL_NATIVE_TBB
// #include <ATen/ParallelNativeTBB.h> // IWYU pragma: keep
// #endif

// #include <ATen/Parallel-inl.h> // IWYU pragma: keep


// Parsed from ATen/Utils.h

// #pragma once

// #include <ATen/EmptyTensor.h>
// #include <ATen/Formatting.h>
// #include <ATen/core/ATenGeneral.h>
// #include <ATen/core/Generator.h>
// #include <c10/core/ScalarType.h>
// #include <c10/core/StorageImpl.h>
// #include <c10/core/UndefinedTensorImpl.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>
// #include <c10/util/accumulate.h>
// #include <c10/util/irange.h>

// #include <algorithm>
// #include <memory>
// #include <numeric>
// #include <sstream>
// #include <typeinfo>

// #define AT_DISALLOW_COPY_AND_ASSIGN(TypeName)
//   TypeName(const TypeName&) = delete;
//   void operator=(const TypeName&) = delete

@Namespace("at") public static native int _crash_if_asan(int arg0);

// Converts a TensorList (i.e. ArrayRef<Tensor> to vector of TensorImpl*)
// NB: This is ONLY used by legacy TH bindings, and ONLY used by cat.
// Once cat is ported entirely to ATen this can be deleted!
@Namespace("at") public static native @ByVal TensorImplVector checked_dense_tensor_list_unwrap(
    @ByVal TensorArrayRef tensors,
    @Cast("const char*") BytePointer name,
    int pos,
    DeviceType device_type,
    ScalarType scalar_type);
@Namespace("at") public static native @ByVal TensorImplVector checked_dense_tensor_list_unwrap(
    @ByVal TensorArrayRef tensors,
    String name,
    int pos,
    @Cast("c10::DeviceType") byte device_type,
    ScalarType scalar_type);
 // namespace detail

 // namespace at


// Parsed from ATen/TracerMode.h

// #pragma once

// #include <c10/core/impl/LocalDispatchKeySet.h>
// #include <c10/macros/Export.h>
// #include <c10/macros/Macros.h>

// NOTE [Tracing Mode Switches]
//
// Historically, tracing function was controlled by two switches:
//
// - `AutoDispatchBelowADInplaceOrView` guard
//
//    Tracing function used to be script-generated inside `VariableType_*.cpp`
//    kernels, sharing the same `Autograd` dispatch key with autograd function.
//    Therefore, before tracing function was moved out of VariableType,
//    `AutoDispatchBelowADInplaceOrView` guard can also disable tracing as a
//    side effect of disabling `Autograd` dispatching.
//
// - `setTracingState()` API in `torch/csrc/jit/frontend/tracer.h`
//
//    It stores tracing data in a `TracingState` object in TLS. If the
//    `TracingState` object in TLS is `null`, then tracing is paused.
//
//    The `TracingState` object is created in `tracer::trace()` - the main
//    entrance of tracing function. It's temporarily set to `null` inside
//    generated VariableType (now TraceType) to bypass tracing for intermediate
//    ops (ops being called by other ops). After the intermediate op call
//    finishes it's set back to the original `TracingState` object.
//
//    The `TracingState` obect in TLS can also be read/written via its Python
//    binding in `python_tracer.cpp`, and `get/setTracingState()` C++ APIs,
//    which are also exposed as `TORCH_API`.
//
// Two new switches were introduced since tracing function was moved out of
// VariableType:
//
// - `tracer::impl::set_dispatch_enabled()` API
//
//    Unlike the special `Autograd` dispatch key which is included in dispatch
//    key set by default, `Tracer` dispatch key is off by default. The
//    dispatching switch can be toggled via this new API.
//
// - `tracer::impl::NoTracerDispatchMode` guard
//
//    It's used to cover the old semantics of `AutoDispatchBelowADInplaceOrView`
//    after tracing was moved out of VariableType.
//
// Before tracing function was moved out of VariableType, tracing was enabled
// when the following conditions are satisfied:
//
//    1) `TracingState` object in TLS != null;
//       - Either inside the execution scope of `tracer::trace()`, or
//       - Eagerly called `setTracingState()` with non-null object.
//    2) Not inside `AutoDispatchBelowADInplaceOrView` scope;
//
// After:
//
//    1) `TracingState` object in TLS != null;
//    2) Has called `tracer::impl::set_dispatch_enabled(true)`;
//    3) Not inside `tracer::impl::NonDispatchGuard` scope;
//
// [TODOs]
//
// - `setTracingState()` v.s. `tracer::impl::set_dispatch_enabled()`
//
//   Currently `set_dispatch_enabled()` is set/unset inside `setTracingState()`
//   to keep the semantics exactly the same as before - it's confusing to keep
//   both switches, though. We should consider simplifying/limiting the exposed
//   `setTracingState()` Python/C++ APIs (and other APIs calling it) so that
//   these two can be unified.
//
// - `AutoDispatchBelowADInplaceOrView` v.s.
// `tracer::impl::NoTracerDispatchMode`
//
//   We don't need to always set both guards together to keep semantics
//   unchanged. For the follow use cases of `AutoDispatchBelowADInplaceOrView`
//   we don't need set the new tracer guard:
//
//   * Script-generated VariableType kernels. The guard is not necessary as
//     tracing is already disabled explicitly by `setTracingState(null)` in
//     generated TraceType kernels - we could keep it as is or use the new guard
//     instead.
//
//   * Custom ops. Will be handled by fallback kernel for `Tracer`.
//
//   * Functions that are not likely to be called in tracing context (no python
//     binding / not an operator), e.g.: all mobile forward() wrappers, test
//     binaries, and etc.
//
//   * Where new threads are spawned, e.g.: ATen/native/ConvolutionMM2d.cpp.
//     It's not necessary as tracing is off by default.
//
//   For the rest of cases we might need have both:
//
//   * Functions that might be reachable from eager mode python (especially
//     factory methods), e.g.:
//     `internal_new_from_data()` in `torch/csrc/utils/tensor_new.cpp`.
//     Without the new guard it will add `aten::empty` to the traced graph.
//
//   * Some manually maintained functions, e.g.:
//     `torch/csrc/autograd/VariableTypeManual.cpp`.
//     Set the new guard if it's not obvious whether `setTracingState(null)`
//     has been called before it reaches the `AutoDispatchBelowADInplaceOrView`
//     guard.
//
//   We might need tweak the usage of the new guard to optimize/fix things.
//   It should only affect the correctness of tracing function, because the
//   guard is essentially no-op when the master `setTracingState()` switch is
//   off.
// TODO: move this from `at::` to `jit::torch::` after
// `aten/src/ATen/cpp_custom_type_hack.h` is removed.

@Namespace("at::tracer::impl") public static native @Cast("bool") boolean is_dispatch_enabled();

@Namespace("at::tracer::impl") public static native void set_dispatch_enabled(@Cast("bool") boolean enabled);
// Targeting ../NoTracerDispatchMode.java



 // namespace impl
 // namespace tracer
 // namespace at


// Parsed from ATen/WrapDimUtils.h

// #pragma once

// #include <ATen/core/IListRef.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/TensorImpl.h>
// #include <c10/core/WrapDimMinimal.h>
// #include <c10/util/irange.h>

// if dim_post_expr is 0 and wrap_scalar is true, then dim must be in the
// range [-1, 0]. This is a special case for scalar tensors and manifests in
// e.g. torch.sum(scalar_tensor, 0) Otherwise, dim should be in the range
// [-dim_post_expr, dim_post_expr-1].

@Namespace("at") public static native @Cast("int64_t") long maybe_wrap_dim(@Cast("int64_t") long dim, TensorImpl tensor);

@Namespace("at") public static native @Cast("int64_t") long maybe_wrap_dim(@Cast("int64_t") long dim, @ByVal TensorArrayRef tensors);

@Namespace("at") public static native @Cast("int64_t") long maybe_wrap_dim(
    @Cast("int64_t") long dim,
    @Cast("std::vector<int64_t>*") @StdVector LongVector tensor_sizes);

// Given an array of dimensions `dims` of length `ndims`, this function "Wraps"
// each dim in-place for a tensor of rank `dim_post_expr`, allowing dims to be
// specified using negative indices.
//
// Additionally, if `wrap_scalar` is true then scalar tensors with rank 0, will
// allow dimensions in the range [-1, 0]. Otherwise, an IndexError is raised for
// dimensions not in the range [-dim_post_expr, dim_post_expr).
@Namespace("at") public static native void maybe_wrap_dims_n(
    @Cast("int64_t*") LongPointer dims,
    @Cast("int64_t") long ndims,
    @Cast("int64_t") long dim_post_expr,
    @Cast("bool") boolean wrap_scalars/*=true*/);
@Namespace("at") public static native void maybe_wrap_dims_n(
    @Cast("int64_t*") LongPointer dims,
    @Cast("int64_t") long ndims,
    @Cast("int64_t") long dim_post_expr);
@Namespace("at") public static native void maybe_wrap_dims_n(
    @Cast("int64_t*") LongBuffer dims,
    @Cast("int64_t") long ndims,
    @Cast("int64_t") long dim_post_expr,
    @Cast("bool") boolean wrap_scalars/*=true*/);
@Namespace("at") public static native void maybe_wrap_dims_n(
    @Cast("int64_t*") LongBuffer dims,
    @Cast("int64_t") long ndims,
    @Cast("int64_t") long dim_post_expr);
@Namespace("at") public static native void maybe_wrap_dims_n(
    @Cast("int64_t*") long[] dims,
    @Cast("int64_t") long ndims,
    @Cast("int64_t") long dim_post_expr,
    @Cast("bool") boolean wrap_scalars/*=true*/);
@Namespace("at") public static native void maybe_wrap_dims_n(
    @Cast("int64_t*") long[] dims,
    @Cast("int64_t") long ndims,
    @Cast("int64_t") long dim_post_expr);

// Given a contiguous container of dimensions `dims`, this function "Wraps"
// each dim in-place for a tensor of rank `dim_post_expr`, allowing dims to be
// specified using negative indices.
//
// Additionally, if `wrap_scalar` is true then scalar tensors with rank 0, will
// allow dimensions in the range [-1, 0]. Otherwise, an IndexError is raised for
// dimensions not in the range [-dim_post_expr, dim_post_expr).

// previously, size [0] tensors were the only possible empty tensors; thus, it
// wasn't possible to cat empty tensors unless all the other tensors were
// 1-dimensional, so we allowed these tensors to be "skipped" (both for wrap
// dimension behavior and dimension size checking). We maintain this behavior
// for backwards compatibility, but only for this specific size (i.e. other
// empty sizes are not skipped).

@Namespace("at") public static native @Cast("int64_t") long legacy_cat_wrap_dim(
    @Cast("int64_t") long dim,
    @Cast("std::vector<int64_t>*") @StdVector LongVector tensor_sizes);

@Namespace("at") public static native @Cast("int64_t") long legacy_cat_wrap_dim_symint(
    @Cast("int64_t") long dim,
    @StdVector SymIntVector tensor_sizes);

// wrap negative dims in a vector
@Namespace("at") public static native void wrap_all_dims(
    @Cast("std::vector<int64_t>*") @ByRef LongVector dims_to_wrap,
    @Cast("int64_t") long tensor_total_dims);

 // namespace at


// Parsed from ATen/Tensor.h

// #pragma once

// #include <ATen/core/Tensor.h>


// Parsed from ATen/TensorGeometry.h

// #pragma once

// #include <ATen/core/TensorBase.h>
// #include <c10/core/WrapDimMinimal.h>

// Return if the tensor geometry represented by `sizes` and `strides` is
// contiguous Although we cache is_contiguous in tensor now, this is till useful
// because it allows checking if a particular geometry is contiguous without
// explicitly constructing a tensor, e.g., when you want to choose a kernel
// strategy based on whether a subgeometry is contiguous.
@Namespace("at") public static native @Cast("bool") boolean geometry_is_contiguous(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides);
@Namespace("at") public static native @Cast("bool") boolean geometry_is_contiguous(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... strides);
// Targeting ../TensorGeometry.java



 // namespace at


// Parsed from ATen/TensorNames.h

// #pragma once

// #include <ATen/WrapDimUtils.h>
// Targeting ../TensorName.java


// Targeting ../TensorNames.java



 // namespace namedinference
 // namespace at


// Parsed from ATen/TensorUtils.h

// #pragma once

// #include <ATen/DimVector.h>
// #include <ATen/EmptyTensor.h>
// #include <ATen/Tensor.h>
// #include <ATen/TensorGeometry.h>
// #include <ATen/Utils.h>

// #include <utility>

// These functions are NOT in Utils.h, because this file has a dep on Tensor.h

// #define TORCH_CHECK_TENSOR_ALL(cond, ...)
//   TORCH_CHECK((cond)._is_all_true().item<bool>(), __VA_ARGS__);
// Targeting ../TensorArg.java


// Targeting ../TensorGeometryArg.java



// A string describing which function did checks on its input
// arguments.
// TODO: Consider generalizing this into a call stack.

// The undefined convention: singular operators assume their arguments
// are defined, but functions which take multiple tensors will
// implicitly filter out undefined tensors (to make it easier to perform
// tests which should apply if the tensor is defined, and should not
// otherwise.)
//
// NB: This means that the n-ary operators take lists of TensorArg,
// not TensorGeometryArg, because the Tensor to TensorGeometry
// conversion will blow up if you have undefined tensors.

@Namespace("at") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @ByVal TensorGeometryArg t);
@Namespace("at") public static native void checkDim(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef Tensor tensor,
    @Cast("const char*") BytePointer name,
    int pos,
    @Cast("int64_t") long dim);
@Namespace("at") public static native void checkDim(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef Tensor tensor,
    String name,
    int pos,
    @Cast("int64_t") long dim);
@Namespace("at") public static native void checkDim(@Cast("at::CheckedFrom") BytePointer c, @Const @ByRef TensorGeometryArg t, @Cast("int64_t") long dim);
@Namespace("at") public static native void checkDim(@Cast("at::CheckedFrom") String c, @Const @ByRef TensorGeometryArg t, @Cast("int64_t") long dim);
// NB: this is an inclusive-exclusive range
@Namespace("at") public static native void checkDimRange(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim_start,
    @Cast("int64_t") long dim_end);
@Namespace("at") public static native void checkDimRange(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim_start,
    @Cast("int64_t") long dim_end);
@Namespace("at") public static native void checkSameDim(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t1,
    @Const @ByRef TensorGeometryArg t2);
@Namespace("at") public static native void checkSameDim(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t1,
    @Const @ByRef TensorGeometryArg t2);
@Namespace("at") public static native void checkContiguous(@Cast("at::CheckedFrom") BytePointer c, @Const @ByRef TensorGeometryArg t);
@Namespace("at") public static native void checkContiguous(@Cast("at::CheckedFrom") String c, @Const @ByRef TensorGeometryArg t);
@Namespace("at") public static native void checkAllContiguous(@Cast("at::CheckedFrom") BytePointer c, @ByVal TensorArgArrayRef ts);
@Namespace("at") public static native void checkAllContiguous(@Cast("at::CheckedFrom") String c, @ByVal TensorArgArrayRef ts);
@Namespace("at") public static native void checkSize(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("at") public static native void checkSize(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sizes);
@Namespace("at") public static native void checkSize_symint(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t,
    @ByVal SymIntRef sizes);
@Namespace("at") public static native void checkSize_symint(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t,
    @ByVal SymIntRef sizes);
@Namespace("at") public static native void checkSize(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim,
    @Cast("int64_t") long size);
@Namespace("at") public static native void checkSize(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim,
    @Cast("int64_t") long size);
@Namespace("at") public static native void checkSize_symint(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim,
    @ByVal SymInt size);
@Namespace("at") public static native void checkSize_symint(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim,
    @ByVal SymInt size);
@Namespace("at") public static native void checkNumel(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long numel);
@Namespace("at") public static native void checkNumel(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long numel);

@Namespace("at") public static native void checkAllSameNumel(@Cast("at::CheckedFrom") BytePointer c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkAllSameNumel(@Cast("at::CheckedFrom") String c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkScalarType(@Cast("at::CheckedFrom") BytePointer c, @Const @ByRef TensorArg t, ScalarType s);
@Namespace("at") public static native void checkScalarType(@Cast("at::CheckedFrom") String c, @Const @ByRef TensorArg t, ScalarType s);
@Namespace("at") public static native void checkScalarTypes(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorArg t,
    @ByVal ScalarTypeArrayRef l);
@Namespace("at") public static native void checkScalarTypes(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorArg t,
    @ByVal ScalarTypeArrayRef l);
@Namespace("at") public static native void checkSameGPU(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkSameGPU(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkAllSameGPU(@Cast("at::CheckedFrom") BytePointer c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkAllSameGPU(@Cast("at::CheckedFrom") String c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkSameType(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkSameType(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkAllSameType(@Cast("at::CheckedFrom") BytePointer c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkAllSameType(@Cast("at::CheckedFrom") String c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkSameSize(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkSameSize(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkDefined(@Cast("at::CheckedFrom") BytePointer c, @Const @ByRef TensorArg t);
@Namespace("at") public static native void checkDefined(@Cast("at::CheckedFrom") String c, @Const @ByRef TensorArg t);
@Namespace("at") public static native void checkAllDefined(@Cast("at::CheckedFrom") BytePointer c, @ByVal TensorArgArrayRef t);
@Namespace("at") public static native void checkAllDefined(@Cast("at::CheckedFrom") String c, @ByVal TensorArgArrayRef t);

// FixMe: does TensorArg slow things down?
@Namespace("at") public static native void checkBackend(
    @Cast("at::CheckedFrom") BytePointer c,
    @ByVal TensorArrayRef t,
    @ByVal Backend backend);
@Namespace("at") public static native void checkBackend(
    @Cast("at::CheckedFrom") String c,
    @ByVal TensorArrayRef t,
    @ByVal Backend backend);

@Namespace("at") public static native void checkDeviceType(
    @Cast("at::CheckedFrom") BytePointer c,
    @ByVal TensorArrayRef tensors,
    @ByVal DeviceType device_type);
@Namespace("at") public static native void checkDeviceType(
    @Cast("at::CheckedFrom") String c,
    @ByVal TensorArrayRef tensors,
    @ByVal DeviceType device_type);

@Namespace("at") public static native void checkLayout(@Cast("at::CheckedFrom") BytePointer c, @Const @ByRef Tensor t, Layout layout);
@Namespace("at") public static native void checkLayout(@Cast("at::CheckedFrom") String c, @Const @ByRef Tensor t, @Cast("c10::Layout") byte layout);

@Namespace("at") public static native void checkLayout(
    @Cast("at::CheckedFrom") BytePointer c,
    @ByVal TensorArrayRef tensors,
    @ByVal Layout layout);
@Namespace("at") public static native void checkLayout(
    @Cast("at::CheckedFrom") String c,
    @ByVal TensorArrayRef tensors,
    @ByVal Layout layout);

// Methods for getting data_ptr if tensor is defined
@Namespace("at") public static native Pointer maybe_data_ptr(@Const @ByRef Tensor tensor);
@Namespace("at") public static native Pointer maybe_data_ptr(@Const @ByRef TensorArg tensor);

@Namespace("at") public static native void check_dim_size(
    @Const @ByRef Tensor tensor,
    @Cast("int64_t") long dim,
    @Cast("int64_t") long dim_size,
    @Cast("int64_t") long size);
@Namespace("at::detail") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector defaultStrides(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("at::detail") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector defaultStrides(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sizes);

@Namespace("at::detail") public static native @ByVal LongVectorOptional computeStride(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef oldshape,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef oldstride,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef newshape);
@Namespace("at::detail") public static native @ByVal LongVectorOptional computeStride(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] oldshape,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] oldstride,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... newshape);

@Namespace("at::detail") public static native @ByVal SymDimVectorOptional computeStride(
    @ByVal SymIntRef oldshape,
    @ByVal SymIntRef oldstride,
    @ByVal SymIntRef newshape);

@Namespace("at::detail") public static native @ByVal DimVectorOptional computeStride(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef oldshape,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef oldstride,
    @Const @ByRef DimVector newshape);
@Namespace("at::detail") public static native @ByVal DimVectorOptional computeStride(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] oldshape,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] oldstride,
    @Const @ByRef DimVector newshape);

 // namespace detail
 // namespace at


// Parsed from ATen/Context.h

// #pragma once

// #include <ATen/CPUGeneratorImpl.h>
// #include <ATen/LinalgBackend.h>
// #include <ATen/core/ATenGeneral.h>
// #include <ATen/core/DeprecatedTypeProperties.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/LegacyTypeDispatch.h>
// #include <ATen/detail/CUDAHooksInterface.h>
// #include <ATen/detail/HIPHooksInterface.h>
// #include <ATen/detail/MPSHooksInterface.h>
// #include <ATen/detail/ORTHooksInterface.h>
// #include <c10/core/QEngine.h>
// #include <c10/core/impl/DeviceGuardImplInterface.h>
// #include <c10/util/CallOnce.h>
// #include <c10/util/Exception.h>
// #include <c10/util/env.h>
// #include <c10/util/irange.h>

// #include <cstdint>
// #include <memory>
// #include <mutex>

@Namespace("at") public enum Float32MatmulPrecision { HIGHEST(0), HIGH(1), MEDIUM(2);

    public final int value;
    private Float32MatmulPrecision(int v) { this.value = v; }
    private Float32MatmulPrecision(Float32MatmulPrecision e) { this.value = e.value; }
    public Float32MatmulPrecision intern() { for (Float32MatmulPrecision e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../Context.java



@Namespace("at") public static native @ByRef Context globalContext();

@Namespace("at") public static native void init();

@Namespace("at") public static native Allocator getCPUAllocator();

@Namespace("at") public static native @ByRef DeprecatedTypeProperties getDeprecatedTypeProperties(
    Backend p,
    ScalarType s);
@Namespace("at") public static native @ByRef DeprecatedTypeProperties getDeprecatedTypeProperties(
    @Cast("c10::Backend") int p,
    ScalarType s);

@Namespace("at") public static native @ByRef DeprecatedTypeProperties CPU(ScalarType s);

@Namespace("at") public static native @ByRef DeprecatedTypeProperties CUDA(ScalarType s);

@Namespace("at") public static native @ByRef DeprecatedTypeProperties HIP(ScalarType s);

@Namespace("at") public static native @ByRef DeprecatedTypeProperties MPS(ScalarType s);

@Namespace("at") public static native @Cast("bool") boolean hasCUDA();

@Namespace("at") public static native @Cast("bool") boolean hasHIP();

@Namespace("at") public static native @Cast("bool") boolean hasIPU();

@Namespace("at") public static native @Cast("bool") boolean hasXLA();

@Namespace("at") public static native @Cast("bool") boolean hasMPS();

@Namespace("at") public static native @Cast("bool") boolean hasORT();

// Despite its name, this function returns the number of *CUDA* GPUs.
@Namespace("at") public static native @Cast("size_t") long getNumGPUs();

@Namespace("at") public static native @Cast("bool") boolean hasOpenMP();

@Namespace("at") public static native @Cast("bool") boolean hasMKL();

@Namespace("at") public static native @Cast("bool") boolean hasLAPACK();

@Namespace("at") public static native @Cast("bool") boolean hasMAGMA();

@Namespace("at") public static native @Cast("bool") boolean hasMKLDNN();

@Namespace("at") public static native void manual_seed(@Cast("uint64_t") long seed);
// Targeting ../NoTF32Guard.java



// #ifdef USE_ROCM
// #endif

 // namespace at


// Parsed from ATen/ExpandUtils.h

// #pragma once

// #ifndef AT_PER_OPERATOR_HEADERS
// #include <ATen/Functions.h>
// #else
// #include <ATen/ops/view.h>
// #include <ATen/ops/view_copy.h>
// #endif

// #include <ATen/Tensor.h>
// #include <ATen/core/DimVector.h>
// #include <c10/util/Exception.h>
// #include <c10/util/MaybeOwned.h>
// #include <c10/util/irange.h>

// #include <functional>
// #include <sstream>
// #include <tuple>
// #include <utility>

@Namespace("at") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector infer_size(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef a, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef b);
@Namespace("at") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector infer_size(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] a, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... b);
@Namespace("at") public static native @ByVal DimVector infer_size_dimvector(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef a, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef b);
@Namespace("at") public static native @ByVal DimVector infer_size_dimvector(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] a, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... b);
@Namespace("at") public static native @ByVal SymDimVector infer_size_symdimvector(@ByVal SymIntRef a, @ByVal SymIntRef b);
// Targeting ../DimVectorInferExpandGeometryResult.java



@Namespace("at") public static native @ByVal @Cast("std::tuple<std::vector<int64_t>,std::vector<int64_t> >*") LongVector inferExpandGeometry(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef tensor_sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef tensor_strides,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("at") public static native @ByVal @Cast("std::tuple<std::vector<int64_t>,std::vector<int64_t> >*") LongVector inferExpandGeometry(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] tensor_sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] tensor_strides,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sizes);

@Namespace("at") public static native @ByVal DimVectorInferExpandGeometryResult inferExpandGeometry_dimvector(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef tensor_sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef tensor_strides,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("at") public static native @ByVal DimVectorInferExpandGeometryResult inferExpandGeometry_dimvector(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] tensor_sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] tensor_strides,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sizes);

@Namespace("at") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector infer_dense_strides(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef tensor_sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef tensor_strides);
@Namespace("at") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector infer_dense_strides(
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] tensor_sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... tensor_strides);

// True if input shapes are expandable
// NOTE: infer_size did a similar check, please keep them sync if change is
// needed
@Namespace("at") public static native @Cast("bool") boolean are_expandable(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape1, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape2);
@Namespace("at") public static native @Cast("bool") boolean are_expandable(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] shape1, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... shape2);

// avoid copy-construction of Tensor by using a reference_wrapper.

// NOTE [ ExpandUtils Borrowing ]
//
// Functions in ExpandUtils return `c10::MaybeOwned<Tensor>` because
// expansion may not actually be needed, in which case we can improve
// efficiency by returning
// `c10::MaybeOwned<Tensor>::borrowed(to_expand)`. However, this means
// that you need to be careful: the returned `c10::MaybeOwned<Tensor>`
// must not outlive the original `Tensor` object that `to_expand`
// referred to! The deleted rvalue reference overloads of these
// functions help with this by preventing trivial use of a temporary
// resulting from a function call, but it is still possible to make a
// mistake.

@Namespace("at") public static native @Cast({"", "c10::MaybeOwned<at::Tensor>&&"}) @StdMove TensorMaybeOwned expand_inplace(
    @Const @ByRef Tensor tensor,
    @Const @ByRef Tensor to_expand);



@Namespace("at") public static native @Cast({"", "c10::MaybeOwned<at::Tensor>&&"}) @StdMove TensorMaybeOwned expand_inplace(
    @Const @ByRef Tensor tensor,
    @Const @ByRef Tensor to_expand,
    @Cast("const char*") BytePointer api_name);
@Namespace("at") public static native @Cast({"", "c10::MaybeOwned<at::Tensor>&&"}) @StdMove TensorMaybeOwned expand_inplace(
    @Const @ByRef Tensor tensor,
    @Const @ByRef Tensor to_expand,
    String api_name);



@Namespace("at") public static native @ByVal TensorMaybeOwnedTensorMaybeOwnedTuple expand_inplace(
    @Const @ByRef Tensor tensor,
    @Const @ByRef Tensor to_expand1,
    @Const @ByRef Tensor to_expand2);





@Namespace("at") public static native @ByVal TensorMaybeOwnedTensorMaybeOwnedTuple expand_inplace(
    @Const @ByRef Tensor tensor,
    @Const @ByRef Tensor to_expand1,
    @Const @ByRef Tensor to_expand2,
    @Cast("const char*") BytePointer api_name);
@Namespace("at") public static native @ByVal TensorMaybeOwnedTensorMaybeOwnedTuple expand_inplace(
    @Const @ByRef Tensor tensor,
    @Const @ByRef Tensor to_expand1,
    @Const @ByRef Tensor to_expand2,
    String api_name);





// See NOTE [ ExpandUtils Borrowing ] above for `MaybeOwned` explanation.
@Namespace("at") public static native @ByVal TensorMaybeOwnedTensorMaybeOwnedTuple expand_outplace(@Const @ByRef Tensor to_expand1, @Const @ByRef Tensor to_expand2);





@Namespace("at") public static native @ByVal TensorMaybeOwnedTensorMaybeOwnedTuple expand_outplace(
    @Const @ByRef Tensor to_expand1,
    @Const @ByRef Tensor to_expand2,
    @Cast("const char*") BytePointer api_name);
@Namespace("at") public static native @ByVal TensorMaybeOwnedTensorMaybeOwnedTuple expand_outplace(
    @Const @ByRef Tensor to_expand1,
    @Const @ByRef Tensor to_expand2,
    String api_name);





@Namespace("at") public static native @ByVal TensorMaybeOwnedTensorMaybeOwnedTensorMaybeOwnedTuple expand_outplace(
    @Const @ByRef Tensor to_expand1,
    @Const @ByRef Tensor to_expand2,
    @Const @ByRef Tensor to_expand3);









@Namespace("at") public static native @ByVal TensorMaybeOwnedTensorMaybeOwnedTensorMaybeOwnedTuple expand_outplace(
    @Const @ByRef Tensor to_expand1,
    @Const @ByRef Tensor to_expand2,
    @Const @ByRef Tensor to_expand3,
    @Cast("const char*") BytePointer api_name);
@Namespace("at") public static native @ByVal TensorMaybeOwnedTensorMaybeOwnedTensorMaybeOwnedTuple expand_outplace(
    @Const @ByRef Tensor to_expand1,
    @Const @ByRef Tensor to_expand2,
    @Const @ByRef Tensor to_expand3,
    String api_name);









@Namespace("at") public static native @Cast({"", "c10::MaybeOwned<at::Tensor>&&"}) @StdMove TensorMaybeOwned expand_size(
    @Const @ByRef Tensor to_expand,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("at") public static native @Cast({"", "c10::MaybeOwned<at::Tensor>&&"}) @StdMove TensorMaybeOwned expand_size(
    @Const @ByRef Tensor to_expand,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sizes);



@Namespace("at") public static native @Cast({"", "c10::MaybeOwned<at::Tensor>&&"}) @StdMove TensorMaybeOwned expand_size(
    @Const @ByRef Tensor to_expand,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @Cast("const char*") BytePointer api_name);
@Namespace("at") public static native @Cast({"", "c10::MaybeOwned<at::Tensor>&&"}) @StdMove TensorMaybeOwned expand_size(
    @Const @ByRef Tensor to_expand,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    String api_name);



@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector expand_outplace(@ByVal TensorArrayRef to_expand);

@Namespace("at") public static native @ByVal Tensor sum_to(
    @ByVal Tensor tensor,
    @Const @ByVal SymIntRef shape,
    @Cast("bool") boolean always_return_non_view/*=false*/);
@Namespace("at") public static native @ByVal Tensor sum_to(
    @ByVal Tensor tensor,
    @Const @ByVal SymIntRef shape);

// Sums `tensor` repeatedly to produce a tensor of shape `shape`.
// Precondition: is_expandable_to(shape, tensor.sizes()) must be true
@Namespace("at") public static native @ByVal Tensor sum_to(
    @ByVal Tensor tensor,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape,
    @Cast("bool") boolean always_return_non_view/*=false*/);
@Namespace("at") public static native @ByVal Tensor sum_to(
    @ByVal Tensor tensor,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape);
@Namespace("at") public static native @ByVal Tensor sum_to(
    @ByVal Tensor tensor,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] shape,
    @Cast("bool") boolean always_return_non_view/*=false*/);
@Namespace("at") public static native @ByVal Tensor sum_to(
    @ByVal Tensor tensor,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... shape);

@Namespace("at") public static native @Cast("bool") boolean is_expandable_to(
    @ByVal SymIntRef shape,
    @ByVal SymIntRef desired);

@Namespace("at") public static native @Cast("bool") boolean is_expandable_to(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef desired);
@Namespace("at") public static native @Cast("bool") boolean is_expandable_to(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] shape, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... desired);

 // namespace at


// Parsed from ATen/Functions.h

// #pragma once

// @generated by torchgen/gen.py from Functions.h

// #ifdef TORCH_ASSERT_NO_OPERATORS
// #error This change adds a dependency on native_functions.yaml,
//   meaning the file will need to be re-compiled every time an operator
//   is changed or added. Consider if your change would be better placed in
//   another file, or if a more specific header might achieve the same goal.
//   See NOTE: [Tensor vs. TensorBase]
// #endif

// #if defined(AT_PER_OPERATOR_HEADERS) && defined(TORCH_ASSERT_ONLY_METHOD_OPERATORS)
// #error This change adds a dependency on all pytorch operators, meaning the
//   file will need to be re-compiled every time an operator is changed or added.
//   Consider including a specific operator from <ATen/ops/{my_operator}.h> and
//   see NOTE [TORCH_ASSERT_ONLY_METHOD_OPERATORS].
// #endif

// NOTE: [TORCH_ASSERT_ONLY_METHOD_OPERATORS]
//
// In ATen, certain generated headers files include the definitions of
// every single operator in PyTorch. Unfortunately this means every
// time an operator signature is updated or changed in
// native_functions.yaml, you (and every other PyTorch developer) need
// to recompile every source file that includes any of these headers.
//
// To break up these header dependencies, and improve incremental
// build times for all PyTorch developers. These headers are split
// into per-operator headers in the `ATen/ops` folder. This limits
// incremental builds to only changes to methods of `Tensor`, or files
// that use the specific operator being changed. With `at::sum` as an
// example, you should include
//
//   <ATen/ops/sum.h>               // instead of ATen/Functions.h
//   <ATen/ops/sum_native.h>        // instead of ATen/NativeFunctions.h
//   <ATen/ops/sum_ops.h>           // instead of ATen/Operators.h
//   <ATen/ops/sum_cpu_dispatch.h>  // instead of ATen/CPUFunctions.h
//
// However, even if you're careful to use this in your own code.
// `Functions.h` might be included indirectly through another header
// without you realising. To avoid this, you can add
//
//   #define TORCH_ASSERT_ONLY_METHOD_OPERATORS
//
// to the top of your source file. This way any time the non-specific
// headers are included, the compiler will error out.
//
// Also, be aware that `ops` are not available in all build
// configurations (namely fb-internal) so you must guard these
// includes with `#ifdef AT_PER_OPERATOR_HEADERS`. e.g.
//
//   #ifndef AT_PER_OPERATOR_HEADERS
//   #include <ATen/Functions.h>
//   #else
//   #include <ATen/ops/sum.h>
//   #endif

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <c10/core/SymInt.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>
// #include <c10/util/OptionalArrayRef.h>

// #include <ATen/ops/from_blob.h>
// #include <ATen/ops/tensor.h>

// #include <ATen/ops/_adaptive_avg_pool2d.h>
// #include <ATen/ops/_adaptive_avg_pool2d_backward.h>
// #include <ATen/ops/_adaptive_avg_pool3d.h>
// #include <ATen/ops/_adaptive_avg_pool3d_backward.h>
// #include <ATen/ops/_add_batch_dim.h>
// #include <ATen/ops/_add_relu.h>
// #include <ATen/ops/_addmm_activation.h>
// #include <ATen/ops/_aminmax.h>
// #include <ATen/ops/_amp_foreach_non_finite_check_and_unscale.h>
// #include <ATen/ops/_amp_update_scale.h>
// #include <ATen/ops/_assert_async.h>
// #include <ATen/ops/_assert_tensor_metadata.h>
// #include <ATen/ops/_autocast_to_full_precision.h>
// #include <ATen/ops/_autocast_to_reduced_precision.h>
// #include <ATen/ops/_backward.h>
// #include <ATen/ops/_batch_norm_impl_index.h>
// #include <ATen/ops/_batch_norm_impl_index_backward.h>
// #include <ATen/ops/_cast_Byte.h>
// #include <ATen/ops/_cast_Char.h>
// #include <ATen/ops/_cast_Double.h>
// #include <ATen/ops/_cast_Float.h>
// #include <ATen/ops/_cast_Half.h>
// #include <ATen/ops/_cast_Int.h>
// #include <ATen/ops/_cast_Long.h>
// #include <ATen/ops/_cast_Short.h>
// #include <ATen/ops/_cdist_backward.h>
// #include <ATen/ops/_cdist_forward.h>
// #include <ATen/ops/_cholesky_solve_helper.h>
// #include <ATen/ops/_choose_qparams_per_tensor.h>
// #include <ATen/ops/_chunk_grad_outputs_efficient_attention.h>
// #include <ATen/ops/_coalesce.h>
// #include <ATen/ops/_coalesced.h>
// #include <ATen/ops/_compute_linear_combination.h>
// #include <ATen/ops/_conj.h>
// #include <ATen/ops/_conj_copy.h>
// #include <ATen/ops/_conj_physical.h>
// #include <ATen/ops/_conv_depthwise2d.h>
// #include <ATen/ops/_convert_indices_from_coo_to_csr.h>
// #include <ATen/ops/_convert_indices_from_csr_to_coo.h>
// #include <ATen/ops/_convolution.h>
// #include <ATen/ops/_convolution_double_backward.h>
// #include <ATen/ops/_convolution_mode.h>
// #include <ATen/ops/_copy_from.h>
// #include <ATen/ops/_copy_from_and_resize.h>
// #include <ATen/ops/_ctc_loss.h>
// #include <ATen/ops/_ctc_loss_backward.h>
// #include <ATen/ops/_cudnn_ctc_loss.h>
// #include <ATen/ops/_cudnn_init_dropout_state.h>
// #include <ATen/ops/_cudnn_rnn.h>
// #include <ATen/ops/_cudnn_rnn_backward.h>
// #include <ATen/ops/_cudnn_rnn_flatten_weight.h>
// #include <ATen/ops/_cufft_clear_plan_cache.h>
// #include <ATen/ops/_cufft_get_plan_cache_max_size.h>
// #include <ATen/ops/_cufft_get_plan_cache_size.h>
// #include <ATen/ops/_cufft_set_plan_cache_max_size.h>
// #include <ATen/ops/_cummax_helper.h>
// #include <ATen/ops/_cummin_helper.h>
// #include <ATen/ops/_debug_has_internal_overlap.h>
// #include <ATen/ops/_dimI.h>
// #include <ATen/ops/_dimV.h>
// #include <ATen/ops/_dim_arange.h>
// #include <ATen/ops/_dirichlet_grad.h>
// #include <ATen/ops/_efficient_attention_backward.h>
// #include <ATen/ops/_efficient_attention_forward.h>
// #include <ATen/ops/_efficientzerotensor.h>
// #include <ATen/ops/_embedding_bag.h>
// #include <ATen/ops/_embedding_bag_backward.h>
// #include <ATen/ops/_embedding_bag_dense_backward.h>
// #include <ATen/ops/_embedding_bag_forward_only.h>
// #include <ATen/ops/_embedding_bag_per_sample_weights_backward.h>
// #include <ATen/ops/_embedding_bag_sparse_backward.h>
// #include <ATen/ops/_empty_affine_quantized.h>
// #include <ATen/ops/_empty_per_channel_affine_quantized.h>
// #include <ATen/ops/_euclidean_dist.h>
// #include <ATen/ops/_fake_quantize_learnable_per_channel_affine.h>
// #include <ATen/ops/_fake_quantize_learnable_per_channel_affine_backward.h>
// #include <ATen/ops/_fake_quantize_learnable_per_tensor_affine.h>
// #include <ATen/ops/_fake_quantize_learnable_per_tensor_affine_backward.h>
// #include <ATen/ops/_fake_quantize_per_tensor_affine_cachemask_tensor_qparams.h>
// #include <ATen/ops/_fft_c2c.h>
// #include <ATen/ops/_fft_c2r.h>
// #include <ATen/ops/_fft_r2c.h>
// #include <ATen/ops/_flash_attention_backward.h>
// #include <ATen/ops/_flash_attention_forward.h>
// #include <ATen/ops/_foobar.h>
// #include <ATen/ops/_foreach_abs.h>
// #include <ATen/ops/_foreach_acos.h>
// #include <ATen/ops/_foreach_add.h>
// #include <ATen/ops/_foreach_addcdiv.h>
// #include <ATen/ops/_foreach_addcmul.h>
// #include <ATen/ops/_foreach_asin.h>
// #include <ATen/ops/_foreach_atan.h>
// #include <ATen/ops/_foreach_ceil.h>
// #include <ATen/ops/_foreach_clamp_max.h>
// #include <ATen/ops/_foreach_clamp_min.h>
// #include <ATen/ops/_foreach_cos.h>
// #include <ATen/ops/_foreach_cosh.h>
// #include <ATen/ops/_foreach_div.h>
// #include <ATen/ops/_foreach_erf.h>
// #include <ATen/ops/_foreach_erfc.h>
// #include <ATen/ops/_foreach_exp.h>
// #include <ATen/ops/_foreach_expm1.h>
// #include <ATen/ops/_foreach_floor.h>
// #include <ATen/ops/_foreach_frac.h>
// #include <ATen/ops/_foreach_lerp.h>
// #include <ATen/ops/_foreach_lgamma.h>
// #include <ATen/ops/_foreach_log.h>
// #include <ATen/ops/_foreach_log10.h>
// #include <ATen/ops/_foreach_log1p.h>
// #include <ATen/ops/_foreach_log2.h>
// #include <ATen/ops/_foreach_maximum.h>
// #include <ATen/ops/_foreach_minimum.h>
// #include <ATen/ops/_foreach_mul.h>
// #include <ATen/ops/_foreach_neg.h>
// #include <ATen/ops/_foreach_norm.h>
// #include <ATen/ops/_foreach_reciprocal.h>
// #include <ATen/ops/_foreach_round.h>
// #include <ATen/ops/_foreach_sigmoid.h>
// #include <ATen/ops/_foreach_sin.h>
// #include <ATen/ops/_foreach_sinh.h>
// #include <ATen/ops/_foreach_sqrt.h>
// #include <ATen/ops/_foreach_sub.h>
// #include <ATen/ops/_foreach_tan.h>
// #include <ATen/ops/_foreach_tanh.h>
// #include <ATen/ops/_foreach_trunc.h>
// #include <ATen/ops/_foreach_zero.h>
// #include <ATen/ops/_fused_adam.h>
// #include <ATen/ops/_fused_adamw.h>
// #include <ATen/ops/_fused_dropout.h>
// #include <ATen/ops/_fused_moving_avg_obs_fq_helper.h>
// #include <ATen/ops/_fused_sdp_choice.h>
// #include <ATen/ops/_fw_primal.h>
// #include <ATen/ops/_fw_primal_copy.h>
// #include <ATen/ops/_gather_sparse_backward.h>
// #include <ATen/ops/_grid_sampler_2d_cpu_fallback.h>
// #include <ATen/ops/_grid_sampler_2d_cpu_fallback_backward.h>
// #include <ATen/ops/_has_compatible_shallow_copy_type.h>
// #include <ATen/ops/_has_same_storage_numel.h>
// #include <ATen/ops/_histogramdd_bin_edges.h>
// #include <ATen/ops/_histogramdd_from_bin_cts.h>
// #include <ATen/ops/_histogramdd_from_bin_tensors.h>
// #include <ATen/ops/_index_put_impl.h>
// #include <ATen/ops/_indices.h>
// #include <ATen/ops/_indices_copy.h>
// #include <ATen/ops/_is_all_true.h>
// #include <ATen/ops/_is_any_true.h>
// #include <ATen/ops/_is_zerotensor.h>
// #include <ATen/ops/_linalg_check_errors.h>
// #include <ATen/ops/_linalg_det.h>
// #include <ATen/ops/_linalg_eigh.h>
// #include <ATen/ops/_linalg_slogdet.h>
// #include <ATen/ops/_linalg_solve_ex.h>
// #include <ATen/ops/_linalg_svd.h>
// #include <ATen/ops/_local_scalar_dense.h>
// #include <ATen/ops/_log_softmax.h>
// #include <ATen/ops/_log_softmax_backward_data.h>
// #include <ATen/ops/_logcumsumexp.h>
// #include <ATen/ops/_lstm_mps.h>
// #include <ATen/ops/_lu_with_info.h>
// #include <ATen/ops/_make_dual.h>
// #include <ATen/ops/_make_dual_copy.h>
// #include <ATen/ops/_make_per_channel_quantized_tensor.h>
// #include <ATen/ops/_make_per_tensor_quantized_tensor.h>
// #include <ATen/ops/_masked_scale.h>
// #include <ATen/ops/_masked_softmax.h>
// #include <ATen/ops/_masked_softmax_backward.h>
// #include <ATen/ops/_mkldnn_reshape.h>
// #include <ATen/ops/_mkldnn_transpose.h>
// #include <ATen/ops/_mps_convolution.h>
// #include <ATen/ops/_mps_convolution_transpose.h>
// #include <ATen/ops/_native_batch_norm_legit.h>
// #include <ATen/ops/_native_decoder_only_multi_head_attention.h>
// #include <ATen/ops/_native_multi_head_attention.h>
// #include <ATen/ops/_neg_view.h>
// #include <ATen/ops/_neg_view_copy.h>
// #include <ATen/ops/_nested_from_padded.h>
// #include <ATen/ops/_nested_from_padded_and_nested_example.h>
// #include <ATen/ops/_nested_select_backward.h>
// #include <ATen/ops/_nested_sum_backward.h>
// #include <ATen/ops/_nested_tensor_from_mask.h>
// #include <ATen/ops/_nested_tensor_from_mask_left_aligned.h>
// #include <ATen/ops/_nested_tensor_from_tensor_list.h>
// #include <ATen/ops/_nested_tensor_offsets.h>
// #include <ATen/ops/_nested_tensor_size.h>
// #include <ATen/ops/_nested_tensor_softmax_with_shape.h>
// #include <ATen/ops/_nested_tensor_strides.h>
// #include <ATen/ops/_nested_view_from_buffer.h>
// #include <ATen/ops/_nested_view_from_buffer_copy.h>
// #include <ATen/ops/_new_zeros_with_same_feature_meta.h>
// #include <ATen/ops/_nnpack_available.h>
// #include <ATen/ops/_nnpack_spatial_convolution.h>
// #include <ATen/ops/_nnz.h>
// #include <ATen/ops/_pack_padded_sequence.h>
// #include <ATen/ops/_pack_padded_sequence_backward.h>
// #include <ATen/ops/_pad_circular.h>
// #include <ATen/ops/_pad_enum.h>
// #include <ATen/ops/_pad_packed_sequence.h>
// #include <ATen/ops/_pdist_backward.h>
// #include <ATen/ops/_pdist_forward.h>
// #include <ATen/ops/_pin_memory.h>
// #include <ATen/ops/_prelu_kernel.h>
// #include <ATen/ops/_prelu_kernel_backward.h>
// #include <ATen/ops/_remove_batch_dim.h>
// #include <ATen/ops/_reshape_alias.h>
// #include <ATen/ops/_reshape_alias_copy.h>
// #include <ATen/ops/_reshape_copy.h>
// #include <ATen/ops/_reshape_from_tensor.h>
// #include <ATen/ops/_resize_output.h>
// #include <ATen/ops/_rowwise_prune.h>
// #include <ATen/ops/_sample_dirichlet.h>
// #include <ATen/ops/_saturate_weight_to_fp16.h>
// #include <ATen/ops/_scaled_dot_product_attention.h>
// #include <ATen/ops/_scaled_dot_product_attention_math.h>
// #include <ATen/ops/_scaled_dot_product_efficient_attention.h>
// #include <ATen/ops/_scaled_dot_product_efficient_attention_backward.h>
// #include <ATen/ops/_scaled_dot_product_flash_attention.h>
// #include <ATen/ops/_scaled_dot_product_flash_attention_backward.h>
// #include <ATen/ops/_segment_reduce_backward.h>
// #include <ATen/ops/_shape_as_tensor.h>
// #include <ATen/ops/_slow_conv2d_backward.h>
// #include <ATen/ops/_slow_conv2d_forward.h>
// #include <ATen/ops/_sobol_engine_draw.h>
// #include <ATen/ops/_sobol_engine_ff.h>
// #include <ATen/ops/_sobol_engine_initialize_state.h>
// #include <ATen/ops/_sobol_engine_scramble.h>
// #include <ATen/ops/_softmax.h>
// #include <ATen/ops/_softmax_backward_data.h>
// #include <ATen/ops/_sparse_addmm.h>
// #include <ATen/ops/_sparse_broadcast_to.h>
// #include <ATen/ops/_sparse_broadcast_to_copy.h>
// #include <ATen/ops/_sparse_bsc_tensor_unsafe.h>
// #include <ATen/ops/_sparse_bsr_tensor_unsafe.h>
// #include <ATen/ops/_sparse_compressed_tensor_unsafe.h>
// #include <ATen/ops/_sparse_coo_tensor_unsafe.h>
// #include <ATen/ops/_sparse_coo_tensor_with_dims.h>
// #include <ATen/ops/_sparse_coo_tensor_with_dims_and_tensors.h>
// #include <ATen/ops/_sparse_csc_tensor_unsafe.h>
// #include <ATen/ops/_sparse_csr_prod.h>
// #include <ATen/ops/_sparse_csr_sum.h>
// #include <ATen/ops/_sparse_csr_tensor_unsafe.h>
// #include <ATen/ops/_sparse_log_softmax.h>
// #include <ATen/ops/_sparse_log_softmax_backward_data.h>
// #include <ATen/ops/_sparse_mm.h>
// #include <ATen/ops/_sparse_mm_reduce_impl.h>
// #include <ATen/ops/_sparse_mm_reduce_impl_backward.h>
// #include <ATen/ops/_sparse_softmax.h>
// #include <ATen/ops/_sparse_softmax_backward_data.h>
// #include <ATen/ops/_sparse_sparse_matmul.h>
// #include <ATen/ops/_sparse_sum.h>
// #include <ATen/ops/_sparse_sum_backward.h>
// #include <ATen/ops/_spdiags.h>
// #include <ATen/ops/_stack.h>
// #include <ATen/ops/_standard_gamma.h>
// #include <ATen/ops/_standard_gamma_grad.h>
// #include <ATen/ops/_test_ambiguous_defaults.h>
// #include <ATen/ops/_test_autograd_multiple_dispatch.h>
// #include <ATen/ops/_test_autograd_multiple_dispatch_view.h>
// #include <ATen/ops/_test_autograd_multiple_dispatch_view_copy.h>
// #include <ATen/ops/_test_check_tensor.h>
// #include <ATen/ops/_test_optional_filled_intlist.h>
// #include <ATen/ops/_test_optional_floatlist.h>
// #include <ATen/ops/_test_optional_intlist.h>
// #include <ATen/ops/_test_serialization_subcmul.h>
// #include <ATen/ops/_test_string_default.h>
// #include <ATen/ops/_test_warn_in_autograd.h>
// #include <ATen/ops/_thnn_differentiable_gru_cell_backward.h>
// #include <ATen/ops/_thnn_differentiable_lstm_cell_backward.h>
// #include <ATen/ops/_thnn_fused_gru_cell.h>
// #include <ATen/ops/_thnn_fused_gru_cell_backward.h>
// #include <ATen/ops/_thnn_fused_lstm_cell.h>
// #include <ATen/ops/_thnn_fused_lstm_cell_backward.h>
// #include <ATen/ops/_thnn_fused_lstm_cell_backward_impl.h>
// #include <ATen/ops/_to_copy.h>
// #include <ATen/ops/_to_cpu.h>
// #include <ATen/ops/_to_dense.h>
// #include <ATen/ops/_transform_bias_rescale_qkv.h>
// #include <ATen/ops/_transformer_decoder_only_layer_fwd.h>
// #include <ATen/ops/_transformer_encoder_layer_fwd.h>
// #include <ATen/ops/_trilinear.h>
// #include <ATen/ops/_triton_multi_head_attention.h>
// #include <ATen/ops/_triton_scaled_dot_attention.h>
// #include <ATen/ops/_unique.h>
// #include <ATen/ops/_unique2.h>
// #include <ATen/ops/_unpack_dual.h>
// #include <ATen/ops/_unsafe_view.h>
// #include <ATen/ops/_upsample_bicubic2d_aa.h>
// #include <ATen/ops/_upsample_bicubic2d_aa_backward.h>
// #include <ATen/ops/_upsample_bilinear2d_aa.h>
// #include <ATen/ops/_upsample_bilinear2d_aa_backward.h>
// #include <ATen/ops/_upsample_nearest_exact1d.h>
// #include <ATen/ops/_upsample_nearest_exact1d_backward.h>
// #include <ATen/ops/_upsample_nearest_exact2d.h>
// #include <ATen/ops/_upsample_nearest_exact2d_backward.h>
// #include <ATen/ops/_upsample_nearest_exact3d.h>
// #include <ATen/ops/_upsample_nearest_exact3d_backward.h>
// #include <ATen/ops/_use_cudnn_ctc_loss.h>
// #include <ATen/ops/_use_cudnn_rnn_flatten_weight.h>
// #include <ATen/ops/_validate_compressed_sparse_indices.h>
// #include <ATen/ops/_validate_sparse_bsc_tensor_args.h>
// #include <ATen/ops/_validate_sparse_bsr_tensor_args.h>
// #include <ATen/ops/_validate_sparse_compressed_tensor_args.h>
// #include <ATen/ops/_validate_sparse_coo_tensor_args.h>
// #include <ATen/ops/_validate_sparse_csc_tensor_args.h>
// #include <ATen/ops/_validate_sparse_csr_tensor_args.h>
// #include <ATen/ops/_values.h>
// #include <ATen/ops/_values_copy.h>
// #include <ATen/ops/_version.h>
// #include <ATen/ops/_weight_norm.h>
// #include <ATen/ops/_weight_norm_differentiable_backward.h>
// #include <ATen/ops/_weight_norm_interface.h>
// #include <ATen/ops/_weight_norm_interface_backward.h>
// #include <ATen/ops/abs.h>
// #include <ATen/ops/absolute.h>
// #include <ATen/ops/acos.h>
// #include <ATen/ops/acosh.h>
// #include <ATen/ops/adaptive_avg_pool1d.h>
// #include <ATen/ops/adaptive_avg_pool2d.h>
// #include <ATen/ops/adaptive_avg_pool3d.h>
// #include <ATen/ops/adaptive_avg_pool3d_backward.h>
// #include <ATen/ops/adaptive_max_pool1d.h>
// #include <ATen/ops/adaptive_max_pool2d.h>
// #include <ATen/ops/adaptive_max_pool2d_backward.h>
// #include <ATen/ops/adaptive_max_pool3d.h>
// #include <ATen/ops/adaptive_max_pool3d_backward.h>
// #include <ATen/ops/add.h>
// #include <ATen/ops/addbmm.h>
// #include <ATen/ops/addcdiv.h>
// #include <ATen/ops/addcmul.h>
// #include <ATen/ops/addmm.h>
// #include <ATen/ops/addmv.h>
// #include <ATen/ops/addr.h>
// #include <ATen/ops/adjoint.h>
// #include <ATen/ops/affine_grid_generator.h>
// #include <ATen/ops/affine_grid_generator_backward.h>
// #include <ATen/ops/alias.h>
// #include <ATen/ops/alias_copy.h>
// #include <ATen/ops/align_as.h>
// #include <ATen/ops/align_tensors.h>
// #include <ATen/ops/align_to.h>
// #include <ATen/ops/all.h>
// #include <ATen/ops/allclose.h>
// #include <ATen/ops/alpha_dropout.h>
// #include <ATen/ops/amax.h>
// #include <ATen/ops/amin.h>
// #include <ATen/ops/aminmax.h>
// #include <ATen/ops/and.h>
// #include <ATen/ops/angle.h>
// #include <ATen/ops/any.h>
// #include <ATen/ops/arange.h>
// #include <ATen/ops/arccos.h>
// #include <ATen/ops/arccosh.h>
// #include <ATen/ops/arcsin.h>
// #include <ATen/ops/arcsinh.h>
// #include <ATen/ops/arctan.h>
// #include <ATen/ops/arctan2.h>
// #include <ATen/ops/arctanh.h>
// #include <ATen/ops/argmax.h>
// #include <ATen/ops/argmin.h>
// #include <ATen/ops/argsort.h>
// #include <ATen/ops/argwhere.h>
// #include <ATen/ops/as_strided.h>
// #include <ATen/ops/as_strided_copy.h>
// #include <ATen/ops/as_strided_scatter.h>
// #include <ATen/ops/asin.h>
// #include <ATen/ops/asinh.h>
// #include <ATen/ops/atan.h>
// #include <ATen/ops/atan2.h>
// #include <ATen/ops/atanh.h>
// #include <ATen/ops/atleast_1d.h>
// #include <ATen/ops/atleast_2d.h>
// #include <ATen/ops/atleast_3d.h>
// #include <ATen/ops/avg_pool1d.h>
// #include <ATen/ops/avg_pool2d.h>
// #include <ATen/ops/avg_pool2d_backward.h>
// #include <ATen/ops/avg_pool3d.h>
// #include <ATen/ops/avg_pool3d_backward.h>
// #include <ATen/ops/baddbmm.h>
// #include <ATen/ops/bartlett_window.h>
// #include <ATen/ops/batch_norm.h>
// #include <ATen/ops/batch_norm_backward_elemt.h>
// #include <ATen/ops/batch_norm_backward_reduce.h>
// #include <ATen/ops/batch_norm_elemt.h>
// #include <ATen/ops/batch_norm_gather_stats.h>
// #include <ATen/ops/batch_norm_gather_stats_with_counts.h>
// #include <ATen/ops/batch_norm_stats.h>
// #include <ATen/ops/batch_norm_update_stats.h>
// #include <ATen/ops/bernoulli.h>
// #include <ATen/ops/bilinear.h>
// #include <ATen/ops/binary_cross_entropy.h>
// #include <ATen/ops/binary_cross_entropy_backward.h>
// #include <ATen/ops/binary_cross_entropy_with_logits.h>
// #include <ATen/ops/bincount.h>
// #include <ATen/ops/binomial.h>
// #include <ATen/ops/bitwise_and.h>
// #include <ATen/ops/bitwise_left_shift.h>
// #include <ATen/ops/bitwise_not.h>
// #include <ATen/ops/bitwise_or.h>
// #include <ATen/ops/bitwise_right_shift.h>
// #include <ATen/ops/bitwise_xor.h>
// #include <ATen/ops/blackman_window.h>
// #include <ATen/ops/block_diag.h>
// #include <ATen/ops/bmm.h>
// #include <ATen/ops/broadcast_tensors.h>
// #include <ATen/ops/broadcast_to.h>
// #include <ATen/ops/bucketize.h>
// #include <ATen/ops/can_cast.h>
// #include <ATen/ops/cartesian_prod.h>
// #include <ATen/ops/cat.h>
// #include <ATen/ops/cauchy.h>
// #include <ATen/ops/ccol_indices.h>
// #include <ATen/ops/ccol_indices_copy.h>
// #include <ATen/ops/cdist.h>
// #include <ATen/ops/ceil.h>
// #include <ATen/ops/celu.h>
// #include <ATen/ops/chain_matmul.h>
// #include <ATen/ops/chalf.h>
// #include <ATen/ops/channel_shuffle.h>
// #include <ATen/ops/cholesky.h>
// #include <ATen/ops/cholesky_inverse.h>
// #include <ATen/ops/cholesky_solve.h>
// #include <ATen/ops/choose_qparams_optimized.h>
// #include <ATen/ops/chunk.h>
// #include <ATen/ops/clamp.h>
// #include <ATen/ops/clamp_max.h>
// #include <ATen/ops/clamp_min.h>
// #include <ATen/ops/clip.h>
// #include <ATen/ops/clone.h>
// #include <ATen/ops/coalesce.h>
// #include <ATen/ops/col2im.h>
// #include <ATen/ops/col_indices.h>
// #include <ATen/ops/col_indices_copy.h>
// #include <ATen/ops/column_stack.h>
// #include <ATen/ops/combinations.h>
// #include <ATen/ops/complex.h>
// #include <ATen/ops/concat.h>
// #include <ATen/ops/concatenate.h>
// #include <ATen/ops/conj.h>
// #include <ATen/ops/conj_physical.h>
// #include <ATen/ops/constant_pad_nd.h>
// #include <ATen/ops/contiguous.h>
// #include <ATen/ops/conv1d.h>
// #include <ATen/ops/conv2d.h>
// #include <ATen/ops/conv3d.h>
// #include <ATen/ops/conv_depthwise3d.h>
// #include <ATen/ops/conv_tbc.h>
// #include <ATen/ops/conv_tbc_backward.h>
// #include <ATen/ops/conv_transpose1d.h>
// #include <ATen/ops/conv_transpose2d.h>
// #include <ATen/ops/conv_transpose3d.h>
// #include <ATen/ops/convolution.h>
// #include <ATen/ops/convolution_backward.h>
// #include <ATen/ops/convolution_backward_overrideable.h>
// #include <ATen/ops/convolution_overrideable.h>
// #include <ATen/ops/copy.h>
// #include <ATen/ops/copy_sparse_to_sparse.h>
// #include <ATen/ops/copysign.h>
// #include <ATen/ops/corrcoef.h>
// #include <ATen/ops/cos.h>
// #include <ATen/ops/cosh.h>
// #include <ATen/ops/cosine_embedding_loss.h>
// #include <ATen/ops/cosine_similarity.h>
// #include <ATen/ops/count_nonzero.h>
// #include <ATen/ops/cov.h>
// #include <ATen/ops/cross.h>
// #include <ATen/ops/cross_entropy_loss.h>
// #include <ATen/ops/crow_indices.h>
// #include <ATen/ops/crow_indices_copy.h>
// #include <ATen/ops/ctc_loss.h>
// #include <ATen/ops/cudnn_affine_grid_generator.h>
// #include <ATen/ops/cudnn_affine_grid_generator_backward.h>
// #include <ATen/ops/cudnn_batch_norm.h>
// #include <ATen/ops/cudnn_batch_norm_backward.h>
// #include <ATen/ops/cudnn_convolution.h>
// #include <ATen/ops/cudnn_convolution_add_relu.h>
// #include <ATen/ops/cudnn_convolution_relu.h>
// #include <ATen/ops/cudnn_convolution_transpose.h>
// #include <ATen/ops/cudnn_grid_sampler.h>
// #include <ATen/ops/cudnn_grid_sampler_backward.h>
// #include <ATen/ops/cudnn_is_acceptable.h>
// #include <ATen/ops/cummax.h>
// #include <ATen/ops/cummaxmin_backward.h>
// #include <ATen/ops/cummin.h>
// #include <ATen/ops/cumprod.h>
// #include <ATen/ops/cumprod_backward.h>
// #include <ATen/ops/cumsum.h>
// #include <ATen/ops/cumulative_trapezoid.h>
// #include <ATen/ops/data.h>
// #include <ATen/ops/deg2rad.h>
// #include <ATen/ops/dense_dim.h>
// #include <ATen/ops/dequantize.h>
// #include <ATen/ops/det.h>
// #include <ATen/ops/detach.h>
// #include <ATen/ops/detach_copy.h>
// #include <ATen/ops/diag.h>
// #include <ATen/ops/diag_embed.h>
// #include <ATen/ops/diagflat.h>
// #include <ATen/ops/diagonal.h>
// #include <ATen/ops/diagonal_backward.h>
// #include <ATen/ops/diagonal_copy.h>
// #include <ATen/ops/diagonal_scatter.h>
// #include <ATen/ops/diff.h>
// #include <ATen/ops/digamma.h>
// #include <ATen/ops/dist.h>
// #include <ATen/ops/div.h>
// #include <ATen/ops/divide.h>
// #include <ATen/ops/dot.h>
// #include <ATen/ops/dropout.h>
// #include <ATen/ops/dsplit.h>
// #include <ATen/ops/dstack.h>
// #include <ATen/ops/einsum.h>
// #include <ATen/ops/elu.h>
// #include <ATen/ops/elu_backward.h>
// #include <ATen/ops/embedding.h>
// #include <ATen/ops/embedding_backward.h>
// #include <ATen/ops/embedding_bag.h>
// #include <ATen/ops/embedding_dense_backward.h>
// #include <ATen/ops/embedding_renorm.h>
// #include <ATen/ops/embedding_sparse_backward.h>
// #include <ATen/ops/empty.h>
// #include <ATen/ops/empty_like.h>
// #include <ATen/ops/empty_quantized.h>
// #include <ATen/ops/empty_strided.h>
// #include <ATen/ops/eq.h>
// #include <ATen/ops/equal.h>
// #include <ATen/ops/erf.h>
// #include <ATen/ops/erfc.h>
// #include <ATen/ops/erfinv.h>
// #include <ATen/ops/exp.h>
// #include <ATen/ops/exp2.h>
// #include <ATen/ops/expand.h>
// #include <ATen/ops/expand_as.h>
// #include <ATen/ops/expand_copy.h>
// #include <ATen/ops/expm1.h>
// #include <ATen/ops/exponential.h>
// #include <ATen/ops/eye.h>
// #include <ATen/ops/fake_quantize_per_channel_affine.h>
// #include <ATen/ops/fake_quantize_per_channel_affine_cachemask.h>
// #include <ATen/ops/fake_quantize_per_channel_affine_cachemask_backward.h>
// #include <ATen/ops/fake_quantize_per_tensor_affine.h>
// #include <ATen/ops/fake_quantize_per_tensor_affine_cachemask.h>
// #include <ATen/ops/fake_quantize_per_tensor_affine_cachemask_backward.h>
// #include <ATen/ops/fbgemm_linear_fp16_weight.h>
// #include <ATen/ops/fbgemm_linear_fp16_weight_fp32_activation.h>
// #include <ATen/ops/fbgemm_linear_int8_weight.h>
// #include <ATen/ops/fbgemm_linear_int8_weight_fp32_activation.h>
// #include <ATen/ops/fbgemm_linear_quantize_weight.h>
// #include <ATen/ops/fbgemm_pack_gemm_matrix_fp16.h>
// #include <ATen/ops/fbgemm_pack_quantized_matrix.h>
// #include <ATen/ops/feature_alpha_dropout.h>
// #include <ATen/ops/feature_dropout.h>
// #include <ATen/ops/fft_fft.h>
// #include <ATen/ops/fft_fft2.h>
// #include <ATen/ops/fft_fftfreq.h>
// #include <ATen/ops/fft_fftn.h>
// #include <ATen/ops/fft_fftshift.h>
// #include <ATen/ops/fft_hfft.h>
// #include <ATen/ops/fft_hfft2.h>
// #include <ATen/ops/fft_hfftn.h>
// #include <ATen/ops/fft_ifft.h>
// #include <ATen/ops/fft_ifft2.h>
// #include <ATen/ops/fft_ifftn.h>
// #include <ATen/ops/fft_ifftshift.h>
// #include <ATen/ops/fft_ihfft.h>
// #include <ATen/ops/fft_ihfft2.h>
// #include <ATen/ops/fft_ihfftn.h>
// #include <ATen/ops/fft_irfft.h>
// #include <ATen/ops/fft_irfft2.h>
// #include <ATen/ops/fft_irfftn.h>
// #include <ATen/ops/fft_rfft.h>
// #include <ATen/ops/fft_rfft2.h>
// #include <ATen/ops/fft_rfftfreq.h>
// #include <ATen/ops/fft_rfftn.h>
// #include <ATen/ops/fill.h>
// #include <ATen/ops/fill_diagonal.h>
// #include <ATen/ops/fix.h>
// #include <ATen/ops/flatten.h>
// #include <ATen/ops/flatten_dense_tensors.h>
// #include <ATen/ops/flip.h>
// #include <ATen/ops/fliplr.h>
// #include <ATen/ops/flipud.h>
// #include <ATen/ops/float_power.h>
// #include <ATen/ops/floor.h>
// #include <ATen/ops/floor_divide.h>
// #include <ATen/ops/fmax.h>
// #include <ATen/ops/fmin.h>
// #include <ATen/ops/fmod.h>
// #include <ATen/ops/frac.h>
// #include <ATen/ops/fractional_max_pool2d.h>
// #include <ATen/ops/fractional_max_pool2d_backward.h>
// #include <ATen/ops/fractional_max_pool3d.h>
// #include <ATen/ops/fractional_max_pool3d_backward.h>
// #include <ATen/ops/frexp.h>
// #include <ATen/ops/frobenius_norm.h>
// #include <ATen/ops/from_file.h>
// #include <ATen/ops/full.h>
// #include <ATen/ops/full_like.h>
// #include <ATen/ops/fused_moving_avg_obs_fake_quant.h>
// #include <ATen/ops/gather.h>
// #include <ATen/ops/gather_backward.h>
// #include <ATen/ops/gcd.h>
// #include <ATen/ops/ge.h>
// #include <ATen/ops/gelu.h>
// #include <ATen/ops/gelu_backward.h>
// #include <ATen/ops/geometric.h>
// #include <ATen/ops/geqrf.h>
// #include <ATen/ops/ger.h>
// #include <ATen/ops/glu.h>
// #include <ATen/ops/glu_backward.h>
// #include <ATen/ops/glu_backward_jvp.h>
// #include <ATen/ops/glu_jvp.h>
// #include <ATen/ops/gradient.h>
// #include <ATen/ops/greater.h>
// #include <ATen/ops/greater_equal.h>
// #include <ATen/ops/grid_sampler.h>
// #include <ATen/ops/grid_sampler_2d.h>
// #include <ATen/ops/grid_sampler_2d_backward.h>
// #include <ATen/ops/grid_sampler_3d.h>
// #include <ATen/ops/grid_sampler_3d_backward.h>
// #include <ATen/ops/group_norm.h>
// #include <ATen/ops/gru.h>
// #include <ATen/ops/gru_cell.h>
// #include <ATen/ops/gt.h>
// #include <ATen/ops/hamming_window.h>
// #include <ATen/ops/hann_window.h>
// #include <ATen/ops/hardshrink.h>
// #include <ATen/ops/hardshrink_backward.h>
// #include <ATen/ops/hardsigmoid.h>
// #include <ATen/ops/hardsigmoid_backward.h>
// #include <ATen/ops/hardswish.h>
// #include <ATen/ops/hardswish_backward.h>
// #include <ATen/ops/hardtanh.h>
// #include <ATen/ops/hardtanh_backward.h>
// #include <ATen/ops/heaviside.h>
// #include <ATen/ops/hinge_embedding_loss.h>
// #include <ATen/ops/histc.h>
// #include <ATen/ops/histogram.h>
// #include <ATen/ops/histogramdd.h>
// #include <ATen/ops/hsplit.h>
// #include <ATen/ops/hspmm.h>
// #include <ATen/ops/hstack.h>
// #include <ATen/ops/huber_loss.h>
// #include <ATen/ops/huber_loss_backward.h>
// #include <ATen/ops/hypot.h>
// #include <ATen/ops/i0.h>
// #include <ATen/ops/igamma.h>
// #include <ATen/ops/igammac.h>
// #include <ATen/ops/im2col.h>
// #include <ATen/ops/imag.h>
// #include <ATen/ops/index.h>
// #include <ATen/ops/index_add.h>
// #include <ATen/ops/index_copy.h>
// #include <ATen/ops/index_fill.h>
// #include <ATen/ops/index_put.h>
// #include <ATen/ops/index_reduce.h>
// #include <ATen/ops/index_select.h>
// #include <ATen/ops/index_select_backward.h>
// #include <ATen/ops/indices.h>
// #include <ATen/ops/indices_copy.h>
// #include <ATen/ops/infinitely_differentiable_gelu_backward.h>
// #include <ATen/ops/inner.h>
// #include <ATen/ops/instance_norm.h>
// #include <ATen/ops/int_repr.h>
// #include <ATen/ops/inverse.h>
// #include <ATen/ops/is_coalesced.h>
// #include <ATen/ops/is_complex.h>
// #include <ATen/ops/is_conj.h>
// #include <ATen/ops/is_distributed.h>
// #include <ATen/ops/is_floating_point.h>
// #include <ATen/ops/is_inference.h>
// #include <ATen/ops/is_leaf.h>
// #include <ATen/ops/is_neg.h>
// #include <ATen/ops/is_nonzero.h>
// #include <ATen/ops/is_pinned.h>
// #include <ATen/ops/is_same_size.h>
// #include <ATen/ops/is_set_to.h>
// #include <ATen/ops/is_signed.h>
// #include <ATen/ops/is_vulkan_available.h>
// #include <ATen/ops/isclose.h>
// #include <ATen/ops/isfinite.h>
// #include <ATen/ops/isin.h>
// #include <ATen/ops/isinf.h>
// #include <ATen/ops/isnan.h>
// #include <ATen/ops/isneginf.h>
// #include <ATen/ops/isposinf.h>
// #include <ATen/ops/isreal.h>
// #include <ATen/ops/istft.h>
// #include <ATen/ops/item.h>
// #include <ATen/ops/kaiser_window.h>
// #include <ATen/ops/kl_div.h>
// #include <ATen/ops/kron.h>
// #include <ATen/ops/kthvalue.h>
// #include <ATen/ops/l1_loss.h>
// #include <ATen/ops/layer_norm.h>
// #include <ATen/ops/lcm.h>
// #include <ATen/ops/ldexp.h>
// #include <ATen/ops/le.h>
// #include <ATen/ops/leaky_relu.h>
// #include <ATen/ops/leaky_relu_backward.h>
// #include <ATen/ops/lerp.h>
// #include <ATen/ops/less.h>
// #include <ATen/ops/less_equal.h>
// #include <ATen/ops/lgamma.h>
// #include <ATen/ops/lift.h>
// #include <ATen/ops/lift_fresh.h>
// #include <ATen/ops/lift_fresh_copy.h>
// #include <ATen/ops/linalg_cholesky.h>
// #include <ATen/ops/linalg_cholesky_ex.h>
// #include <ATen/ops/linalg_cond.h>
// #include <ATen/ops/linalg_cross.h>
// #include <ATen/ops/linalg_det.h>
// #include <ATen/ops/linalg_diagonal.h>
// #include <ATen/ops/linalg_eig.h>
// #include <ATen/ops/linalg_eigh.h>
// #include <ATen/ops/linalg_eigvals.h>
// #include <ATen/ops/linalg_eigvalsh.h>
// #include <ATen/ops/linalg_householder_product.h>
// #include <ATen/ops/linalg_inv.h>
// #include <ATen/ops/linalg_inv_ex.h>
// #include <ATen/ops/linalg_ldl_factor.h>
// #include <ATen/ops/linalg_ldl_factor_ex.h>
// #include <ATen/ops/linalg_ldl_solve.h>
// #include <ATen/ops/linalg_lstsq.h>
// #include <ATen/ops/linalg_lu.h>
// #include <ATen/ops/linalg_lu_factor.h>
// #include <ATen/ops/linalg_lu_factor_ex.h>
// #include <ATen/ops/linalg_lu_solve.h>
// #include <ATen/ops/linalg_matmul.h>
// #include <ATen/ops/linalg_matrix_exp.h>
// #include <ATen/ops/linalg_matrix_norm.h>
// #include <ATen/ops/linalg_matrix_power.h>
// #include <ATen/ops/linalg_matrix_rank.h>
// #include <ATen/ops/linalg_multi_dot.h>
// #include <ATen/ops/linalg_norm.h>
// #include <ATen/ops/linalg_pinv.h>
// #include <ATen/ops/linalg_qr.h>
// #include <ATen/ops/linalg_slogdet.h>
// #include <ATen/ops/linalg_solve.h>
// #include <ATen/ops/linalg_solve_ex.h>
// #include <ATen/ops/linalg_solve_triangular.h>
// #include <ATen/ops/linalg_svd.h>
// #include <ATen/ops/linalg_svdvals.h>
// #include <ATen/ops/linalg_tensorinv.h>
// #include <ATen/ops/linalg_tensorsolve.h>
// #include <ATen/ops/linalg_vander.h>
// #include <ATen/ops/linalg_vecdot.h>
// #include <ATen/ops/linalg_vector_norm.h>
// #include <ATen/ops/linear.h>
// #include <ATen/ops/linear_backward.h>
// #include <ATen/ops/linspace.h>
// #include <ATen/ops/log.h>
// #include <ATen/ops/log10.h>
// #include <ATen/ops/log1p.h>
// #include <ATen/ops/log2.h>
// #include <ATen/ops/log_normal.h>
// #include <ATen/ops/log_sigmoid.h>
// #include <ATen/ops/log_sigmoid_backward.h>
// #include <ATen/ops/log_sigmoid_forward.h>
// #include <ATen/ops/log_softmax.h>
// #include <ATen/ops/logaddexp.h>
// #include <ATen/ops/logaddexp2.h>
// #include <ATen/ops/logcumsumexp.h>
// #include <ATen/ops/logdet.h>
// #include <ATen/ops/logical_and.h>
// #include <ATen/ops/logical_not.h>
// #include <ATen/ops/logical_or.h>
// #include <ATen/ops/logical_xor.h>
// #include <ATen/ops/logit.h>
// #include <ATen/ops/logit_backward.h>
// #include <ATen/ops/logspace.h>
// #include <ATen/ops/logsumexp.h>
// #include <ATen/ops/lshift.h>
// #include <ATen/ops/lstm.h>
// #include <ATen/ops/lstm_cell.h>
// #include <ATen/ops/lstm_mps_backward.h>
// #include <ATen/ops/lt.h>
// #include <ATen/ops/lu_solve.h>
// #include <ATen/ops/lu_unpack.h>
// #include <ATen/ops/mH.h>
// #include <ATen/ops/mT.h>
// #include <ATen/ops/margin_ranking_loss.h>
// #include <ATen/ops/masked_fill.h>
// #include <ATen/ops/masked_scatter.h>
// #include <ATen/ops/masked_select.h>
// #include <ATen/ops/masked_select_backward.h>
// #include <ATen/ops/matmul.h>
// #include <ATen/ops/matmul_backward.h>
// #include <ATen/ops/matrix_H.h>
// #include <ATen/ops/matrix_exp.h>
// #include <ATen/ops/matrix_exp_backward.h>
// #include <ATen/ops/matrix_power.h>
// #include <ATen/ops/max.h>
// #include <ATen/ops/max_pool1d.h>
// #include <ATen/ops/max_pool1d_with_indices.h>
// #include <ATen/ops/max_pool2d.h>
// #include <ATen/ops/max_pool2d_backward.h>
// #include <ATen/ops/max_pool2d_with_indices.h>
// #include <ATen/ops/max_pool2d_with_indices_backward.h>
// #include <ATen/ops/max_pool3d.h>
// #include <ATen/ops/max_pool3d_with_indices.h>
// #include <ATen/ops/max_pool3d_with_indices_backward.h>
// #include <ATen/ops/max_unpool2d.h>
// #include <ATen/ops/max_unpool3d.h>
// #include <ATen/ops/maximum.h>
// #include <ATen/ops/mean.h>
// #include <ATen/ops/median.h>
// #include <ATen/ops/meshgrid.h>
// #include <ATen/ops/min.h>
// #include <ATen/ops/minimum.h>
// #include <ATen/ops/miopen_batch_norm.h>
// #include <ATen/ops/miopen_batch_norm_backward.h>
// #include <ATen/ops/miopen_convolution.h>
// #include <ATen/ops/miopen_convolution_add_relu.h>
// #include <ATen/ops/miopen_convolution_relu.h>
// #include <ATen/ops/miopen_convolution_transpose.h>
// #include <ATen/ops/miopen_depthwise_convolution.h>
// #include <ATen/ops/miopen_rnn.h>
// #include <ATen/ops/miopen_rnn_backward.h>
// #include <ATen/ops/mish.h>
// #include <ATen/ops/mish_backward.h>
// #include <ATen/ops/mkldnn_adaptive_avg_pool2d.h>
// #include <ATen/ops/mkldnn_adaptive_avg_pool2d_backward.h>
// #include <ATen/ops/mkldnn_convolution.h>
// #include <ATen/ops/mkldnn_linear.h>
// #include <ATen/ops/mkldnn_linear_backward.h>
// #include <ATen/ops/mkldnn_linear_backward_input.h>
// #include <ATen/ops/mkldnn_linear_backward_weights.h>
// #include <ATen/ops/mkldnn_max_pool2d.h>
// #include <ATen/ops/mkldnn_max_pool2d_backward.h>
// #include <ATen/ops/mkldnn_max_pool3d.h>
// #include <ATen/ops/mkldnn_max_pool3d_backward.h>
// #include <ATen/ops/mkldnn_reorder_conv2d_weight.h>
// #include <ATen/ops/mkldnn_reorder_conv3d_weight.h>
// #include <ATen/ops/mkldnn_rnn_layer.h>
// #include <ATen/ops/mkldnn_rnn_layer_backward.h>
// #include <ATen/ops/mm.h>
// #include <ATen/ops/mode.h>
// #include <ATen/ops/moveaxis.h>
// #include <ATen/ops/movedim.h>
// #include <ATen/ops/mps_convolution_backward.h>
// #include <ATen/ops/mps_convolution_transpose_backward.h>
// #include <ATen/ops/mse_loss.h>
// #include <ATen/ops/mse_loss_backward.h>
// #include <ATen/ops/msort.h>
// #include <ATen/ops/mul.h>
// #include <ATen/ops/multi_margin_loss.h>
// #include <ATen/ops/multi_margin_loss_backward.h>
// #include <ATen/ops/multilabel_margin_loss.h>
// #include <ATen/ops/multilabel_margin_loss_backward.h>
// #include <ATen/ops/multilabel_margin_loss_forward.h>
// #include <ATen/ops/multinomial.h>
// #include <ATen/ops/multiply.h>
// #include <ATen/ops/mv.h>
// #include <ATen/ops/mvlgamma.h>
// #include <ATen/ops/nan_to_num.h>
// #include <ATen/ops/nanmean.h>
// #include <ATen/ops/nanmedian.h>
// #include <ATen/ops/nanquantile.h>
// #include <ATen/ops/nansum.h>
// #include <ATen/ops/narrow.h>
// #include <ATen/ops/narrow_copy.h>
// #include <ATen/ops/native_batch_norm.h>
// #include <ATen/ops/native_batch_norm_backward.h>
// #include <ATen/ops/native_channel_shuffle.h>
// #include <ATen/ops/native_dropout.h>
// #include <ATen/ops/native_dropout_backward.h>
// #include <ATen/ops/native_group_norm.h>
// #include <ATen/ops/native_group_norm_backward.h>
// #include <ATen/ops/native_layer_norm.h>
// #include <ATen/ops/native_layer_norm_backward.h>
// #include <ATen/ops/native_norm.h>
// #include <ATen/ops/ne.h>
// #include <ATen/ops/neg.h>
// #include <ATen/ops/negative.h>
// #include <ATen/ops/nested_to_padded_tensor.h>
// #include <ATen/ops/new_empty.h>
// #include <ATen/ops/new_empty_strided.h>
// #include <ATen/ops/new_full.h>
// #include <ATen/ops/new_ones.h>
// #include <ATen/ops/new_zeros.h>
// #include <ATen/ops/nextafter.h>
// #include <ATen/ops/nll_loss.h>
// #include <ATen/ops/nll_loss2d.h>
// #include <ATen/ops/nll_loss2d_backward.h>
// #include <ATen/ops/nll_loss2d_forward.h>
// #include <ATen/ops/nll_loss_backward.h>
// #include <ATen/ops/nll_loss_forward.h>
// #include <ATen/ops/nll_loss_nd.h>
// #include <ATen/ops/nonzero.h>
// #include <ATen/ops/nonzero_numpy.h>
// #include <ATen/ops/norm.h>
// #include <ATen/ops/norm_except_dim.h>
// #include <ATen/ops/normal.h>
// #include <ATen/ops/not_equal.h>
// #include <ATen/ops/nuclear_norm.h>
// #include <ATen/ops/numpy_T.h>
// #include <ATen/ops/one_hot.h>
// #include <ATen/ops/ones.h>
// #include <ATen/ops/ones_like.h>
// #include <ATen/ops/or.h>
// #include <ATen/ops/orgqr.h>
// #include <ATen/ops/ormqr.h>
// #include <ATen/ops/outer.h>
// #include <ATen/ops/output_nr.h>
// #include <ATen/ops/pad.h>
// #include <ATen/ops/pad_sequence.h>
// #include <ATen/ops/pairwise_distance.h>
// #include <ATen/ops/pdist.h>
// #include <ATen/ops/permute.h>
// #include <ATen/ops/permute_copy.h>
// #include <ATen/ops/pin_memory.h>
// #include <ATen/ops/pinverse.h>
// #include <ATen/ops/pixel_shuffle.h>
// #include <ATen/ops/pixel_unshuffle.h>
// #include <ATen/ops/poisson.h>
// #include <ATen/ops/poisson_nll_loss.h>
// #include <ATen/ops/polar.h>
// #include <ATen/ops/polygamma.h>
// #include <ATen/ops/positive.h>
// #include <ATen/ops/pow.h>
// #include <ATen/ops/prelu.h>
// #include <ATen/ops/prod.h>
// #include <ATen/ops/promote_types.h>
// #include <ATen/ops/put.h>
// #include <ATen/ops/q_per_channel_axis.h>
// #include <ATen/ops/q_per_channel_scales.h>
// #include <ATen/ops/q_per_channel_zero_points.h>
// #include <ATen/ops/q_scale.h>
// #include <ATen/ops/q_zero_point.h>
// #include <ATen/ops/qr.h>
// #include <ATen/ops/qscheme.h>
// #include <ATen/ops/quantile.h>
// #include <ATen/ops/quantize_per_channel.h>
// #include <ATen/ops/quantize_per_tensor.h>
// #include <ATen/ops/quantize_per_tensor_dynamic.h>
// #include <ATen/ops/quantized_batch_norm.h>
// #include <ATen/ops/quantized_gru_cell.h>
// #include <ATen/ops/quantized_lstm_cell.h>
// #include <ATen/ops/quantized_max_pool1d.h>
// #include <ATen/ops/quantized_max_pool2d.h>
// #include <ATen/ops/quantized_rnn_relu_cell.h>
// #include <ATen/ops/quantized_rnn_tanh_cell.h>
// #include <ATen/ops/rad2deg.h>
// #include <ATen/ops/rand.h>
// #include <ATen/ops/rand_like.h>
// #include <ATen/ops/randint.h>
// #include <ATen/ops/randint_like.h>
// #include <ATen/ops/randn.h>
// #include <ATen/ops/randn_like.h>
// #include <ATen/ops/random.h>
// #include <ATen/ops/randperm.h>
// #include <ATen/ops/range.h>
// #include <ATen/ops/ravel.h>
// #include <ATen/ops/real.h>
// #include <ATen/ops/reciprocal.h>
// #include <ATen/ops/record_stream.h>
// #include <ATen/ops/refine_names.h>
// #include <ATen/ops/reflection_pad1d.h>
// #include <ATen/ops/reflection_pad1d_backward.h>
// #include <ATen/ops/reflection_pad2d.h>
// #include <ATen/ops/reflection_pad2d_backward.h>
// #include <ATen/ops/reflection_pad3d.h>
// #include <ATen/ops/reflection_pad3d_backward.h>
// #include <ATen/ops/relu.h>
// #include <ATen/ops/relu6.h>
// #include <ATen/ops/remainder.h>
// #include <ATen/ops/rename.h>
// #include <ATen/ops/renorm.h>
// #include <ATen/ops/repeat.h>
// #include <ATen/ops/repeat_interleave.h>
// #include <ATen/ops/replication_pad1d.h>
// #include <ATen/ops/replication_pad1d_backward.h>
// #include <ATen/ops/replication_pad2d.h>
// #include <ATen/ops/replication_pad2d_backward.h>
// #include <ATen/ops/replication_pad3d.h>
// #include <ATen/ops/replication_pad3d_backward.h>
// #include <ATen/ops/requires_grad.h>
// #include <ATen/ops/reshape.h>
// #include <ATen/ops/reshape_as.h>
// #include <ATen/ops/resize.h>
// #include <ATen/ops/resize_as.h>
// #include <ATen/ops/resize_as_sparse.h>
// #include <ATen/ops/resolve_conj.h>
// #include <ATen/ops/resolve_neg.h>
// #include <ATen/ops/result_type.h>
// #include <ATen/ops/retain_grad.h>
// #include <ATen/ops/retains_grad.h>
// #include <ATen/ops/rnn_relu.h>
// #include <ATen/ops/rnn_relu_cell.h>
// #include <ATen/ops/rnn_tanh.h>
// #include <ATen/ops/rnn_tanh_cell.h>
// #include <ATen/ops/roll.h>
// #include <ATen/ops/rot90.h>
// #include <ATen/ops/round.h>
// #include <ATen/ops/row_indices.h>
// #include <ATen/ops/row_indices_copy.h>
// #include <ATen/ops/row_stack.h>
// #include <ATen/ops/rrelu.h>
// #include <ATen/ops/rrelu_with_noise.h>
// #include <ATen/ops/rrelu_with_noise_backward.h>
// #include <ATen/ops/rshift.h>
// #include <ATen/ops/rsqrt.h>
// #include <ATen/ops/rsub.h>
// #include <ATen/ops/scalar_tensor.h>
// #include <ATen/ops/scaled_dot_product_attention.h>
// #include <ATen/ops/scatter.h>
// #include <ATen/ops/scatter_add.h>
// #include <ATen/ops/scatter_reduce.h>
// #include <ATen/ops/searchsorted.h>
// #include <ATen/ops/segment_reduce.h>
// #include <ATen/ops/select.h>
// #include <ATen/ops/select_backward.h>
// #include <ATen/ops/select_copy.h>
// #include <ATen/ops/select_scatter.h>
// #include <ATen/ops/selu.h>
// #include <ATen/ops/set.h>
// #include <ATen/ops/set_data.h>
// #include <ATen/ops/sgn.h>
// #include <ATen/ops/sigmoid.h>
// #include <ATen/ops/sigmoid_backward.h>
// #include <ATen/ops/sign.h>
// #include <ATen/ops/signbit.h>
// #include <ATen/ops/silu.h>
// #include <ATen/ops/silu_backward.h>
// #include <ATen/ops/sin.h>
// #include <ATen/ops/sinc.h>
// #include <ATen/ops/sinh.h>
// #include <ATen/ops/size.h>
// #include <ATen/ops/slice.h>
// #include <ATen/ops/slice_backward.h>
// #include <ATen/ops/slice_copy.h>
// #include <ATen/ops/slice_scatter.h>
// #include <ATen/ops/slogdet.h>
// #include <ATen/ops/slow_conv3d.h>
// #include <ATen/ops/slow_conv3d_forward.h>
// #include <ATen/ops/slow_conv_dilated2d.h>
// #include <ATen/ops/slow_conv_dilated3d.h>
// #include <ATen/ops/slow_conv_transpose2d.h>
// #include <ATen/ops/slow_conv_transpose3d.h>
// #include <ATen/ops/smm.h>
// #include <ATen/ops/smooth_l1_loss.h>
// #include <ATen/ops/smooth_l1_loss_backward.h>
// #include <ATen/ops/soft_margin_loss.h>
// #include <ATen/ops/soft_margin_loss_backward.h>
// #include <ATen/ops/softmax.h>
// #include <ATen/ops/softplus.h>
// #include <ATen/ops/softplus_backward.h>
// #include <ATen/ops/softshrink.h>
// #include <ATen/ops/softshrink_backward.h>
// #include <ATen/ops/sort.h>
// #include <ATen/ops/sparse_bsc_tensor.h>
// #include <ATen/ops/sparse_bsr_tensor.h>
// #include <ATen/ops/sparse_compressed_tensor.h>
// #include <ATen/ops/sparse_coo_tensor.h>
// #include <ATen/ops/sparse_csc_tensor.h>
// #include <ATen/ops/sparse_csr_tensor.h>
// #include <ATen/ops/sparse_dim.h>
// #include <ATen/ops/sparse_mask.h>
// #include <ATen/ops/sparse_resize.h>
// #include <ATen/ops/sparse_resize_and_clear.h>
// #include <ATen/ops/sparse_sampled_addmm.h>
// #include <ATen/ops/special_airy_ai.h>
// #include <ATen/ops/special_bessel_j0.h>
// #include <ATen/ops/special_bessel_j1.h>
// #include <ATen/ops/special_bessel_y0.h>
// #include <ATen/ops/special_bessel_y1.h>
// #include <ATen/ops/special_chebyshev_polynomial_t.h>
// #include <ATen/ops/special_chebyshev_polynomial_u.h>
// #include <ATen/ops/special_chebyshev_polynomial_v.h>
// #include <ATen/ops/special_chebyshev_polynomial_w.h>
// #include <ATen/ops/special_digamma.h>
// #include <ATen/ops/special_entr.h>
// #include <ATen/ops/special_erf.h>
// #include <ATen/ops/special_erfc.h>
// #include <ATen/ops/special_erfcx.h>
// #include <ATen/ops/special_erfinv.h>
// #include <ATen/ops/special_exp2.h>
// #include <ATen/ops/special_expit.h>
// #include <ATen/ops/special_expm1.h>
// #include <ATen/ops/special_gammainc.h>
// #include <ATen/ops/special_gammaincc.h>
// #include <ATen/ops/special_gammaln.h>
// #include <ATen/ops/special_hermite_polynomial_h.h>
// #include <ATen/ops/special_hermite_polynomial_he.h>
// #include <ATen/ops/special_i0.h>
// #include <ATen/ops/special_i0e.h>
// #include <ATen/ops/special_i1.h>
// #include <ATen/ops/special_i1e.h>
// #include <ATen/ops/special_laguerre_polynomial_l.h>
// #include <ATen/ops/special_legendre_polynomial_p.h>
// #include <ATen/ops/special_log1p.h>
// #include <ATen/ops/special_log_ndtr.h>
// #include <ATen/ops/special_log_softmax.h>
// #include <ATen/ops/special_logit.h>
// #include <ATen/ops/special_logsumexp.h>
// #include <ATen/ops/special_modified_bessel_i0.h>
// #include <ATen/ops/special_modified_bessel_i1.h>
// #include <ATen/ops/special_modified_bessel_k0.h>
// #include <ATen/ops/special_modified_bessel_k1.h>
// #include <ATen/ops/special_multigammaln.h>
// #include <ATen/ops/special_ndtr.h>
// #include <ATen/ops/special_ndtri.h>
// #include <ATen/ops/special_polygamma.h>
// #include <ATen/ops/special_psi.h>
// #include <ATen/ops/special_round.h>
// #include <ATen/ops/special_scaled_modified_bessel_k0.h>
// #include <ATen/ops/special_scaled_modified_bessel_k1.h>
// #include <ATen/ops/special_shifted_chebyshev_polynomial_t.h>
// #include <ATen/ops/special_shifted_chebyshev_polynomial_u.h>
// #include <ATen/ops/special_shifted_chebyshev_polynomial_v.h>
// #include <ATen/ops/special_shifted_chebyshev_polynomial_w.h>
// #include <ATen/ops/special_sinc.h>
// #include <ATen/ops/special_softmax.h>
// #include <ATen/ops/special_spherical_bessel_j0.h>
// #include <ATen/ops/special_xlog1py.h>
// #include <ATen/ops/special_xlogy.h>
// #include <ATen/ops/special_zeta.h>
// #include <ATen/ops/split.h>
// #include <ATen/ops/split_copy.h>
// #include <ATen/ops/split_with_sizes.h>
// #include <ATen/ops/split_with_sizes_copy.h>
// #include <ATen/ops/sqrt.h>
// #include <ATen/ops/square.h>
// #include <ATen/ops/squeeze.h>
// #include <ATen/ops/squeeze_copy.h>
// #include <ATen/ops/sspaddmm.h>
// #include <ATen/ops/stack.h>
// #include <ATen/ops/std.h>
// #include <ATen/ops/std_mean.h>
// #include <ATen/ops/stft.h>
// #include <ATen/ops/stride.h>
// #include <ATen/ops/sub.h>
// #include <ATen/ops/subtract.h>
// #include <ATen/ops/sum.h>
// #include <ATen/ops/sum_to_size.h>
// #include <ATen/ops/svd.h>
// #include <ATen/ops/swapaxes.h>
// #include <ATen/ops/swapdims.h>
// #include <ATen/ops/t.h>
// #include <ATen/ops/t_copy.h>
// #include <ATen/ops/take.h>
// #include <ATen/ops/take_along_dim.h>
// #include <ATen/ops/tan.h>
// #include <ATen/ops/tanh.h>
// #include <ATen/ops/tanh_backward.h>
// #include <ATen/ops/tensor_split.h>
// #include <ATen/ops/tensordot.h>
// #include <ATen/ops/thnn_conv2d.h>
// #include <ATen/ops/threshold.h>
// #include <ATen/ops/threshold_backward.h>
// #include <ATen/ops/tile.h>
// #include <ATen/ops/to.h>
// #include <ATen/ops/to_dense.h>
// #include <ATen/ops/to_dense_backward.h>
// #include <ATen/ops/to_mkldnn.h>
// #include <ATen/ops/to_mkldnn_backward.h>
// #include <ATen/ops/to_padded_tensor.h>
// #include <ATen/ops/to_sparse.h>
// #include <ATen/ops/to_sparse_bsc.h>
// #include <ATen/ops/to_sparse_bsr.h>
// #include <ATen/ops/to_sparse_csc.h>
// #include <ATen/ops/to_sparse_csr.h>
// #include <ATen/ops/topk.h>
// #include <ATen/ops/trace.h>
// #include <ATen/ops/trace_backward.h>
// #include <ATen/ops/transpose.h>
// #include <ATen/ops/transpose_copy.h>
// #include <ATen/ops/trapezoid.h>
// #include <ATen/ops/trapz.h>
// #include <ATen/ops/triangular_solve.h>
// #include <ATen/ops/tril.h>
// #include <ATen/ops/tril_indices.h>
// #include <ATen/ops/triplet_margin_loss.h>
// #include <ATen/ops/triu.h>
// #include <ATen/ops/triu_indices.h>
// #include <ATen/ops/true_divide.h>
// #include <ATen/ops/trunc.h>
// #include <ATen/ops/type_as.h>
// #include <ATen/ops/unbind.h>
// #include <ATen/ops/unbind_copy.h>
// #include <ATen/ops/unflatten.h>
// #include <ATen/ops/unflatten_dense_tensors.h>
// #include <ATen/ops/unfold.h>
// #include <ATen/ops/unfold_backward.h>
// #include <ATen/ops/unfold_copy.h>
// #include <ATen/ops/uniform.h>
// #include <ATen/ops/unique_consecutive.h>
// #include <ATen/ops/unique_dim.h>
// #include <ATen/ops/unique_dim_consecutive.h>
// #include <ATen/ops/unsafe_chunk.h>
// #include <ATen/ops/unsafe_split.h>
// #include <ATen/ops/unsafe_split_with_sizes.h>
// #include <ATen/ops/unsqueeze.h>
// #include <ATen/ops/unsqueeze_copy.h>
// #include <ATen/ops/upsample_bicubic2d.h>
// #include <ATen/ops/upsample_bicubic2d_backward.h>
// #include <ATen/ops/upsample_bilinear2d.h>
// #include <ATen/ops/upsample_bilinear2d_backward.h>
// #include <ATen/ops/upsample_linear1d.h>
// #include <ATen/ops/upsample_linear1d_backward.h>
// #include <ATen/ops/upsample_nearest1d.h>
// #include <ATen/ops/upsample_nearest1d_backward.h>
// #include <ATen/ops/upsample_nearest2d.h>
// #include <ATen/ops/upsample_nearest2d_backward.h>
// #include <ATen/ops/upsample_nearest3d.h>
// #include <ATen/ops/upsample_nearest3d_backward.h>
// #include <ATen/ops/upsample_trilinear3d.h>
// #include <ATen/ops/upsample_trilinear3d_backward.h>
// #include <ATen/ops/value_selecting_reduction_backward.h>
// #include <ATen/ops/values.h>
// #include <ATen/ops/values_copy.h>
// #include <ATen/ops/vander.h>
// #include <ATen/ops/var.h>
// #include <ATen/ops/var_mean.h>
// #include <ATen/ops/vdot.h>
// #include <ATen/ops/view.h>
// #include <ATen/ops/view_as.h>
// #include <ATen/ops/view_as_complex.h>
// #include <ATen/ops/view_as_complex_copy.h>
// #include <ATen/ops/view_as_real.h>
// #include <ATen/ops/view_as_real_copy.h>
// #include <ATen/ops/view_copy.h>
// #include <ATen/ops/vsplit.h>
// #include <ATen/ops/vstack.h>
// #include <ATen/ops/where.h>
// #include <ATen/ops/xlogy.h>
// #include <ATen/ops/xor.h>
// #include <ATen/ops/zero.h>
// #include <ATen/ops/zeros.h>
// #include <ATen/ops/zeros_like.h>



// Special C++ only overloads for std()-like functions (See gh-40287)
// These are needed because int -> bool conversion takes precedence over int -> IntArrayRef
// So, for example std(0) would select the std(unbiased=False) overload
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, int dim);
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, int dim);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, int dim);
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, int dim);

@Namespace("at") public static native @Cast("int64_t") long numel(@Const @ByRef Tensor tensor);

@Namespace("at") public static native @Cast("int64_t") long size(@Const @ByRef Tensor tensor, @Cast("int64_t") long dim);

@Namespace("at") public static native @Cast("int64_t") long stride(@Const @ByRef Tensor tensor, @Cast("int64_t") long dim);

@Namespace("at") public static native @Cast("bool") boolean is_complex(@Const @ByRef Tensor tensor);

@Namespace("at") public static native @Cast("bool") boolean is_floating_point(@Const @ByRef Tensor tensor);

@Namespace("at") public static native @Cast("bool") boolean is_signed(@Const @ByRef Tensor tensor);

@Namespace("at") public static native @Cast("bool") boolean is_inference(@Const @ByRef Tensor tensor);

@Namespace("at") public static native @Cast("bool") boolean _is_zerotensor(@Const @ByRef Tensor tensor);

@Namespace("at") public static native @Cast("bool") boolean is_conj(@Const @ByRef Tensor tensor);

@Namespace("at") public static native @ByVal Tensor conj(@Const @ByRef Tensor tensor);

@Namespace("at") public static native @Cast("bool") boolean is_neg(@Const @ByRef Tensor tensor);




// Parsed from ATen/NamedTensor.h

// #include <ATen/core/NamedTensor.h>


// Parsed from ATen/NestedTensorImpl.h

// #pragma once
// #include <ATen/MemoryOverlap.h>
// #include <ATen/Tensor.h>
// #include <c10/core/DispatchKey.h>
// #include <c10/core/DispatchKeySet.h>
// #include <c10/core/MemoryFormat.h>
// #include <c10/core/TensorImpl.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Metaprogramming.h>
// #include <c10/util/irange.h>
@Namespace("at::native") public static native @Cast("bool") boolean nested_tensor_impl_is_contiguous(@Const NestedTensorImpl nt);
// Targeting ../NestedTensorImpl.java



@Namespace("at::native") public static native NestedTensorImpl get_nested_tensor_impl_or_null(
    @Const @ByRef Tensor tensor);

@Namespace("at::native") public static native NestedTensorImpl get_nested_tensor_impl(@Const @ByRef Tensor tensor);

@Namespace("at::native") public static native @Const @ByRef Tensor get_nested_size_tensor(@Const @ByRef Tensor tensor);

 // namespace native
 // namespace at


// Parsed from ATen/NamedTensorUtils.h

// #pragma once
// #include <ATen/NamedTensor.h>
// #include <ATen/TensorNames.h>
// #include <ATen/WrapDimUtilsMulti.h>

// #include <ATen/core/DimVector.h>
// #include <ATen/core/Tensor.h>
// #include <functional>

@Namespace("at") public static native @Cast("bool") boolean has_names(@ByVal TensorArrayRef tensors);

// Converts dim to an positional index. Errors if `dim` cannot be used to
// refer to any dimension of tensor.
@Namespace("at") public static native @Cast("int64_t") long dimname_to_position(@Const @ByRef Tensor tensor, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector dimnames_to_positions(
    @Const @ByRef Tensor tensor,
    @ByVal DimnameArrayRef dims);

// Unifies two DimnameList to produce a third. This is useful for implementing
// the named inference rule for binary broadcasting operations like add.
//
// There are three main constraints:
// 1) Check matching: Names must match positionally from the right.
// 2) Check misaligned: If a name `n` is in `names`, then it must appear at
//    the same index from the right in other.
// 3) The output names are obtained by unifying the names individually from the
// right.
@Namespace("at") public static native @StdMove DimnameVector unify_from_right(
    @ByVal DimnameArrayRef names,
    @ByVal DimnameArrayRef other,
    @Cast("const char*") BytePointer action/*="broadcast"*/);
@Namespace("at") public static native @StdMove DimnameVector unify_from_right(
    @ByVal DimnameArrayRef names,
    @ByVal DimnameArrayRef other);
@Namespace("at") public static native @StdMove DimnameVector unify_from_right(
    @ByVal DimnameArrayRef names,
    @ByVal DimnameArrayRef other,
    String action/*="broadcast"*/);

@Namespace("at") public static native void reportNYIDimnameOverload(@Cast("const char*") BytePointer op_name);
@Namespace("at") public static native void reportNYIDimnameOverload(String op_name);

// [NOTE] Writing name inference rules
//
// Operators that support named tensors are either composed of operations that
// support named tensors or implement some name inference rule. An op that
// implements its own name inference rule generally looks like the following:
//
// Tensor op(...) {
//   perform_shape_checks(...);
//   # (1)
//   auto maybe_outnames = compute_outnames(...);
//   auto result = [&]() {
//     NoNamesGuard guard;
//     return op_impl(...);
//   }();
//   # (2)
//   propagate_names_if_nonempty(result, maybe_outnames);
//
// Each op has (1) a compute outnames step and (2) a propagate names step.
//
// compute_outnames is responsible for checking that input names match and
// determining what the output names should be. It returns either:
// - {} (if the inputs tensors are all unnamed)
// - non-empty outnames.
//
// propagate_names_if_nonempty propagates the outnames if they exist to the
// result tensors.
//
// The {} case is an optimization; if the user does not use named tensors they
// pay no perf cost for it.


// Propagates `names` to `result` if `names` is not empty.
// `names` can be empty; see [NOTE] Writing name inference rules
// If `names` is not empty, `names.size()` should equal `result.dim()`.
// When in doubt, use this overload instead of the others.
@Namespace("at::namedinference") public static native @Const @ByRef Tensor propagate_names_if_nonempty(
    @Const @ByRef Tensor result,
    @ByVal DimnameArrayRef maybe_names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native @Const @ByRef Tensor propagate_names_if_nonempty(
    @Const @ByRef Tensor result,
    @ByVal DimnameArrayRef maybe_names);

// Propagates `names` to `result`. Only use this if we are certain that there
// are names to propagate (that names is not empty).
@Namespace("at::namedinference") public static native @Const @ByRef Tensor propagate_names(
    @Const @ByRef Tensor result,
    @ByVal DimnameArrayRef names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native @Const @ByRef Tensor propagate_names(
    @Const @ByRef Tensor result,
    @ByVal DimnameArrayRef names);

// Propagates all names from src to result.
@Namespace("at::namedinference") public static native void propagate_names(@Const @ByRef Tensor result, @Const @ByRef Tensor src);

// Propagates all names except for those at the excluded_idxs.
@Namespace("at::namedinference") public static native void propagate_names_except(
    @Const @ByRef Tensor result,
    @Const @ByRef Tensor src,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef excluded_idxs);
@Namespace("at::namedinference") public static native void propagate_names_except(
    @Const @ByRef Tensor result,
    @Const @ByRef Tensor src,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... excluded_idxs);

// Used for reduction ops that have a `keepdim` arg.
@Namespace("at::namedinference") public static native void propagate_names_for_reduction(
    @Const @ByRef Tensor result,
    @Const @ByRef Tensor src,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef excluded_idxs,
    @Cast("bool") boolean keepdim);
@Namespace("at::namedinference") public static native void propagate_names_for_reduction(
    @Const @ByRef Tensor result,
    @Const @ByRef Tensor src,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] excluded_idxs,
    @Cast("bool") boolean keepdim);

@Namespace("at::namedinference") public static native void propagate_names_for_expand(
    @Const @ByRef Tensor result,
    @Const @ByRef Tensor self);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_broadcast_outnames(
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other);

@Namespace("at::namedinference") public static native @StdMove DimnameVector broadcast_to_outnames(
    @Const @ByRef Tensor tensor,
    @Const @ByRef Tensor reference_tensor,
    @Cast("const char*") BytePointer op_name);
@Namespace("at::namedinference") public static native @StdMove DimnameVector broadcast_to_outnames(
    @Const @ByRef Tensor tensor,
    @Const @ByRef Tensor reference_tensor,
    String op_name);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_matmul_outnames(
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_cdist_outnames(
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_bmm_outnames(
    @Const @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_squeeze_outnames(@Const @ByRef Tensor tensor);



// TensorImpl* overloads for Legacy TH/THC code. Use these sparingly.

@Namespace("at::namedinference") public static native TensorImpl propagate_names_if_nonempty(
    TensorImpl result,
    @ByVal DimnameArrayRef maybe_names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native TensorImpl propagate_names_if_nonempty(
    TensorImpl result,
    @ByVal DimnameArrayRef maybe_names);

@Namespace("at::namedinference") public static native TensorImpl propagate_names(
    TensorImpl result,
    @ByVal DimnameArrayRef names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native TensorImpl propagate_names(
    TensorImpl result,
    @ByVal DimnameArrayRef names);

@Namespace("at::namedinference") public static native void propagate_names(TensorImpl result, TensorImpl src);

@Namespace("at::namedinference") public static native void propagate_names(
    @Const @ByRef TensorBase result,
    @ByVal DimnameArrayRef names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native void propagate_names(
    @Const @ByRef TensorBase result,
    @ByVal DimnameArrayRef names);

@Namespace("at::namedinference") public static native void propagate_names_if_nonempty(
    @Const @ByRef TensorBase result,
    @ByVal DimnameArrayRef names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native void propagate_names_if_nonempty(
    @Const @ByRef TensorBase result,
    @ByVal DimnameArrayRef names);

@Namespace("at::namedinference") public static native void propagate_names(
    @Const @ByRef TensorBase result,
    @Const @ByRef TensorBase src);

// result = m1 @ m2 + bias
@Namespace("at::namedinference") public static native @StdMove DimnameVector propagate_names_for_addmm(
    @Const @ByRef Tensor m1,
    @Const @ByRef Tensor m2,
    @Const @ByRef Tensor bias);

@Namespace("at::namedinference") public static native @StdMove DimnameVector propagate_names_for_addmv(
    @Const @ByRef Tensor mat,
    @Const @ByRef Tensor vec,
    @Const @ByRef Tensor bias);

@Namespace("at::namedinference") public static native void check_names_for_dot(TensorImpl vec1, TensorImpl vec2);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_baddbmm_outnames(
    @Const @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other,
    @Const @ByRef Tensor bias);

@Namespace("at::namedinference") public static native @Cast("bool") boolean are_names_equal(TensorImpl self, TensorImpl other);

 // namespace namedinference

 // namespace at


// Parsed from ATen/SavedTensorHooks.h

// #pragma once

// #include <c10/macros/Export.h>
// #include <c10/util/Optional.h>
// #include <c10/util/python_stub.h>
// #include <stack>
// #include <string>

// #include <utility>
// Targeting ../SavedTensorDefaultHooksTLS.java




// Targeting ../SavedTensorDefaultHooks.java



 // namespace at


// Parsed from ATen/ScalarOps.h

// #pragma once

// #include <ATen/Tensor.h>
// #include <c10/core/Scalar.h>

// #ifndef AT_PER_OPERATOR_HEADERS
// #include <ATen/Functions.h>
// #else
// #include <ATen/ops/scalar_tensor.h>
// #endif
// When filling a number to 1-element CPU tensor, we want to skip
// everything but manipulate data ptr directly.
// Ideally this fast pass should be implemented in TensorIterator,
// but we also want to skip compute_types which in not avoidable
// in TensorIterator for now.

@Namespace("at::detail") public static native @ByVal Tensor scalar_tensor_static(
    @Const @ByRef Scalar s,
    @ByVal ScalarTypeOptional dtype_opt,
    @ByVal DeviceOptional device_opt);
 // namespace detail
 // namespace at

// This is in the c10 namespace because we use ADL to find the functions in it.

// FIXME: this should be (and was) Scalar::toTensor, but there is currently no
// way to implement this without going through Derived Types (which are not part
// of core).
@Namespace("c10") public static native @ByVal Tensor scalar_to_tensor(
    @Const @ByRef Scalar s,
    @Const @ByVal(nullValue = "c10::Device(at::kCPU)") Device device);
@Namespace("c10") public static native @ByVal Tensor scalar_to_tensor(
    @Const @ByRef Scalar s);

 // namespace c10

@Namespace("at::native") public static native @ByVal Tensor wrapped_scalar_tensor(
    @Const @ByRef Scalar scalar,
    @Const @ByVal(nullValue = "c10::Device(at::kCPU)") Device device);
@Namespace("at::native") public static native @ByVal Tensor wrapped_scalar_tensor(
    @Const @ByRef Scalar scalar);

 // namespace native
 // namespace at


// Parsed from ATen/SequenceNumber.h

// #pragma once

// #include <c10/macros/Export.h>
// #include <cstdint>

// A simple thread local enumeration, used to link forward and backward pass
// ops and is used by autograd and observers framework

@Namespace("at::sequence_number") public static native @Cast("uint64_t") long peek();
@Namespace("at::sequence_number") public static native @Cast("uint64_t") long get_and_increment();

 // namespace sequence_number
 // namespace at


// Parsed from ATen/TensorIndexing.h

// #pragma once

// #include <ATen/ExpandUtils.h>
// #include <ATen/ScalarOps.h>
// #include <ATen/core/Tensor.h>
// #include <ATen/core/TensorBody.h>
// #include <c10/core/SymInt.h>
// #include <c10/util/Optional.h>
// #include <c10/util/irange.h>

// #ifndef AT_PER_OPERATOR_HEADERS
// #include <ATen/Functions.h>
// #include <ATen/NativeFunctions.h>
// #else
// #include <ATen/ops/alias.h>
// #include <ATen/ops/empty.h>
// #include <ATen/ops/scalar_tensor.h>
// #include <ATen/ops/zeros.h>
// #endif

// #include <ATen/core/List.h>

// #include <utility>

@Namespace("at::indexing") @MemberGetter public static native @Cast("const int64_t") long INDEX_MIN();
@Namespace("at::indexing") @MemberGetter public static native @Cast("const int64_t") long INDEX_MAX();

@Namespace("at::indexing") public enum TensorIndexType { None(0), Ellipsis(1), Integer(2), Boolean(3), Slice(4), Tensor(5);

    public final int value;
    private TensorIndexType(int v) { this.value = v; }
    private TensorIndexType(TensorIndexType e) { this.value = e.value; }
    public TensorIndexType intern() { for (TensorIndexType e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("at::indexing") @MemberGetter public static native @ByRef @Cast("const c10::nullopt_t*") Pointer None();
// Targeting ../EllipsisIndexType.java


@Namespace("at::indexing") @MemberGetter public static native @Const @ByRef EllipsisIndexType Ellipsis();
// Targeting ../Slice.java



@Namespace("at::indexing") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer stream, @Const @ByRef Slice slice);
// Targeting ../TensorIndex.java



@Namespace("at::indexing") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(
    @Cast("std::ostream*") @ByRef Pointer stream,
    @Const @ByRef TensorIndex tensor_index);
@Namespace("at::indexing") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(
    @Cast("std::ostream*") @ByRef Pointer stream,
    @Const @ByRef TensorIndexVector tensor_indices);
@Namespace("at::indexing::impl") public static native @ByVal Tensor applySlice(
    @Const @ByRef Tensor self,
    @Cast("int64_t") long dim,
    @ByVal SymInt start,
    @ByVal SymInt stop,
    @ByVal SymInt step,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device self_device,
    @Const @ByRef SymIntArrayRefOptional self_sizes);

@Namespace("at::indexing::impl") public static native @ByVal Tensor applySelect(
    @Const @ByRef Tensor self,
    @Cast("int64_t") long dim,
    @Cast("int64_t") long index,
    @Cast("int64_t") long real_dim,
    @Const @ByRef Device arg4,
    @Const @ByRef SymIntArrayRefOptional self_sizes);

@Namespace("at::indexing::impl") public static native @ByVal Tensor boolToIndexingTensorCPUOrCUDA(
    @Const @ByRef Tensor self,
    @Cast("bool") boolean value);

@Namespace("at::indexing::impl") public static native @ByVal Tensor boolToIndexingTensorNonNativeDeviceType(
    @Const @ByRef Tensor self,
    @Cast("bool") boolean value);

@Namespace("at::indexing::impl") public static native @ByVal Tensor boolToIndexingTensor(
    @Const @ByRef Tensor self,
    @Cast("bool") boolean value,
    @Const @ByRef Device self_device);

@Namespace("at::indexing::impl") public static native @ByVal Tensor scalarToTensorNonNativeDeviceType(
    @Const @ByRef Scalar v,
    @Const @ByRef TensorOptions options);

@Namespace("at::indexing::impl") public static native void recordTensorIndex(
    @Const @ByRef Tensor tensor,
    @ByRef TensorVector outIndices,
    @Cast("int64_t*") LongPointer dim_ptr);
@Namespace("at::indexing::impl") public static native void recordTensorIndex(
    @Const @ByRef Tensor tensor,
    @ByRef TensorVector outIndices,
    @Cast("int64_t*") LongBuffer dim_ptr);
@Namespace("at::indexing::impl") public static native void recordTensorIndex(
    @Const @ByRef Tensor tensor,
    @ByRef TensorVector outIndices,
    @Cast("int64_t*") long[] dim_ptr);

// NOTE: Why do we mirror instead of replace the `count_specified_dimensions`
// function in torch/csrc/autograd/python_variable_indexing.cpp? It's because
// `count_specified_dimensions` is on the hot path of Python tensor multi-dim
// indexing (i.e. it's called by `applySlicing` which is called by
// `THPVariable_getitem` / `THPVariable_setitem` when handling indexing of more
// than one dimension). If we were to merge the Python/C++
// `count_specified_dimensions` function, on the Python side we would have to
// construct a `std::vector` container to be consumed by the C++
// `count_specified_dimensions` function, which adds 100s of nanoseconds
// overhead and is undesirable.
@Namespace("at::indexing::impl") public static native @Cast("int64_t") long count_specified_dimensions(
    @Const @ByRef TensorIndexArrayRef indices);
 // namespace impl

// NOTE: Many functions below are only for consumption from Python indexing
// implementation, they include:
//
// - `Tensor scalarToTensor(...)`
// - `IntArrayRef slicePrefix1sSize(...)`
// - `void copy_to(...)`
// - `Tensor handleDimInMultiDimIndexing(...)`
// - `Tensor dispatch_index(...)`
// - `Tensor dispatch_index_put_(...)`
// - `Tensor get_item(...)`
// - `void set_item(...)`
//
// The rest of the functions are in `at::indexing::impl` namespace, signifying
// that they shouldn't be used from Python indexing implementation.
@Namespace("at::indexing") public static native @ByVal Tensor scalarToTensor(
    @Const @ByRef Scalar v,
    @Const @ByRef TensorOptions options,
    @Const @ByRef Device self_device);

// To match numpy semantics:
// As a special case for backwards compatibility,
// strip away unit dimensions from the left of 'src'
@Namespace("at::indexing") public static native @ByVal SymIntRef slicePrefix1sSize(@Const @ByRef SymIntRef sizes);

@Namespace("at::indexing") public static native void copy_to(@Const @ByRef Tensor dst, @Const @ByRef Tensor src);

// See NOTE [ Setting `disable_slice_optimization` when calling C++ tensor
// indexing functions from Python ]
@Namespace("at::indexing") public static native @ByVal Tensor handleDimInMultiDimIndexing(
    @Const @ByRef Tensor prev_dim_result,
    @Const @ByRef Tensor original_tensor,
    @Const @ByRef TensorIndex index,
    @Cast("int64_t*") LongPointer dim_ptr,
    @Cast("int64_t*") LongPointer specified_dims_ptr,
    @Cast("int64_t") long real_dim,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device original_tensor_device,
    @Const @ByRef SymIntArrayRefOptional prev_dim_result_sizes);
@Namespace("at::indexing") public static native @ByVal Tensor handleDimInMultiDimIndexing(
    @Const @ByRef Tensor prev_dim_result,
    @Const @ByRef Tensor original_tensor,
    @Const @ByRef TensorIndex index,
    @Cast("int64_t*") LongBuffer dim_ptr,
    @Cast("int64_t*") LongBuffer specified_dims_ptr,
    @Cast("int64_t") long real_dim,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device original_tensor_device,
    @Const @ByRef SymIntArrayRefOptional prev_dim_result_sizes);
@Namespace("at::indexing") public static native @ByVal Tensor handleDimInMultiDimIndexing(
    @Const @ByRef Tensor prev_dim_result,
    @Const @ByRef Tensor original_tensor,
    @Const @ByRef TensorIndex index,
    @Cast("int64_t*") long[] dim_ptr,
    @Cast("int64_t*") long[] specified_dims_ptr,
    @Cast("int64_t") long real_dim,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device original_tensor_device,
    @Const @ByRef SymIntArrayRefOptional prev_dim_result_sizes);
// This mirrors `applySlicing` in
// torch/csrc/autograd/python_variable_indexing.cpp
@Namespace("at::indexing::impl") public static native @ByVal Tensor applySlicing(
    @Const @ByRef Tensor self,
    @Const @ByRef TensorIndexArrayRef indices,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device self_device,
    @Const @ByRef SymIntArrayRefOptional self_sizes);
 // namespace impl

@Namespace("at::indexing") public static native @ByVal Tensor dispatch_index(
    @Const @ByRef Tensor self,
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector indices);

@Namespace("at::indexing") public static native @ByVal Tensor dispatch_index_put_(
    @ByRef Tensor self,
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector indices,
    @Const @ByRef Tensor value);

// NOTE [ Setting `disable_slice_optimization` when calling C++ tensor indexing
// functions from Python ]
//
// Question: When should we set `disable_slice_optimization` to `true` when
// calling C++ tensor indexing functions from Python indexing code?
//
// Answer: What "slice optimization" means: when we have a slicing expression
// like `x[0:5, 0]`, where the sliced tensor was of size 5 in dimension 0, we
// would skip dispatching the actual slice call as an optimization. However,
// here are the cases where we DON'T want this optimization:
//
// 1. When we are doing 1-D slicing (e.g. `tensor[:]`).
//    Reason: we always return a shallow copy for expressions such as
//    `tensor[:]` / `tensor[...]` / `tensor[:, :]`. (Note that for `tensor[:,
//    :]`, we return an alias of `tensor` by doing the following:
//    ```
//    Tensor sliced = impl::applySlicing(self, indices, tensorIndices,
//    disable_slice_optimization, self_device, self_sizes); if
//    (tensorIndices.empty()) {
//      if (sliced.is_same(self)) {
//        // ensure we return a shallow copy for things like x[...]
//        sliced = at::alias(sliced);
//      }
//      return sliced;
//    }
//    ```)
// 2. When we are doing JIT tracing.
//    Reason: JIT tracing needs the `self.slice(...)` call to properly trace the
//    slice operation.

// This mirrors `THPVariable_getitem` in
// torch/csrc/autograd/python_variable_indexing.cpp See NOTE [ Setting
// `disable_slice_optimization` when calling C++ tensor indexing functions from
// Python ]
@Namespace("at::indexing") public static native @ByVal Tensor get_item(
    @Const @ByRef Tensor self,
    @Const @ByRef TensorIndexArrayRef indices,
    @Cast("bool") boolean disable_slice_optimization/*=false*/);
@Namespace("at::indexing") public static native @ByVal Tensor get_item(
    @Const @ByRef Tensor self,
    @Const @ByRef TensorIndexArrayRef indices);

// This mirrors `THPVariable_setitem` in
// torch/csrc/autograd/python_variable_indexing.cpp for "the assigned value is a
// Tensor" case See NOTE [ Setting `disable_slice_optimization` when calling C++
// tensor indexing functions from Python ]
@Namespace("at::indexing") public static native void set_item(
    @Const @ByRef Tensor self,
    @Const @ByRef TensorIndexArrayRef indices,
    @Const @ByRef Tensor value,
    @Cast("bool") boolean disable_slice_optimization/*=false*/);
@Namespace("at::indexing") public static native void set_item(
    @Const @ByRef Tensor self,
    @Const @ByRef TensorIndexArrayRef indices,
    @Const @ByRef Tensor value);

 // namespace indexing
 // namespace at


// Parsed from ATen/TensorOperators.h

// #pragma once

// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>

// #ifndef AT_PER_OPERATOR_HEADERS
// #include <ATen/Functions.h>
// #else
// #include <ATen/ops/empty_like.h>
// #endif

// #include <stdexcept>
// #include <string>

// #define AT_FORALL_BINARY_OPS(_)
//   _(+, x.add(y), y.add(x))
//   _(*, x.mul(y), y.mul(x))
//   _(-,
//     x.sub(y),
//     ::at::empty_like(y, at::MemoryFormat::Preserve).fi_(x).sub_(y))
//   _(/,
//     x.div(y),
//     ::at::empty_like(y, at::MemoryFormat::Preserve).fi_(x).div_(y))
//   _(%,
//     x.remainder(y),
//     ::at::empty_like(y, at::MemoryFormat::Preserve).fi_(x).remainder_(y))
//   _(&, x.bitwise_and(y), y.bitwise_and(x))
//   _(|, x.bitwise_or(y), y.bitwise_or(x))
//   _(^, x.bitwise_xor(y), y.bitwise_xor(x))
//   _(<, x.t(y), y.gt(x))
//   _(<=, x.e(y), y.ge(x))
//   _(>, x.gt(y), y.t(x))
//   _(>=, x.ge(y), y.e(x))
//   _(==, x.eq(y), y.eq(x))
//   _(!=, x.ne(y), y.ne(x))

// #define DEFINE_OPERATOR(op, body, reverse_scalar_body)
//   static inline Tensor operator op(const Tensor& x, const Tensor& y) {
//     return body;
//   }
//   static inline Tensor operator op(const Tensor& x, const Scalar& y) {
//     return body;
//   }
//   static inline Tensor operator op(const Scalar& x, const Tensor& y) {
//     return reverse_scalar_body;
//   }

@Namespace("at") public static native @ByVal @Name("operator +") Tensor add(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator +") Tensor add(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator +") Tensor add(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator *") Tensor multiply(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator *") Tensor multiply(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator *") Tensor multiply(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator -") Tensor subtract(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator -") Tensor subtract(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator -") Tensor subtract(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator /") Tensor divide(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator /") Tensor divide(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator /") Tensor divide(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator %") Tensor mod(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator %") Tensor mod(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator %") Tensor mod(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator &") Tensor and(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator &") Tensor and(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator &") Tensor and(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator |") Tensor or(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator |") Tensor or(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator |") Tensor or(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator ^") Tensor xor(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator ^") Tensor xor(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator ^") Tensor xor(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator <") Tensor lessThan(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator <") Tensor lessThan(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator <") Tensor lessThan(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator <=") Tensor lessThanEquals(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator <=") Tensor lessThanEquals(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator <=") Tensor lessThanEquals(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator >") Tensor greaterThan(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator >") Tensor greaterThan(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator >") Tensor greaterThan(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator >=") Tensor greaterThanEquals(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator >=") Tensor greaterThanEquals(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator >=") Tensor greaterThanEquals(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator ==") Tensor equals(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator ==") Tensor equals(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator ==") Tensor equals(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator !=") Tensor notEquals(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
  @Namespace("at") public static native @ByVal @Name("operator !=") Tensor notEquals(@Const @ByRef Tensor x, @Const @ByRef Scalar y);
  @Namespace("at") public static native @ByVal @Name("operator !=") Tensor notEquals(@Const @ByRef Scalar x, @Const @ByRef Tensor y);
// #undef DEFINE_OPERATOR
// #undef AT_FORALL_BINARY_OPS

 // namespace at


// Parsed from ATen/Version.h

// #include <ATen/Context.h>

/** Returns a detailed string describing the configuration PyTorch. */
@Namespace("at") public static native @StdString BytePointer show_config();

@Namespace("at") public static native @StdString BytePointer get_mkl_version();

@Namespace("at") public static native @StdString BytePointer get_mkldnn_version();

@Namespace("at") public static native @StdString BytePointer get_openmp_version();

@Namespace("at") public static native @StdString BytePointer get_cxx_flags();

 // namespace at


// Parsed from ATen/WrapDimUtilsMulti.h

// #pragma once

// #include <ATen/WrapDimUtils.h>
// #include <c10/core/TensorImpl.h>
// #include <c10/util/irange.h>
// #include <bitset>
// #include <sstream>

// This is in an extra file to work around strange interaction of
// bitset on Windows with operator overloading

@Namespace("at") @MemberGetter public static native @Cast("const size_t") long dim_bitset_size();

 // namespace at


// Parsed from ATen/ops/from_blob.h

// #pragma once
// #include <ATen/core/Tensor.h>

@Namespace("at::detail") public static native void noopDelete(Pointer arg0);


// Targeting ../TensorMaker.java



@Namespace("at") public static native @ByVal @NoException(true) TensorMaker for_blob(Pointer data, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("at") public static native @ByVal @NoException(true) TensorMaker for_blob(Pointer data, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sizes);

@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @Const @ByRef Deleter deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options,
    @Const @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional target_device);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @Const @ByRef Deleter deleter);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @ByRef @Cast("void(*)(void*)") Pointer deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options,
    @Const @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional target_device);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @ByRef @Cast("void(*)(void*)") Pointer deleter);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @ByRef @Cast("void(*)(void*)") long deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options,
    @Const @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional target_device);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @ByRef @Cast("void(*)(void*)") long deleter);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @Const @ByRef Deleter deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options,
    @Const @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional target_device);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @Const @ByRef Deleter deleter);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @ByRef @Cast("void(*)(void*)") Pointer deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options,
    @Const @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional target_device);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @ByRef @Cast("void(*)(void*)") Pointer deleter);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @ByRef @Cast("void(*)(void*)") long deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options,
    @Const @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional target_device);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @ByRef @Cast("void(*)(void*)") long deleter);

@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @Cast("int64_t") long storage_offset,
    @Const @ByRef Deleter deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options,
    @Const @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional target_device);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @Cast("int64_t") long storage_offset,
    @Const @ByRef Deleter deleter);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @Cast("int64_t") long storage_offset,
    @ByRef @Cast("void(*)(void*)") Pointer deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options,
    @Const @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional target_device);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @Cast("int64_t") long storage_offset,
    @ByRef @Cast("void(*)(void*)") Pointer deleter);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @Cast("int64_t") long storage_offset,
    @ByRef @Cast("void(*)(void*)") long deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options,
    @Const @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional target_device);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @Cast("int64_t") long storage_offset,
    @ByRef @Cast("void(*)(void*)") long deleter);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @Cast("int64_t") long storage_offset,
    @Const @ByRef Deleter deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options,
    @Const @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional target_device);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @Cast("int64_t") long storage_offset,
    @Const @ByRef Deleter deleter);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @Cast("int64_t") long storage_offset,
    @ByRef @Cast("void(*)(void*)") Pointer deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options,
    @Const @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional target_device);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @Cast("int64_t") long storage_offset,
    @ByRef @Cast("void(*)(void*)") Pointer deleter);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @Cast("int64_t") long storage_offset,
    @ByRef @Cast("void(*)(void*)") long deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options,
    @Const @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional target_device);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @Cast("int64_t") long storage_offset,
    @ByRef @Cast("void(*)(void*)") long deleter);

@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @Const @ByRef Deleter deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @Const @ByRef Deleter deleter);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByRef @Cast("void(*)(void*)") Pointer deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByRef @Cast("void(*)(void*)") Pointer deleter);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByRef @Cast("void(*)(void*)") long deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByRef @Cast("void(*)(void*)") long deleter);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @Const @ByRef Deleter deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @Const @ByRef Deleter deleter);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByRef @Cast("void(*)(void*)") Pointer deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByRef @Cast("void(*)(void*)") Pointer deleter);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByRef @Cast("void(*)(void*)") long deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByRef @Cast("void(*)(void*)") long deleter);

@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... strides);

@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sizes);

  // namespace at


// Parsed from ATen/ops/tensor.h

// #pragma once
// #include <ATen/core/Tensor.h>
// #include <c10/core/ScalarType.h>

// These functions are defined in ATen/Utils.cpp.
// #define TENSOR(T, S)
//   TORCH_API Tensor tensor(ArrayRef<T> values, const TensorOptions& options);
//   inline Tensor tensor(
//       std::initializer_list<T> values, const TensorOptions& options) {
//     return at::tensor(ArrayRef<T>(values), options);
//   }
//   inline Tensor tensor(T value, const TensorOptions& options) {
//     return at::tensor(ArrayRef<T>(value), options);
//   }
//   inline Tensor tensor(ArrayRef<T> values) {
//     return at::tensor(std::move(values), at::dtype(k##S));
//   }
//   inline Tensor tensor(std::initializer_list<T> values) {
//     return at::tensor(ArrayRef<T>(values));
//   }
//   inline Tensor tensor(T value) {
//     return at::tensor(ArrayRef<T>(value));
//   }
@Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<uint8_t>*") ByteArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("uint8_t") byte value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<uint8_t>*") ByteArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("uint8_t") byte value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<int16_t>*") ShortArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(short value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<int16_t>*") ShortArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(short value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<int>*") IntArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(int value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<int>*") IntArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(int value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("int64_t") long value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... values);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("int64_t") long value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal FloatArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(float value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal FloatArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(float value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal DoubleArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(double value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal DoubleArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(double value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BoolArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("decltype(::c10::impl::ScalarTypeToCPPType<::c10::ScalarType::Bool>::t)") boolean value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BoolArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("decltype(::c10::impl::ScalarTypeToCPPType<::c10::ScalarType::Bool>::t)") boolean value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal HalfArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal Half value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal HalfArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal Half value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BFloat16ArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BFloat16 value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BFloat16ArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BFloat16 value);
@Namespace("at") public static native @ByVal Tensor tensor(@ByVal FloatComplexrrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal FloatComplexrrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal DoubleComplexrrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal DoubleComplexrrayRef values);
// #undef TENSOR

  // namespace at


// Parsed from ATen/ops/_adaptive_avg_pool2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_adaptive_avg_pool2d_ops.h>


// aten::_adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor _adaptive_avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor _adaptive_avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);


// aten::_adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor _adaptive_avg_pool2d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size);


// aten::_adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _adaptive_avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor _adaptive_avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);


// aten::_adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _adaptive_avg_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _adaptive_avg_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByRef Tensor out);


// aten::_adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _adaptive_avg_pool2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size);


// aten::_adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _adaptive_avg_pool2d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByRef Tensor out);





// Parsed from ATen/ops/_adaptive_avg_pool2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_adaptive_avg_pool2d_backward_ops.h>


// aten::_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _adaptive_avg_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);

// aten::_adaptive_avg_pool2d_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _adaptive_avg_pool2d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
// aten::_adaptive_avg_pool2d_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _adaptive_avg_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/_adaptive_avg_pool3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_adaptive_avg_pool3d_ops.h>


// aten::_adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor _adaptive_avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor _adaptive_avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);


// aten::_adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor _adaptive_avg_pool3d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size);


// aten::_adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _adaptive_avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor _adaptive_avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);


// aten::_adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _adaptive_avg_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _adaptive_avg_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByRef Tensor out);


// aten::_adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _adaptive_avg_pool3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size);


// aten::_adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _adaptive_avg_pool3d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByRef Tensor out);





// Parsed from ATen/ops/_adaptive_avg_pool3d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_adaptive_avg_pool3d_backward_ops.h>


// aten::_adaptive_avg_pool3d_backward(Tensor grad_output, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _adaptive_avg_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);

// aten::_adaptive_avg_pool3d_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _adaptive_avg_pool3d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
// aten::_adaptive_avg_pool3d_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _adaptive_avg_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/_add_batch_dim.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_add_batch_dim_ops.h>


// aten::_add_batch_dim(Tensor self, int batch_dim, int level) -> Tensor
@Namespace("at") public static native @ByVal Tensor _add_batch_dim(@Const @ByRef Tensor self, @Cast("int64_t") long batch_dim, @Cast("int64_t") long level);




// Parsed from ATen/ops/_add_relu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_add_relu_ops.h>


// aten::_add_relu.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _add_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor _add_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::_add_relu_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _add_relu_(@ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor _add_relu_(@ByRef Tensor self, @Const @ByRef Tensor other);

// aten::_add_relu.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _add_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor _add_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::_add_relu.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _add_relu_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::_add_relu.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _add_relu(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor _add_relu(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::_add_relu_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _add_relu_(@ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor _add_relu_(@ByRef Tensor self, @Const @ByRef Scalar other);

// aten::_add_relu.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _add_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor _add_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::_add_relu.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _add_relu_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef Scalar alpha, @ByRef Tensor out);




// Parsed from ATen/ops/_addmm_activation.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_addmm_activation_ops.h>


// aten::_addmm_activation.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, bool use_gelu=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _addmm_activation_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha, @Cast("bool") boolean use_gelu/*=false*/);
@Namespace("at") public static native @ByRef Tensor _addmm_activation_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);
// aten::_addmm_activation.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, bool use_gelu=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _addmm_activation_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef Scalar beta, @Const @ByRef Scalar alpha, @Cast("bool") boolean use_gelu, @ByRef Tensor out);

// aten::_addmm_activation(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, bool use_gelu=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _addmm_activation(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha, @Cast("bool") boolean use_gelu/*=false*/);
@Namespace("at") public static native @ByVal Tensor _addmm_activation(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);




// Parsed from ATen/ops/_aminmax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_aminmax_ops.h>


// aten::_aminmax(Tensor self) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _aminmax(@Const @ByRef Tensor self);

// aten::_aminmax.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _aminmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple _aminmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::_aminmax.out(Tensor self, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _aminmax_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self);
// aten::_aminmax.out(Tensor self, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _aminmax_outf(@Const @ByRef Tensor self, @ByRef Tensor out0, @ByRef Tensor out1);

// aten::_aminmax.dim_out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _aminmax_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _aminmax_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::_aminmax.dim_out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _aminmax_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/_amp_foreach_non_finite_check_and_unscale.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_amp_foreach_non_finite_check_and_unscale_ops.h>


// aten::_amp_foreach_non_finite_check_and_unscale_(Tensor(a!)[] self, Tensor(b!) found_inf, Tensor inv_scale) -> ()
@Namespace("at") public static native void _amp_foreach_non_finite_check_and_unscale_(@ByVal TensorArrayRef self, @ByRef Tensor found_inf, @Const @ByRef Tensor inv_scale);

// aten::_amp_foreach_non_finite_check_and_unscale.out(Tensor[] self, Tensor(b!) found_inf, Tensor inv_scale, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _amp_foreach_non_finite_check_and_unscale_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByRef Tensor found_inf, @Const @ByRef Tensor inv_scale);
// aten::_amp_foreach_non_finite_check_and_unscale.out(Tensor[] self, Tensor(b!) found_inf, Tensor inv_scale, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _amp_foreach_non_finite_check_and_unscale_outf(@ByVal TensorArrayRef self, @ByRef Tensor found_inf, @Const @ByRef Tensor inv_scale, @ByVal TensorArrayRef out);

// aten::_amp_foreach_non_finite_check_and_unscale(Tensor[] self, Tensor found_inf, Tensor inv_scale) -> (Tensor[] self_out, Tensor found_inf_out)
@Namespace("at") public static native @ByVal TensorVectorTensorTuple _amp_foreach_non_finite_check_and_unscale(@ByVal TensorArrayRef self, @Const @ByRef Tensor found_inf, @Const @ByRef Tensor inv_scale);




// Parsed from ATen/ops/_amp_update_scale.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_amp_update_scale_ops.h>


// aten::_amp_update_scale_(Tensor(a!) self, Tensor(b!) growth_tracker, Tensor found_inf, float scale_growth_factor, float scale_backoff_factor, int growth_interval) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _amp_update_scale_(@ByRef Tensor self, @ByRef Tensor growth_tracker, @Const @ByRef Tensor found_inf, double scale_growth_factor, double scale_backoff_factor, @Cast("int64_t") long growth_interval);

// aten::_amp_update_scale.out(Tensor self, Tensor(b!) growth_tracker, Tensor found_inf, float scale_growth_factor, float scale_backoff_factor, int growth_interval, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _amp_update_scale_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByRef Tensor growth_tracker, @Const @ByRef Tensor found_inf, double scale_growth_factor, double scale_backoff_factor, @Cast("int64_t") long growth_interval);
// aten::_amp_update_scale.out(Tensor self, Tensor(b!) growth_tracker, Tensor found_inf, float scale_growth_factor, float scale_backoff_factor, int growth_interval, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _amp_update_scale_outf(@Const @ByRef Tensor self, @ByRef Tensor growth_tracker, @Const @ByRef Tensor found_inf, double scale_growth_factor, double scale_backoff_factor, @Cast("int64_t") long growth_interval, @ByRef Tensor out);

// aten::_amp_update_scale(Tensor self, Tensor growth_tracker, Tensor found_inf, float scale_growth_factor, float scale_backoff_factor, int growth_interval) -> (Tensor, Tensor growth_tracker_out)
@Namespace("at") public static native @ByVal TensorTensorTuple _amp_update_scale(@Const @ByRef Tensor self, @Const @ByRef Tensor growth_tracker, @Const @ByRef Tensor found_inf, double scale_growth_factor, double scale_backoff_factor, @Cast("int64_t") long growth_interval);




// Parsed from ATen/ops/_assert_async.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_assert_async_ops.h>


// aten::_assert_async(Tensor self) -> ()
@Namespace("at") public static native void _assert_async(@Const @ByRef Tensor self);




// Parsed from ATen/ops/_assert_tensor_metadata.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_assert_tensor_metadata_ops.h>


// aten::_assert_tensor_metadata(Tensor a, int[]? size=None, int[]? stride=None, ScalarType? dtype=None) -> ()
@Namespace("at") public static native void _assert_tensor_metadata(@Const @ByRef Tensor a, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional size, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional stride, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native void _assert_tensor_metadata(@Const @ByRef Tensor a);
@Namespace("at") public static native void _assert_tensor_metadata(@Const @ByRef Tensor a, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);




// Parsed from ATen/ops/_autocast_to_full_precision.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_autocast_to_full_precision_ops.h>






// Parsed from ATen/ops/_autocast_to_reduced_precision.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_autocast_to_reduced_precision_ops.h>






// Parsed from ATen/ops/_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_backward_ops.h>






// Parsed from ATen/ops/_batch_norm_impl_index.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_batch_norm_impl_index_ops.h>


// aten::_batch_norm_impl_index(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -> (Tensor, Tensor, Tensor, Tensor, int)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorLongTuple _batch_norm_impl_index(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double momentum, double eps, @Cast("bool") boolean cudnn_enabled);




// Parsed from ATen/ops/_batch_norm_impl_index_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_batch_norm_impl_index_backward_ops.h>


// aten::_batch_norm_impl_index_backward(int impl_index, Tensor input, Tensor grad_output, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var_transform, bool train, float eps, bool[3] output_mask, Tensor reservedSpace) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _batch_norm_impl_index_backward(@Cast("int64_t") long impl_index, @Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_var_transform, @Cast("bool") boolean train, double eps, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @Const @ByRef Tensor reservedSpace);




// Parsed from ATen/ops/_cast_Byte.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_cast_Byte_ops.h>


// aten::_cast_Byte(Tensor self, bool non_blocking=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cast_Byte(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Byte(@Const @ByRef Tensor self);




// Parsed from ATen/ops/_cast_Char.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_cast_Char_ops.h>


// aten::_cast_Char(Tensor self, bool non_blocking=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cast_Char(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Char(@Const @ByRef Tensor self);




// Parsed from ATen/ops/_cast_Double.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_cast_Double_ops.h>


// aten::_cast_Double(Tensor self, bool non_blocking=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cast_Double(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Double(@Const @ByRef Tensor self);




// Parsed from ATen/ops/_cast_Float.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_cast_Float_ops.h>


// aten::_cast_Float(Tensor self, bool non_blocking=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cast_Float(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Float(@Const @ByRef Tensor self);




// Parsed from ATen/ops/_cast_Half.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_cast_Half_ops.h>


// aten::_cast_Half(Tensor self, bool non_blocking=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cast_Half(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Half(@Const @ByRef Tensor self);




// Parsed from ATen/ops/_cast_Int.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_cast_Int_ops.h>


// aten::_cast_Int(Tensor self, bool non_blocking=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cast_Int(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Int(@Const @ByRef Tensor self);




// Parsed from ATen/ops/_cast_Long.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_cast_Long_ops.h>


// aten::_cast_Long(Tensor self, bool non_blocking=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cast_Long(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Long(@Const @ByRef Tensor self);




// Parsed from ATen/ops/_cast_Short.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_cast_Short_ops.h>


// aten::_cast_Short(Tensor self, bool non_blocking=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cast_Short(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Short(@Const @ByRef Tensor self);




// Parsed from ATen/ops/_cdist_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_cdist_backward_ops.h>


// aten::_cdist_backward(Tensor grad, Tensor x1, Tensor x2, float p, Tensor cdist) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cdist_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor x1, @Const @ByRef Tensor x2, double p, @Const @ByRef Tensor cdist);

// aten::_cdist_backward.out(Tensor grad, Tensor x1, Tensor x2, float p, Tensor cdist, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _cdist_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad, @Const @ByRef Tensor x1, @Const @ByRef Tensor x2, double p, @Const @ByRef Tensor cdist);
// aten::_cdist_backward.out(Tensor grad, Tensor x1, Tensor x2, float p, Tensor cdist, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _cdist_backward_outf(@Const @ByRef Tensor grad, @Const @ByRef Tensor x1, @Const @ByRef Tensor x2, double p, @Const @ByRef Tensor cdist, @ByRef Tensor out);




// Parsed from ATen/ops/_cdist_forward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_cdist_forward_ops.h>


// aten::_cdist_forward(Tensor x1, Tensor x2, float p, int? compute_mode) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cdist_forward(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2, double p, @ByVal LongOptional compute_mode);

// aten::_cdist_forward.out(Tensor x1, Tensor x2, float p, int? compute_mode, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _cdist_forward_out(@ByRef Tensor out, @Const @ByRef Tensor x1, @Const @ByRef Tensor x2, double p, @ByVal LongOptional compute_mode);
// aten::_cdist_forward.out(Tensor x1, Tensor x2, float p, int? compute_mode, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _cdist_forward_outf(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2, double p, @ByVal LongOptional compute_mode, @ByRef Tensor out);




// Parsed from ATen/ops/_cholesky_solve_helper.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_cholesky_solve_helper_ops.h>


// aten::_cholesky_solve_helper(Tensor self, Tensor A, bool upper) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cholesky_solve_helper(@Const @ByRef Tensor self, @Const @ByRef Tensor A, @Cast("bool") boolean upper);

// aten::_cholesky_solve_helper.out(Tensor self, Tensor A, bool upper, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _cholesky_solve_helper_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor A, @Cast("bool") boolean upper);
// aten::_cholesky_solve_helper.out(Tensor self, Tensor A, bool upper, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _cholesky_solve_helper_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor A, @Cast("bool") boolean upper, @ByRef Tensor out);




// Parsed from ATen/ops/_choose_qparams_per_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_choose_qparams_per_tensor_ops.h>


// aten::_choose_qparams_per_tensor(Tensor self, bool reduce_range=False) -> (float, int)
@Namespace("at") public static native @ByVal @Cast("std::tuple<double,int64_t>*") LongPointer _choose_qparams_per_tensor(@Const @ByRef Tensor self, @Cast("bool") boolean reduce_range/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<double,int64_t>*") LongPointer _choose_qparams_per_tensor(@Const @ByRef Tensor self);




// Parsed from ATen/ops/_chunk_grad_outputs_efficient_attention.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_chunk_grad_outputs_efficient_attention_ops.h>


// aten::_chunk_grad_outputs_efficient_attention(Tensor query, Tensor key, Tensor value, bool is_causal=False) -> bool
@Namespace("at") public static native @Cast("bool") boolean _chunk_grad_outputs_efficient_attention(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Cast("bool") boolean is_causal/*=false*/);
@Namespace("at") public static native @Cast("bool") boolean _chunk_grad_outputs_efficient_attention(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value);




// Parsed from ATen/ops/_coalesce.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_coalesce_ops.h>


// aten::_coalesce(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _coalesce(@Const @ByRef Tensor self);

// aten::_coalesce.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _coalesce_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::_coalesce.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _coalesce_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/_coalesced.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_coalesced_ops.h>


// aten::_coalesced.out(Tensor self, bool coalesced, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _coalesced_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("bool") boolean coalesced);
// aten::_coalesced.out(Tensor self, bool coalesced, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _coalesced_outf(@Const @ByRef Tensor self, @Cast("bool") boolean coalesced, @ByRef Tensor out);

// aten::_coalesced(Tensor self, bool coalesced) -> Tensor
@Namespace("at") public static native @ByVal Tensor _coalesced(@Const @ByRef Tensor self, @Cast("bool") boolean coalesced);




// Parsed from ATen/ops/_compute_linear_combination.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_compute_linear_combination_ops.h>


// aten::_compute_linear_combination(Tensor input, Tensor coefficients) -> Tensor
@Namespace("at") public static native @ByVal Tensor _compute_linear_combination(@Const @ByRef Tensor input, @Const @ByRef Tensor coefficients);

// aten::_compute_linear_combination.out(Tensor input, Tensor coefficients, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _compute_linear_combination_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor coefficients);
// aten::_compute_linear_combination.out(Tensor input, Tensor coefficients, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _compute_linear_combination_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor coefficients, @ByRef Tensor out);




// Parsed from ATen/ops/_conj.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_conj_ops.h>


// aten::_conj(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor _conj(@Const @ByRef Tensor self);




// Parsed from ATen/ops/_conj_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_conj_copy_ops.h>


// aten::_conj_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _conj_copy(@Const @ByRef Tensor self);

// aten::_conj_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _conj_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::_conj_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _conj_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/_conj_physical.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_conj_physical_ops.h>


// aten::_conj_physical(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _conj_physical(@Const @ByRef Tensor self);

// aten::_conj_physical.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _conj_physical_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::_conj_physical.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _conj_physical_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/_conv_depthwise2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_conv_depthwise2d_ops.h>


// aten::_conv_depthwise2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, SymInt[2] padding, int[2] dilation, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor _conv_depthwise2d_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @Const @ByRef Tensor _conv_depthwise2d_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);


// aten::_conv_depthwise2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, SymInt[2] padding, int[2] dilation, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor _conv_depthwise2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Const @ByRef Tensor out);
@Namespace("at") public static native @Const @ByRef Tensor _conv_depthwise2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Const @ByRef Tensor out);


// aten::_conv_depthwise2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, SymInt[2] padding, int[2] dilation, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor _conv_depthwise2d_symint_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @Const @ByRef Tensor _conv_depthwise2d_symint_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);


// aten::_conv_depthwise2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, SymInt[2] padding, int[2] dilation, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor _conv_depthwise2d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Const @ByRef Tensor out);
@Namespace("at") public static native @Const @ByRef Tensor _conv_depthwise2d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Const @ByRef Tensor out);


// aten::_conv_depthwise2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, SymInt[2] padding, int[2] dilation) -> Tensor
@Namespace("at") public static native @ByVal Tensor _conv_depthwise2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor _conv_depthwise2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);


// aten::_conv_depthwise2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, SymInt[2] padding, int[2] dilation) -> Tensor
@Namespace("at") public static native @ByVal Tensor _conv_depthwise2d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor _conv_depthwise2d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);





// Parsed from ATen/ops/_convert_indices_from_coo_to_csr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_convert_indices_from_coo_to_csr_ops.h>


// aten::_convert_indices_from_coo_to_csr(Tensor self, int size, *, bool out_int32=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _convert_indices_from_coo_to_csr(@Const @ByRef Tensor self, @Cast("int64_t") long size, @Cast("bool") boolean out_int32/*=false*/);
@Namespace("at") public static native @ByVal Tensor _convert_indices_from_coo_to_csr(@Const @ByRef Tensor self, @Cast("int64_t") long size);

// aten::_convert_indices_from_coo_to_csr.out(Tensor self, int size, *, bool out_int32=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _convert_indices_from_coo_to_csr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long size, @Cast("bool") boolean out_int32/*=false*/);
@Namespace("at") public static native @ByRef Tensor _convert_indices_from_coo_to_csr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long size);
// aten::_convert_indices_from_coo_to_csr.out(Tensor self, int size, *, bool out_int32=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _convert_indices_from_coo_to_csr_outf(@Const @ByRef Tensor self, @Cast("int64_t") long size, @Cast("bool") boolean out_int32, @ByRef Tensor out);




// Parsed from ATen/ops/_convert_indices_from_csr_to_coo.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_convert_indices_from_csr_to_coo_ops.h>


// aten::_convert_indices_from_csr_to_coo(Tensor crow_indices, Tensor col_indices, *, bool out_int32=False, bool transpose=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _convert_indices_from_csr_to_coo(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean transpose/*=false*/);
@Namespace("at") public static native @ByVal Tensor _convert_indices_from_csr_to_coo(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices);

// aten::_convert_indices_from_csr_to_coo.out(Tensor crow_indices, Tensor col_indices, *, bool out_int32=False, bool transpose=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _convert_indices_from_csr_to_coo_out(@ByRef Tensor out, @Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean transpose/*=false*/);
@Namespace("at") public static native @ByRef Tensor _convert_indices_from_csr_to_coo_out(@ByRef Tensor out, @Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices);
// aten::_convert_indices_from_csr_to_coo.out(Tensor crow_indices, Tensor col_indices, *, bool out_int32=False, bool transpose=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _convert_indices_from_csr_to_coo_outf(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Cast("bool") boolean out_int32, @Cast("bool") boolean transpose, @ByRef Tensor out);




// Parsed from ATen/ops/_convolution.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_convolution_ops.h>


// aten::_convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32) -> Tensor
@Namespace("at") public static native @ByVal Tensor _convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor _convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled, @Cast("bool") boolean allow_tf32);


// aten::_convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32) -> Tensor
@Namespace("at") public static native @ByVal Tensor _convolution_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal SymIntRef output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor _convolution_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal SymIntRef output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled, @Cast("bool") boolean allow_tf32);


// aten::_convolution.deprecated(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled) -> Tensor
@Namespace("at") public static native @ByVal Tensor _convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled);
@Namespace("at") public static native @ByVal Tensor _convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled);

// aten::_convolution.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _convolution_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByRef Tensor _convolution_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled, @Cast("bool") boolean allow_tf32);


// aten::_convolution.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _convolution_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled, @Cast("bool") boolean allow_tf32, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _convolution_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled, @Cast("bool") boolean allow_tf32, @ByRef Tensor out);


// aten::_convolution.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _convolution_symint_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal SymIntRef output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByRef Tensor _convolution_symint_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal SymIntRef output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled, @Cast("bool") boolean allow_tf32);


// aten::_convolution.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _convolution_symint_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal SymIntRef output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled, @Cast("bool") boolean allow_tf32, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _convolution_symint_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal SymIntRef output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled, @Cast("bool") boolean allow_tf32, @ByRef Tensor out);





// Parsed from ATen/ops/_convolution_double_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_convolution_double_backward_ops.h>


// aten::_convolution_double_backward(Tensor? ggI, Tensor? ggW, Tensor? ggb, Tensor gO, Tensor weight, Tensor self, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _convolution_double_backward(@Const @ByRef TensorOptional ggI, @Const @ByRef TensorOptional ggW, @Const @ByRef TensorOptional ggb, @Const @ByRef Tensor gO, @Const @ByRef Tensor weight, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _convolution_double_backward(@Const @ByRef TensorOptional ggI, @Const @ByRef TensorOptional ggW, @Const @ByRef TensorOptional ggb, @Const @ByRef Tensor gO, @Const @ByRef Tensor weight, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::_convolution_double_backward(Tensor? ggI, Tensor? ggW, Tensor? ggb, Tensor gO, Tensor weight, Tensor self, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _convolution_double_backward_symint(@Const @ByRef TensorOptional ggI, @Const @ByRef TensorOptional ggW, @Const @ByRef TensorOptional ggb, @Const @ByRef Tensor gO, @Const @ByRef Tensor weight, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal SymIntRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _convolution_double_backward_symint(@Const @ByRef TensorOptional ggI, @Const @ByRef TensorOptional ggW, @Const @ByRef TensorOptional ggb, @Const @ByRef Tensor gO, @Const @ByRef Tensor weight, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal SymIntRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);





// Parsed from ATen/ops/_convolution_mode.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_convolution_mode_ops.h>


// aten::_convolution_mode(Tensor input, Tensor weight, Tensor? bias, int[] stride, str padding, int[] dilation, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor _convolution_mode(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::string_view*") Pointer padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor _convolution_mode(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast("c10::string_view*") Pointer padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups);




// Parsed from ATen/ops/_copy_from.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_copy_from_ops.h>


// aten::_copy_from(Tensor self, Tensor dst, bool non_blocking=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _copy_from(@Const @ByRef Tensor self, @Const @ByRef Tensor dst, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _copy_from(@Const @ByRef Tensor self, @Const @ByRef Tensor dst);

// aten::_copy_from.out(Tensor self, Tensor dst, bool non_blocking=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _copy_from_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor dst, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByRef Tensor _copy_from_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor dst);
// aten::_copy_from.out(Tensor self, Tensor dst, bool non_blocking=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _copy_from_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor dst, @Cast("bool") boolean non_blocking, @ByRef Tensor out);




// Parsed from ATen/ops/_copy_from_and_resize.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_copy_from_and_resize_ops.h>


// aten::_copy_from_and_resize(Tensor self, Tensor dst) -> Tensor
@Namespace("at") public static native @ByVal Tensor _copy_from_and_resize(@Const @ByRef Tensor self, @Const @ByRef Tensor dst);

// aten::_copy_from_and_resize.out(Tensor self, Tensor dst, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _copy_from_and_resize_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor dst);
// aten::_copy_from_and_resize.out(Tensor self, Tensor dst, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _copy_from_and_resize_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor dst, @ByRef Tensor out);




// Parsed from ATen/ops/_ctc_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_ctc_loss_ops.h>


// aten::_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, bool zero_infinity=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Cast("int64_t") long blank/*=0*/, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple _ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths);
@Namespace("at") public static native @ByVal TensorTensorTuple _ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Cast("int64_t") long blank/*=0*/, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple _ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... target_lengths);

// aten::_ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, bool zero_infinity=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @Const @ByRef Tensor input_lengths, @Const @ByRef Tensor target_lengths, @Cast("int64_t") long blank/*=0*/, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple _ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @Const @ByRef Tensor input_lengths, @Const @ByRef Tensor target_lengths);

// aten::_ctc_loss.out(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, bool zero_infinity=False, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _ctc_loss_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Cast("int64_t") long blank/*=0*/, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _ctc_loss_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _ctc_loss_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Cast("int64_t") long blank/*=0*/, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _ctc_loss_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... target_lengths);
// aten::_ctc_loss.out(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, bool zero_infinity=False, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _ctc_loss_outf(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Cast("int64_t") long blank, @Cast("bool") boolean zero_infinity, @ByRef Tensor out0, @ByRef Tensor out1);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _ctc_loss_outf(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Cast("int64_t") long blank, @Cast("bool") boolean zero_infinity, @ByRef Tensor out0, @ByRef Tensor out1);

// aten::_ctc_loss.Tensor_out(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, bool zero_infinity=False, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _ctc_loss_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @Const @ByRef Tensor input_lengths, @Const @ByRef Tensor target_lengths, @Cast("int64_t") long blank/*=0*/, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _ctc_loss_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @Const @ByRef Tensor input_lengths, @Const @ByRef Tensor target_lengths);
// aten::_ctc_loss.Tensor_out(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, bool zero_infinity=False, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _ctc_loss_outf(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @Const @ByRef Tensor input_lengths, @Const @ByRef Tensor target_lengths, @Cast("int64_t") long blank, @Cast("bool") boolean zero_infinity, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/_ctc_loss_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_ctc_loss_backward_ops.h>


// aten::_ctc_loss_backward(Tensor grad, Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, bool zero_infinity=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _ctc_loss_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Const @ByRef Tensor neg_log_likelihood, @Const @ByRef Tensor log_alpha, @Cast("int64_t") long blank, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal Tensor _ctc_loss_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Const @ByRef Tensor neg_log_likelihood, @Const @ByRef Tensor log_alpha, @Cast("int64_t") long blank);
@Namespace("at") public static native @ByVal Tensor _ctc_loss_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Const @ByRef Tensor neg_log_likelihood, @Const @ByRef Tensor log_alpha, @Cast("int64_t") long blank, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal Tensor _ctc_loss_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Const @ByRef Tensor neg_log_likelihood, @Const @ByRef Tensor log_alpha, @Cast("int64_t") long blank);

// aten::_ctc_loss_backward.Tensor(Tensor grad, Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, bool zero_infinity=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _ctc_loss_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @Const @ByRef Tensor input_lengths, @Const @ByRef Tensor target_lengths, @Const @ByRef Tensor neg_log_likelihood, @Const @ByRef Tensor log_alpha, @Cast("int64_t") long blank, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal Tensor _ctc_loss_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @Const @ByRef Tensor input_lengths, @Const @ByRef Tensor target_lengths, @Const @ByRef Tensor neg_log_likelihood, @Const @ByRef Tensor log_alpha, @Cast("int64_t") long blank);

// aten::_ctc_loss_backward.out(Tensor grad, Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, bool zero_infinity=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _ctc_loss_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Const @ByRef Tensor neg_log_likelihood, @Const @ByRef Tensor log_alpha, @Cast("int64_t") long blank, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByRef Tensor _ctc_loss_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Const @ByRef Tensor neg_log_likelihood, @Const @ByRef Tensor log_alpha, @Cast("int64_t") long blank);
@Namespace("at") public static native @ByRef Tensor _ctc_loss_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Const @ByRef Tensor neg_log_likelihood, @Const @ByRef Tensor log_alpha, @Cast("int64_t") long blank, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByRef Tensor _ctc_loss_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Const @ByRef Tensor neg_log_likelihood, @Const @ByRef Tensor log_alpha, @Cast("int64_t") long blank);
// aten::_ctc_loss_backward.out(Tensor grad, Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, bool zero_infinity=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _ctc_loss_backward_outf(@Const @ByRef Tensor grad, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Const @ByRef Tensor neg_log_likelihood, @Const @ByRef Tensor log_alpha, @Cast("int64_t") long blank, @Cast("bool") boolean zero_infinity, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _ctc_loss_backward_outf(@Const @ByRef Tensor grad, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Const @ByRef Tensor neg_log_likelihood, @Const @ByRef Tensor log_alpha, @Cast("int64_t") long blank, @Cast("bool") boolean zero_infinity, @ByRef Tensor out);




// Parsed from ATen/ops/_cudnn_ctc_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_cudnn_ctc_loss_ops.h>


// aten::_cudnn_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank, bool deterministic, bool zero_infinity) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _cudnn_ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Cast("int64_t") long blank, @Cast("bool") boolean deterministic, @Cast("bool") boolean zero_infinity);
@Namespace("at") public static native @ByVal TensorTensorTuple _cudnn_ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Cast("int64_t") long blank, @Cast("bool") boolean deterministic, @Cast("bool") boolean zero_infinity);

// aten::_cudnn_ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank, bool deterministic, bool zero_infinity) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _cudnn_ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @Const @ByRef Tensor input_lengths, @Const @ByRef Tensor target_lengths, @Cast("int64_t") long blank, @Cast("bool") boolean deterministic, @Cast("bool") boolean zero_infinity);

// aten::_cudnn_ctc_loss.out(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank, bool deterministic, bool zero_infinity, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _cudnn_ctc_loss_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Cast("int64_t") long blank, @Cast("bool") boolean deterministic, @Cast("bool") boolean zero_infinity);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _cudnn_ctc_loss_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Cast("int64_t") long blank, @Cast("bool") boolean deterministic, @Cast("bool") boolean zero_infinity);
// aten::_cudnn_ctc_loss.out(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank, bool deterministic, bool zero_infinity, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _cudnn_ctc_loss_outf(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Cast("int64_t") long blank, @Cast("bool") boolean deterministic, @Cast("bool") boolean zero_infinity, @ByRef Tensor out0, @ByRef Tensor out1);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _cudnn_ctc_loss_outf(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Cast("int64_t") long blank, @Cast("bool") boolean deterministic, @Cast("bool") boolean zero_infinity, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/_cudnn_init_dropout_state.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_cudnn_init_dropout_state_ops.h>


// aten::_cudnn_init_dropout_state(float dropout, bool train, int dropout_seed, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cudnn_init_dropout_state(double dropout, @Cast("bool") boolean train, @Cast("int64_t") long dropout_seed, @ByVal TensorOptions options);
// aten::_cudnn_init_dropout_state(float dropout, bool train, int dropout_seed, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cudnn_init_dropout_state(double dropout, @Cast("bool") boolean train, @Cast("int64_t") long dropout_seed, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::_cudnn_init_dropout_state.out(float dropout, bool train, int dropout_seed, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _cudnn_init_dropout_state_out(@ByRef Tensor out, double dropout, @Cast("bool") boolean train, @Cast("int64_t") long dropout_seed);
// aten::_cudnn_init_dropout_state.out(float dropout, bool train, int dropout_seed, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _cudnn_init_dropout_state_outf(double dropout, @Cast("bool") boolean train, @Cast("int64_t") long dropout_seed, @ByRef Tensor out);




// Parsed from ATen/ops/_cudnn_rnn.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_cudnn_rnn_ops.h>


// aten::_cudnn_rnn(Tensor input, Tensor[] weight, int weight_stride0, Tensor? weight_buf, Tensor hx, Tensor? cx, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTensorTuple _cudnn_rnn(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef TensorOptional weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state);
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTensorTuple _cudnn_rnn(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef TensorOptional weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Const @ByRef TensorOptional dropout_state);


// aten::_cudnn_rnn(Tensor input, Tensor[] weight, int weight_stride0, Tensor? weight_buf, Tensor hx, Tensor? cx, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTensorTuple _cudnn_rnn_symint(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef TensorOptional weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @ByVal SymInt hidden_size, @ByVal SymInt proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal SymIntRef batch_sizes, @Const @ByRef TensorOptional dropout_state);


// aten::_cudnn_rnn.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor? weight_buf, Tensor hx, Tensor? cx, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _cudnn_rnn_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4, @Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef TensorOptional weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _cudnn_rnn_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4, @Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef TensorOptional weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Const @ByRef TensorOptional dropout_state);


// aten::_cudnn_rnn.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor? weight_buf, Tensor hx, Tensor? cx, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _cudnn_rnn_outf(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef TensorOptional weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _cudnn_rnn_outf(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef TensorOptional weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Const @ByRef TensorOptional dropout_state, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4);


// aten::_cudnn_rnn.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor? weight_buf, Tensor hx, Tensor? cx, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _cudnn_rnn_symint_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4, @Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef TensorOptional weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @ByVal SymInt hidden_size, @ByVal SymInt proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal SymIntRef batch_sizes, @Const @ByRef TensorOptional dropout_state);


// aten::_cudnn_rnn.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor? weight_buf, Tensor hx, Tensor? cx, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _cudnn_rnn_symint_outf(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef TensorOptional weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @ByVal SymInt hidden_size, @ByVal SymInt proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal SymIntRef batch_sizes, @Const @ByRef TensorOptional dropout_state, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4);





// Parsed from ATen/ops/_cudnn_rnn_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_cudnn_rnn_backward_ops.h>


// aten::_cudnn_rnn_backward(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask) -> (Tensor, Tensor, Tensor, Tensor[])
@Namespace("at") public static native @ByVal TensorTensorTensorTensorVectorTuple _cudnn_rnn_backward(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTensorVectorTuple _cudnn_rnn_backward(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask);


// aten::_cudnn_rnn_backward(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask) -> (Tensor, Tensor, Tensor, Tensor[])
@Namespace("at") public static native @ByVal TensorTensorTensorTensorVectorTuple _cudnn_rnn_backward_symint(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @ByVal SymInt hidden_size, @ByVal SymInt proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal SymIntRef batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask);


// aten::_cudnn_rnn_backward.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!)[] out3) -> ()
@Namespace("at") public static native void _cudnn_rnn_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByVal TensorArrayRef out3, @Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask);
@Namespace("at") public static native void _cudnn_rnn_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByVal TensorArrayRef out3, @Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask);


// aten::_cudnn_rnn_backward.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!)[] out3) -> ()
@Namespace("at") public static native void _cudnn_rnn_backward_outf(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByVal TensorArrayRef out3);
@Namespace("at") public static native void _cudnn_rnn_backward_outf(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByVal TensorArrayRef out3);


// aten::_cudnn_rnn_backward.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!)[] out3) -> ()
@Namespace("at") public static native void _cudnn_rnn_backward_symint_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByVal TensorArrayRef out3, @Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @ByVal SymInt hidden_size, @ByVal SymInt proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal SymIntRef batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask);


// aten::_cudnn_rnn_backward.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!)[] out3) -> ()
@Namespace("at") public static native void _cudnn_rnn_backward_symint_outf(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @ByVal SymInt hidden_size, @ByVal SymInt proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal SymIntRef batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByVal TensorArrayRef out3);





// Parsed from ATen/ops/_cudnn_rnn_flatten_weight.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_cudnn_rnn_flatten_weight_ops.h>


// aten::_cudnn_rnn_flatten_weight(Tensor[] weight_arr, int weight_stride0, SymInt input_size, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, bool bidirectional) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cudnn_rnn_flatten_weight(@ByVal TensorArrayRef weight_arr, @Cast("int64_t") long weight_stride0, @Cast("int64_t") long input_size, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, @Cast("bool") boolean bidirectional);


// aten::_cudnn_rnn_flatten_weight(Tensor[] weight_arr, int weight_stride0, SymInt input_size, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, bool bidirectional) -> Tensor
@Namespace("at") public static native @ByVal Tensor _cudnn_rnn_flatten_weight_symint(@ByVal TensorArrayRef weight_arr, @Cast("int64_t") long weight_stride0, @ByVal SymInt input_size, @Cast("int64_t") long mode, @ByVal SymInt hidden_size, @ByVal SymInt proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, @Cast("bool") boolean bidirectional);


// aten::_cudnn_rnn_flatten_weight.out(Tensor[] weight_arr, int weight_stride0, SymInt input_size, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, bool bidirectional, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _cudnn_rnn_flatten_weight_out(@ByRef Tensor out, @ByVal TensorArrayRef weight_arr, @Cast("int64_t") long weight_stride0, @Cast("int64_t") long input_size, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, @Cast("bool") boolean bidirectional);


// aten::_cudnn_rnn_flatten_weight.out(Tensor[] weight_arr, int weight_stride0, SymInt input_size, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, bool bidirectional, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _cudnn_rnn_flatten_weight_outf(@ByVal TensorArrayRef weight_arr, @Cast("int64_t") long weight_stride0, @Cast("int64_t") long input_size, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, @Cast("bool") boolean bidirectional, @ByRef Tensor out);


// aten::_cudnn_rnn_flatten_weight.out(Tensor[] weight_arr, int weight_stride0, SymInt input_size, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, bool bidirectional, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _cudnn_rnn_flatten_weight_symint_out(@ByRef Tensor out, @ByVal TensorArrayRef weight_arr, @Cast("int64_t") long weight_stride0, @ByVal SymInt input_size, @Cast("int64_t") long mode, @ByVal SymInt hidden_size, @ByVal SymInt proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, @Cast("bool") boolean bidirectional);


// aten::_cudnn_rnn_flatten_weight.out(Tensor[] weight_arr, int weight_stride0, SymInt input_size, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, bool bidirectional, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _cudnn_rnn_flatten_weight_symint_outf(@ByVal TensorArrayRef weight_arr, @Cast("int64_t") long weight_stride0, @ByVal SymInt input_size, @Cast("int64_t") long mode, @ByVal SymInt hidden_size, @ByVal SymInt proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, @Cast("bool") boolean bidirectional, @ByRef Tensor out);





// Parsed from ATen/ops/_cufft_clear_plan_cache.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_cufft_clear_plan_cache_ops.h>


// aten::_cufft_clear_plan_cache(int device_index) -> ()
@Namespace("at") public static native void _cufft_clear_plan_cache(@Cast("int64_t") long device_index);




// Parsed from ATen/ops/_cufft_get_plan_cache_max_size.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_cufft_get_plan_cache_max_size_ops.h>


// aten::_cufft_get_plan_cache_max_size(int device_index) -> int
@Namespace("at") public static native @Cast("int64_t") long _cufft_get_plan_cache_max_size(@Cast("int64_t") long device_index);




// Parsed from ATen/ops/_cufft_get_plan_cache_size.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_cufft_get_plan_cache_size_ops.h>


// aten::_cufft_get_plan_cache_size(int device_index) -> int
@Namespace("at") public static native @Cast("int64_t") long _cufft_get_plan_cache_size(@Cast("int64_t") long device_index);




// Parsed from ATen/ops/_cufft_set_plan_cache_max_size.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_cufft_set_plan_cache_max_size_ops.h>


// aten::_cufft_set_plan_cache_max_size(int device_index, int max_size) -> ()
@Namespace("at") public static native void _cufft_set_plan_cache_max_size(@Cast("int64_t") long device_index, @Cast("int64_t") long max_size);




// Parsed from ATen/ops/_cummax_helper.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_cummax_helper_ops.h>


// aten::_cummax_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()
@Namespace("at") public static native void _cummax_helper(@Const @ByRef Tensor self, @ByRef Tensor values, @ByRef Tensor indices, @Cast("int64_t") long dim);




// Parsed from ATen/ops/_cummin_helper.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_cummin_helper_ops.h>


// aten::_cummin_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()
@Namespace("at") public static native void _cummin_helper(@Const @ByRef Tensor self, @ByRef Tensor values, @ByRef Tensor indices, @Cast("int64_t") long dim);




// Parsed from ATen/ops/_debug_has_internal_overlap.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_debug_has_internal_overlap_ops.h>


// aten::_debug_has_internal_overlap(Tensor self) -> int
@Namespace("at") public static native @Cast("int64_t") long _debug_has_internal_overlap(@Const @ByRef Tensor self);




// Parsed from ATen/ops/_dimI.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_dimI_ops.h>






// Parsed from ATen/ops/_dimV.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_dimV_ops.h>






// Parsed from ATen/ops/_dim_arange.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_dim_arange_ops.h>


// aten::_dim_arange(Tensor like, int dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor _dim_arange(@Const @ByRef Tensor like, @Cast("int64_t") long dim);




// Parsed from ATen/ops/_dirichlet_grad.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_dirichlet_grad_ops.h>


// aten::_dirichlet_grad(Tensor x, Tensor alpha, Tensor total) -> Tensor
@Namespace("at") public static native @ByVal Tensor _dirichlet_grad(@Const @ByRef Tensor x, @Const @ByRef Tensor alpha, @Const @ByRef Tensor total);

// aten::_dirichlet_grad.out(Tensor x, Tensor alpha, Tensor total, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _dirichlet_grad_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor alpha, @Const @ByRef Tensor total);
// aten::_dirichlet_grad.out(Tensor x, Tensor alpha, Tensor total, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _dirichlet_grad_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor alpha, @Const @ByRef Tensor total, @ByRef Tensor out);




// Parsed from ATen/ops/_efficient_attention_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_efficient_attention_backward_ops.h>


// aten::_efficient_attention_backward(Tensor grad_out_, Tensor query, Tensor key, Tensor value, Tensor out, Tensor logsumexp, bool is_causal=False, bool chunk_grad_outputs=False) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _efficient_attention_backward(@Const @ByRef Tensor grad_out_, @Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Const @ByRef Tensor out, @Const @ByRef Tensor logsumexp, @Cast("bool") boolean is_causal/*=false*/, @Cast("bool") boolean chunk_grad_outputs/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _efficient_attention_backward(@Const @ByRef Tensor grad_out_, @Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Const @ByRef Tensor out, @Const @ByRef Tensor logsumexp);




// Parsed from ATen/ops/_efficient_attention_forward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_efficient_attention_forward_ops.h>


// aten::_efficient_attention_forward(Tensor query, Tensor key, Tensor value, Tensor? cu_seqlens_q, Tensor? cu_seqlens_k, int? max_seqlen_q, bool compute_log_sumexp=False, bool causal=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _efficient_attention_forward(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Const @ByRef TensorOptional cu_seqlens_q, @Const @ByRef TensorOptional cu_seqlens_k, @ByVal LongOptional max_seqlen_q, @Cast("bool") boolean compute_log_sumexp/*=false*/, @Cast("bool") boolean causal/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple _efficient_attention_forward(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Const @ByRef TensorOptional cu_seqlens_q, @Const @ByRef TensorOptional cu_seqlens_k, @ByVal LongOptional max_seqlen_q);




// Parsed from ATen/ops/_efficientzerotensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_efficientzerotensor_ops.h>


// aten::_efficientzerotensor(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _efficientzerotensor(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _efficientzerotensor(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor _efficientzerotensor(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _efficientzerotensor(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
// aten::_efficientzerotensor(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _efficientzerotensor(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor _efficientzerotensor(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::_efficientzerotensor.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _efficientzerotensor_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor _efficientzerotensor_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
// aten::_efficientzerotensor.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _efficientzerotensor_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _efficientzerotensor_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);




// Parsed from ATen/ops/_embedding_bag.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_embedding_bag_ops.h>


// aten::_embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple _embedding_bag(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Cast("bool") boolean scale_grad_by_freq/*=false*/, @Cast("int64_t") long mode/*=0*/, @Cast("bool") boolean sparse/*=false*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional per_sample_weights, @Cast("bool") boolean include_last_offset/*=false*/, @Cast("int64_t") long padding_idx/*=-1*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple _embedding_bag(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets);

// aten::_embedding_bag.out(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _embedding_bag_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Cast("bool") boolean scale_grad_by_freq/*=false*/, @Cast("int64_t") long mode/*=0*/, @Cast("bool") boolean sparse/*=false*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional per_sample_weights, @Cast("bool") boolean include_last_offset/*=false*/, @Cast("int64_t") long padding_idx/*=-1*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _embedding_bag_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets);
// aten::_embedding_bag.out(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _embedding_bag_outf(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Cast("bool") boolean sparse, @Const @ByRef TensorOptional per_sample_weights, @Cast("bool") boolean include_last_offset, @Cast("int64_t") long padding_idx, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3);




// Parsed from ATen/ops/_embedding_bag_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_embedding_bag_backward_ops.h>


// aten::_embedding_bag_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, SymInt num_weights, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, int padding_idx=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _embedding_bag_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Const @ByRef Tensor maximum_indices, @Cast("int64_t") long num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Cast("bool") boolean sparse, @Const @ByRef TensorOptional per_sample_weights, @Cast("int64_t") long padding_idx/*=-1*/);
@Namespace("at") public static native @ByVal Tensor _embedding_bag_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Const @ByRef Tensor maximum_indices, @Cast("int64_t") long num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Cast("bool") boolean sparse, @Const @ByRef TensorOptional per_sample_weights);


// aten::_embedding_bag_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, SymInt num_weights, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, int padding_idx=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _embedding_bag_backward_symint(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Const @ByRef Tensor maximum_indices, @ByVal SymInt num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Cast("bool") boolean sparse, @Const @ByRef TensorOptional per_sample_weights, @Cast("int64_t") long padding_idx/*=-1*/);
@Namespace("at") public static native @ByVal Tensor _embedding_bag_backward_symint(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Const @ByRef Tensor maximum_indices, @ByVal SymInt num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Cast("bool") boolean sparse, @Const @ByRef TensorOptional per_sample_weights);





// Parsed from ATen/ops/_embedding_bag_dense_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_embedding_bag_dense_backward_ops.h>


// aten::_embedding_bag_dense_backward(Tensor grad, Tensor indices, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, SymInt num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _embedding_bag_dense_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Const @ByRef Tensor maximum_indices, @Cast("int64_t") long num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Const @ByRef TensorOptional per_sample_weights, @Cast("int64_t") long padding_idx/*=-1*/);
@Namespace("at") public static native @ByVal Tensor _embedding_bag_dense_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Const @ByRef Tensor maximum_indices, @Cast("int64_t") long num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Const @ByRef TensorOptional per_sample_weights);


// aten::_embedding_bag_dense_backward(Tensor grad, Tensor indices, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, SymInt num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _embedding_bag_dense_backward_symint(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Const @ByRef Tensor maximum_indices, @ByVal SymInt num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Const @ByRef TensorOptional per_sample_weights, @Cast("int64_t") long padding_idx/*=-1*/);
@Namespace("at") public static native @ByVal Tensor _embedding_bag_dense_backward_symint(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Const @ByRef Tensor maximum_indices, @ByVal SymInt num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Const @ByRef TensorOptional per_sample_weights);


// aten::_embedding_bag_dense_backward.out(Tensor grad, Tensor indices, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, SymInt num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _embedding_bag_dense_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Const @ByRef Tensor maximum_indices, @Cast("int64_t") long num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Const @ByRef TensorOptional per_sample_weights, @Cast("int64_t") long padding_idx/*=-1*/);
@Namespace("at") public static native @ByRef Tensor _embedding_bag_dense_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Const @ByRef Tensor maximum_indices, @Cast("int64_t") long num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Const @ByRef TensorOptional per_sample_weights);


// aten::_embedding_bag_dense_backward.out(Tensor grad, Tensor indices, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, SymInt num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _embedding_bag_dense_backward_outf(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Const @ByRef Tensor maximum_indices, @Cast("int64_t") long num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Const @ByRef TensorOptional per_sample_weights, @Cast("int64_t") long padding_idx, @ByRef Tensor out);


// aten::_embedding_bag_dense_backward.out(Tensor grad, Tensor indices, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, SymInt num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _embedding_bag_dense_backward_symint_out(@ByRef Tensor out, @Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Const @ByRef Tensor maximum_indices, @ByVal SymInt num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Const @ByRef TensorOptional per_sample_weights, @Cast("int64_t") long padding_idx/*=-1*/);
@Namespace("at") public static native @ByRef Tensor _embedding_bag_dense_backward_symint_out(@ByRef Tensor out, @Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Const @ByRef Tensor maximum_indices, @ByVal SymInt num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Const @ByRef TensorOptional per_sample_weights);


// aten::_embedding_bag_dense_backward.out(Tensor grad, Tensor indices, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, SymInt num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _embedding_bag_dense_backward_symint_outf(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Const @ByRef Tensor maximum_indices, @ByVal SymInt num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Const @ByRef TensorOptional per_sample_weights, @Cast("int64_t") long padding_idx, @ByRef Tensor out);





// Parsed from ATen/ops/_embedding_bag_forward_only.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_embedding_bag_forward_only_ops.h>


// aten::_embedding_bag_forward_only(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple _embedding_bag_forward_only(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Cast("bool") boolean scale_grad_by_freq/*=false*/, @Cast("int64_t") long mode/*=0*/, @Cast("bool") boolean sparse/*=false*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional per_sample_weights, @Cast("bool") boolean include_last_offset/*=false*/, @Cast("int64_t") long padding_idx/*=-1*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple _embedding_bag_forward_only(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets);

// aten::_embedding_bag_forward_only.out(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _embedding_bag_forward_only_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Cast("bool") boolean scale_grad_by_freq/*=false*/, @Cast("int64_t") long mode/*=0*/, @Cast("bool") boolean sparse/*=false*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional per_sample_weights, @Cast("bool") boolean include_last_offset/*=false*/, @Cast("int64_t") long padding_idx/*=-1*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _embedding_bag_forward_only_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets);
// aten::_embedding_bag_forward_only.out(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _embedding_bag_forward_only_outf(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Cast("bool") boolean sparse, @Const @ByRef TensorOptional per_sample_weights, @Cast("bool") boolean include_last_offset, @Cast("int64_t") long padding_idx, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3);




// Parsed from ATen/ops/_embedding_bag_per_sample_weights_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_embedding_bag_per_sample_weights_backward_ops.h>


// aten::_embedding_bag_per_sample_weights_backward(Tensor grad, Tensor weight, Tensor indices, Tensor offsets, Tensor offset2bag, int mode, int padding_idx=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _embedding_bag_per_sample_weights_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Const @ByRef Tensor offset2bag, @Cast("int64_t") long mode, @Cast("int64_t") long padding_idx/*=-1*/);
@Namespace("at") public static native @ByVal Tensor _embedding_bag_per_sample_weights_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Const @ByRef Tensor offset2bag, @Cast("int64_t") long mode);

// aten::_embedding_bag_per_sample_weights_backward.out(Tensor grad, Tensor weight, Tensor indices, Tensor offsets, Tensor offset2bag, int mode, int padding_idx=-1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _embedding_bag_per_sample_weights_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad, @Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Const @ByRef Tensor offset2bag, @Cast("int64_t") long mode, @Cast("int64_t") long padding_idx/*=-1*/);
@Namespace("at") public static native @ByRef Tensor _embedding_bag_per_sample_weights_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad, @Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Const @ByRef Tensor offset2bag, @Cast("int64_t") long mode);
// aten::_embedding_bag_per_sample_weights_backward.out(Tensor grad, Tensor weight, Tensor indices, Tensor offsets, Tensor offset2bag, int mode, int padding_idx=-1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _embedding_bag_per_sample_weights_backward_outf(@Const @ByRef Tensor grad, @Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Const @ByRef Tensor offset2bag, @Cast("int64_t") long mode, @Cast("int64_t") long padding_idx, @ByRef Tensor out);




// Parsed from ATen/ops/_embedding_bag_sparse_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_embedding_bag_sparse_backward_ops.h>


// aten::_embedding_bag_sparse_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, SymInt num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _embedding_bag_sparse_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Cast("int64_t") long num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Const @ByRef TensorOptional per_sample_weights, @Cast("int64_t") long padding_idx/*=-1*/);
@Namespace("at") public static native @ByVal Tensor _embedding_bag_sparse_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Cast("int64_t") long num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Const @ByRef TensorOptional per_sample_weights);


// aten::_embedding_bag_sparse_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, SymInt num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _embedding_bag_sparse_backward_symint(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @ByVal SymInt num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Const @ByRef TensorOptional per_sample_weights, @Cast("int64_t") long padding_idx/*=-1*/);
@Namespace("at") public static native @ByVal Tensor _embedding_bag_sparse_backward_symint(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @ByVal SymInt num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Const @ByRef TensorOptional per_sample_weights);





// Parsed from ATen/ops/_empty_affine_quantized.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_empty_affine_quantized_ops.h>


// aten::_empty_affine_quantized(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format) -> Tensor
@Namespace("at") public static native @ByVal Tensor _empty_affine_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, double scale/*=1*/, @Cast("int64_t") long zero_point/*=0*/, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor _empty_affine_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor _empty_affine_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, double scale/*=1*/, @Cast("int64_t") long zero_point/*=0*/, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor _empty_affine_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
// aten::_empty_affine_quantized(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format) -> Tensor
@Namespace("at") public static native @ByVal Tensor _empty_affine_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, double scale, @Cast("int64_t") long zero_point, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor _empty_affine_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, double scale, @Cast("int64_t") long zero_point, @ByVal MemoryFormatOptional memory_format);

// aten::_empty_affine_quantized.out(int[] size, *, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _empty_affine_quantized_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, double scale/*=1*/, @Cast("int64_t") long zero_point/*=0*/, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor _empty_affine_quantized_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor _empty_affine_quantized_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, double scale/*=1*/, @Cast("int64_t") long zero_point/*=0*/, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor _empty_affine_quantized_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
// aten::_empty_affine_quantized.out(int[] size, *, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _empty_affine_quantized_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, double scale, @Cast("int64_t") long zero_point, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _empty_affine_quantized_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, double scale, @Cast("int64_t") long zero_point, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);




// Parsed from ATen/ops/_empty_per_channel_affine_quantized.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_empty_per_channel_affine_quantized_ops.h>


// aten::_empty_per_channel_affine_quantized(int[] size, *, Tensor scales, Tensor zero_points, int axis, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=contiguous_format) -> Tensor
@Namespace("at") public static native @ByVal Tensor _empty_per_channel_affine_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor _empty_per_channel_affine_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis);
@Namespace("at") public static native @ByVal Tensor _empty_per_channel_affine_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor _empty_per_channel_affine_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis);
// aten::_empty_per_channel_affine_quantized(int[] size, *, Tensor scales, Tensor zero_points, int axis, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=contiguous_format) -> Tensor
@Namespace("at") public static native @ByVal Tensor _empty_per_channel_affine_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor _empty_per_channel_affine_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::_empty_per_channel_affine_quantized.out(int[] size, *, Tensor scales, Tensor zero_points, int axis, MemoryFormat? memory_format=contiguous_format, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _empty_per_channel_affine_quantized_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor _empty_per_channel_affine_quantized_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis);
@Namespace("at") public static native @ByRef Tensor _empty_per_channel_affine_quantized_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor _empty_per_channel_affine_quantized_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis);
// aten::_empty_per_channel_affine_quantized.out(int[] size, *, Tensor scales, Tensor zero_points, int axis, MemoryFormat? memory_format=contiguous_format, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _empty_per_channel_affine_quantized_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _empty_per_channel_affine_quantized_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);




// Parsed from ATen/ops/_euclidean_dist.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_euclidean_dist_ops.h>


// aten::_euclidean_dist(Tensor x1, Tensor x2) -> Tensor
@Namespace("at") public static native @ByVal Tensor _euclidean_dist(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2);

// aten::_euclidean_dist.out(Tensor x1, Tensor x2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _euclidean_dist_out(@ByRef Tensor out, @Const @ByRef Tensor x1, @Const @ByRef Tensor x2);
// aten::_euclidean_dist.out(Tensor x1, Tensor x2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _euclidean_dist_outf(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2, @ByRef Tensor out);




// Parsed from ATen/ops/_fake_quantize_learnable_per_channel_affine.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_fake_quantize_learnable_per_channel_affine_ops.h>


// aten::_fake_quantize_learnable_per_channel_affine(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -> Tensor
@Namespace("at") public static native @ByVal Tensor _fake_quantize_learnable_per_channel_affine(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, double grad_factor/*=1.0*/);
@Namespace("at") public static native @ByVal Tensor _fake_quantize_learnable_per_channel_affine(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);

// aten::_fake_quantize_learnable_per_channel_affine.out(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _fake_quantize_learnable_per_channel_affine_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, double grad_factor/*=1.0*/);
@Namespace("at") public static native @ByRef Tensor _fake_quantize_learnable_per_channel_affine_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);
// aten::_fake_quantize_learnable_per_channel_affine.out(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _fake_quantize_learnable_per_channel_affine_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, double grad_factor, @ByRef Tensor out);




// Parsed from ATen/ops/_fake_quantize_learnable_per_channel_affine_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_fake_quantize_learnable_per_channel_affine_backward_ops.h>


// aten::_fake_quantize_learnable_per_channel_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _fake_quantize_learnable_per_channel_affine_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, double grad_factor/*=1.0*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _fake_quantize_learnable_per_channel_affine_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);




// Parsed from ATen/ops/_fake_quantize_learnable_per_tensor_affine.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_fake_quantize_learnable_per_tensor_affine_ops.h>


// aten::_fake_quantize_learnable_per_tensor_affine(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.0) -> Tensor
@Namespace("at") public static native @ByVal Tensor _fake_quantize_learnable_per_tensor_affine(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, double grad_factor/*=1.0*/);
@Namespace("at") public static native @ByVal Tensor _fake_quantize_learnable_per_tensor_affine(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);

// aten::_fake_quantize_learnable_per_tensor_affine.out(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _fake_quantize_learnable_per_tensor_affine_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, double grad_factor/*=1.0*/);
@Namespace("at") public static native @ByRef Tensor _fake_quantize_learnable_per_tensor_affine_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);
// aten::_fake_quantize_learnable_per_tensor_affine.out(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _fake_quantize_learnable_per_tensor_affine_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, double grad_factor, @ByRef Tensor out);




// Parsed from ATen/ops/_fake_quantize_learnable_per_tensor_affine_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_fake_quantize_learnable_per_tensor_affine_backward_ops.h>


// aten::_fake_quantize_learnable_per_tensor_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.0) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _fake_quantize_learnable_per_tensor_affine_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, double grad_factor/*=1.0*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _fake_quantize_learnable_per_tensor_affine_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);




// Parsed from ATen/ops/_fake_quantize_per_tensor_affine_cachemask_tensor_qparams.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_fake_quantize_per_tensor_affine_cachemask_tensor_qparams_ops.h>


// aten::_fake_quantize_per_tensor_affine_cachemask_tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, Tensor fake_quant_enabled, int quant_min, int quant_max) -> (Tensor output, Tensor mask)
@Namespace("at") public static native @ByVal TensorTensorTuple _fake_quantize_per_tensor_affine_cachemask_tensor_qparams(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Const @ByRef Tensor fake_quant_enabled, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);

// aten::_fake_quantize_per_tensor_affine_cachemask_tensor_qparams.out(Tensor self, Tensor scale, Tensor zero_point, Tensor fake_quant_enabled, int quant_min, int quant_max, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _fake_quantize_per_tensor_affine_cachemask_tensor_qparams_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Const @ByRef Tensor fake_quant_enabled, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);
// aten::_fake_quantize_per_tensor_affine_cachemask_tensor_qparams.out(Tensor self, Tensor scale, Tensor zero_point, Tensor fake_quant_enabled, int quant_min, int quant_max, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _fake_quantize_per_tensor_affine_cachemask_tensor_qparams_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Const @ByRef Tensor fake_quant_enabled, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/_fft_c2c.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_fft_c2c_ops.h>


// aten::_fft_c2c(Tensor self, SymInt[] dim, int normalization, bool forward) -> Tensor
@Namespace("at") public static native @ByVal Tensor _fft_c2c(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("bool") boolean forward);
@Namespace("at") public static native @ByVal Tensor _fft_c2c(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("bool") boolean forward);


// aten::_fft_c2c(Tensor self, SymInt[] dim, int normalization, bool forward) -> Tensor
@Namespace("at") public static native @ByVal Tensor _fft_c2c_symint(@Const @ByRef Tensor self, @ByVal SymIntRef dim, @Cast("int64_t") long normalization, @Cast("bool") boolean forward);


// aten::_fft_c2c.out(Tensor self, SymInt[] dim, int normalization, bool forward, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _fft_c2c_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("bool") boolean forward);
@Namespace("at") public static native @ByRef Tensor _fft_c2c_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("bool") boolean forward);


// aten::_fft_c2c.out(Tensor self, SymInt[] dim, int normalization, bool forward, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _fft_c2c_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("bool") boolean forward, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _fft_c2c_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("bool") boolean forward, @ByRef Tensor out);


// aten::_fft_c2c.out(Tensor self, SymInt[] dim, int normalization, bool forward, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _fft_c2c_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef dim, @Cast("int64_t") long normalization, @Cast("bool") boolean forward);


// aten::_fft_c2c.out(Tensor self, SymInt[] dim, int normalization, bool forward, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _fft_c2c_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef dim, @Cast("int64_t") long normalization, @Cast("bool") boolean forward, @ByRef Tensor out);





// Parsed from ATen/ops/_fft_c2r.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_fft_c2r_ops.h>


// aten::_fft_c2r(Tensor self, int[] dim, int normalization, int last_dim_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor _fft_c2r(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("int64_t") long last_dim_size);
@Namespace("at") public static native @ByVal Tensor _fft_c2r(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("int64_t") long last_dim_size);

// aten::_fft_c2r.out(Tensor self, int[] dim, int normalization, int last_dim_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _fft_c2r_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("int64_t") long last_dim_size);
@Namespace("at") public static native @ByRef Tensor _fft_c2r_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("int64_t") long last_dim_size);
// aten::_fft_c2r.out(Tensor self, int[] dim, int normalization, int last_dim_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _fft_c2r_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("int64_t") long last_dim_size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _fft_c2r_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("int64_t") long last_dim_size, @ByRef Tensor out);




// Parsed from ATen/ops/_fft_r2c.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_fft_r2c_ops.h>


// aten::_fft_r2c(Tensor self, int[] dim, int normalization, bool onesided) -> Tensor
@Namespace("at") public static native @ByVal Tensor _fft_r2c(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("bool") boolean onesided);
@Namespace("at") public static native @ByVal Tensor _fft_r2c(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("bool") boolean onesided);

// aten::_fft_r2c.out(Tensor self, int[] dim, int normalization, bool onesided, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _fft_r2c_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("bool") boolean onesided);
@Namespace("at") public static native @ByRef Tensor _fft_r2c_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("bool") boolean onesided);
// aten::_fft_r2c.out(Tensor self, int[] dim, int normalization, bool onesided, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _fft_r2c_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("bool") boolean onesided, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _fft_r2c_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("bool") boolean onesided, @ByRef Tensor out);




// Parsed from ATen/ops/_flash_attention_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_flash_attention_backward_ops.h>


// aten::_flash_attention_backward(Tensor grad_out, Tensor query, Tensor key, Tensor value, Tensor out, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, int max_q, int max_k, float dropout_p, bool is_causal, int philox_seed, int philox_offset) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _flash_attention_backward(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Const @ByRef Tensor out, @Const @ByRef Tensor logsumexp, @Const @ByRef Tensor cum_seq_q, @Const @ByRef Tensor cum_seq_k, @Cast("int64_t") long max_q, @Cast("int64_t") long max_k, double dropout_p, @Cast("bool") boolean is_causal, @Cast("int64_t") long philox_seed, @Cast("int64_t") long philox_offset);




// Parsed from ATen/ops/_flash_attention_forward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_flash_attention_forward_ops.h>


// aten::_flash_attention_forward(Tensor query, Tensor key, Tensor value, Tensor cum_seq_q, Tensor cum_seq_k, int max_q, int max_k, float dropout_p, bool is_causal, bool return_debug_mask) -> (Tensor output, Tensor softmax_logsumexp, int philox_seed, int philox_offset, Tensor debug_attn_mask)
@Namespace("at") public static native @ByVal TensorTensorLongLongTensorTuple _flash_attention_forward(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Const @ByRef Tensor cum_seq_q, @Const @ByRef Tensor cum_seq_k, @Cast("int64_t") long max_q, @Cast("int64_t") long max_k, double dropout_p, @Cast("bool") boolean is_causal, @Cast("bool") boolean return_debug_mask);




// Parsed from ATen/ops/_foobar.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foobar_ops.h>


// aten::_foobar(Tensor self, bool arg1=True, bool arg2=True, *, bool arg3=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor _foobar(@Const @ByRef Tensor self, @Cast("bool") boolean arg1/*=true*/, @Cast("bool") boolean arg2/*=true*/, @Cast("bool") boolean arg3/*=true*/);
@Namespace("at") public static native @ByVal Tensor _foobar(@Const @ByRef Tensor self);

// aten::_foobar.out(Tensor self, bool arg1=True, bool arg2=True, *, bool arg3=True, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _foobar_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("bool") boolean arg1/*=true*/, @Cast("bool") boolean arg2/*=true*/, @Cast("bool") boolean arg3/*=true*/);
@Namespace("at") public static native @ByRef Tensor _foobar_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::_foobar.out(Tensor self, bool arg1=True, bool arg2=True, *, bool arg3=True, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _foobar_outf(@Const @ByRef Tensor self, @Cast("bool") boolean arg1, @Cast("bool") boolean arg2, @Cast("bool") boolean arg3, @ByRef Tensor out);




// Parsed from ATen/ops/_foreach_abs.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_abs_ops.h>


// aten::_foreach_abs(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_abs(@ByVal TensorArrayRef self);

// aten::_foreach_abs_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_abs_(@ByVal TensorArrayRef self);

// aten::_foreach_abs.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_abs_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_abs.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_abs_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_acos.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_acos_ops.h>


// aten::_foreach_acos(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_acos(@ByVal TensorArrayRef self);

// aten::_foreach_acos_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_acos_(@ByVal TensorArrayRef self);

// aten::_foreach_acos.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_acos_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_acos.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_acos_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_add.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_add_ops.h>


// aten::_foreach_add.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_add(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);

// aten::_foreach_add_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
@Namespace("at") public static native void _foreach_add_(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);

// aten::_foreach_add.List(Tensor[] self, Tensor[] other, *, Scalar alpha=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_add(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_add(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);

// aten::_foreach_add_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()
@Namespace("at") public static native void _foreach_add_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native void _foreach_add_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);

// aten::_foreach_add.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_add(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);

// aten::_foreach_add_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
@Namespace("at") public static native void _foreach_add_(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);

// aten::_foreach_add.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_add_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);
// aten::_foreach_add.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_add_outf(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar, @ByVal TensorArrayRef out);

// aten::_foreach_add.List_out(Tensor[] self, Tensor[] other, *, Scalar alpha=1, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_add_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native void _foreach_add_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef other);
// aten::_foreach_add.List_out(Tensor[] self, Tensor[] other, *, Scalar alpha=1, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_add_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other, @Const @ByRef Scalar alpha, @ByVal TensorArrayRef out);

// aten::_foreach_add.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_add_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);
// aten::_foreach_add.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_add_outf(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_addcdiv.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_addcdiv_ops.h>


// aten::_foreach_addcdiv_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()
@Namespace("at") public static native void _foreach_addcdiv_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar value);
@Namespace("at") public static native void _foreach_addcdiv_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2);

// aten::_foreach_addcdiv_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()
@Namespace("at") public static native void _foreach_addcdiv_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @ByVal ScalarArrayRef scalars);

// aten::_foreach_addcdiv_.Tensor(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> ()
@Namespace("at") public static native void _foreach_addcdiv_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @Const @ByRef Tensor scalars);

// aten::_foreach_addcdiv.Scalar(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_addcdiv(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar value);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_addcdiv(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2);

// aten::_foreach_addcdiv.ScalarList(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_addcdiv(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @ByVal ScalarArrayRef scalars);

// aten::_foreach_addcdiv.Tensor(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_addcdiv(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @Const @ByRef Tensor scalars);

// aten::_foreach_addcdiv.Scalar_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_addcdiv_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar value);
@Namespace("at") public static native void _foreach_addcdiv_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2);
// aten::_foreach_addcdiv.Scalar_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_addcdiv_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @Const @ByRef Scalar value, @ByVal TensorArrayRef out);

// aten::_foreach_addcdiv.ScalarList_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_addcdiv_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @ByVal ScalarArrayRef scalars);
// aten::_foreach_addcdiv.ScalarList_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_addcdiv_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @ByVal ScalarArrayRef scalars, @ByVal TensorArrayRef out);

// aten::_foreach_addcdiv.Tensor_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_addcdiv_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @Const @ByRef Tensor scalars);
// aten::_foreach_addcdiv.Tensor_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_addcdiv_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @Const @ByRef Tensor scalars, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_addcmul.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_addcmul_ops.h>


// aten::_foreach_addcmul_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()
@Namespace("at") public static native void _foreach_addcmul_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar value);
@Namespace("at") public static native void _foreach_addcmul_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2);

// aten::_foreach_addcmul_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()
@Namespace("at") public static native void _foreach_addcmul_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @ByVal ScalarArrayRef scalars);

// aten::_foreach_addcmul_.Tensor(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> ()
@Namespace("at") public static native void _foreach_addcmul_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @Const @ByRef Tensor scalars);

// aten::_foreach_addcmul.Scalar(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_addcmul(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar value);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_addcmul(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2);

// aten::_foreach_addcmul.ScalarList(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_addcmul(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @ByVal ScalarArrayRef scalars);

// aten::_foreach_addcmul.Tensor(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_addcmul(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @Const @ByRef Tensor scalars);

// aten::_foreach_addcmul.Scalar_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_addcmul_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar value);
@Namespace("at") public static native void _foreach_addcmul_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2);
// aten::_foreach_addcmul.Scalar_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_addcmul_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @Const @ByRef Scalar value, @ByVal TensorArrayRef out);

// aten::_foreach_addcmul.ScalarList_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_addcmul_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @ByVal ScalarArrayRef scalars);
// aten::_foreach_addcmul.ScalarList_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_addcmul_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @ByVal ScalarArrayRef scalars, @ByVal TensorArrayRef out);

// aten::_foreach_addcmul.Tensor_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_addcmul_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @Const @ByRef Tensor scalars);
// aten::_foreach_addcmul.Tensor_out(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_addcmul_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @Const @ByRef Tensor scalars, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_asin.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_asin_ops.h>


// aten::_foreach_asin(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_asin(@ByVal TensorArrayRef self);

// aten::_foreach_asin_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_asin_(@ByVal TensorArrayRef self);

// aten::_foreach_asin.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_asin_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_asin.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_asin_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_atan.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_atan_ops.h>


// aten::_foreach_atan(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_atan(@ByVal TensorArrayRef self);

// aten::_foreach_atan_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_atan_(@ByVal TensorArrayRef self);

// aten::_foreach_atan.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_atan_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_atan.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_atan_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_ceil.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_ceil_ops.h>


// aten::_foreach_ceil(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_ceil(@ByVal TensorArrayRef self);

// aten::_foreach_ceil_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_ceil_(@ByVal TensorArrayRef self);

// aten::_foreach_ceil.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_ceil_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_ceil.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_ceil_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_clamp_max.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_clamp_max_ops.h>


// aten::_foreach_clamp_max.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_clamp_max(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);

// aten::_foreach_clamp_max_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
@Namespace("at") public static native void _foreach_clamp_max_(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);

// aten::_foreach_clamp_max.List(Tensor[] self, Tensor[] other) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_clamp_max(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);

// aten::_foreach_clamp_max_.List(Tensor(a!)[] self, Tensor[] other) -> ()
@Namespace("at") public static native void _foreach_clamp_max_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);

// aten::_foreach_clamp_max.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_clamp_max(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);

// aten::_foreach_clamp_max_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
@Namespace("at") public static native void _foreach_clamp_max_(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);

// aten::_foreach_clamp_max.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_clamp_max_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);
// aten::_foreach_clamp_max.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_clamp_max_outf(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar, @ByVal TensorArrayRef out);

// aten::_foreach_clamp_max.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_clamp_max_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef other);
// aten::_foreach_clamp_max.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_clamp_max_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other, @ByVal TensorArrayRef out);

// aten::_foreach_clamp_max.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_clamp_max_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);
// aten::_foreach_clamp_max.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_clamp_max_outf(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_clamp_min.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_clamp_min_ops.h>


// aten::_foreach_clamp_min.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_clamp_min(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);

// aten::_foreach_clamp_min_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
@Namespace("at") public static native void _foreach_clamp_min_(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);

// aten::_foreach_clamp_min.List(Tensor[] self, Tensor[] other) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_clamp_min(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);

// aten::_foreach_clamp_min_.List(Tensor(a!)[] self, Tensor[] other) -> ()
@Namespace("at") public static native void _foreach_clamp_min_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);

// aten::_foreach_clamp_min.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_clamp_min(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);

// aten::_foreach_clamp_min_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
@Namespace("at") public static native void _foreach_clamp_min_(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);

// aten::_foreach_clamp_min.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_clamp_min_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);
// aten::_foreach_clamp_min.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_clamp_min_outf(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar, @ByVal TensorArrayRef out);

// aten::_foreach_clamp_min.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_clamp_min_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef other);
// aten::_foreach_clamp_min.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_clamp_min_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other, @ByVal TensorArrayRef out);

// aten::_foreach_clamp_min.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_clamp_min_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);
// aten::_foreach_clamp_min.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_clamp_min_outf(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_cos.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_cos_ops.h>


// aten::_foreach_cos(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_cos(@ByVal TensorArrayRef self);

// aten::_foreach_cos_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_cos_(@ByVal TensorArrayRef self);

// aten::_foreach_cos.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_cos_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_cos.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_cos_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_cosh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_cosh_ops.h>


// aten::_foreach_cosh(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_cosh(@ByVal TensorArrayRef self);

// aten::_foreach_cosh_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_cosh_(@ByVal TensorArrayRef self);

// aten::_foreach_cosh.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_cosh_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_cosh.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_cosh_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_div.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_div_ops.h>


// aten::_foreach_div.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_div(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);

// aten::_foreach_div_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
@Namespace("at") public static native void _foreach_div_(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);

// aten::_foreach_div.List(Tensor[] self, Tensor[] other) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_div(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);

// aten::_foreach_div_.List(Tensor(a!)[] self, Tensor[] other) -> ()
@Namespace("at") public static native void _foreach_div_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);

// aten::_foreach_div.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_div(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);

// aten::_foreach_div_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
@Namespace("at") public static native void _foreach_div_(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);

// aten::_foreach_div.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_div_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);
// aten::_foreach_div.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_div_outf(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar, @ByVal TensorArrayRef out);

// aten::_foreach_div.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_div_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef other);
// aten::_foreach_div.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_div_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other, @ByVal TensorArrayRef out);

// aten::_foreach_div.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_div_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);
// aten::_foreach_div.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_div_outf(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_erf.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_erf_ops.h>


// aten::_foreach_erf(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_erf(@ByVal TensorArrayRef self);

// aten::_foreach_erf_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_erf_(@ByVal TensorArrayRef self);

// aten::_foreach_erf.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_erf_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_erf.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_erf_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_erfc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_erfc_ops.h>


// aten::_foreach_erfc(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_erfc(@ByVal TensorArrayRef self);

// aten::_foreach_erfc_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_erfc_(@ByVal TensorArrayRef self);

// aten::_foreach_erfc.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_erfc_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_erfc.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_erfc_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_exp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_exp_ops.h>


// aten::_foreach_exp(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_exp(@ByVal TensorArrayRef self);

// aten::_foreach_exp_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_exp_(@ByVal TensorArrayRef self);

// aten::_foreach_exp.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_exp_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_exp.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_exp_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_expm1.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_expm1_ops.h>


// aten::_foreach_expm1(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_expm1(@ByVal TensorArrayRef self);

// aten::_foreach_expm1_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_expm1_(@ByVal TensorArrayRef self);

// aten::_foreach_expm1.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_expm1_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_expm1.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_expm1_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_floor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_floor_ops.h>


// aten::_foreach_floor(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_floor(@ByVal TensorArrayRef self);

// aten::_foreach_floor_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_floor_(@ByVal TensorArrayRef self);

// aten::_foreach_floor.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_floor_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_floor.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_floor_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_frac.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_frac_ops.h>


// aten::_foreach_frac(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_frac(@ByVal TensorArrayRef self);

// aten::_foreach_frac_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_frac_(@ByVal TensorArrayRef self);

// aten::_foreach_frac.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_frac_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_frac.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_frac_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_lerp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_lerp_ops.h>


// aten::_foreach_lerp.List(Tensor[] self, Tensor[] tensors1, Tensor[] weights) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_lerp(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensors1, @ByVal TensorArrayRef weights);

// aten::_foreach_lerp_.List(Tensor(a!)[] self, Tensor[] tensors1, Tensor[] weights) -> ()
@Namespace("at") public static native void _foreach_lerp_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensors1, @ByVal TensorArrayRef weights);

// aten::_foreach_lerp.Scalar(Tensor[] self, Tensor[] tensors1, Scalar weight) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_lerp(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensors1, @Const @ByRef Scalar weight);

// aten::_foreach_lerp_.Scalar(Tensor(a!)[] self, Tensor[] tensors1, Scalar weight) -> ()
@Namespace("at") public static native void _foreach_lerp_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensors1, @Const @ByRef Scalar weight);

// aten::_foreach_lerp.List_out(Tensor[] self, Tensor[] tensors1, Tensor[] weights, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_lerp_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef tensors1, @ByVal TensorArrayRef weights);
// aten::_foreach_lerp.List_out(Tensor[] self, Tensor[] tensors1, Tensor[] weights, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_lerp_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensors1, @ByVal TensorArrayRef weights, @ByVal TensorArrayRef out);

// aten::_foreach_lerp.Scalar_out(Tensor[] self, Tensor[] tensors1, Scalar weight, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_lerp_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef tensors1, @Const @ByRef Scalar weight);
// aten::_foreach_lerp.Scalar_out(Tensor[] self, Tensor[] tensors1, Scalar weight, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_lerp_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensors1, @Const @ByRef Scalar weight, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_lgamma.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_lgamma_ops.h>


// aten::_foreach_lgamma(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_lgamma(@ByVal TensorArrayRef self);

// aten::_foreach_lgamma_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_lgamma_(@ByVal TensorArrayRef self);

// aten::_foreach_lgamma.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_lgamma_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_lgamma.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_lgamma_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_log.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_log_ops.h>


// aten::_foreach_log(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_log(@ByVal TensorArrayRef self);

// aten::_foreach_log_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_log_(@ByVal TensorArrayRef self);

// aten::_foreach_log.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_log_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_log.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_log_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_log10.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_log10_ops.h>


// aten::_foreach_log10(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_log10(@ByVal TensorArrayRef self);

// aten::_foreach_log10_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_log10_(@ByVal TensorArrayRef self);

// aten::_foreach_log10.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_log10_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_log10.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_log10_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_log1p.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_log1p_ops.h>


// aten::_foreach_log1p(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_log1p(@ByVal TensorArrayRef self);

// aten::_foreach_log1p_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_log1p_(@ByVal TensorArrayRef self);

// aten::_foreach_log1p.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_log1p_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_log1p.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_log1p_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_log2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_log2_ops.h>


// aten::_foreach_log2(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_log2(@ByVal TensorArrayRef self);

// aten::_foreach_log2_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_log2_(@ByVal TensorArrayRef self);

// aten::_foreach_log2.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_log2_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_log2.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_log2_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_maximum.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_maximum_ops.h>


// aten::_foreach_maximum.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_maximum(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);

// aten::_foreach_maximum_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
@Namespace("at") public static native void _foreach_maximum_(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);

// aten::_foreach_maximum.List(Tensor[] self, Tensor[] other) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_maximum(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);

// aten::_foreach_maximum_.List(Tensor(a!)[] self, Tensor[] other) -> ()
@Namespace("at") public static native void _foreach_maximum_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);

// aten::_foreach_maximum.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_maximum(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);

// aten::_foreach_maximum_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
@Namespace("at") public static native void _foreach_maximum_(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);

// aten::_foreach_maximum.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_maximum_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);
// aten::_foreach_maximum.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_maximum_outf(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar, @ByVal TensorArrayRef out);

// aten::_foreach_maximum.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_maximum_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef other);
// aten::_foreach_maximum.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_maximum_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other, @ByVal TensorArrayRef out);

// aten::_foreach_maximum.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_maximum_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);
// aten::_foreach_maximum.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_maximum_outf(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_minimum.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_minimum_ops.h>


// aten::_foreach_minimum.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_minimum(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);

// aten::_foreach_minimum_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
@Namespace("at") public static native void _foreach_minimum_(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);

// aten::_foreach_minimum.List(Tensor[] self, Tensor[] other) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_minimum(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);

// aten::_foreach_minimum_.List(Tensor(a!)[] self, Tensor[] other) -> ()
@Namespace("at") public static native void _foreach_minimum_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);

// aten::_foreach_minimum.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_minimum(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);

// aten::_foreach_minimum_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
@Namespace("at") public static native void _foreach_minimum_(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);

// aten::_foreach_minimum.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_minimum_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);
// aten::_foreach_minimum.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_minimum_outf(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar, @ByVal TensorArrayRef out);

// aten::_foreach_minimum.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_minimum_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef other);
// aten::_foreach_minimum.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_minimum_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other, @ByVal TensorArrayRef out);

// aten::_foreach_minimum.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_minimum_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);
// aten::_foreach_minimum.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_minimum_outf(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_mul.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_mul_ops.h>


// aten::_foreach_mul.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_mul(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);

// aten::_foreach_mul_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
@Namespace("at") public static native void _foreach_mul_(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);

// aten::_foreach_mul.List(Tensor[] self, Tensor[] other) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_mul(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);

// aten::_foreach_mul_.List(Tensor(a!)[] self, Tensor[] other) -> ()
@Namespace("at") public static native void _foreach_mul_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);

// aten::_foreach_mul.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_mul(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);

// aten::_foreach_mul_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
@Namespace("at") public static native void _foreach_mul_(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);

// aten::_foreach_mul.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_mul_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);
// aten::_foreach_mul.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_mul_outf(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar, @ByVal TensorArrayRef out);

// aten::_foreach_mul.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_mul_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef other);
// aten::_foreach_mul.List_out(Tensor[] self, Tensor[] other, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_mul_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other, @ByVal TensorArrayRef out);

// aten::_foreach_mul.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_mul_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);
// aten::_foreach_mul.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_mul_outf(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_neg.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_neg_ops.h>


// aten::_foreach_neg(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_neg(@ByVal TensorArrayRef self);

// aten::_foreach_neg_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_neg_(@ByVal TensorArrayRef self);

// aten::_foreach_neg.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_neg_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_neg.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_neg_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_norm_ops.h>


// aten::_foreach_norm.Scalar(Tensor[] self, Scalar ord=2) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_norm(@ByVal TensorArrayRef self, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar ord);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_norm(@ByVal TensorArrayRef self);

// aten::_foreach_norm.Scalar_out(Tensor[] self, Scalar ord=2, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_norm_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar ord);
@Namespace("at") public static native void _foreach_norm_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_norm.Scalar_out(Tensor[] self, Scalar ord=2, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_norm_outf(@ByVal TensorArrayRef self, @Const @ByRef Scalar ord, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_reciprocal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_reciprocal_ops.h>


// aten::_foreach_reciprocal(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_reciprocal(@ByVal TensorArrayRef self);

// aten::_foreach_reciprocal_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_reciprocal_(@ByVal TensorArrayRef self);

// aten::_foreach_reciprocal.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_reciprocal_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_reciprocal.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_reciprocal_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_round.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_round_ops.h>


// aten::_foreach_round(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_round(@ByVal TensorArrayRef self);

// aten::_foreach_round_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_round_(@ByVal TensorArrayRef self);

// aten::_foreach_round.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_round_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_round.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_round_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_sigmoid.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_sigmoid_ops.h>


// aten::_foreach_sigmoid(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_sigmoid(@ByVal TensorArrayRef self);

// aten::_foreach_sigmoid_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_sigmoid_(@ByVal TensorArrayRef self);

// aten::_foreach_sigmoid.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_sigmoid_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_sigmoid.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_sigmoid_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_sin.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_sin_ops.h>


// aten::_foreach_sin(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_sin(@ByVal TensorArrayRef self);

// aten::_foreach_sin_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_sin_(@ByVal TensorArrayRef self);

// aten::_foreach_sin.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_sin_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_sin.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_sin_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_sinh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_sinh_ops.h>


// aten::_foreach_sinh(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_sinh(@ByVal TensorArrayRef self);

// aten::_foreach_sinh_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_sinh_(@ByVal TensorArrayRef self);

// aten::_foreach_sinh.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_sinh_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_sinh.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_sinh_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_sqrt.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_sqrt_ops.h>


// aten::_foreach_sqrt(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_sqrt(@ByVal TensorArrayRef self);

// aten::_foreach_sqrt_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_sqrt_(@ByVal TensorArrayRef self);

// aten::_foreach_sqrt.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_sqrt_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_sqrt.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_sqrt_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_sub.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_sub_ops.h>


// aten::_foreach_sub.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_sub(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);

// aten::_foreach_sub_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
@Namespace("at") public static native void _foreach_sub_(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);

// aten::_foreach_sub.List(Tensor[] self, Tensor[] other, *, Scalar alpha=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_sub(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_sub(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);

// aten::_foreach_sub_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()
@Namespace("at") public static native void _foreach_sub_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native void _foreach_sub_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);

// aten::_foreach_sub.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_sub(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);

// aten::_foreach_sub_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
@Namespace("at") public static native void _foreach_sub_(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);

// aten::_foreach_sub.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_sub_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @Const @ByRef Scalar scalar);
// aten::_foreach_sub.Scalar_out(Tensor[] self, Scalar scalar, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_sub_outf(@ByVal TensorArrayRef self, @Const @ByRef Scalar scalar, @ByVal TensorArrayRef out);

// aten::_foreach_sub.List_out(Tensor[] self, Tensor[] other, *, Scalar alpha=1, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_sub_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native void _foreach_sub_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef other);
// aten::_foreach_sub.List_out(Tensor[] self, Tensor[] other, *, Scalar alpha=1, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_sub_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other, @Const @ByRef Scalar alpha, @ByVal TensorArrayRef out);

// aten::_foreach_sub.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_sub_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars);
// aten::_foreach_sub.ScalarList_out(Tensor[] self, Scalar[] scalars, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_sub_outf(@ByVal TensorArrayRef self, @ByVal ScalarArrayRef scalars, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_tan.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_tan_ops.h>


// aten::_foreach_tan(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_tan(@ByVal TensorArrayRef self);

// aten::_foreach_tan_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_tan_(@ByVal TensorArrayRef self);

// aten::_foreach_tan.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_tan_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_tan.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_tan_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_tanh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_tanh_ops.h>


// aten::_foreach_tanh(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_tanh(@ByVal TensorArrayRef self);

// aten::_foreach_tanh_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_tanh_(@ByVal TensorArrayRef self);

// aten::_foreach_tanh.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_tanh_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_tanh.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_tanh_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_trunc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_trunc_ops.h>


// aten::_foreach_trunc(Tensor[] self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_trunc(@ByVal TensorArrayRef self);

// aten::_foreach_trunc_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_trunc_(@ByVal TensorArrayRef self);

// aten::_foreach_trunc.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_trunc_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_trunc.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_trunc_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_foreach_zero.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_foreach_zero_ops.h>


// aten::_foreach_zero_(Tensor(a!)[] self) -> ()
@Namespace("at") public static native void _foreach_zero_(@ByVal TensorArrayRef self);

// aten::_foreach_zero.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_zero_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self);
// aten::_foreach_zero.out(Tensor[] self, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _foreach_zero_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef out);

// aten::_foreach_zero(Tensor[] self) -> Tensor[] self_out
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _foreach_zero(@ByVal TensorArrayRef self);




// Parsed from ATen/ops/_fused_adam.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_fused_adam_ops.h>


// aten::_fused_adam_(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> ()
@Namespace("at") public static native void _fused_adam_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef grads, @ByVal TensorArrayRef exp_avgs, @ByVal TensorArrayRef exp_avg_sqs, @ByVal TensorArrayRef max_exp_avg_sqs, @ByVal TensorArrayRef state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, @Cast("bool") boolean amsgrad, @Cast("bool") boolean maximize, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional grad_scale, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional found_inf);
@Namespace("at") public static native void _fused_adam_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef grads, @ByVal TensorArrayRef exp_avgs, @ByVal TensorArrayRef exp_avg_sqs, @ByVal TensorArrayRef max_exp_avg_sqs, @ByVal TensorArrayRef state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, @Cast("bool") boolean amsgrad, @Cast("bool") boolean maximize);

// aten::_fused_adam.out(Tensor[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _fused_adam_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef grads, @ByVal TensorArrayRef exp_avgs, @ByVal TensorArrayRef exp_avg_sqs, @ByVal TensorArrayRef max_exp_avg_sqs, @ByVal TensorArrayRef state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, @Cast("bool") boolean amsgrad, @Cast("bool") boolean maximize, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional grad_scale, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional found_inf);
@Namespace("at") public static native void _fused_adam_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef grads, @ByVal TensorArrayRef exp_avgs, @ByVal TensorArrayRef exp_avg_sqs, @ByVal TensorArrayRef max_exp_avg_sqs, @ByVal TensorArrayRef state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, @Cast("bool") boolean amsgrad, @Cast("bool") boolean maximize);
// aten::_fused_adam.out(Tensor[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _fused_adam_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef grads, @ByVal TensorArrayRef exp_avgs, @ByVal TensorArrayRef exp_avg_sqs, @ByVal TensorArrayRef max_exp_avg_sqs, @ByVal TensorArrayRef state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, @Cast("bool") boolean amsgrad, @Cast("bool") boolean maximize, @Const @ByRef TensorOptional grad_scale, @Const @ByRef TensorOptional found_inf, @ByVal TensorArrayRef out);

// aten::_fused_adam(Tensor[] self, Tensor[] grads, Tensor[] exp_avgs, Tensor[] exp_avg_sqs, Tensor[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> (Tensor[] self_out, Tensor[] grads_out, Tensor[] exp_avgs_out, Tensor[] exp_avg_sqs_out, Tensor[] max_exp_avg_sqs_out)
@Namespace("at") public static native @ByVal TensorVectorTensorVectorTensorVectorTensorVectorTensorVectorTuple _fused_adam(@ByVal TensorArrayRef self, @ByVal TensorArrayRef grads, @ByVal TensorArrayRef exp_avgs, @ByVal TensorArrayRef exp_avg_sqs, @ByVal TensorArrayRef max_exp_avg_sqs, @ByVal TensorArrayRef state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, @Cast("bool") boolean amsgrad, @Cast("bool") boolean maximize, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional grad_scale, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional found_inf);
@Namespace("at") public static native @ByVal TensorVectorTensorVectorTensorVectorTensorVectorTensorVectorTuple _fused_adam(@ByVal TensorArrayRef self, @ByVal TensorArrayRef grads, @ByVal TensorArrayRef exp_avgs, @ByVal TensorArrayRef exp_avg_sqs, @ByVal TensorArrayRef max_exp_avg_sqs, @ByVal TensorArrayRef state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, @Cast("bool") boolean amsgrad, @Cast("bool") boolean maximize);




// Parsed from ATen/ops/_fused_adamw.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_fused_adamw_ops.h>


// aten::_fused_adamw_(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> ()
@Namespace("at") public static native void _fused_adamw_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef grads, @ByVal TensorArrayRef exp_avgs, @ByVal TensorArrayRef exp_avg_sqs, @ByVal TensorArrayRef max_exp_avg_sqs, @ByVal TensorArrayRef state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, @Cast("bool") boolean amsgrad, @Cast("bool") boolean maximize, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional grad_scale, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional found_inf);
@Namespace("at") public static native void _fused_adamw_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef grads, @ByVal TensorArrayRef exp_avgs, @ByVal TensorArrayRef exp_avg_sqs, @ByVal TensorArrayRef max_exp_avg_sqs, @ByVal TensorArrayRef state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, @Cast("bool") boolean amsgrad, @Cast("bool") boolean maximize);

// aten::_fused_adamw.out(Tensor[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _fused_adamw_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef grads, @ByVal TensorArrayRef exp_avgs, @ByVal TensorArrayRef exp_avg_sqs, @ByVal TensorArrayRef max_exp_avg_sqs, @ByVal TensorArrayRef state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, @Cast("bool") boolean amsgrad, @Cast("bool") boolean maximize, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional grad_scale, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional found_inf);
@Namespace("at") public static native void _fused_adamw_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef self, @ByVal TensorArrayRef grads, @ByVal TensorArrayRef exp_avgs, @ByVal TensorArrayRef exp_avg_sqs, @ByVal TensorArrayRef max_exp_avg_sqs, @ByVal TensorArrayRef state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, @Cast("bool") boolean amsgrad, @Cast("bool") boolean maximize);
// aten::_fused_adamw.out(Tensor[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _fused_adamw_outf(@ByVal TensorArrayRef self, @ByVal TensorArrayRef grads, @ByVal TensorArrayRef exp_avgs, @ByVal TensorArrayRef exp_avg_sqs, @ByVal TensorArrayRef max_exp_avg_sqs, @ByVal TensorArrayRef state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, @Cast("bool") boolean amsgrad, @Cast("bool") boolean maximize, @Const @ByRef TensorOptional grad_scale, @Const @ByRef TensorOptional found_inf, @ByVal TensorArrayRef out);

// aten::_fused_adamw(Tensor[] self, Tensor[] grads, Tensor[] exp_avgs, Tensor[] exp_avg_sqs, Tensor[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> (Tensor[] self_out, Tensor[] grads_out, Tensor[] exp_avgs_out, Tensor[] exp_avg_sqs_out, Tensor[] max_exp_avg_sqs_out)
@Namespace("at") public static native @ByVal TensorVectorTensorVectorTensorVectorTensorVectorTensorVectorTuple _fused_adamw(@ByVal TensorArrayRef self, @ByVal TensorArrayRef grads, @ByVal TensorArrayRef exp_avgs, @ByVal TensorArrayRef exp_avg_sqs, @ByVal TensorArrayRef max_exp_avg_sqs, @ByVal TensorArrayRef state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, @Cast("bool") boolean amsgrad, @Cast("bool") boolean maximize, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional grad_scale, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional found_inf);
@Namespace("at") public static native @ByVal TensorVectorTensorVectorTensorVectorTensorVectorTensorVectorTuple _fused_adamw(@ByVal TensorArrayRef self, @ByVal TensorArrayRef grads, @ByVal TensorArrayRef exp_avgs, @ByVal TensorArrayRef exp_avg_sqs, @ByVal TensorArrayRef max_exp_avg_sqs, @ByVal TensorArrayRef state_steps, double lr, double beta1, double beta2, double weight_decay, double eps, @Cast("bool") boolean amsgrad, @Cast("bool") boolean maximize);




// Parsed from ATen/ops/_fused_dropout.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_fused_dropout_ops.h>


// aten::_fused_dropout(Tensor self, float p, Generator? generator=None) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _fused_dropout(@Const @ByRef Tensor self, double p, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal TensorTensorTuple _fused_dropout(@Const @ByRef Tensor self, double p);

// aten::_fused_dropout.out(Tensor self, float p, Generator? generator=None, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _fused_dropout_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, double p, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _fused_dropout_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, double p);
// aten::_fused_dropout.out(Tensor self, float p, Generator? generator=None, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _fused_dropout_outf(@Const @ByRef Tensor self, double p, @ByVal GeneratorOptional generator, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/_fused_moving_avg_obs_fq_helper.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_fused_moving_avg_obs_fq_helper_ops.h>


// aten::_fused_moving_avg_obs_fq_helper(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -> (Tensor output, Tensor mask)
@Namespace("at") public static native @ByVal TensorTensorTuple _fused_moving_avg_obs_fq_helper(@Const @ByRef Tensor self, @Const @ByRef Tensor observer_on, @Const @ByRef Tensor fake_quant_on, @ByRef Tensor running_min, @ByRef Tensor running_max, @ByRef Tensor scale, @ByRef Tensor zero_point, double averaging_const, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, @Cast("int64_t") long ch_axis, @Cast("bool") boolean per_row_fake_quant/*=false*/, @Cast("bool") boolean symmetric_quant/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple _fused_moving_avg_obs_fq_helper(@Const @ByRef Tensor self, @Const @ByRef Tensor observer_on, @Const @ByRef Tensor fake_quant_on, @ByRef Tensor running_min, @ByRef Tensor running_max, @ByRef Tensor scale, @ByRef Tensor zero_point, double averaging_const, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, @Cast("int64_t") long ch_axis);

// aten::_fused_moving_avg_obs_fq_helper.out(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False, *, Tensor(e!) out0, Tensor(f!) out1) -> (Tensor(e!), Tensor(f!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _fused_moving_avg_obs_fq_helper_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, @Const @ByRef Tensor observer_on, @Const @ByRef Tensor fake_quant_on, @ByRef Tensor running_min, @ByRef Tensor running_max, @ByRef Tensor scale, @ByRef Tensor zero_point, double averaging_const, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, @Cast("int64_t") long ch_axis, @Cast("bool") boolean per_row_fake_quant/*=false*/, @Cast("bool") boolean symmetric_quant/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _fused_moving_avg_obs_fq_helper_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, @Const @ByRef Tensor observer_on, @Const @ByRef Tensor fake_quant_on, @ByRef Tensor running_min, @ByRef Tensor running_max, @ByRef Tensor scale, @ByRef Tensor zero_point, double averaging_const, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, @Cast("int64_t") long ch_axis);
// aten::_fused_moving_avg_obs_fq_helper.out(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False, *, Tensor(e!) out0, Tensor(f!) out1) -> (Tensor(e!), Tensor(f!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _fused_moving_avg_obs_fq_helper_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor observer_on, @Const @ByRef Tensor fake_quant_on, @ByRef Tensor running_min, @ByRef Tensor running_max, @ByRef Tensor scale, @ByRef Tensor zero_point, double averaging_const, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, @Cast("int64_t") long ch_axis, @Cast("bool") boolean per_row_fake_quant, @Cast("bool") boolean symmetric_quant, @ByRef Tensor out0, @ByRef Tensor out1);

// aten::_fused_moving_avg_obs_fq_helper_functional(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor running_min, Tensor running_max, Tensor scale, Tensor zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -> (Tensor output, Tensor mask, Tensor running_min_out, Tensor running_max_out, Tensor scale_out, Tensor zero_point_out)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTensorTensorTuple _fused_moving_avg_obs_fq_helper_functional(@Const @ByRef Tensor self, @Const @ByRef Tensor observer_on, @Const @ByRef Tensor fake_quant_on, @Const @ByRef Tensor running_min, @Const @ByRef Tensor running_max, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, double averaging_const, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, @Cast("int64_t") long ch_axis, @Cast("bool") boolean per_row_fake_quant/*=false*/, @Cast("bool") boolean symmetric_quant/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTensorTensorTuple _fused_moving_avg_obs_fq_helper_functional(@Const @ByRef Tensor self, @Const @ByRef Tensor observer_on, @Const @ByRef Tensor fake_quant_on, @Const @ByRef Tensor running_min, @Const @ByRef Tensor running_max, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, double averaging_const, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, @Cast("int64_t") long ch_axis);




// Parsed from ATen/ops/_fused_sdp_choice.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_fused_sdp_choice_ops.h>


// aten::_fused_sdp_choice(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False) -> int
@Namespace("at") public static native @Cast("int64_t") long _fused_sdp_choice(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional attn_mask, double dropout_p/*=0.0*/, @Cast("bool") boolean is_causal/*=false*/);
@Namespace("at") public static native @Cast("int64_t") long _fused_sdp_choice(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value);




// Parsed from ATen/ops/_fw_primal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_fw_primal_ops.h>






// Parsed from ATen/ops/_fw_primal_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_fw_primal_copy_ops.h>


// aten::_fw_primal_copy(Tensor self, int level) -> Tensor
@Namespace("at") public static native @ByVal Tensor _fw_primal_copy(@Const @ByRef Tensor self, @Cast("int64_t") long level);

// aten::_fw_primal_copy.out(Tensor self, int level, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _fw_primal_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long level);
// aten::_fw_primal_copy.out(Tensor self, int level, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _fw_primal_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long level, @ByRef Tensor out);




// Parsed from ATen/ops/_gather_sparse_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_gather_sparse_backward_ops.h>


// aten::_gather_sparse_backward(Tensor self, int dim, Tensor index, Tensor grad) -> Tensor
@Namespace("at") public static native @ByVal Tensor _gather_sparse_backward(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor grad);




// Parsed from ATen/ops/_grid_sampler_2d_cpu_fallback.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_grid_sampler_2d_cpu_fallback_ops.h>


// aten::_grid_sampler_2d_cpu_fallback(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
@Namespace("at") public static native @ByVal Tensor _grid_sampler_2d_cpu_fallback(@Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);

// aten::_grid_sampler_2d_cpu_fallback.out(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _grid_sampler_2d_cpu_fallback_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);
// aten::_grid_sampler_2d_cpu_fallback.out(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _grid_sampler_2d_cpu_fallback_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners, @ByRef Tensor out);




// Parsed from ATen/ops/_grid_sampler_2d_cpu_fallback_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_grid_sampler_2d_cpu_fallback_backward_ops.h>


// aten::_grid_sampler_2d_cpu_fallback_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _grid_sampler_2d_cpu_fallback_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);




// Parsed from ATen/ops/_has_compatible_shallow_copy_type.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_has_compatible_shallow_copy_type_ops.h>


// aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool
@Namespace("at") public static native @Cast("bool") boolean _has_compatible_shallow_copy_type(@Const @ByRef Tensor self, @Const @ByRef Tensor from);




// Parsed from ATen/ops/_has_same_storage_numel.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_has_same_storage_numel_ops.h>


// aten::_has_same_storage_numel(Tensor self, Tensor other) -> bool
@Namespace("at") public static native @Cast("bool") boolean _has_same_storage_numel(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/_histogramdd_bin_edges.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_histogramdd_bin_edges_ops.h>


// aten::_histogramdd_bin_edges(Tensor self, int[] bins, *, float[]? range=None, Tensor? weight=None, bool density=False) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _histogramdd_bin_edges(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef bins, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") DoubleArrayRefOptional range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _histogramdd_bin_edges(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef bins);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _histogramdd_bin_edges(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] bins, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") DoubleArrayRefOptional range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _histogramdd_bin_edges(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... bins);

// aten::_histogramdd_bin_edges.out(Tensor self, int[] bins, *, float[]? range=None, Tensor? weight=None, bool density=False, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _histogramdd_bin_edges_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef bins, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") DoubleArrayRefOptional range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native void _histogramdd_bin_edges_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef bins);
@Namespace("at") public static native void _histogramdd_bin_edges_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] bins, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") DoubleArrayRefOptional range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native void _histogramdd_bin_edges_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... bins);
// aten::_histogramdd_bin_edges.out(Tensor self, int[] bins, *, float[]? range=None, Tensor? weight=None, bool density=False, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void _histogramdd_bin_edges_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef bins, @ByVal DoubleArrayRefOptional range, @Const @ByRef TensorOptional weight, @Cast("bool") boolean density, @ByVal TensorArrayRef out);
@Namespace("at") public static native void _histogramdd_bin_edges_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] bins, @ByVal DoubleArrayRefOptional range, @Const @ByRef TensorOptional weight, @Cast("bool") boolean density, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/_histogramdd_from_bin_cts.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_histogramdd_from_bin_cts_ops.h>


// aten::_histogramdd_from_bin_cts(Tensor self, int[] bins, *, float[]? range=None, Tensor? weight=None, bool density=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _histogramdd_from_bin_cts(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef bins, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") DoubleArrayRefOptional range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal Tensor _histogramdd_from_bin_cts(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef bins);
@Namespace("at") public static native @ByVal Tensor _histogramdd_from_bin_cts(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] bins, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") DoubleArrayRefOptional range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal Tensor _histogramdd_from_bin_cts(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... bins);

// aten::_histogramdd_from_bin_cts.out(Tensor self, int[] bins, *, float[]? range=None, Tensor? weight=None, bool density=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _histogramdd_from_bin_cts_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef bins, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") DoubleArrayRefOptional range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByRef Tensor _histogramdd_from_bin_cts_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef bins);
@Namespace("at") public static native @ByRef Tensor _histogramdd_from_bin_cts_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] bins, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") DoubleArrayRefOptional range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByRef Tensor _histogramdd_from_bin_cts_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... bins);
// aten::_histogramdd_from_bin_cts.out(Tensor self, int[] bins, *, float[]? range=None, Tensor? weight=None, bool density=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _histogramdd_from_bin_cts_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef bins, @ByVal DoubleArrayRefOptional range, @Const @ByRef TensorOptional weight, @Cast("bool") boolean density, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _histogramdd_from_bin_cts_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] bins, @ByVal DoubleArrayRefOptional range, @Const @ByRef TensorOptional weight, @Cast("bool") boolean density, @ByRef Tensor out);




// Parsed from ATen/ops/_histogramdd_from_bin_tensors.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_histogramdd_from_bin_tensors_ops.h>


// aten::_histogramdd_from_bin_tensors(Tensor self, Tensor[] bins, *, Tensor? weight=None, bool density=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _histogramdd_from_bin_tensors(@Const @ByRef Tensor self, @ByVal TensorArrayRef bins, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal Tensor _histogramdd_from_bin_tensors(@Const @ByRef Tensor self, @ByVal TensorArrayRef bins);

// aten::_histogramdd_from_bin_tensors.out(Tensor self, Tensor[] bins, *, Tensor? weight=None, bool density=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _histogramdd_from_bin_tensors_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal TensorArrayRef bins, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByRef Tensor _histogramdd_from_bin_tensors_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal TensorArrayRef bins);
// aten::_histogramdd_from_bin_tensors.out(Tensor self, Tensor[] bins, *, Tensor? weight=None, bool density=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _histogramdd_from_bin_tensors_outf(@Const @ByRef Tensor self, @ByVal TensorArrayRef bins, @Const @ByRef TensorOptional weight, @Cast("bool") boolean density, @ByRef Tensor out);




// Parsed from ATen/ops/_index_put_impl.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_index_put_impl_ops.h>


// aten::_index_put_impl_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False, bool unsafe=False) -> Tensor(a!)

// aten::_index_put_impl.out(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False, bool unsafe=False, *, Tensor(a!) out) -> Tensor(a!)
// aten::_index_put_impl.out(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False, bool unsafe=False, *, Tensor(a!) out) -> Tensor(a!)

// aten::_index_put_impl(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False, bool unsafe=False) -> Tensor




// Parsed from ATen/ops/_indices.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_indices_ops.h>






// Parsed from ATen/ops/_indices_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_indices_copy_ops.h>


// aten::_indices_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _indices_copy(@Const @ByRef Tensor self);

// aten::_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _indices_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _indices_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/_is_all_true.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_is_all_true_ops.h>


// aten::_is_all_true(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _is_all_true(@Const @ByRef Tensor self);




// Parsed from ATen/ops/_is_any_true.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_is_any_true_ops.h>


// aten::_is_any_true(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _is_any_true(@Const @ByRef Tensor self);




// Parsed from ATen/ops/_is_zerotensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_is_zerotensor_ops.h>


// aten::_is_zerotensor(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean __dispatch__is_zerotensor(@Const @ByRef Tensor self);




// Parsed from ATen/ops/_linalg_check_errors.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_linalg_check_errors_ops.h>


// aten::_linalg_check_errors(Tensor info, str api_name, *, bool is_matrix) -> ()
@Namespace("at") public static native void _linalg_check_errors(@Const @ByRef Tensor info, @ByVal @Cast("c10::string_view*") Pointer api_name, @Cast("bool") boolean is_matrix);




// Parsed from ATen/ops/_linalg_det.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_linalg_det_ops.h>


// aten::_linalg_det(Tensor A) -> (Tensor result, Tensor LU, Tensor pivots)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _linalg_det(@Const @ByRef Tensor A);

// aten::_linalg_det.result(Tensor A, *, Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots) -> (Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _linalg_det_out(@ByRef Tensor result, @ByRef Tensor LU, @ByRef Tensor pivots, @Const @ByRef Tensor A);
// aten::_linalg_det.result(Tensor A, *, Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots) -> (Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _linalg_det_outf(@Const @ByRef Tensor A, @ByRef Tensor result, @ByRef Tensor LU, @ByRef Tensor pivots);




// Parsed from ATen/ops/_linalg_eigh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_linalg_eigh_ops.h>


// aten::_linalg_eigh(Tensor A, str UPLO="L", bool compute_v=True) -> (Tensor eigenvalues, Tensor eigenvectors)
@Namespace("at") public static native @ByVal TensorTensorTuple _linalg_eigh(@Const @ByRef Tensor A, @ByVal(nullValue = "c10::string_view(\"L\")") @Cast("c10::string_view*") Pointer UPLO, @Cast("bool") boolean compute_v/*=true*/);
@Namespace("at") public static native @ByVal TensorTensorTuple _linalg_eigh(@Const @ByRef Tensor A);

// aten::_linalg_eigh.eigenvalues(Tensor A, str UPLO="L", bool compute_v=True, *, Tensor(a!) eigenvalues, Tensor(b!) eigenvectors) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _linalg_eigh_out(@ByRef Tensor eigenvalues, @ByRef Tensor eigenvectors, @Const @ByRef Tensor A, @ByVal(nullValue = "c10::string_view(\"L\")") @Cast("c10::string_view*") Pointer UPLO, @Cast("bool") boolean compute_v/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _linalg_eigh_out(@ByRef Tensor eigenvalues, @ByRef Tensor eigenvectors, @Const @ByRef Tensor A);
// aten::_linalg_eigh.eigenvalues(Tensor A, str UPLO="L", bool compute_v=True, *, Tensor(a!) eigenvalues, Tensor(b!) eigenvectors) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _linalg_eigh_outf(@Const @ByRef Tensor A, @ByVal @Cast("c10::string_view*") Pointer UPLO, @Cast("bool") boolean compute_v, @ByRef Tensor eigenvalues, @ByRef Tensor eigenvectors);




// Parsed from ATen/ops/_linalg_slogdet.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_linalg_slogdet_ops.h>


// aten::_linalg_slogdet(Tensor A) -> (Tensor sign, Tensor logabsdet, Tensor LU, Tensor pivots)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple _linalg_slogdet(@Const @ByRef Tensor A);

// aten::_linalg_slogdet.sign(Tensor A, *, Tensor(a!) sign, Tensor(b!) logabsdet, Tensor(c!) LU, Tensor(d!) pivots) -> (Tensor(a!) sign, Tensor(b!) logabsdet, Tensor(c!) LU, Tensor(d!) pivots)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _linalg_slogdet_out(@ByRef Tensor sign, @ByRef Tensor logabsdet, @ByRef Tensor LU, @ByRef Tensor pivots, @Const @ByRef Tensor A);
// aten::_linalg_slogdet.sign(Tensor A, *, Tensor(a!) sign, Tensor(b!) logabsdet, Tensor(c!) LU, Tensor(d!) pivots) -> (Tensor(a!) sign, Tensor(b!) logabsdet, Tensor(c!) LU, Tensor(d!) pivots)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _linalg_slogdet_outf(@Const @ByRef Tensor A, @ByRef Tensor sign, @ByRef Tensor logabsdet, @ByRef Tensor LU, @ByRef Tensor pivots);




// Parsed from ATen/ops/_linalg_solve_ex.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_linalg_solve_ex_ops.h>


// aten::_linalg_solve_ex(Tensor A, Tensor B, *, bool left=True, bool check_errors=False) -> (Tensor result, Tensor LU, Tensor pivots, Tensor info)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple _linalg_solve_ex(@Const @ByRef Tensor A, @Const @ByRef Tensor B, @Cast("bool") boolean left/*=true*/, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple _linalg_solve_ex(@Const @ByRef Tensor A, @Const @ByRef Tensor B);

// aten::_linalg_solve_ex.result(Tensor A, Tensor B, *, bool left=True, bool check_errors=False, Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots, Tensor(d!) info) -> (Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots, Tensor(d!) info)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _linalg_solve_ex_out(@ByRef Tensor result, @ByRef Tensor LU, @ByRef Tensor pivots, @ByRef Tensor info, @Const @ByRef Tensor A, @Const @ByRef Tensor B, @Cast("bool") boolean left/*=true*/, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _linalg_solve_ex_out(@ByRef Tensor result, @ByRef Tensor LU, @ByRef Tensor pivots, @ByRef Tensor info, @Const @ByRef Tensor A, @Const @ByRef Tensor B);
// aten::_linalg_solve_ex.result(Tensor A, Tensor B, *, bool left=True, bool check_errors=False, Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots, Tensor(d!) info) -> (Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots, Tensor(d!) info)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _linalg_solve_ex_outf(@Const @ByRef Tensor A, @Const @ByRef Tensor B, @Cast("bool") boolean left, @Cast("bool") boolean check_errors, @ByRef Tensor result, @ByRef Tensor LU, @ByRef Tensor pivots, @ByRef Tensor info);




// Parsed from ATen/ops/_linalg_svd.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_linalg_svd_ops.h>


// aten::_linalg_svd(Tensor A, bool full_matrices=False, bool compute_uv=True, *, str? driver=None) -> (Tensor U, Tensor S, Tensor Vh)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _linalg_svd(@Const @ByRef Tensor A, @Cast("bool") boolean full_matrices/*=false*/, @Cast("bool") boolean compute_uv/*=true*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer driver);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _linalg_svd(@Const @ByRef Tensor A);

// aten::_linalg_svd.U(Tensor A, bool full_matrices=False, bool compute_uv=True, *, str? driver=None, Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _linalg_svd_out(@ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor Vh, @Const @ByRef Tensor A, @Cast("bool") boolean full_matrices/*=false*/, @Cast("bool") boolean compute_uv/*=true*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer driver);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _linalg_svd_out(@ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor Vh, @Const @ByRef Tensor A);
// aten::_linalg_svd.U(Tensor A, bool full_matrices=False, bool compute_uv=True, *, str? driver=None, Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _linalg_svd_outf(@Const @ByRef Tensor A, @Cast("bool") boolean full_matrices, @Cast("bool") boolean compute_uv, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer driver, @ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor Vh);




// Parsed from ATen/ops/_local_scalar_dense.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_local_scalar_dense_ops.h>


// aten::_local_scalar_dense(Tensor self) -> Scalar
@Namespace("at") public static native @ByVal Scalar _local_scalar_dense(@Const @ByRef Tensor self);




// Parsed from ATen/ops/_log_softmax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_log_softmax_ops.h>


// aten::_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
@Namespace("at") public static native @ByVal Tensor _log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float);

// aten::_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _log_softmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float);
// aten::_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _log_softmax_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float, @ByRef Tensor out);




// Parsed from ATen/ops/_log_softmax_backward_data.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_log_softmax_backward_data_ops.h>


// aten::_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor _log_softmax_backward_data(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, ScalarType input_dtype);

// aten::_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _log_softmax_backward_data_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, ScalarType input_dtype);
// aten::_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _log_softmax_backward_data_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, ScalarType input_dtype, @ByRef Tensor out);




// Parsed from ATen/ops/_logcumsumexp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_logcumsumexp_ops.h>


// aten::_logcumsumexp(Tensor self, int dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor _logcumsumexp(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::_logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _logcumsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::_logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _logcumsumexp_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor out);




// Parsed from ATen/ops/_lstm_mps.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_lstm_mps_ops.h>


// aten::_lstm_mps(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTensorTensorTuple _lstm_mps(@Const @ByRef Tensor input, @ByVal TensorArrayRef hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);

// aten::_lstm_mps.out(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4, Tensor(f!) out5) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!), Tensor(f!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _lstm_mps_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4, @ByRef Tensor out5, @Const @ByRef Tensor input, @ByVal TensorArrayRef hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);
// aten::_lstm_mps.out(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4, Tensor(f!) out5) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!), Tensor(f!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _lstm_mps_outf(@Const @ByRef Tensor input, @ByVal TensorArrayRef hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4, @ByRef Tensor out5);




// Parsed from ATen/ops/_lu_with_info.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_lu_with_info_ops.h>


// aten::_lu_with_info(Tensor self, bool pivot=True, bool check_errors=True) -> (Tensor LU, Tensor pivots, Tensor info)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _lu_with_info(@Const @ByRef Tensor self, @Cast("bool") boolean pivot/*=true*/, @Cast("bool") boolean check_errors/*=true*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _lu_with_info(@Const @ByRef Tensor self);




// Parsed from ATen/ops/_make_dual.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_make_dual_ops.h>


// aten::_make_dual(Tensor(a) primal, Tensor tangent, int level) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor _make_dual(@Const @ByRef Tensor primal, @Const @ByRef Tensor tangent, @Cast("int64_t") long level);




// Parsed from ATen/ops/_make_dual_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_make_dual_copy_ops.h>


// aten::_make_dual_copy(Tensor primal, Tensor tangent, int level) -> Tensor
@Namespace("at") public static native @ByVal Tensor _make_dual_copy(@Const @ByRef Tensor primal, @Const @ByRef Tensor tangent, @Cast("int64_t") long level);

// aten::_make_dual_copy.out(Tensor primal, Tensor tangent, int level, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _make_dual_copy_out(@ByRef Tensor out, @Const @ByRef Tensor primal, @Const @ByRef Tensor tangent, @Cast("int64_t") long level);
// aten::_make_dual_copy.out(Tensor primal, Tensor tangent, int level, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _make_dual_copy_outf(@Const @ByRef Tensor primal, @Const @ByRef Tensor tangent, @Cast("int64_t") long level, @ByRef Tensor out);




// Parsed from ATen/ops/_make_per_channel_quantized_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_make_per_channel_quantized_tensor_ops.h>


// aten::_make_per_channel_quantized_tensor(Tensor self, Tensor scale, Tensor zero_point, int axis) -> Tensor
@Namespace("at") public static native @ByVal Tensor _make_per_channel_quantized_tensor(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis);

// aten::_make_per_channel_quantized_tensor.out(Tensor self, Tensor scale, Tensor zero_point, int axis, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _make_per_channel_quantized_tensor_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis);
// aten::_make_per_channel_quantized_tensor.out(Tensor self, Tensor scale, Tensor zero_point, int axis, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _make_per_channel_quantized_tensor_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @ByRef Tensor out);




// Parsed from ATen/ops/_make_per_tensor_quantized_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_make_per_tensor_quantized_tensor_ops.h>


// aten::_make_per_tensor_quantized_tensor(Tensor self, float scale, int zero_point) -> Tensor
@Namespace("at") public static native @ByVal Tensor _make_per_tensor_quantized_tensor(@Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point);

// aten::_make_per_tensor_quantized_tensor.out(Tensor self, float scale, int zero_point, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _make_per_tensor_quantized_tensor_out(@ByRef Tensor out, @Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point);
// aten::_make_per_tensor_quantized_tensor.out(Tensor self, float scale, int zero_point, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _make_per_tensor_quantized_tensor_outf(@Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point, @ByRef Tensor out);




// Parsed from ATen/ops/_masked_scale.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_masked_scale_ops.h>


// aten::_masked_scale(Tensor self, Tensor mask, float scale) -> Tensor
@Namespace("at") public static native @ByVal Tensor _masked_scale(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, double scale);

// aten::_masked_scale.out(Tensor self, Tensor mask, float scale, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _masked_scale_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mask, double scale);
// aten::_masked_scale.out(Tensor self, Tensor mask, float scale, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _masked_scale_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, double scale, @ByRef Tensor out);




// Parsed from ATen/ops/_masked_softmax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_masked_softmax_ops.h>


// aten::_masked_softmax(Tensor self, Tensor mask, int? dim=None, int? mask_type=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _masked_softmax(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional mask_type);
@Namespace("at") public static native @ByVal Tensor _masked_softmax(@Const @ByRef Tensor self, @Const @ByRef Tensor mask);

// aten::_masked_softmax.out(Tensor self, Tensor mask, int? dim=None, int? mask_type=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _masked_softmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mask, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional mask_type);
@Namespace("at") public static native @ByRef Tensor _masked_softmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mask);
// aten::_masked_softmax.out(Tensor self, Tensor mask, int? dim=None, int? mask_type=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _masked_softmax_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @ByVal LongOptional dim, @ByVal LongOptional mask_type, @ByRef Tensor out);




// Parsed from ATen/ops/_masked_softmax_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_masked_softmax_backward_ops.h>


// aten::_masked_softmax_backward(Tensor grad_output, Tensor output, Tensor mask, int? dim=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _masked_softmax_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor mask, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByVal Tensor _masked_softmax_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor mask);

// aten::_masked_softmax_backward.out(Tensor grad_output, Tensor output, Tensor mask, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _masked_softmax_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor mask, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByRef Tensor _masked_softmax_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor mask);
// aten::_masked_softmax_backward.out(Tensor grad_output, Tensor output, Tensor mask, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _masked_softmax_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor mask, @ByVal LongOptional dim, @ByRef Tensor out);




// Parsed from ATen/ops/_mkldnn_reshape.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_mkldnn_reshape_ops.h>


// aten::_mkldnn_reshape(Tensor self, int[] shape) -> Tensor
@Namespace("at") public static native @ByVal Tensor _mkldnn_reshape(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape);
@Namespace("at") public static native @ByVal Tensor _mkldnn_reshape(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... shape);

// aten::_mkldnn_reshape.out(Tensor self, int[] shape, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _mkldnn_reshape_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape);
@Namespace("at") public static native @ByRef Tensor _mkldnn_reshape_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... shape);
// aten::_mkldnn_reshape.out(Tensor self, int[] shape, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _mkldnn_reshape_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _mkldnn_reshape_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] shape, @ByRef Tensor out);




// Parsed from ATen/ops/_mkldnn_transpose.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_mkldnn_transpose_ops.h>


// aten::_mkldnn_transpose(Tensor self, int dim0, int dim1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _mkldnn_transpose(@Const @ByRef Tensor self, @Cast("int64_t") long dim0, @Cast("int64_t") long dim1);

// aten::_mkldnn_transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _mkldnn_transpose_(@ByRef Tensor self, @Cast("int64_t") long dim0, @Cast("int64_t") long dim1);

// aten::_mkldnn_transpose.out(Tensor self, int dim0, int dim1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _mkldnn_transpose_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim0, @Cast("int64_t") long dim1);
// aten::_mkldnn_transpose.out(Tensor self, int dim0, int dim1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _mkldnn_transpose_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim0, @Cast("int64_t") long dim1, @ByRef Tensor out);




// Parsed from ATen/ops/_mps_convolution.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_mps_convolution_ops.h>


// aten::_mps_convolution(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor _mps_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor _mps_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups);

// aten::_mps_convolution.out(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _mps_convolution_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByRef Tensor _mps_convolution_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups);
// aten::_mps_convolution.out(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _mps_convolution_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _mps_convolution_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @ByRef Tensor out);




// Parsed from ATen/ops/_mps_convolution_transpose.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_mps_convolution_transpose_ops.h>


// aten::_mps_convolution_transpose(Tensor self, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor _mps_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor _mps_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups);

// aten::_mps_convolution_transpose.out(Tensor self, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _mps_convolution_transpose_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByRef Tensor _mps_convolution_transpose_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups);
// aten::_mps_convolution_transpose.out(Tensor self, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _mps_convolution_transpose_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _mps_convolution_transpose_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @ByRef Tensor out);




// Parsed from ATen/ops/_native_batch_norm_legit.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_native_batch_norm_legit_ops.h>


// aten::_native_batch_norm_legit(Tensor input, Tensor? weight, Tensor? bias, Tensor(a!) running_mean, Tensor(b!) running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _native_batch_norm_legit(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByRef Tensor running_mean, @ByRef Tensor running_var, @Cast("bool") boolean training, double momentum, double eps);

// aten::_native_batch_norm_legit.out(Tensor input, Tensor? weight, Tensor? bias, Tensor(a!) running_mean, Tensor(b!) running_var, bool training, float momentum, float eps, *, Tensor(d!) out, Tensor(e!) save_mean, Tensor(f!) save_invstd) -> (Tensor(d!), Tensor(e!), Tensor(f!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _native_batch_norm_legit_out(@ByRef Tensor out, @ByRef Tensor save_mean, @ByRef Tensor save_invstd, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByRef Tensor running_mean, @ByRef Tensor running_var, @Cast("bool") boolean training, double momentum, double eps);
// aten::_native_batch_norm_legit.out(Tensor input, Tensor? weight, Tensor? bias, Tensor(a!) running_mean, Tensor(b!) running_var, bool training, float momentum, float eps, *, Tensor(d!) out, Tensor(e!) save_mean, Tensor(f!) save_invstd) -> (Tensor(d!), Tensor(e!), Tensor(f!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _native_batch_norm_legit_outf(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByRef Tensor running_mean, @ByRef Tensor running_var, @Cast("bool") boolean training, double momentum, double eps, @ByRef Tensor out, @ByRef Tensor save_mean, @ByRef Tensor save_invstd);

// aten::_native_batch_norm_legit.no_stats(Tensor input, Tensor? weight, Tensor? bias, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _native_batch_norm_legit(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Cast("bool") boolean training, double momentum, double eps);

// aten::_native_batch_norm_legit.no_stats_out(Tensor input, Tensor? weight, Tensor? bias, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _native_batch_norm_legit_out(@ByRef Tensor out, @ByRef Tensor save_mean, @ByRef Tensor save_invstd, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Cast("bool") boolean training, double momentum, double eps);
// aten::_native_batch_norm_legit.no_stats_out(Tensor input, Tensor? weight, Tensor? bias, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _native_batch_norm_legit_outf(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Cast("bool") boolean training, double momentum, double eps, @ByRef Tensor out, @ByRef Tensor save_mean, @ByRef Tensor save_invstd);

// aten::_native_batch_norm_legit_functional(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor, Tensor running_mean_out, Tensor running_var_out)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTensorTuple _native_batch_norm_legit_functional(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef Tensor running_mean, @Const @ByRef Tensor running_var, @Cast("bool") boolean training, double momentum, double eps);




// Parsed from ATen/ops/_native_decoder_only_multi_head_attention.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_native_decoder_only_multi_head_attention_ops.h>


// aten::_native_decoder_only_multi_head_attention(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, Tensor? incr_key=None, Tensor? incr_value=None, bool need_weights=True, bool average_attn_weights=True) -> (Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple _native_decoder_only_multi_head_attention(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_head, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional mask, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional incr_key, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional incr_value, @Cast("bool") boolean need_weights/*=true*/, @Cast("bool") boolean average_attn_weights/*=true*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple _native_decoder_only_multi_head_attention(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_head, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias);

// aten::_native_decoder_only_multi_head_attention.out(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, Tensor? incr_key=None, Tensor? incr_value=None, bool need_weights=True, bool average_attn_weights=True, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _native_decoder_only_multi_head_attention_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_head, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional mask, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional incr_key, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional incr_value, @Cast("bool") boolean need_weights/*=true*/, @Cast("bool") boolean average_attn_weights/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _native_decoder_only_multi_head_attention_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_head, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias);
// aten::_native_decoder_only_multi_head_attention.out(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, Tensor? incr_key=None, Tensor? incr_value=None, bool need_weights=True, bool average_attn_weights=True, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _native_decoder_only_multi_head_attention_outf(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_head, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias, @Const @ByRef TensorOptional mask, @Const @ByRef TensorOptional incr_key, @Const @ByRef TensorOptional incr_value, @Cast("bool") boolean need_weights, @Cast("bool") boolean average_attn_weights, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3);




// Parsed from ATen/ops/_native_multi_head_attention.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_native_multi_head_attention_ops.h>


// aten::_native_multi_head_attention(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, bool need_weights=True, bool average_attn_weights=True, int? mask_type=None) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _native_multi_head_attention(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_head, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional mask, @Cast("bool") boolean need_weights/*=true*/, @Cast("bool") boolean average_attn_weights/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional mask_type);
@Namespace("at") public static native @ByVal TensorTensorTuple _native_multi_head_attention(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_head, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias);

// aten::_native_multi_head_attention.out(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, bool need_weights=True, bool average_attn_weights=True, int? mask_type=None, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _native_multi_head_attention_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_head, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional mask, @Cast("bool") boolean need_weights/*=true*/, @Cast("bool") boolean average_attn_weights/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional mask_type);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _native_multi_head_attention_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_head, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias);
// aten::_native_multi_head_attention.out(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, bool need_weights=True, bool average_attn_weights=True, int? mask_type=None, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _native_multi_head_attention_outf(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_head, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias, @Const @ByRef TensorOptional mask, @Cast("bool") boolean need_weights, @Cast("bool") boolean average_attn_weights, @ByVal LongOptional mask_type, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/_neg_view.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_neg_view_ops.h>


// aten::_neg_view(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor _neg_view(@Const @ByRef Tensor self);




// Parsed from ATen/ops/_neg_view_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_neg_view_copy_ops.h>


// aten::_neg_view_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _neg_view_copy(@Const @ByRef Tensor self);

// aten::_neg_view_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _neg_view_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::_neg_view_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _neg_view_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/_nested_from_padded.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_nested_from_padded_ops.h>


// aten::_nested_from_padded(Tensor padded, Tensor cpu_nested_shape_example, bool fuse_transform_0213=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _nested_from_padded(@Const @ByRef Tensor padded, @Const @ByRef Tensor cpu_nested_shape_example, @Cast("bool") boolean fuse_transform_0213/*=false*/);
@Namespace("at") public static native @ByVal Tensor _nested_from_padded(@Const @ByRef Tensor padded, @Const @ByRef Tensor cpu_nested_shape_example);

// aten::_nested_from_padded.out(Tensor padded, Tensor cpu_nested_shape_example, bool fuse_transform_0213=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _nested_from_padded_out(@ByRef Tensor out, @Const @ByRef Tensor padded, @Const @ByRef Tensor cpu_nested_shape_example, @Cast("bool") boolean fuse_transform_0213/*=false*/);
@Namespace("at") public static native @ByRef Tensor _nested_from_padded_out(@ByRef Tensor out, @Const @ByRef Tensor padded, @Const @ByRef Tensor cpu_nested_shape_example);
// aten::_nested_from_padded.out(Tensor padded, Tensor cpu_nested_shape_example, bool fuse_transform_0213=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _nested_from_padded_outf(@Const @ByRef Tensor padded, @Const @ByRef Tensor cpu_nested_shape_example, @Cast("bool") boolean fuse_transform_0213, @ByRef Tensor out);




// Parsed from ATen/ops/_nested_from_padded_and_nested_example.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_nested_from_padded_and_nested_example_ops.h>


// aten::_nested_from_padded_and_nested_example(Tensor padded, Tensor nt_example) -> Tensor
@Namespace("at") public static native @ByVal Tensor _nested_from_padded_and_nested_example(@Const @ByRef Tensor padded, @Const @ByRef Tensor nt_example);

// aten::_nested_from_padded_and_nested_example.out(Tensor padded, Tensor nt_example, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _nested_from_padded_and_nested_example_out(@ByRef Tensor out, @Const @ByRef Tensor padded, @Const @ByRef Tensor nt_example);
// aten::_nested_from_padded_and_nested_example.out(Tensor padded, Tensor nt_example, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _nested_from_padded_and_nested_example_outf(@Const @ByRef Tensor padded, @Const @ByRef Tensor nt_example, @ByRef Tensor out);




// Parsed from ATen/ops/_nested_select_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_nested_select_backward_ops.h>


// aten::_nested_select_backward(Tensor grad_output, Tensor self, int dim, SymInt index) -> Tensor
@Namespace("at") public static native @ByVal Tensor _nested_select_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long index);


// aten::_nested_select_backward(Tensor grad_output, Tensor self, int dim, SymInt index) -> Tensor
@Namespace("at") public static native @ByVal Tensor _nested_select_backward_symint(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal SymInt index);





// Parsed from ATen/ops/_nested_sum_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_nested_sum_backward_ops.h>


// aten::_nested_sum_backward(Tensor grad, Tensor self, int[1]? dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _nested_sum_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor _nested_sum_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim);
@Namespace("at") public static native @ByVal Tensor _nested_sum_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor _nested_sum_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);




// Parsed from ATen/ops/_nested_tensor_from_mask.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_nested_tensor_from_mask_ops.h>


// aten::_nested_tensor_from_mask(Tensor t, Tensor mask, bool mask_check=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor _nested_tensor_from_mask(@Const @ByRef Tensor t, @Const @ByRef Tensor mask, @Cast("bool") boolean mask_check/*=true*/);
@Namespace("at") public static native @ByVal Tensor _nested_tensor_from_mask(@Const @ByRef Tensor t, @Const @ByRef Tensor mask);

// aten::_nested_tensor_from_mask.out(Tensor t, Tensor mask, bool mask_check=True, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _nested_tensor_from_mask_out(@ByRef Tensor out, @Const @ByRef Tensor t, @Const @ByRef Tensor mask, @Cast("bool") boolean mask_check/*=true*/);
@Namespace("at") public static native @ByRef Tensor _nested_tensor_from_mask_out(@ByRef Tensor out, @Const @ByRef Tensor t, @Const @ByRef Tensor mask);
// aten::_nested_tensor_from_mask.out(Tensor t, Tensor mask, bool mask_check=True, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _nested_tensor_from_mask_outf(@Const @ByRef Tensor t, @Const @ByRef Tensor mask, @Cast("bool") boolean mask_check, @ByRef Tensor out);




// Parsed from ATen/ops/_nested_tensor_from_mask_left_aligned.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_nested_tensor_from_mask_left_aligned_ops.h>


// aten::_nested_tensor_from_mask_left_aligned(Tensor t, Tensor mask) -> bool
@Namespace("at") public static native @Cast("bool") boolean _nested_tensor_from_mask_left_aligned(@Const @ByRef Tensor t, @Const @ByRef Tensor mask);




// Parsed from ATen/ops/_nested_tensor_from_tensor_list.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_nested_tensor_from_tensor_list_ops.h>


// aten::_nested_tensor_from_tensor_list(Tensor[] list, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _nested_tensor_from_tensor_list(@ByVal TensorArrayRef list, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype, @ByVal(nullValue = "c10::optional<at::Layout>(c10::nullopt)") LayoutOptional layout, @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional device, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor _nested_tensor_from_tensor_list(@ByVal TensorArrayRef list);

// aten::_nested_tensor_from_tensor_list.out(Tensor[] list, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _nested_tensor_from_tensor_list_out(@ByRef Tensor out, @ByVal TensorArrayRef list, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype, @ByVal(nullValue = "c10::optional<at::Layout>(c10::nullopt)") LayoutOptional layout, @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional device, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional pin_memory);
@Namespace("at") public static native @ByRef Tensor _nested_tensor_from_tensor_list_out(@ByRef Tensor out, @ByVal TensorArrayRef list);
// aten::_nested_tensor_from_tensor_list.out(Tensor[] list, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _nested_tensor_from_tensor_list_outf(@ByVal TensorArrayRef list, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByRef Tensor out);




// Parsed from ATen/ops/_nested_tensor_offsets.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_nested_tensor_offsets_ops.h>






// Parsed from ATen/ops/_nested_tensor_size.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_nested_tensor_size_ops.h>


// aten::_nested_tensor_size.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _nested_tensor_size_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::_nested_tensor_size.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _nested_tensor_size_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/_nested_tensor_softmax_with_shape.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_nested_tensor_softmax_with_shape_ops.h>


// aten::_nested_tensor_softmax_with_shape(Tensor self, Tensor query) -> Tensor
@Namespace("at") public static native @ByVal Tensor _nested_tensor_softmax_with_shape(@Const @ByRef Tensor self, @Const @ByRef Tensor query);




// Parsed from ATen/ops/_nested_tensor_strides.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_nested_tensor_strides_ops.h>


// aten::_nested_tensor_strides.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _nested_tensor_strides_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::_nested_tensor_strides.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _nested_tensor_strides_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/_nested_view_from_buffer.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_nested_view_from_buffer_ops.h>


// aten::_nested_view_from_buffer(Tensor(a) self, Tensor nested_size, Tensor nested_strides, int[] offsets) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor _nested_view_from_buffer(@Const @ByRef Tensor self, @Const @ByRef Tensor nested_size, @Const @ByRef Tensor nested_strides, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef offsets);
@Namespace("at") public static native @ByVal Tensor _nested_view_from_buffer(@Const @ByRef Tensor self, @Const @ByRef Tensor nested_size, @Const @ByRef Tensor nested_strides, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... offsets);




// Parsed from ATen/ops/_nested_view_from_buffer_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_nested_view_from_buffer_copy_ops.h>


// aten::_nested_view_from_buffer_copy(Tensor self, Tensor nested_size, Tensor nested_strides, int[] offsets) -> Tensor
@Namespace("at") public static native @ByVal Tensor _nested_view_from_buffer_copy(@Const @ByRef Tensor self, @Const @ByRef Tensor nested_size, @Const @ByRef Tensor nested_strides, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef offsets);
@Namespace("at") public static native @ByVal Tensor _nested_view_from_buffer_copy(@Const @ByRef Tensor self, @Const @ByRef Tensor nested_size, @Const @ByRef Tensor nested_strides, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... offsets);

// aten::_nested_view_from_buffer_copy.out(Tensor self, Tensor nested_size, Tensor nested_strides, int[] offsets, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _nested_view_from_buffer_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor nested_size, @Const @ByRef Tensor nested_strides, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef offsets);
@Namespace("at") public static native @ByRef Tensor _nested_view_from_buffer_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor nested_size, @Const @ByRef Tensor nested_strides, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... offsets);
// aten::_nested_view_from_buffer_copy.out(Tensor self, Tensor nested_size, Tensor nested_strides, int[] offsets, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _nested_view_from_buffer_copy_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor nested_size, @Const @ByRef Tensor nested_strides, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef offsets, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _nested_view_from_buffer_copy_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor nested_size, @Const @ByRef Tensor nested_strides, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] offsets, @ByRef Tensor out);




// Parsed from ATen/ops/_new_zeros_with_same_feature_meta.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_new_zeros_with_same_feature_meta_ops.h>


// aten::_new_zeros_with_same_feature_meta(Tensor self, Tensor other, *, int self_num_batch_dims=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor _new_zeros_with_same_feature_meta(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Cast("int64_t") long self_num_batch_dims/*=0*/);
@Namespace("at") public static native @ByVal Tensor _new_zeros_with_same_feature_meta(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::_new_zeros_with_same_feature_meta.out(Tensor self, Tensor other, *, int self_num_batch_dims=0, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _new_zeros_with_same_feature_meta_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @Cast("int64_t") long self_num_batch_dims/*=0*/);
@Namespace("at") public static native @ByRef Tensor _new_zeros_with_same_feature_meta_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::_new_zeros_with_same_feature_meta.out(Tensor self, Tensor other, *, int self_num_batch_dims=0, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _new_zeros_with_same_feature_meta_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Cast("int64_t") long self_num_batch_dims, @ByRef Tensor out);




// Parsed from ATen/ops/_nnpack_spatial_convolution.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_nnpack_spatial_convolution_ops.h>


// aten::_nnpack_spatial_convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[2] padding, int[2] stride=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _nnpack_spatial_convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor _nnpack_spatial_convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor _nnpack_spatial_convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);
@Namespace("at") public static native @ByVal Tensor _nnpack_spatial_convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::_nnpack_spatial_convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[2] padding, int[2] stride=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _nnpack_spatial_convolution_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor _nnpack_spatial_convolution_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding);
@Namespace("at") public static native @ByVal Tensor _nnpack_spatial_convolution_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);


// aten::_nnpack_spatial_convolution.out(Tensor input, Tensor weight, Tensor? bias, SymInt[2] padding, int[2] stride=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _nnpack_spatial_convolution_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor _nnpack_spatial_convolution_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor _nnpack_spatial_convolution_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);
@Namespace("at") public static native @ByRef Tensor _nnpack_spatial_convolution_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::_nnpack_spatial_convolution.out(Tensor input, Tensor weight, Tensor? bias, SymInt[2] padding, int[2] stride=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _nnpack_spatial_convolution_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _nnpack_spatial_convolution_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByRef Tensor out);


// aten::_nnpack_spatial_convolution.out(Tensor input, Tensor weight, Tensor? bias, SymInt[2] padding, int[2] stride=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _nnpack_spatial_convolution_symint_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor _nnpack_spatial_convolution_symint_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding);
@Namespace("at") public static native @ByRef Tensor _nnpack_spatial_convolution_symint_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);


// aten::_nnpack_spatial_convolution.out(Tensor input, Tensor weight, Tensor? bias, SymInt[2] padding, int[2] stride=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _nnpack_spatial_convolution_symint_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _nnpack_spatial_convolution_symint_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByRef Tensor out);





// Parsed from ATen/ops/_nnz.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_nnz_ops.h>






// Parsed from ATen/ops/_pack_padded_sequence.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_pack_padded_sequence_ops.h>


// aten::_pack_padded_sequence(Tensor input, Tensor lengths, bool batch_first) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _pack_padded_sequence(@Const @ByRef Tensor input, @Const @ByRef Tensor lengths, @Cast("bool") boolean batch_first);

// aten::_pack_padded_sequence.out(Tensor input, Tensor lengths, bool batch_first, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _pack_padded_sequence_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor input, @Const @ByRef Tensor lengths, @Cast("bool") boolean batch_first);
// aten::_pack_padded_sequence.out(Tensor input, Tensor lengths, bool batch_first, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _pack_padded_sequence_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor lengths, @Cast("bool") boolean batch_first, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/_pack_padded_sequence_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_pack_padded_sequence_backward_ops.h>


// aten::_pack_padded_sequence_backward(Tensor grad, SymInt[] input_size, Tensor batch_sizes, bool batch_first) -> Tensor
@Namespace("at") public static native @ByVal Tensor _pack_padded_sequence_backward(@Const @ByRef Tensor grad, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Const @ByRef Tensor batch_sizes, @Cast("bool") boolean batch_first);
@Namespace("at") public static native @ByVal Tensor _pack_padded_sequence_backward(@Const @ByRef Tensor grad, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Const @ByRef Tensor batch_sizes, @Cast("bool") boolean batch_first);


// aten::_pack_padded_sequence_backward(Tensor grad, SymInt[] input_size, Tensor batch_sizes, bool batch_first) -> Tensor
@Namespace("at") public static native @ByVal Tensor _pack_padded_sequence_backward_symint(@Const @ByRef Tensor grad, @ByVal SymIntRef input_size, @Const @ByRef Tensor batch_sizes, @Cast("bool") boolean batch_first);





// Parsed from ATen/ops/_pad_circular.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_pad_circular_ops.h>


// aten::_pad_circular(Tensor self, SymInt[] pad) -> Tensor
@Namespace("at") public static native @ByVal Tensor _pad_circular(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef pad);
@Namespace("at") public static native @ByVal Tensor _pad_circular(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... pad);


// aten::_pad_circular(Tensor self, SymInt[] pad) -> Tensor
@Namespace("at") public static native @ByVal Tensor _pad_circular_symint(@Const @ByRef Tensor self, @ByVal SymIntRef pad);





// Parsed from ATen/ops/_pad_enum.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_pad_enum_ops.h>


// aten::_pad_enum(Tensor self, SymInt[] pad, int mode, float? value=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _pad_enum(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef pad, @Cast("int64_t") long mode, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional value);
@Namespace("at") public static native @ByVal Tensor _pad_enum(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef pad, @Cast("int64_t") long mode);
@Namespace("at") public static native @ByVal Tensor _pad_enum(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] pad, @Cast("int64_t") long mode, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional value);
@Namespace("at") public static native @ByVal Tensor _pad_enum(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] pad, @Cast("int64_t") long mode);


// aten::_pad_enum(Tensor self, SymInt[] pad, int mode, float? value=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _pad_enum_symint(@Const @ByRef Tensor self, @ByVal SymIntRef pad, @Cast("int64_t") long mode, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional value);
@Namespace("at") public static native @ByVal Tensor _pad_enum_symint(@Const @ByRef Tensor self, @ByVal SymIntRef pad, @Cast("int64_t") long mode);





// Parsed from ATen/ops/_pad_packed_sequence.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_pad_packed_sequence_ops.h>


// aten::_pad_packed_sequence(Tensor data, Tensor batch_sizes, bool batch_first, Scalar padding_value, int total_length) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _pad_packed_sequence(@Const @ByRef Tensor data, @Const @ByRef Tensor batch_sizes, @Cast("bool") boolean batch_first, @Const @ByRef Scalar padding_value, @Cast("int64_t") long total_length);




// Parsed from ATen/ops/_pdist_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_pdist_backward_ops.h>


// aten::_pdist_backward(Tensor grad, Tensor self, float p, Tensor pdist) -> Tensor
@Namespace("at") public static native @ByVal Tensor _pdist_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, double p, @Const @ByRef Tensor pdist);

// aten::_pdist_backward.out(Tensor grad, Tensor self, float p, Tensor pdist, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _pdist_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad, @Const @ByRef Tensor self, double p, @Const @ByRef Tensor pdist);
// aten::_pdist_backward.out(Tensor grad, Tensor self, float p, Tensor pdist, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _pdist_backward_outf(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, double p, @Const @ByRef Tensor pdist, @ByRef Tensor out);




// Parsed from ATen/ops/_pdist_forward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_pdist_forward_ops.h>


// aten::_pdist_forward(Tensor self, float p=2) -> Tensor
@Namespace("at") public static native @ByVal Tensor _pdist_forward(@Const @ByRef Tensor self, double p/*=2*/);
@Namespace("at") public static native @ByVal Tensor _pdist_forward(@Const @ByRef Tensor self);

// aten::_pdist_forward.out(Tensor self, float p=2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _pdist_forward_out(@ByRef Tensor out, @Const @ByRef Tensor self, double p/*=2*/);
@Namespace("at") public static native @ByRef Tensor _pdist_forward_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::_pdist_forward.out(Tensor self, float p=2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _pdist_forward_outf(@Const @ByRef Tensor self, double p, @ByRef Tensor out);




// Parsed from ATen/ops/_pin_memory.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_pin_memory_ops.h>


// aten::_pin_memory(Tensor self, Device? device=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _pin_memory(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("at") public static native @ByVal Tensor _pin_memory(@Const @ByRef Tensor self);

// aten::_pin_memory.out(Tensor self, Device? device=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _pin_memory_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("at") public static native @ByRef Tensor _pin_memory_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::_pin_memory.out(Tensor self, Device? device=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _pin_memory_outf(@Const @ByRef Tensor self, @ByVal DeviceOptional device, @ByRef Tensor out);




// Parsed from ATen/ops/_prelu_kernel.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_prelu_kernel_ops.h>


// aten::_prelu_kernel(Tensor self, Tensor weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor _prelu_kernel(@Const @ByRef Tensor self, @Const @ByRef Tensor weight);




// Parsed from ATen/ops/_prelu_kernel_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_prelu_kernel_backward_ops.h>


// aten::_prelu_kernel_backward(Tensor grad_output, Tensor self, Tensor weight) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _prelu_kernel_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight);




// Parsed from ATen/ops/_remove_batch_dim.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_remove_batch_dim_ops.h>


// aten::_remove_batch_dim(Tensor self, int level, int batch_size, int out_dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor _remove_batch_dim(@Const @ByRef Tensor self, @Cast("int64_t") long level, @Cast("int64_t") long batch_size, @Cast("int64_t") long out_dim);




// Parsed from ATen/ops/_reshape_alias.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_reshape_alias_ops.h>


// aten::_reshape_alias(Tensor(a) self, SymInt[] size, SymInt[] stride) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor _reshape_alias(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor _reshape_alias(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);


// aten::_reshape_alias(Tensor(a) self, SymInt[] size, SymInt[] stride) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor _reshape_alias_symint(@Const @ByRef Tensor self, @ByVal SymIntRef size, @ByVal SymIntRef stride);





// Parsed from ATen/ops/_reshape_alias_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_reshape_alias_copy_ops.h>


// aten::_reshape_alias_copy(Tensor self, SymInt[] size, SymInt[] stride) -> Tensor
@Namespace("at") public static native @ByVal Tensor _reshape_alias_copy(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor _reshape_alias_copy(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);


// aten::_reshape_alias_copy(Tensor self, SymInt[] size, SymInt[] stride) -> Tensor
@Namespace("at") public static native @ByVal Tensor _reshape_alias_copy_symint(@Const @ByRef Tensor self, @ByVal SymIntRef size, @ByVal SymIntRef stride);


// aten::_reshape_alias_copy.out(Tensor self, SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _reshape_alias_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor _reshape_alias_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);


// aten::_reshape_alias_copy.out(Tensor self, SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _reshape_alias_copy_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _reshape_alias_copy_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByRef Tensor out);


// aten::_reshape_alias_copy.out(Tensor self, SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _reshape_alias_copy_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef size, @ByVal SymIntRef stride);


// aten::_reshape_alias_copy.out(Tensor self, SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _reshape_alias_copy_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef size, @ByVal SymIntRef stride, @ByRef Tensor out);





// Parsed from ATen/ops/_reshape_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_reshape_copy_ops.h>


// aten::_reshape_copy(Tensor self, SymInt[] size) -> Tensor
@Namespace("at") public static native @ByVal Tensor _reshape_copy(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor _reshape_copy(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::_reshape_copy(Tensor self, SymInt[] size) -> Tensor
@Namespace("at") public static native @ByVal Tensor _reshape_copy_symint(@Const @ByRef Tensor self, @ByVal SymIntRef size);





// Parsed from ATen/ops/_reshape_from_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_reshape_from_tensor_ops.h>


// aten::_reshape_from_tensor(Tensor self, Tensor shape) -> Tensor
@Namespace("at") public static native @ByVal Tensor _reshape_from_tensor(@Const @ByRef Tensor self, @Const @ByRef Tensor shape);




// Parsed from ATen/ops/_resize_output.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_resize_output_ops.h>


// aten::_resize_output_(Tensor(a!) self, int[] size, Device device) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor _resize_output_(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal Device device);
@Namespace("at") public static native @Const @ByRef Tensor _resize_output_(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal Device device);

// aten::_resize_output.out(Tensor self, int[] size, Device device, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor _resize_output_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal Device device);
@Namespace("at") public static native @Const @ByRef Tensor _resize_output_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal Device device);
// aten::_resize_output.out(Tensor self, int[] size, Device device, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor _resize_output_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal Device device, @Const @ByRef Tensor out);
@Namespace("at") public static native @Const @ByRef Tensor _resize_output_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal Device device, @Const @ByRef Tensor out);

// aten::_resize_output(Tensor self, int[] size, Device device) -> Tensor
@Namespace("at") public static native @ByVal Tensor _resize_output(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal Device device);
@Namespace("at") public static native @ByVal Tensor _resize_output(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal Device device);




// Parsed from ATen/ops/_rowwise_prune.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_rowwise_prune_ops.h>


// aten::_rowwise_prune(Tensor weight, Tensor mask, ScalarType compressed_indices_dtype) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _rowwise_prune(@Const @ByRef Tensor weight, @Const @ByRef Tensor mask, ScalarType compressed_indices_dtype);




// Parsed from ATen/ops/_sample_dirichlet.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sample_dirichlet_ops.h>


// aten::_sample_dirichlet(Tensor self, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sample_dirichlet(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor _sample_dirichlet(@Const @ByRef Tensor self);

// aten::_sample_dirichlet.out(Tensor self, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sample_dirichlet_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor _sample_dirichlet_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::_sample_dirichlet.out(Tensor self, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sample_dirichlet_outf(@Const @ByRef Tensor self, @ByVal GeneratorOptional generator, @ByRef Tensor out);




// Parsed from ATen/ops/_saturate_weight_to_fp16.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_saturate_weight_to_fp16_ops.h>


// aten::_saturate_weight_to_fp16(Tensor weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor _saturate_weight_to_fp16(@Const @ByRef Tensor weight);




// Parsed from ATen/ops/_scaled_dot_product_attention.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_scaled_dot_product_attention_ops.h>


// aten::_scaled_dot_product_attention(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool need_attn_weights=False, bool is_causal=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _scaled_dot_product_attention(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional attn_mask, double dropout_p/*=0.0*/, @Cast("bool") boolean need_attn_weights/*=false*/, @Cast("bool") boolean is_causal/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple _scaled_dot_product_attention(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value);




// Parsed from ATen/ops/_scaled_dot_product_attention_math.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_scaled_dot_product_attention_math_ops.h>


// aten::_scaled_dot_product_attention_math(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, Tensor? dropout_mask=None) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _scaled_dot_product_attention_math(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional attn_mask, double dropout_p/*=0.0*/, @Cast("bool") boolean is_causal/*=false*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional dropout_mask);
@Namespace("at") public static native @ByVal TensorTensorTuple _scaled_dot_product_attention_math(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value);




// Parsed from ATen/ops/_scaled_dot_product_efficient_attention.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_scaled_dot_product_efficient_attention_ops.h>


// aten::_scaled_dot_product_efficient_attention(Tensor query, Tensor key, Tensor value, bool compute_log_sumexp, bool is_causal=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _scaled_dot_product_efficient_attention(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Cast("bool") boolean compute_log_sumexp, @Cast("bool") boolean is_causal/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple _scaled_dot_product_efficient_attention(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Cast("bool") boolean compute_log_sumexp);




// Parsed from ATen/ops/_scaled_dot_product_efficient_attention_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_scaled_dot_product_efficient_attention_backward_ops.h>


// aten::_scaled_dot_product_efficient_attention_backward(Tensor grad_out_, Tensor query, Tensor key, Tensor value, Tensor out, Tensor logsumexp, bool is_causal=False, bool chunk_grad_outputs=False) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _scaled_dot_product_efficient_attention_backward(@Const @ByRef Tensor grad_out_, @Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Const @ByRef Tensor out, @Const @ByRef Tensor logsumexp, @Cast("bool") boolean is_causal/*=false*/, @Cast("bool") boolean chunk_grad_outputs/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _scaled_dot_product_efficient_attention_backward(@Const @ByRef Tensor grad_out_, @Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Const @ByRef Tensor out, @Const @ByRef Tensor logsumexp);




// Parsed from ATen/ops/_scaled_dot_product_flash_attention_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_scaled_dot_product_flash_attention_backward_ops.h>


// aten::_scaled_dot_product_flash_attention_backward(Tensor grad_out, Tensor query, Tensor key, Tensor value, Tensor out, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, int max_q, int max_k, float dropout_p, bool is_causal, int philox_seed, int philox_offset) -> (Tensor grad_query, Tensor grad_key, Tensor grad_value)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _scaled_dot_product_flash_attention_backward(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Const @ByRef Tensor out, @Const @ByRef Tensor logsumexp, @Const @ByRef Tensor cum_seq_q, @Const @ByRef Tensor cum_seq_k, @Cast("int64_t") long max_q, @Cast("int64_t") long max_k, double dropout_p, @Cast("bool") boolean is_causal, @Cast("int64_t") long philox_seed, @Cast("int64_t") long philox_offset);




// Parsed from ATen/ops/_segment_reduce_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_segment_reduce_backward_ops.h>


// aten::_segment_reduce_backward(Tensor grad, Tensor output, Tensor data, str reduce, *, Tensor? lengths=None, Tensor? offsets=None, int axis=0, Scalar? initial=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _segment_reduce_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor output, @Const @ByRef Tensor data, @ByVal @Cast("c10::string_view*") Pointer reduce, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional lengths, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional offsets, @Cast("int64_t") long axis/*=0*/, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional initial);
@Namespace("at") public static native @ByVal Tensor _segment_reduce_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor output, @Const @ByRef Tensor data, @ByVal @Cast("c10::string_view*") Pointer reduce);

// aten::_segment_reduce_backward.out(Tensor grad, Tensor output, Tensor data, str reduce, *, Tensor? lengths=None, Tensor? offsets=None, int axis=0, Scalar? initial=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _segment_reduce_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad, @Const @ByRef Tensor output, @Const @ByRef Tensor data, @ByVal @Cast("c10::string_view*") Pointer reduce, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional lengths, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional offsets, @Cast("int64_t") long axis/*=0*/, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional initial);
@Namespace("at") public static native @ByRef Tensor _segment_reduce_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad, @Const @ByRef Tensor output, @Const @ByRef Tensor data, @ByVal @Cast("c10::string_view*") Pointer reduce);
// aten::_segment_reduce_backward.out(Tensor grad, Tensor output, Tensor data, str reduce, *, Tensor? lengths=None, Tensor? offsets=None, int axis=0, Scalar? initial=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _segment_reduce_backward_outf(@Const @ByRef Tensor grad, @Const @ByRef Tensor output, @Const @ByRef Tensor data, @ByVal @Cast("c10::string_view*") Pointer reduce, @Const @ByRef TensorOptional lengths, @Const @ByRef TensorOptional offsets, @Cast("int64_t") long axis, @Const @ByRef ScalarOptional initial, @ByRef Tensor out);




// Parsed from ATen/ops/_shape_as_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_shape_as_tensor_ops.h>


// aten::_shape_as_tensor(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _shape_as_tensor(@Const @ByRef Tensor self);




// Parsed from ATen/ops/_slow_conv2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_slow_conv2d_backward_ops.h>


// aten::_slow_conv2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _slow_conv2d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _slow_conv2d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);
// aten::_slow_conv2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _slow_conv2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _slow_conv2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias);

// aten::_slow_conv2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _slow_conv2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _slow_conv2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::_slow_conv2d_backward.output_mask_out(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _slow_conv2d_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _slow_conv2d_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
// aten::_slow_conv2d_backward.output_mask_out(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _slow_conv2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _slow_conv2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/_slow_conv2d_forward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_slow_conv2d_forward_ops.h>


// aten::_slow_conv2d_forward.output(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, *, Tensor(a!) output) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _slow_conv2d_forward_out(@ByRef Tensor output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor _slow_conv2d_forward_out(@ByRef Tensor output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);
// aten::_slow_conv2d_forward.output(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, *, Tensor(a!) output) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _slow_conv2d_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor output);
@Namespace("at") public static native @ByRef Tensor _slow_conv2d_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor output);

// aten::_slow_conv2d_forward(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor _slow_conv2d_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor _slow_conv2d_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);




// Parsed from ATen/ops/_sobol_engine_draw.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sobol_engine_draw_ops.h>


// aten::_sobol_engine_draw(Tensor quasi, int n, Tensor sobolstate, int dimension, int num_generated, ScalarType? dtype) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _sobol_engine_draw(@Const @ByRef Tensor quasi, @Cast("int64_t") long n, @Const @ByRef Tensor sobolstate, @Cast("int64_t") long dimension, @Cast("int64_t") long num_generated, @ByVal ScalarTypeOptional dtype);




// Parsed from ATen/ops/_sobol_engine_ff.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sobol_engine_ff_ops.h>


// aten::_sobol_engine_ff_(Tensor(a!) self, int n, Tensor sobolstate, int dimension, int num_generated) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sobol_engine_ff_(@ByRef Tensor self, @Cast("int64_t") long n, @Const @ByRef Tensor sobolstate, @Cast("int64_t") long dimension, @Cast("int64_t") long num_generated);




// Parsed from ATen/ops/_sobol_engine_initialize_state.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sobol_engine_initialize_state_ops.h>


// aten::_sobol_engine_initialize_state_(Tensor(a!) self, int dimension) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sobol_engine_initialize_state_(@ByRef Tensor self, @Cast("int64_t") long dimension);




// Parsed from ATen/ops/_sobol_engine_scramble.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sobol_engine_scramble_ops.h>


// aten::_sobol_engine_scramble_(Tensor(a!) self, Tensor ltm, int dimension) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sobol_engine_scramble_(@ByRef Tensor self, @Const @ByRef Tensor ltm, @Cast("int64_t") long dimension);




// Parsed from ATen/ops/_softmax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_softmax_ops.h>


// aten::_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
@Namespace("at") public static native @ByVal Tensor _softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float);

// aten::_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _softmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float);
// aten::_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _softmax_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float, @ByRef Tensor out);




// Parsed from ATen/ops/_softmax_backward_data.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_softmax_backward_data_ops.h>


// aten::_softmax_backward_data(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor _softmax_backward_data(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, ScalarType input_dtype);

// aten::_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _softmax_backward_data_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, ScalarType input_dtype);
// aten::_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _softmax_backward_data_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, ScalarType input_dtype, @ByRef Tensor grad_input);




// Parsed from ATen/ops/_sparse_addmm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sparse_addmm_ops.h>


// aten::_sparse_addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_addmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor _sparse_addmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);

// aten::_sparse_addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_addmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor _sparse_addmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);
// aten::_sparse_addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_addmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef Scalar beta, @Const @ByRef Scalar alpha, @ByRef Tensor out);




// Parsed from ATen/ops/_sparse_broadcast_to.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sparse_broadcast_to_ops.h>


// aten::_sparse_broadcast_to(Tensor(a) self, int[] size) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor _sparse_broadcast_to(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor _sparse_broadcast_to(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);




// Parsed from ATen/ops/_sparse_broadcast_to_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sparse_broadcast_to_copy_ops.h>


// aten::_sparse_broadcast_to_copy(Tensor self, int[] size) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_broadcast_to_copy(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor _sparse_broadcast_to_copy(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// aten::_sparse_broadcast_to_copy.out(Tensor self, int[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_broadcast_to_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor _sparse_broadcast_to_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
// aten::_sparse_broadcast_to_copy.out(Tensor self, int[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_broadcast_to_copy_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _sparse_broadcast_to_copy_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);




// Parsed from ATen/ops/_sparse_bsc_tensor_unsafe.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sparse_bsc_tensor_unsafe_ops.h>


// aten::_sparse_bsc_tensor_unsafe(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_bsc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_bsc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor _sparse_bsc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_bsc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
// aten::_sparse_bsc_tensor_unsafe(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_bsc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor _sparse_bsc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);




// Parsed from ATen/ops/_sparse_bsr_tensor_unsafe.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sparse_bsr_tensor_unsafe_ops.h>


// aten::_sparse_bsr_tensor_unsafe(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_bsr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_bsr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor _sparse_bsr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_bsr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
// aten::_sparse_bsr_tensor_unsafe(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_bsr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor _sparse_bsr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);




// Parsed from ATen/ops/_sparse_compressed_tensor_unsafe.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sparse_compressed_tensor_unsafe_ops.h>


// aten::_sparse_compressed_tensor_unsafe(Tensor compressed_indices, Tensor plain_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_compressed_tensor_unsafe(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_compressed_tensor_unsafe(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor _sparse_compressed_tensor_unsafe(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_compressed_tensor_unsafe(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
// aten::_sparse_compressed_tensor_unsafe(Tensor compressed_indices, Tensor plain_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_compressed_tensor_unsafe(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor _sparse_compressed_tensor_unsafe(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);




// Parsed from ATen/ops/_sparse_coo_tensor_unsafe.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sparse_coo_tensor_unsafe_ops.h>


// aten::_sparse_coo_tensor_unsafe(Tensor indices, Tensor values, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::_sparse_coo_tensor_unsafe(Tensor indices, Tensor values, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::_sparse_coo_tensor_unsafe(Tensor indices, Tensor values, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_unsafe_symint(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal SymIntRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_unsafe_symint(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal SymIntRef size);


// aten::_sparse_coo_tensor_unsafe(Tensor indices, Tensor values, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_unsafe_symint(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal SymIntRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);





// Parsed from ATen/ops/_sparse_coo_tensor_with_dims.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sparse_coo_tensor_with_dims_ops.h>


// aten::_sparse_coo_tensor_with_dims(int sparse_dim, int dense_dim, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal TensorOptions options);
// aten::_sparse_coo_tensor_with_dims(int sparse_dim, int dense_dim, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::_sparse_coo_tensor_with_dims.out(int sparse_dim, int dense_dim, int[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_coo_tensor_with_dims_out(@ByRef Tensor out, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor _sparse_coo_tensor_with_dims_out(@ByRef Tensor out, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
// aten::_sparse_coo_tensor_with_dims.out(int sparse_dim, int dense_dim, int[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_coo_tensor_with_dims_outf(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _sparse_coo_tensor_with_dims_outf(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);




// Parsed from ATen/ops/_sparse_coo_tensor_with_dims_and_tensors.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sparse_coo_tensor_with_dims_and_tensors_ops.h>


// aten::_sparse_coo_tensor_with_dims_and_tensors(int sparse_dim, int dense_dim, SymInt[] size, Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims_and_tensors(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims_and_tensors(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);


// aten::_sparse_coo_tensor_with_dims_and_tensors(int sparse_dim, int dense_dim, SymInt[] size, Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims_and_tensors(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims_and_tensors(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::_sparse_coo_tensor_with_dims_and_tensors(int sparse_dim, int dense_dim, SymInt[] size, Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims_and_tensors_symint(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal SymIntRef size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);


// aten::_sparse_coo_tensor_with_dims_and_tensors(int sparse_dim, int dense_dim, SymInt[] size, Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims_and_tensors_symint(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal SymIntRef size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::_sparse_coo_tensor_with_dims_and_tensors.out(int sparse_dim, int dense_dim, SymInt[] size, Tensor indices, Tensor values, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_coo_tensor_with_dims_and_tensors_out(@ByRef Tensor out, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values);
@Namespace("at") public static native @ByRef Tensor _sparse_coo_tensor_with_dims_and_tensors_out(@ByRef Tensor out, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values);


// aten::_sparse_coo_tensor_with_dims_and_tensors.out(int sparse_dim, int dense_dim, SymInt[] size, Tensor indices, Tensor values, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_coo_tensor_with_dims_and_tensors_outf(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _sparse_coo_tensor_with_dims_and_tensors_outf(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByRef Tensor out);


// aten::_sparse_coo_tensor_with_dims_and_tensors.out(int sparse_dim, int dense_dim, SymInt[] size, Tensor indices, Tensor values, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_coo_tensor_with_dims_and_tensors_symint_out(@ByRef Tensor out, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal SymIntRef size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values);


// aten::_sparse_coo_tensor_with_dims_and_tensors.out(int sparse_dim, int dense_dim, SymInt[] size, Tensor indices, Tensor values, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_coo_tensor_with_dims_and_tensors_symint_outf(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal SymIntRef size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByRef Tensor out);





// Parsed from ATen/ops/_sparse_csc_tensor_unsafe.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sparse_csc_tensor_unsafe_ops.h>


// aten::_sparse_csc_tensor_unsafe(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_csc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_csc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor _sparse_csc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_csc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
// aten::_sparse_csc_tensor_unsafe(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_csc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor _sparse_csc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);




// Parsed from ATen/ops/_sparse_csr_prod.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sparse_csr_prod_ops.h>


// aten::_sparse_csr_prod.dim_dtype(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_csr_prod(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor _sparse_csr_prod(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor _sparse_csr_prod(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor _sparse_csr_prod(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::_sparse_csr_prod.dim_dtype_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_csr_prod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor _sparse_csr_prod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor _sparse_csr_prod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor _sparse_csr_prod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);
// aten::_sparse_csr_prod.dim_dtype_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_csr_prod_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _sparse_csr_prod_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/_sparse_csr_sum.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sparse_csr_sum_ops.h>


// aten::_sparse_csr_sum.dim_dtype(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_csr_sum(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor _sparse_csr_sum(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor _sparse_csr_sum(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor _sparse_csr_sum(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::_sparse_csr_sum.dim_dtype_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_csr_sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor _sparse_csr_sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor _sparse_csr_sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor _sparse_csr_sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);
// aten::_sparse_csr_sum.dim_dtype_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_csr_sum_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _sparse_csr_sum_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/_sparse_csr_tensor_unsafe.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sparse_csr_tensor_unsafe_ops.h>


// aten::_sparse_csr_tensor_unsafe(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_csr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_csr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor _sparse_csr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_csr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
// aten::_sparse_csr_tensor_unsafe(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_csr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor _sparse_csr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);




// Parsed from ATen/ops/_sparse_log_softmax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sparse_log_softmax_ops.h>


// aten::_sparse_log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor _sparse_log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::_sparse_log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_log_softmax(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor _sparse_log_softmax(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::_sparse_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float);

// aten::_sparse_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_log_softmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float);
// aten::_sparse_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_log_softmax_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float, @ByRef Tensor out);




// Parsed from ATen/ops/_sparse_log_softmax_backward_data.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sparse_log_softmax_backward_data_ops.h>


// aten::_sparse_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_log_softmax_backward_data(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, @Const @ByRef Tensor self);

// aten::_sparse_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_log_softmax_backward_data_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, @Const @ByRef Tensor self);
// aten::_sparse_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_log_softmax_backward_data_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, @Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/_sparse_mm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sparse_mm_ops.h>


// aten::_sparse_mm(Tensor sparse, Tensor dense) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_mm(@Const @ByRef Tensor sparse, @Const @ByRef Tensor dense);

// aten::_sparse_mm.reduce(Tensor sparse, Tensor dense, str reduce) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_mm(@Const @ByRef Tensor sparse, @Const @ByRef Tensor dense, @ByVal @Cast("c10::string_view*") Pointer reduce);




// Parsed from ATen/ops/_sparse_mm_reduce_impl.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sparse_mm_reduce_impl_ops.h>


// aten::_sparse_mm_reduce_impl(Tensor self, Tensor other, str reduce) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _sparse_mm_reduce_impl(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("c10::string_view*") Pointer reduce);




// Parsed from ATen/ops/_sparse_mm_reduce_impl_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sparse_mm_reduce_impl_backward_ops.h>


// aten::_sparse_mm_reduce_impl_backward(Tensor self, Tensor grad_out, Tensor weight, str reduce, Tensor arg_out, bool[2] output_mask) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _sparse_mm_reduce_impl_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_out, @Const @ByRef Tensor weight, @ByVal @Cast("c10::string_view*") Pointer reduce, @Const @ByRef Tensor arg_out, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);




// Parsed from ATen/ops/_sparse_softmax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sparse_softmax_ops.h>


// aten::_sparse_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor _sparse_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::_sparse_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_softmax(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor _sparse_softmax(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::_sparse_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float);

// aten::_sparse_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_softmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float);
// aten::_sparse_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_softmax_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float, @ByRef Tensor out);




// Parsed from ATen/ops/_sparse_softmax_backward_data.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sparse_softmax_backward_data_ops.h>


// aten::_sparse_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_softmax_backward_data(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, @Const @ByRef Tensor self);

// aten::_sparse_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_softmax_backward_data_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, @Const @ByRef Tensor self);
// aten::_sparse_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_softmax_backward_data_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, @Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/_sparse_sparse_matmul.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sparse_sparse_matmul_ops.h>


// aten::_sparse_sparse_matmul(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_sparse_matmul(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::_sparse_sparse_matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_sparse_matmul_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::_sparse_sparse_matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_sparse_matmul_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/_sparse_sum.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sparse_sum_ops.h>


// aten::_sparse_sum(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_sum(@Const @ByRef Tensor self);

// aten::_sparse_sum.dtype(Tensor self, *, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_sum(@Const @ByRef Tensor self, ScalarType dtype);

// aten::_sparse_sum.dim(Tensor self, int[1] dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_sum(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor _sparse_sum(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::_sparse_sum.dim_dtype(Tensor self, int[1] dim, *, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_sum(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, ScalarType dtype);
@Namespace("at") public static native @ByVal Tensor _sparse_sum(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, ScalarType dtype);

// aten::_sparse_sum.dim_out(Tensor self, int[1] dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor _sparse_sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);
// aten::_sparse_sum.dim_out(Tensor self, int[1] dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_sum_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _sparse_sum_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByRef Tensor out);




// Parsed from ATen/ops/_sparse_sum_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_sparse_sum_backward_ops.h>


// aten::_sparse_sum_backward(Tensor grad, Tensor self, int[] dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor _sparse_sum_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor _sparse_sum_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::_sparse_sum_backward.out(Tensor grad, Tensor self, int[] dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_sum_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor _sparse_sum_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);
// aten::_sparse_sum_backward.out(Tensor grad, Tensor self, int[] dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _sparse_sum_backward_outf(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _sparse_sum_backward_outf(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByRef Tensor out);




// Parsed from ATen/ops/_spdiags.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_spdiags_ops.h>


// aten::_spdiags(Tensor diagonals, Tensor offsets, int[] shape, Layout? layout=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _spdiags(@Const @ByRef Tensor diagonals, @Const @ByRef Tensor offsets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape, @ByVal(nullValue = "c10::optional<at::Layout>(c10::nullopt)") LayoutOptional layout);
@Namespace("at") public static native @ByVal Tensor _spdiags(@Const @ByRef Tensor diagonals, @Const @ByRef Tensor offsets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape);
@Namespace("at") public static native @ByVal Tensor _spdiags(@Const @ByRef Tensor diagonals, @Const @ByRef Tensor offsets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] shape, @ByVal(nullValue = "c10::optional<at::Layout>(c10::nullopt)") LayoutOptional layout);
@Namespace("at") public static native @ByVal Tensor _spdiags(@Const @ByRef Tensor diagonals, @Const @ByRef Tensor offsets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... shape);

// aten::_spdiags.out(Tensor diagonals, Tensor offsets, int[] shape, Layout? layout=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _spdiags_out(@ByRef Tensor out, @Const @ByRef Tensor diagonals, @Const @ByRef Tensor offsets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape, @ByVal(nullValue = "c10::optional<at::Layout>(c10::nullopt)") LayoutOptional layout);
@Namespace("at") public static native @ByRef Tensor _spdiags_out(@ByRef Tensor out, @Const @ByRef Tensor diagonals, @Const @ByRef Tensor offsets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape);
@Namespace("at") public static native @ByRef Tensor _spdiags_out(@ByRef Tensor out, @Const @ByRef Tensor diagonals, @Const @ByRef Tensor offsets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] shape, @ByVal(nullValue = "c10::optional<at::Layout>(c10::nullopt)") LayoutOptional layout);
@Namespace("at") public static native @ByRef Tensor _spdiags_out(@ByRef Tensor out, @Const @ByRef Tensor diagonals, @Const @ByRef Tensor offsets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... shape);
// aten::_spdiags.out(Tensor diagonals, Tensor offsets, int[] shape, Layout? layout=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _spdiags_outf(@Const @ByRef Tensor diagonals, @Const @ByRef Tensor offsets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape, @ByVal LayoutOptional layout, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _spdiags_outf(@Const @ByRef Tensor diagonals, @Const @ByRef Tensor offsets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] shape, @ByVal LayoutOptional layout, @ByRef Tensor out);




// Parsed from ATen/ops/_stack.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_stack_ops.h>


// aten::_stack(Tensor[] tensors, int dim=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor _stack(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor _stack(@ByVal TensorArrayRef tensors);

// aten::_stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _stack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByRef Tensor _stack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
// aten::_stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _stack_outf(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim, @ByRef Tensor out);




// Parsed from ATen/ops/_standard_gamma.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_standard_gamma_ops.h>


// aten::_standard_gamma(Tensor self, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _standard_gamma(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor _standard_gamma(@Const @ByRef Tensor self);

// aten::_standard_gamma.out(Tensor self, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _standard_gamma_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor _standard_gamma_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::_standard_gamma.out(Tensor self, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _standard_gamma_outf(@Const @ByRef Tensor self, @ByVal GeneratorOptional generator, @ByRef Tensor out);




// Parsed from ATen/ops/_standard_gamma_grad.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_standard_gamma_grad_ops.h>


// aten::_standard_gamma_grad(Tensor self, Tensor output) -> Tensor
@Namespace("at") public static native @ByVal Tensor _standard_gamma_grad(@Const @ByRef Tensor self, @Const @ByRef Tensor output);

// aten::_standard_gamma_grad.out(Tensor self, Tensor output, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _standard_gamma_grad_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor output);
// aten::_standard_gamma_grad.out(Tensor self, Tensor output, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _standard_gamma_grad_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor output, @ByRef Tensor out);




// Parsed from ATen/ops/_test_ambiguous_defaults.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_test_ambiguous_defaults_ops.h>


// aten::_test_ambiguous_defaults.a(Tensor dummy, int a=1, int b=1) -> Tensor


// aten::_test_ambiguous_defaults.b(Tensor dummy, int a=2, str b="2") -> Tensor





// Parsed from ATen/ops/_test_autograd_multiple_dispatch.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_test_autograd_multiple_dispatch_ops.h>


// aten::_test_autograd_multiple_dispatch.fullcoverage(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _test_autograd_multiple_dispatch(@Const @ByRef Tensor self);

// aten::_test_autograd_multiple_dispatch.ntonly(Tensor self, bool b) -> Tensor
@Namespace("at") public static native @ByVal Tensor _test_autograd_multiple_dispatch(@Const @ByRef Tensor self, @Cast("bool") boolean b);

// aten::_test_autograd_multiple_dispatch.fullcoverage_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _test_autograd_multiple_dispatch_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::_test_autograd_multiple_dispatch.fullcoverage_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _test_autograd_multiple_dispatch_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/_test_autograd_multiple_dispatch_view.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_test_autograd_multiple_dispatch_view_ops.h>


// aten::_test_autograd_multiple_dispatch_view(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor _test_autograd_multiple_dispatch_view(@Const @ByRef Tensor self);




// Parsed from ATen/ops/_test_autograd_multiple_dispatch_view_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_test_autograd_multiple_dispatch_view_copy_ops.h>


// aten::_test_autograd_multiple_dispatch_view_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _test_autograd_multiple_dispatch_view_copy(@Const @ByRef Tensor self);

// aten::_test_autograd_multiple_dispatch_view_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _test_autograd_multiple_dispatch_view_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::_test_autograd_multiple_dispatch_view_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _test_autograd_multiple_dispatch_view_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/_test_check_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_test_check_tensor_ops.h>


// aten::_test_check_tensor(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _test_check_tensor(@Const @ByRef Tensor self);




// Parsed from ATen/ops/_test_optional_filled_intlist.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_test_optional_filled_intlist_ops.h>


// aten::_test_optional_filled_intlist(Tensor values, int[2]? addends) -> Tensor


// aten::_test_optional_filled_intlist.out(Tensor values, int[2]? addends, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _test_optional_filled_intlist_out(@ByRef Tensor out, @Const @ByRef Tensor values, @ByVal LongArrayRefOptional addends);
@Namespace("at") public static native @ByRef Tensor _test_optional_filled_intlist_out(@ByRef Tensor out, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... addends);
// aten::_test_optional_filled_intlist.out(Tensor values, int[2]? addends, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _test_optional_filled_intlist_outf(@Const @ByRef Tensor values, @ByVal LongArrayRefOptional addends, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _test_optional_filled_intlist_outf(@Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] addends, @ByRef Tensor out);




// Parsed from ATen/ops/_test_optional_floatlist.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_test_optional_floatlist_ops.h>


// aten::_test_optional_floatlist(Tensor values, float[]? addends) -> Tensor


// aten::_test_optional_floatlist.out(Tensor values, float[]? addends, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _test_optional_floatlist_out(@ByRef Tensor out, @Const @ByRef Tensor values, @ByVal DoubleArrayRefOptional addends);
// aten::_test_optional_floatlist.out(Tensor values, float[]? addends, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _test_optional_floatlist_outf(@Const @ByRef Tensor values, @ByVal DoubleArrayRefOptional addends, @ByRef Tensor out);




// Parsed from ATen/ops/_test_optional_intlist.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_test_optional_intlist_ops.h>


// aten::_test_optional_intlist(Tensor values, int[]? addends) -> Tensor


// aten::_test_optional_intlist.out(Tensor values, int[]? addends, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _test_optional_intlist_out(@ByRef Tensor out, @Const @ByRef Tensor values, @ByVal LongArrayRefOptional addends);
@Namespace("at") public static native @ByRef Tensor _test_optional_intlist_out(@ByRef Tensor out, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... addends);
// aten::_test_optional_intlist.out(Tensor values, int[]? addends, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _test_optional_intlist_outf(@Const @ByRef Tensor values, @ByVal LongArrayRefOptional addends, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _test_optional_intlist_outf(@Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] addends, @ByRef Tensor out);




// Parsed from ATen/ops/_test_serialization_subcmul.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_test_serialization_subcmul_ops.h>


// aten::_test_serialization_subcmul(Tensor self, Tensor other, Scalar alpha=1) -> Tensor





// Parsed from ATen/ops/_test_string_default.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_test_string_default_ops.h>


// aten::_test_string_default(Tensor dummy, str a="\"'\\", str b='"\'\\') -> Tensor





// Parsed from ATen/ops/_test_warn_in_autograd.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_test_warn_in_autograd_ops.h>


// aten::_test_warn_in_autograd(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _test_warn_in_autograd(@Const @ByRef Tensor self);

// aten::_test_warn_in_autograd.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _test_warn_in_autograd_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::_test_warn_in_autograd.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _test_warn_in_autograd_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/_thnn_differentiable_gru_cell_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_thnn_differentiable_gru_cell_backward_ops.h>


// aten::_thnn_differentiable_gru_cell_backward(Tensor grad_hy, Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias, Tensor? hidden_bias) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTensorTuple _thnn_differentiable_gru_cell_backward(@Const @ByRef Tensor grad_hy, @Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional input_bias, @Const @ByRef TensorOptional hidden_bias);




// Parsed from ATen/ops/_thnn_differentiable_lstm_cell_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_thnn_differentiable_lstm_cell_backward_ops.h>


// aten::_thnn_differentiable_lstm_cell_backward(Tensor? grad_hy, Tensor? grad_cy, Tensor input_gates, Tensor hidden_gates, Tensor? input_bias, Tensor? hidden_bias, Tensor cx, Tensor cy) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTensorTuple _thnn_differentiable_lstm_cell_backward(@Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef TensorOptional input_bias, @Const @ByRef TensorOptional hidden_bias, @Const @ByRef Tensor cx, @Const @ByRef Tensor cy);




// Parsed from ATen/ops/_thnn_fused_gru_cell.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_thnn_fused_gru_cell_ops.h>


// aten::_thnn_fused_gru_cell(Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _thnn_fused_gru_cell(@Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef Tensor hx, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional input_bias, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional hidden_bias);
@Namespace("at") public static native @ByVal TensorTensorTuple _thnn_fused_gru_cell(@Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef Tensor hx);

// aten::_thnn_fused_gru_cell.out(Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias=None, Tensor? hidden_bias=None, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _thnn_fused_gru_cell_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef Tensor hx, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional input_bias, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional hidden_bias);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _thnn_fused_gru_cell_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef Tensor hx);
// aten::_thnn_fused_gru_cell.out(Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias=None, Tensor? hidden_bias=None, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _thnn_fused_gru_cell_outf(@Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional input_bias, @Const @ByRef TensorOptional hidden_bias, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/_thnn_fused_gru_cell_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_thnn_fused_gru_cell_backward_ops.h>


// aten::_thnn_fused_gru_cell_backward(Tensor grad_hy, Tensor workspace, bool has_bias) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTensorTuple _thnn_fused_gru_cell_backward(@Const @ByRef Tensor grad_hy, @Const @ByRef Tensor workspace, @Cast("bool") boolean has_bias);

// aten::_thnn_fused_gru_cell_backward.out(Tensor grad_hy, Tensor workspace, bool has_bias, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _thnn_fused_gru_cell_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4, @Const @ByRef Tensor grad_hy, @Const @ByRef Tensor workspace, @Cast("bool") boolean has_bias);
// aten::_thnn_fused_gru_cell_backward.out(Tensor grad_hy, Tensor workspace, bool has_bias, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _thnn_fused_gru_cell_backward_outf(@Const @ByRef Tensor grad_hy, @Const @ByRef Tensor workspace, @Cast("bool") boolean has_bias, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4);




// Parsed from ATen/ops/_thnn_fused_lstm_cell.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_thnn_fused_lstm_cell_ops.h>


// aten::_thnn_fused_lstm_cell(Tensor input_gates, Tensor hidden_gates, Tensor cx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _thnn_fused_lstm_cell(@Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef Tensor cx, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional input_bias, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional hidden_bias);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _thnn_fused_lstm_cell(@Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef Tensor cx);

// aten::_thnn_fused_lstm_cell.out(Tensor input_gates, Tensor hidden_gates, Tensor cx, Tensor? input_bias=None, Tensor? hidden_bias=None, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _thnn_fused_lstm_cell_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef Tensor cx, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional input_bias, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional hidden_bias);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _thnn_fused_lstm_cell_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef Tensor cx);
// aten::_thnn_fused_lstm_cell.out(Tensor input_gates, Tensor hidden_gates, Tensor cx, Tensor? input_bias=None, Tensor? hidden_bias=None, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _thnn_fused_lstm_cell_outf(@Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef Tensor cx, @Const @ByRef TensorOptional input_bias, @Const @ByRef TensorOptional hidden_bias, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/_thnn_fused_lstm_cell_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_thnn_fused_lstm_cell_backward_ops.h>


// aten::_thnn_fused_lstm_cell_backward(Tensor? grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, Tensor workspace, bool has_bias) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTensorTuple _thnn_fused_lstm_cell_backward(@Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Const @ByRef Tensor cx, @Const @ByRef Tensor cy, @Const @ByRef Tensor workspace, @Cast("bool") boolean has_bias);




// Parsed from ATen/ops/_thnn_fused_lstm_cell_backward_impl.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_thnn_fused_lstm_cell_backward_impl_ops.h>


// aten::_thnn_fused_lstm_cell_backward_impl(Tensor? grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, Tensor workspace, bool has_bias) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _thnn_fused_lstm_cell_backward_impl(@Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Const @ByRef Tensor cx, @Const @ByRef Tensor cy, @Const @ByRef Tensor workspace, @Cast("bool") boolean has_bias);

// aten::_thnn_fused_lstm_cell_backward_impl.out(Tensor? grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, Tensor workspace, bool has_bias, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _thnn_fused_lstm_cell_backward_impl_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Const @ByRef Tensor cx, @Const @ByRef Tensor cy, @Const @ByRef Tensor workspace, @Cast("bool") boolean has_bias);
// aten::_thnn_fused_lstm_cell_backward_impl.out(Tensor? grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, Tensor workspace, bool has_bias, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _thnn_fused_lstm_cell_backward_impl_outf(@Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Const @ByRef Tensor cx, @Const @ByRef Tensor cy, @Const @ByRef Tensor workspace, @Cast("bool") boolean has_bias, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/_to_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_to_copy_ops.h>


// aten::_to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _to_copy(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @Cast("bool") boolean non_blocking/*=false*/, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor _to_copy(@Const @ByRef Tensor self);
// aten::_to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _to_copy(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @Cast("bool") boolean non_blocking, @ByVal MemoryFormatOptional memory_format);

// aten::_to_copy.out(Tensor self, *, bool non_blocking=False, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _to_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor _to_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::_to_copy.out(Tensor self, *, bool non_blocking=False, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _to_copy_outf(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);




// Parsed from ATen/ops/_to_cpu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_to_cpu_ops.h>


// aten::_to_cpu(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector _to_cpu(@ByVal TensorArrayRef tensors);




// Parsed from ATen/ops/_to_dense.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_to_dense_ops.h>


// aten::_to_dense.out(Tensor self, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _to_dense_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor _to_dense_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::_to_dense.out(Tensor self, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _to_dense_outf(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/_transform_bias_rescale_qkv.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_transform_bias_rescale_qkv_ops.h>


// aten::_transform_bias_rescale_qkv(Tensor qkv, Tensor qkv_bias, int num_heads) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _transform_bias_rescale_qkv(@Const @ByRef Tensor qkv, @Const @ByRef Tensor qkv_bias, @Cast("int64_t") long num_heads);

// aten::_transform_bias_rescale_qkv.out(Tensor qkv, Tensor qkv_bias, int num_heads, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _transform_bias_rescale_qkv_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor qkv, @Const @ByRef Tensor qkv_bias, @Cast("int64_t") long num_heads);
// aten::_transform_bias_rescale_qkv.out(Tensor qkv, Tensor qkv_bias, int num_heads, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _transform_bias_rescale_qkv_outf(@Const @ByRef Tensor qkv, @Const @ByRef Tensor qkv_bias, @Cast("int64_t") long num_heads, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/_transformer_decoder_only_layer_fwd.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_transformer_decoder_only_layer_fwd_ops.h>


// aten::_transformer_decoder_only_layer_fwd(Tensor src, int embed_dim, int num_heads, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, bool use_gelu, bool norm_first, float eps, Tensor norm_weight_1, Tensor norm_bias_1, Tensor norm_weight_2, Tensor norm_bias_2, Tensor ffn_weight_1, Tensor ffn_bias_1, Tensor ffn_weight_2, Tensor ffn_bias_2, Tensor? mask=None, Tensor? incr_key=None, Tensor? incr_value=None) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _transformer_decoder_only_layer_fwd(@Const @ByRef Tensor src, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_heads, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias, @Cast("bool") boolean use_gelu, @Cast("bool") boolean norm_first, double eps, @Const @ByRef Tensor norm_weight_1, @Const @ByRef Tensor norm_bias_1, @Const @ByRef Tensor norm_weight_2, @Const @ByRef Tensor norm_bias_2, @Const @ByRef Tensor ffn_weight_1, @Const @ByRef Tensor ffn_bias_1, @Const @ByRef Tensor ffn_weight_2, @Const @ByRef Tensor ffn_bias_2, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional mask, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional incr_key, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional incr_value);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _transformer_decoder_only_layer_fwd(@Const @ByRef Tensor src, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_heads, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias, @Cast("bool") boolean use_gelu, @Cast("bool") boolean norm_first, double eps, @Const @ByRef Tensor norm_weight_1, @Const @ByRef Tensor norm_bias_1, @Const @ByRef Tensor norm_weight_2, @Const @ByRef Tensor norm_bias_2, @Const @ByRef Tensor ffn_weight_1, @Const @ByRef Tensor ffn_bias_1, @Const @ByRef Tensor ffn_weight_2, @Const @ByRef Tensor ffn_bias_2);

// aten::_transformer_decoder_only_layer_fwd.out(Tensor src, int embed_dim, int num_heads, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, bool use_gelu, bool norm_first, float eps, Tensor norm_weight_1, Tensor norm_bias_1, Tensor norm_weight_2, Tensor norm_bias_2, Tensor ffn_weight_1, Tensor ffn_bias_1, Tensor ffn_weight_2, Tensor ffn_bias_2, Tensor? mask=None, Tensor? incr_key=None, Tensor? incr_value=None, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _transformer_decoder_only_layer_fwd_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor src, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_heads, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias, @Cast("bool") boolean use_gelu, @Cast("bool") boolean norm_first, double eps, @Const @ByRef Tensor norm_weight_1, @Const @ByRef Tensor norm_bias_1, @Const @ByRef Tensor norm_weight_2, @Const @ByRef Tensor norm_bias_2, @Const @ByRef Tensor ffn_weight_1, @Const @ByRef Tensor ffn_bias_1, @Const @ByRef Tensor ffn_weight_2, @Const @ByRef Tensor ffn_bias_2, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional mask, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional incr_key, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional incr_value);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _transformer_decoder_only_layer_fwd_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor src, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_heads, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias, @Cast("bool") boolean use_gelu, @Cast("bool") boolean norm_first, double eps, @Const @ByRef Tensor norm_weight_1, @Const @ByRef Tensor norm_bias_1, @Const @ByRef Tensor norm_weight_2, @Const @ByRef Tensor norm_bias_2, @Const @ByRef Tensor ffn_weight_1, @Const @ByRef Tensor ffn_bias_1, @Const @ByRef Tensor ffn_weight_2, @Const @ByRef Tensor ffn_bias_2);
// aten::_transformer_decoder_only_layer_fwd.out(Tensor src, int embed_dim, int num_heads, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, bool use_gelu, bool norm_first, float eps, Tensor norm_weight_1, Tensor norm_bias_1, Tensor norm_weight_2, Tensor norm_bias_2, Tensor ffn_weight_1, Tensor ffn_bias_1, Tensor ffn_weight_2, Tensor ffn_bias_2, Tensor? mask=None, Tensor? incr_key=None, Tensor? incr_value=None, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _transformer_decoder_only_layer_fwd_outf(@Const @ByRef Tensor src, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_heads, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias, @Cast("bool") boolean use_gelu, @Cast("bool") boolean norm_first, double eps, @Const @ByRef Tensor norm_weight_1, @Const @ByRef Tensor norm_bias_1, @Const @ByRef Tensor norm_weight_2, @Const @ByRef Tensor norm_bias_2, @Const @ByRef Tensor ffn_weight_1, @Const @ByRef Tensor ffn_bias_1, @Const @ByRef Tensor ffn_weight_2, @Const @ByRef Tensor ffn_bias_2, @Const @ByRef TensorOptional mask, @Const @ByRef TensorOptional incr_key, @Const @ByRef TensorOptional incr_value, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/_transformer_encoder_layer_fwd.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_transformer_encoder_layer_fwd_ops.h>


// aten::_transformer_encoder_layer_fwd(Tensor src, int embed_dim, int num_heads, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, bool use_gelu, bool norm_first, float eps, Tensor norm_weight_1, Tensor norm_bias_1, Tensor norm_weight_2, Tensor norm_bias_2, Tensor ffn_weight_1, Tensor ffn_bias_1, Tensor ffn_weight_2, Tensor ffn_bias_2, Tensor? mask=None, int? mask_type=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _transformer_encoder_layer_fwd(@Const @ByRef Tensor src, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_heads, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias, @Cast("bool") boolean use_gelu, @Cast("bool") boolean norm_first, double eps, @Const @ByRef Tensor norm_weight_1, @Const @ByRef Tensor norm_bias_1, @Const @ByRef Tensor norm_weight_2, @Const @ByRef Tensor norm_bias_2, @Const @ByRef Tensor ffn_weight_1, @Const @ByRef Tensor ffn_bias_1, @Const @ByRef Tensor ffn_weight_2, @Const @ByRef Tensor ffn_bias_2, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional mask, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional mask_type);
@Namespace("at") public static native @ByVal Tensor _transformer_encoder_layer_fwd(@Const @ByRef Tensor src, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_heads, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias, @Cast("bool") boolean use_gelu, @Cast("bool") boolean norm_first, double eps, @Const @ByRef Tensor norm_weight_1, @Const @ByRef Tensor norm_bias_1, @Const @ByRef Tensor norm_weight_2, @Const @ByRef Tensor norm_bias_2, @Const @ByRef Tensor ffn_weight_1, @Const @ByRef Tensor ffn_bias_1, @Const @ByRef Tensor ffn_weight_2, @Const @ByRef Tensor ffn_bias_2);

// aten::_transformer_encoder_layer_fwd.out(Tensor src, int embed_dim, int num_heads, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, bool use_gelu, bool norm_first, float eps, Tensor norm_weight_1, Tensor norm_bias_1, Tensor norm_weight_2, Tensor norm_bias_2, Tensor ffn_weight_1, Tensor ffn_bias_1, Tensor ffn_weight_2, Tensor ffn_bias_2, Tensor? mask=None, int? mask_type=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _transformer_encoder_layer_fwd_out(@ByRef Tensor out, @Const @ByRef Tensor src, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_heads, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias, @Cast("bool") boolean use_gelu, @Cast("bool") boolean norm_first, double eps, @Const @ByRef Tensor norm_weight_1, @Const @ByRef Tensor norm_bias_1, @Const @ByRef Tensor norm_weight_2, @Const @ByRef Tensor norm_bias_2, @Const @ByRef Tensor ffn_weight_1, @Const @ByRef Tensor ffn_bias_1, @Const @ByRef Tensor ffn_weight_2, @Const @ByRef Tensor ffn_bias_2, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional mask, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional mask_type);
@Namespace("at") public static native @ByRef Tensor _transformer_encoder_layer_fwd_out(@ByRef Tensor out, @Const @ByRef Tensor src, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_heads, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias, @Cast("bool") boolean use_gelu, @Cast("bool") boolean norm_first, double eps, @Const @ByRef Tensor norm_weight_1, @Const @ByRef Tensor norm_bias_1, @Const @ByRef Tensor norm_weight_2, @Const @ByRef Tensor norm_bias_2, @Const @ByRef Tensor ffn_weight_1, @Const @ByRef Tensor ffn_bias_1, @Const @ByRef Tensor ffn_weight_2, @Const @ByRef Tensor ffn_bias_2);
// aten::_transformer_encoder_layer_fwd.out(Tensor src, int embed_dim, int num_heads, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, bool use_gelu, bool norm_first, float eps, Tensor norm_weight_1, Tensor norm_bias_1, Tensor norm_weight_2, Tensor norm_bias_2, Tensor ffn_weight_1, Tensor ffn_bias_1, Tensor ffn_weight_2, Tensor ffn_bias_2, Tensor? mask=None, int? mask_type=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _transformer_encoder_layer_fwd_outf(@Const @ByRef Tensor src, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_heads, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias, @Cast("bool") boolean use_gelu, @Cast("bool") boolean norm_first, double eps, @Const @ByRef Tensor norm_weight_1, @Const @ByRef Tensor norm_bias_1, @Const @ByRef Tensor norm_weight_2, @Const @ByRef Tensor norm_bias_2, @Const @ByRef Tensor ffn_weight_1, @Const @ByRef Tensor ffn_bias_1, @Const @ByRef Tensor ffn_weight_2, @Const @ByRef Tensor ffn_bias_2, @Const @ByRef TensorOptional mask, @ByVal LongOptional mask_type, @ByRef Tensor out);




// Parsed from ATen/ops/_trilinear.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_trilinear_ops.h>


// aten::_trilinear(Tensor i1, Tensor i2, Tensor i3, int[] expand1, int[] expand2, int[] expand3, int[] sumdim, int unroll_dim=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor _trilinear(@Const @ByRef Tensor i1, @Const @ByRef Tensor i2, @Const @ByRef Tensor i3, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand1, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand2, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand3, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sumdim, @Cast("int64_t") long unroll_dim/*=1*/);
@Namespace("at") public static native @ByVal Tensor _trilinear(@Const @ByRef Tensor i1, @Const @ByRef Tensor i2, @Const @ByRef Tensor i3, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand1, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand2, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand3, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sumdim);
@Namespace("at") public static native @ByVal Tensor _trilinear(@Const @ByRef Tensor i1, @Const @ByRef Tensor i2, @Const @ByRef Tensor i3, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] expand1, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] expand2, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] expand3, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sumdim, @Cast("int64_t") long unroll_dim/*=1*/);
@Namespace("at") public static native @ByVal Tensor _trilinear(@Const @ByRef Tensor i1, @Const @ByRef Tensor i2, @Const @ByRef Tensor i3, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] expand1, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] expand2, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] expand3, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sumdim);

// aten::_trilinear.out(Tensor i1, Tensor i2, Tensor i3, int[] expand1, int[] expand2, int[] expand3, int[] sumdim, int unroll_dim=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _trilinear_out(@ByRef Tensor out, @Const @ByRef Tensor i1, @Const @ByRef Tensor i2, @Const @ByRef Tensor i3, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand1, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand2, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand3, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sumdim, @Cast("int64_t") long unroll_dim/*=1*/);
@Namespace("at") public static native @ByRef Tensor _trilinear_out(@ByRef Tensor out, @Const @ByRef Tensor i1, @Const @ByRef Tensor i2, @Const @ByRef Tensor i3, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand1, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand2, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand3, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sumdim);
@Namespace("at") public static native @ByRef Tensor _trilinear_out(@ByRef Tensor out, @Const @ByRef Tensor i1, @Const @ByRef Tensor i2, @Const @ByRef Tensor i3, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] expand1, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] expand2, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] expand3, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sumdim, @Cast("int64_t") long unroll_dim/*=1*/);
@Namespace("at") public static native @ByRef Tensor _trilinear_out(@ByRef Tensor out, @Const @ByRef Tensor i1, @Const @ByRef Tensor i2, @Const @ByRef Tensor i3, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] expand1, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] expand2, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] expand3, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sumdim);
// aten::_trilinear.out(Tensor i1, Tensor i2, Tensor i3, int[] expand1, int[] expand2, int[] expand3, int[] sumdim, int unroll_dim=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _trilinear_outf(@Const @ByRef Tensor i1, @Const @ByRef Tensor i2, @Const @ByRef Tensor i3, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand1, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand2, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand3, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sumdim, @Cast("int64_t") long unroll_dim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _trilinear_outf(@Const @ByRef Tensor i1, @Const @ByRef Tensor i2, @Const @ByRef Tensor i3, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] expand1, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] expand2, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] expand3, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sumdim, @Cast("int64_t") long unroll_dim, @ByRef Tensor out);




// Parsed from ATen/ops/_triton_multi_head_attention.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_triton_multi_head_attention_ops.h>


// aten::_triton_multi_head_attention(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _triton_multi_head_attention(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_head, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional mask);
@Namespace("at") public static native @ByVal Tensor _triton_multi_head_attention(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_head, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias);

// aten::_triton_multi_head_attention.out(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _triton_multi_head_attention_out(@ByRef Tensor out, @Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_head, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional mask);
@Namespace("at") public static native @ByRef Tensor _triton_multi_head_attention_out(@ByRef Tensor out, @Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_head, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias);
// aten::_triton_multi_head_attention.out(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _triton_multi_head_attention_outf(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Cast("int64_t") long embed_dim, @Cast("int64_t") long num_head, @Const @ByRef Tensor qkv_weight, @Const @ByRef Tensor qkv_bias, @Const @ByRef Tensor proj_weight, @Const @ByRef Tensor proj_bias, @Const @ByRef TensorOptional mask, @ByRef Tensor out);




// Parsed from ATen/ops/_triton_scaled_dot_attention.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_triton_scaled_dot_attention_ops.h>


// aten::_triton_scaled_dot_attention(Tensor q, Tensor k, Tensor v, float dropout_p=0.0) -> Tensor
@Namespace("at") public static native @ByVal Tensor _triton_scaled_dot_attention(@Const @ByRef Tensor q, @Const @ByRef Tensor k, @Const @ByRef Tensor v, double dropout_p/*=0.0*/);
@Namespace("at") public static native @ByVal Tensor _triton_scaled_dot_attention(@Const @ByRef Tensor q, @Const @ByRef Tensor k, @Const @ByRef Tensor v);

// aten::_triton_scaled_dot_attention.out(Tensor q, Tensor k, Tensor v, float dropout_p=0.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _triton_scaled_dot_attention_out(@ByRef Tensor out, @Const @ByRef Tensor q, @Const @ByRef Tensor k, @Const @ByRef Tensor v, double dropout_p/*=0.0*/);
@Namespace("at") public static native @ByRef Tensor _triton_scaled_dot_attention_out(@ByRef Tensor out, @Const @ByRef Tensor q, @Const @ByRef Tensor k, @Const @ByRef Tensor v);
// aten::_triton_scaled_dot_attention.out(Tensor q, Tensor k, Tensor v, float dropout_p=0.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _triton_scaled_dot_attention_outf(@Const @ByRef Tensor q, @Const @ByRef Tensor k, @Const @ByRef Tensor v, double dropout_p, @ByRef Tensor out);




// Parsed from ATen/ops/_unique.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_unique_ops.h>


// aten::_unique(Tensor self, bool sorted=True, bool return_inverse=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _unique(@Const @ByRef Tensor self, @Cast("bool") boolean sorted/*=true*/, @Cast("bool") boolean return_inverse/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple _unique(@Const @ByRef Tensor self);

// aten::_unique.out(Tensor self, bool sorted=True, bool return_inverse=False, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _unique_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, @Cast("bool") boolean sorted/*=true*/, @Cast("bool") boolean return_inverse/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _unique_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self);
// aten::_unique.out(Tensor self, bool sorted=True, bool return_inverse=False, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _unique_outf(@Const @ByRef Tensor self, @Cast("bool") boolean sorted, @Cast("bool") boolean return_inverse, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/_unique2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_unique2_ops.h>


// aten::_unique2(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _unique2(@Const @ByRef Tensor self, @Cast("bool") boolean sorted/*=true*/, @Cast("bool") boolean return_inverse/*=false*/, @Cast("bool") boolean return_counts/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple _unique2(@Const @ByRef Tensor self);

// aten::_unique2.out(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _unique2_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor self, @Cast("bool") boolean sorted/*=true*/, @Cast("bool") boolean return_inverse/*=false*/, @Cast("bool") boolean return_counts/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _unique2_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor self);
// aten::_unique2.out(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _unique2_outf(@Const @ByRef Tensor self, @Cast("bool") boolean sorted, @Cast("bool") boolean return_inverse, @Cast("bool") boolean return_counts, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/_unpack_dual.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_unpack_dual_ops.h>


// aten::_unpack_dual(Tensor(a) dual, int level) -> (Tensor(a) primal, Tensor tangent)
@Namespace("at") public static native @ByVal TensorTensorTuple _unpack_dual(@Const @ByRef Tensor dual, @Cast("int64_t") long level);




// Parsed from ATen/ops/_unsafe_view.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_unsafe_view_ops.h>


// aten::_unsafe_view(Tensor self, SymInt[] size) -> Tensor
@Namespace("at") public static native @ByVal Tensor _unsafe_view(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor _unsafe_view(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::_unsafe_view(Tensor self, SymInt[] size) -> Tensor
@Namespace("at") public static native @ByVal Tensor _unsafe_view_symint(@Const @ByRef Tensor self, @ByVal SymIntRef size);


// aten::_unsafe_view.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _unsafe_view_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor _unsafe_view_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::_unsafe_view.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _unsafe_view_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _unsafe_view_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);


// aten::_unsafe_view.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _unsafe_view_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef size);


// aten::_unsafe_view.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _unsafe_view_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef size, @ByRef Tensor out);





// Parsed from ATen/ops/_upsample_bicubic2d_aa.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_upsample_bicubic2d_aa_ops.h>


// aten::_upsample_bicubic2d_aa.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_bicubic2d_aa(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor _upsample_bicubic2d_aa(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);


// aten::_upsample_bicubic2d_aa.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_bicubic2d_aa_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);


// aten::_upsample_bicubic2d_aa.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_bicubic2d_aa_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_bicubic2d_aa_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor _upsample_bicubic2d_aa_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_bicubic2d_aa_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);


// aten::_upsample_bicubic2d_aa.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_bicubic2d_aa_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _upsample_bicubic2d_aa_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::_upsample_bicubic2d_aa.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_bicubic2d_aa_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_bicubic2d_aa_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners);


// aten::_upsample_bicubic2d_aa.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_bicubic2d_aa_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::_upsample_bicubic2d_aa(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_bicubic2d_aa(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_bicubic2d_aa(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor _upsample_bicubic2d_aa(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_bicubic2d_aa(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);


// aten::_upsample_bicubic2d_aa(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_bicubic2d_aa_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_bicubic2d_aa_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners);





// Parsed from ATen/ops/_upsample_bicubic2d_aa_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_upsample_bicubic2d_aa_backward_ops.h>


// aten::_upsample_bicubic2d_aa_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_bicubic2d_aa_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_bicubic2d_aa_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor _upsample_bicubic2d_aa_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_bicubic2d_aa_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);


// aten::_upsample_bicubic2d_aa_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_bicubic2d_aa_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor _upsample_bicubic2d_aa_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::_upsample_bicubic2d_aa_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_bicubic2d_aa_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_bicubic2d_aa_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners);


// aten::_upsample_bicubic2d_aa_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_bicubic2d_aa_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::_upsample_bicubic2d_aa_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_bicubic2d_aa_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_bicubic2d_aa_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor _upsample_bicubic2d_aa_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_bicubic2d_aa_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);


// aten::_upsample_bicubic2d_aa_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_bicubic2d_aa_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_bicubic2d_aa_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners);





// Parsed from ATen/ops/_upsample_bilinear2d_aa.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_upsample_bilinear2d_aa_ops.h>


// aten::_upsample_bilinear2d_aa.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_bilinear2d_aa(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor _upsample_bilinear2d_aa(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);


// aten::_upsample_bilinear2d_aa.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_bilinear2d_aa_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);


// aten::_upsample_bilinear2d_aa.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_bilinear2d_aa_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_bilinear2d_aa_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor _upsample_bilinear2d_aa_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_bilinear2d_aa_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);


// aten::_upsample_bilinear2d_aa.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_bilinear2d_aa_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _upsample_bilinear2d_aa_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::_upsample_bilinear2d_aa.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_bilinear2d_aa_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_bilinear2d_aa_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners);


// aten::_upsample_bilinear2d_aa.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_bilinear2d_aa_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::_upsample_bilinear2d_aa(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_bilinear2d_aa(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_bilinear2d_aa(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor _upsample_bilinear2d_aa(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_bilinear2d_aa(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);


// aten::_upsample_bilinear2d_aa(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_bilinear2d_aa_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_bilinear2d_aa_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners);





// Parsed from ATen/ops/_upsample_bilinear2d_aa_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_upsample_bilinear2d_aa_backward_ops.h>


// aten::_upsample_bilinear2d_aa_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_bilinear2d_aa_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_bilinear2d_aa_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor _upsample_bilinear2d_aa_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_bilinear2d_aa_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);


// aten::_upsample_bilinear2d_aa_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_bilinear2d_aa_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor _upsample_bilinear2d_aa_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::_upsample_bilinear2d_aa_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_bilinear2d_aa_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_bilinear2d_aa_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners);


// aten::_upsample_bilinear2d_aa_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_bilinear2d_aa_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::_upsample_bilinear2d_aa_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_bilinear2d_aa_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_bilinear2d_aa_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor _upsample_bilinear2d_aa_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_bilinear2d_aa_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);


// aten::_upsample_bilinear2d_aa_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_bilinear2d_aa_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_bilinear2d_aa_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners);





// Parsed from ATen/ops/_upsample_nearest_exact1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_upsample_nearest_exact1d_ops.h>


// aten::_upsample_nearest_exact1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact1d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact1d(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal DoubleArrayRefOptional scale_factors);


// aten::_upsample_nearest_exact1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact1d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);


// aten::_upsample_nearest_exact1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);


// aten::_upsample_nearest_exact1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact1d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal DoubleOptional scales, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact1d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal DoubleOptional scales, @ByRef Tensor out);


// aten::_upsample_nearest_exact1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact1d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact1d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size);


// aten::_upsample_nearest_exact1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact1d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal DoubleOptional scales, @ByRef Tensor out);


// aten::_upsample_nearest_exact1d(Tensor self, SymInt[1] output_size, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);


// aten::_upsample_nearest_exact1d(Tensor self, SymInt[1] output_size, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact1d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact1d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size);





// Parsed from ATen/ops/_upsample_nearest_exact1d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_upsample_nearest_exact1d_backward_ops.h>


// aten::_upsample_nearest_exact1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... input_size);


// aten::_upsample_nearest_exact1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact1d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal DoubleOptional scales, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact1d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal DoubleOptional scales, @ByRef Tensor grad_input);


// aten::_upsample_nearest_exact1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact1d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact1d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size);


// aten::_upsample_nearest_exact1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact1d_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @ByVal DoubleOptional scales, @ByRef Tensor grad_input);


// aten::_upsample_nearest_exact1d_backward(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... input_size);


// aten::_upsample_nearest_exact1d_backward(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact1d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact1d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size);





// Parsed from ATen/ops/_upsample_nearest_exact2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_upsample_nearest_exact2d_ops.h>


// aten::_upsample_nearest_exact2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact2d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact2d(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal DoubleArrayRefOptional scale_factors);


// aten::_upsample_nearest_exact2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact2d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);


// aten::_upsample_nearest_exact2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);


// aten::_upsample_nearest_exact2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::_upsample_nearest_exact2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size);


// aten::_upsample_nearest_exact2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact2d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::_upsample_nearest_exact2d(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);


// aten::_upsample_nearest_exact2d(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact2d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact2d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size);





// Parsed from ATen/ops/_upsample_nearest_exact2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_upsample_nearest_exact2d_backward_ops.h>


// aten::_upsample_nearest_exact2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... input_size);


// aten::_upsample_nearest_exact2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::_upsample_nearest_exact2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact2d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact2d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size);


// aten::_upsample_nearest_exact2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact2d_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::_upsample_nearest_exact2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... input_size);


// aten::_upsample_nearest_exact2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact2d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact2d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size);





// Parsed from ATen/ops/_upsample_nearest_exact3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_upsample_nearest_exact3d_ops.h>


// aten::_upsample_nearest_exact3d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact3d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact3d(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal DoubleArrayRefOptional scale_factors);


// aten::_upsample_nearest_exact3d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact3d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);


// aten::_upsample_nearest_exact3d.out(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);


// aten::_upsample_nearest_exact3d.out(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::_upsample_nearest_exact3d.out(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size);


// aten::_upsample_nearest_exact3d.out(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact3d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::_upsample_nearest_exact3d(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);


// aten::_upsample_nearest_exact3d(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact3d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact3d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size);





// Parsed from ATen/ops/_upsample_nearest_exact3d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_upsample_nearest_exact3d_backward_ops.h>


// aten::_upsample_nearest_exact3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... input_size);


// aten::_upsample_nearest_exact3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact3d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact3d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::_upsample_nearest_exact3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact3d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact3d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size);


// aten::_upsample_nearest_exact3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _upsample_nearest_exact3d_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::_upsample_nearest_exact3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... input_size);


// aten::_upsample_nearest_exact3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact3d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor _upsample_nearest_exact3d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size);





// Parsed from ATen/ops/_use_cudnn_ctc_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_use_cudnn_ctc_loss_ops.h>


// aten::_use_cudnn_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank) -> bool
@Namespace("at") public static native @Cast("bool") boolean _use_cudnn_ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Cast("int64_t") long blank);
@Namespace("at") public static native @Cast("bool") boolean _use_cudnn_ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Cast("int64_t") long blank);

// aten::_use_cudnn_ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank) -> bool
@Namespace("at") public static native @Cast("bool") boolean _use_cudnn_ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @Const @ByRef Tensor input_lengths, @Const @ByRef Tensor target_lengths, @Cast("int64_t") long blank);




// Parsed from ATen/ops/_validate_compressed_sparse_indices.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_validate_compressed_sparse_indices_ops.h>


// aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()
@Namespace("at") public static native void _validate_compressed_sparse_indices(@Cast("bool") boolean is_crow, @Const @ByRef Tensor compressed_idx, @Const @ByRef Tensor plain_idx, @Cast("int64_t") long cdim, @Cast("int64_t") long dim, @Cast("int64_t") long nnz);




// Parsed from ATen/ops/_validate_sparse_bsc_tensor_args.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_validate_sparse_bsc_tensor_args_ops.h>


// aten::_validate_sparse_bsc_tensor_args(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size) -> ()
@Namespace("at") public static native void _validate_sparse_bsc_tensor_args(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native void _validate_sparse_bsc_tensor_args(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);




// Parsed from ATen/ops/_validate_sparse_bsr_tensor_args.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_validate_sparse_bsr_tensor_args_ops.h>


// aten::_validate_sparse_bsr_tensor_args(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size) -> ()
@Namespace("at") public static native void _validate_sparse_bsr_tensor_args(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native void _validate_sparse_bsr_tensor_args(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);




// Parsed from ATen/ops/_validate_sparse_compressed_tensor_args.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_validate_sparse_compressed_tensor_args_ops.h>


// aten::_validate_sparse_compressed_tensor_args(Tensor compressed_indices, Tensor plain_indices, Tensor values, int[] size, Layout layout) -> ()
@Namespace("at") public static native void _validate_sparse_compressed_tensor_args(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal Layout layout);
@Namespace("at") public static native void _validate_sparse_compressed_tensor_args(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal Layout layout);




// Parsed from ATen/ops/_validate_sparse_coo_tensor_args.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_validate_sparse_coo_tensor_args_ops.h>


// aten::_validate_sparse_coo_tensor_args(Tensor indices, Tensor values, int[] size) -> ()
@Namespace("at") public static native void _validate_sparse_coo_tensor_args(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native void _validate_sparse_coo_tensor_args(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);




// Parsed from ATen/ops/_validate_sparse_csc_tensor_args.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_validate_sparse_csc_tensor_args_ops.h>


// aten::_validate_sparse_csc_tensor_args(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size) -> ()
@Namespace("at") public static native void _validate_sparse_csc_tensor_args(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native void _validate_sparse_csc_tensor_args(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);




// Parsed from ATen/ops/_validate_sparse_csr_tensor_args.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_validate_sparse_csr_tensor_args_ops.h>


// aten::_validate_sparse_csr_tensor_args(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size) -> ()
@Namespace("at") public static native void _validate_sparse_csr_tensor_args(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native void _validate_sparse_csr_tensor_args(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);




// Parsed from ATen/ops/_values.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_values_ops.h>






// Parsed from ATen/ops/_values_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_values_copy_ops.h>


// aten::_values_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor _values_copy(@Const @ByRef Tensor self);

// aten::_values_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _values_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::_values_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor _values_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/_version.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_version_ops.h>






// Parsed from ATen/ops/_weight_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_weight_norm_ops.h>


// aten::_weight_norm(Tensor v, Tensor g, int dim=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor _weight_norm(@Const @ByRef Tensor v, @Const @ByRef Tensor g, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor _weight_norm(@Const @ByRef Tensor v, @Const @ByRef Tensor g);




// Parsed from ATen/ops/_weight_norm_differentiable_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_weight_norm_differentiable_backward_ops.h>


// aten::_weight_norm_differentiable_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _weight_norm_differentiable_backward(@Const @ByRef Tensor grad_w, @Const @ByRef Tensor saved_v, @Const @ByRef Tensor saved_g, @Const @ByRef Tensor saved_norms, @Cast("int64_t") long dim);




// Parsed from ATen/ops/_weight_norm_interface.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_weight_norm_interface_ops.h>


// aten::_weight_norm_interface(Tensor v, Tensor g, int dim=0) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _weight_norm_interface(@Const @ByRef Tensor v, @Const @ByRef Tensor g, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal TensorTensorTuple _weight_norm_interface(@Const @ByRef Tensor v, @Const @ByRef Tensor g);

// aten::_weight_norm_interface.out(Tensor v, Tensor g, int dim=0, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _weight_norm_interface_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor v, @Const @ByRef Tensor g, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _weight_norm_interface_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor v, @Const @ByRef Tensor g);
// aten::_weight_norm_interface.out(Tensor v, Tensor g, int dim=0, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _weight_norm_interface_outf(@Const @ByRef Tensor v, @Const @ByRef Tensor g, @Cast("int64_t") long dim, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/_weight_norm_interface_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/_weight_norm_interface_backward_ops.h>


// aten::_weight_norm_interface_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple _weight_norm_interface_backward(@Const @ByRef Tensor grad_w, @Const @ByRef Tensor saved_v, @Const @ByRef Tensor saved_g, @Const @ByRef Tensor saved_norms, @Cast("int64_t") long dim);

// aten::_weight_norm_interface_backward.out(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _weight_norm_interface_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor grad_w, @Const @ByRef Tensor saved_v, @Const @ByRef Tensor saved_g, @Const @ByRef Tensor saved_norms, @Cast("int64_t") long dim);
// aten::_weight_norm_interface_backward.out(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _weight_norm_interface_backward_outf(@Const @ByRef Tensor grad_w, @Const @ByRef Tensor saved_v, @Const @ByRef Tensor saved_g, @Const @ByRef Tensor saved_norms, @Cast("int64_t") long dim, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/abs.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/abs_ops.h>


// aten::abs(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor abs(@Const @ByRef Tensor self);

// aten::abs_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor abs_(@ByRef Tensor self);

// aten::abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor abs_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor abs_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/absolute.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/absolute_ops.h>


// aten::absolute(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor absolute(@Const @ByRef Tensor self);

// aten::absolute.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor absolute_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::absolute.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor absolute_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/acos.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/acos_ops.h>


// aten::acos(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor acos(@Const @ByRef Tensor self);

// aten::acos_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor acos_(@ByRef Tensor self);

// aten::acos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor acos_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::acos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor acos_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/acosh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/acosh_ops.h>


// aten::acosh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor acosh(@Const @ByRef Tensor self);

// aten::acosh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor acosh_(@ByRef Tensor self);

// aten::acosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor acosh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::acosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor acosh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/adaptive_avg_pool1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/adaptive_avg_pool1d_ops.h>


// aten::adaptive_avg_pool1d(Tensor self, int[1] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);




// Parsed from ATen/ops/adaptive_avg_pool2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/adaptive_avg_pool2d_ops.h>


// aten::adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);


// aten::adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByRef Tensor out);


// aten::adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size);


// aten::adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool2d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByRef Tensor out);


// aten::adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);


// aten::adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool2d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size);





// Parsed from ATen/ops/adaptive_avg_pool3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/adaptive_avg_pool3d_ops.h>


// aten::adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);


// aten::adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByRef Tensor out);


// aten::adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size);


// aten::adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByRef Tensor out);


// aten::adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);


// aten::adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool3d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size);





// Parsed from ATen/ops/adaptive_avg_pool3d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/adaptive_avg_pool3d_backward_ops.h>


// aten::adaptive_avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
// aten::adaptive_avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByRef Tensor grad_input);




// Parsed from ATen/ops/adaptive_max_pool1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/adaptive_max_pool1d_ops.h>


// aten::adaptive_max_pool1d(Tensor self, int[1] output_size) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple adaptive_max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal TensorTensorTuple adaptive_max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);




// Parsed from ATen/ops/adaptive_max_pool2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/adaptive_max_pool2d_ops.h>


// aten::adaptive_max_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool2d_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool2d_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);
// aten::adaptive_max_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByRef Tensor out, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByRef Tensor out, @ByRef Tensor indices);

// aten::adaptive_max_pool2d(Tensor self, int[2] output_size) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple adaptive_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal TensorTensorTuple adaptive_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);




// Parsed from ATen/ops/adaptive_max_pool2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/adaptive_max_pool2d_backward_ops.h>


// aten::adaptive_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_max_pool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices);
// aten::adaptive_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_max_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);

// aten::adaptive_max_pool2d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor
@Namespace("at") public static native @ByVal Tensor adaptive_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices);




// Parsed from ATen/ops/adaptive_max_pool3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/adaptive_max_pool3d_ops.h>


// aten::adaptive_max_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool3d_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool3d_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);
// aten::adaptive_max_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByRef Tensor out, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByRef Tensor out, @ByRef Tensor indices);

// aten::adaptive_max_pool3d(Tensor self, int[3] output_size) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple adaptive_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal TensorTensorTuple adaptive_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);




// Parsed from ATen/ops/adaptive_max_pool3d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/adaptive_max_pool3d_backward_ops.h>


// aten::adaptive_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_max_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices);
// aten::adaptive_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor adaptive_max_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);

// aten::adaptive_max_pool3d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor
@Namespace("at") public static native @ByVal Tensor adaptive_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices);




// Parsed from ATen/ops/add.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/add_ops.h>


// aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor add(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);

// aten::add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor add_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor add_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor add_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::add.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor add(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);

// aten::add.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor add_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor add_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::add.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor add_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef Scalar alpha, @ByRef Tensor out);




// Parsed from ATen/ops/addbmm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/addbmm_ops.h>


// aten::addbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addbmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor addbmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2);
// aten::addbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addbmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @Const @ByRef Scalar beta, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor addbmm(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor addbmm(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2);




// Parsed from ATen/ops/addcdiv.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/addcdiv_ops.h>


// aten::addcdiv.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addcdiv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar value);
@Namespace("at") public static native @ByRef Tensor addcdiv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2);
// aten::addcdiv.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addcdiv_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @Const @ByRef Scalar value, @ByRef Tensor out);

// aten::addcdiv(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor addcdiv(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar value);
@Namespace("at") public static native @ByVal Tensor addcdiv(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2);




// Parsed from ATen/ops/addcmul.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/addcmul_ops.h>


// aten::addcmul.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addcmul_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar value);
@Namespace("at") public static native @ByRef Tensor addcmul_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2);
// aten::addcmul.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addcmul_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @Const @ByRef Scalar value, @ByRef Tensor out);

// aten::addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor addcmul(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar value);
@Namespace("at") public static native @ByVal Tensor addcmul(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2);




// Parsed from ATen/ops/addmm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/addmm_ops.h>


// aten::addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor addmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);
// aten::addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef Scalar beta, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor addmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor addmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);




// Parsed from ATen/ops/addmv.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/addmv_ops.h>


// aten::addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor addmv(@Const @ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor addmv(@Const @ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec);

// aten::addmv_(Tensor(a!) self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addmv_(@ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor addmv_(@ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec);

// aten::addmv.out(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addmv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor addmv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec);
// aten::addmv.out(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addmv_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec, @Const @ByRef Scalar beta, @Const @ByRef Scalar alpha, @ByRef Tensor out);




// Parsed from ATen/ops/addr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/addr_ops.h>


// aten::addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor addr(@Const @ByRef Tensor self, @Const @ByRef Tensor vec1, @Const @ByRef Tensor vec2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor addr(@Const @ByRef Tensor self, @Const @ByRef Tensor vec1, @Const @ByRef Tensor vec2);

// aten::addr.out(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor vec1, @Const @ByRef Tensor vec2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor addr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor vec1, @Const @ByRef Tensor vec2);
// aten::addr.out(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor addr_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor vec1, @Const @ByRef Tensor vec2, @Const @ByRef Scalar beta, @Const @ByRef Scalar alpha, @ByRef Tensor out);




// Parsed from ATen/ops/adjoint.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/adjoint_ops.h>


// aten::adjoint(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor adjoint(@Const @ByRef Tensor self);




// Parsed from ATen/ops/affine_grid_generator.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/affine_grid_generator_ops.h>


// aten::affine_grid_generator(Tensor theta, int[] size, bool align_corners) -> Tensor
@Namespace("at") public static native @ByVal Tensor affine_grid_generator(@Const @ByRef Tensor theta, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor affine_grid_generator(@Const @ByRef Tensor theta, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Cast("bool") boolean align_corners);

// aten::affine_grid_generator.out(Tensor theta, int[] size, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor affine_grid_generator_out(@ByRef Tensor out, @Const @ByRef Tensor theta, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor affine_grid_generator_out(@ByRef Tensor out, @Const @ByRef Tensor theta, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Cast("bool") boolean align_corners);
// aten::affine_grid_generator.out(Tensor theta, int[] size, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor affine_grid_generator_outf(@Const @ByRef Tensor theta, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Cast("bool") boolean align_corners, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor affine_grid_generator_outf(@Const @ByRef Tensor theta, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Cast("bool") boolean align_corners, @ByRef Tensor out);




// Parsed from ATen/ops/affine_grid_generator_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/affine_grid_generator_backward_ops.h>


// aten::affine_grid_generator_backward(Tensor grad, int[] size, bool align_corners) -> Tensor
@Namespace("at") public static native @ByVal Tensor affine_grid_generator_backward(@Const @ByRef Tensor grad, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor affine_grid_generator_backward(@Const @ByRef Tensor grad, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Cast("bool") boolean align_corners);




// Parsed from ATen/ops/alias.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/alias_ops.h>


// aten::alias(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor alias(@Const @ByRef Tensor self);




// Parsed from ATen/ops/alias_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/alias_copy_ops.h>


// aten::alias_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor alias_copy(@Const @ByRef Tensor self);

// aten::alias_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor alias_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::alias_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor alias_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/align_as.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/align_as_ops.h>






// Parsed from ATen/ops/align_tensors.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/align_tensors_ops.h>


// aten::align_tensors(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector align_tensors(@ByVal TensorArrayRef tensors);




// Parsed from ATen/ops/align_to.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/align_to_ops.h>






// Parsed from ATen/ops/all.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/all_ops.h>


// aten::all.dim(Tensor self, int dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor all(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor all(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::all.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor all_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor all_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::all.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor all_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::all.dimname(Tensor self, Dimname dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor all(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor all(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::all.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor all_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor all_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::all.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor all_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::all(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor all(@Const @ByRef Tensor self);

// aten::all.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor all_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::all.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor all_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/allclose.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/allclose_ops.h>


// aten::allclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> bool
@Namespace("at") public static native @Cast("bool") boolean allclose(@Const @ByRef Tensor self, @Const @ByRef Tensor other, double rtol/*=1e-05*/, double atol/*=1e-08*/, @Cast("bool") boolean equal_nan/*=false*/);
@Namespace("at") public static native @Cast("bool") boolean allclose(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/alpha_dropout.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/alpha_dropout_ops.h>


// aten::alpha_dropout(Tensor input, float p, bool train) -> Tensor
@Namespace("at") public static native @ByVal Tensor alpha_dropout(@Const @ByRef Tensor input, double p, @Cast("bool") boolean train);

// aten::alpha_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor alpha_dropout_(@ByRef Tensor self, double p, @Cast("bool") boolean train);




// Parsed from ATen/ops/amax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/amax_ops.h>


// aten::amax(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor amax(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor amax(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor amax(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);

// aten::amax.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor amax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor amax_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor amax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
// aten::amax.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor amax_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor amax_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);




// Parsed from ATen/ops/amin.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/amin_ops.h>


// aten::amin(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor amin(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor amin(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor amin(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);

// aten::amin.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor amin_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor amin_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor amin_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
// aten::amin.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor amin_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor amin_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);




// Parsed from ATen/ops/aminmax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/aminmax_ops.h>


// aten::aminmax(Tensor self, *, int? dim=None, bool keepdim=False) -> (Tensor min, Tensor max)
@Namespace("at") public static native @ByVal TensorTensorTuple aminmax(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple aminmax(@Const @ByRef Tensor self);

// aten::aminmax.out(Tensor self, *, int? dim=None, bool keepdim=False, Tensor(a!) min, Tensor(b!) max) -> (Tensor(a!) min, Tensor(b!) max)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> aminmax_out(@ByRef Tensor min, @ByRef Tensor max, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> aminmax_out(@ByRef Tensor min, @ByRef Tensor max, @Const @ByRef Tensor self);
// aten::aminmax.out(Tensor self, *, int? dim=None, bool keepdim=False, Tensor(a!) min, Tensor(b!) max) -> (Tensor(a!) min, Tensor(b!) max)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> aminmax_outf(@Const @ByRef Tensor self, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByRef Tensor min, @ByRef Tensor max);




// Parsed from ATen/ops/and.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/and_ops.h>


// aten::__and__.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __and__(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::__and__.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __and__(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/angle.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/angle_ops.h>


// aten::angle(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor angle(@Const @ByRef Tensor self);

// aten::angle.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor angle_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::angle.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor angle_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/any.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/any_ops.h>


// aten::any.dim(Tensor self, int dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor any(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor any(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::any.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor any_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor any_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::any.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor any_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::any.dimname(Tensor self, Dimname dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor any(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor any(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::any.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor any_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor any_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::any.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor any_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::any(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor any(@Const @ByRef Tensor self);

// aten::any.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor any_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::any.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor any_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/arange.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/arange_ops.h>


// aten::arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar end, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar end);
// aten::arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar end, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end);
// aten::arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::arange.start_step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step);
// aten::arange.start_step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::arange.out(Scalar end, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arange_out(@ByRef Tensor out, @Const @ByRef Scalar end);
// aten::arange.out(Scalar end, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arange_outf(@Const @ByRef Scalar end, @ByRef Tensor out);

// aten::arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arange_out(@ByRef Tensor out, @Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step);
// aten::arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arange_outf(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step, @ByRef Tensor out);




// Parsed from ATen/ops/arccos.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/arccos_ops.h>


// aten::arccos(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor arccos(@Const @ByRef Tensor self);

// aten::arccos_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arccos_(@ByRef Tensor self);

// aten::arccos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arccos_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::arccos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arccos_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/arccosh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/arccosh_ops.h>


// aten::arccosh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor arccosh(@Const @ByRef Tensor self);

// aten::arccosh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arccosh_(@ByRef Tensor self);

// aten::arccosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arccosh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::arccosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arccosh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/arcsin.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/arcsin_ops.h>


// aten::arcsin(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor arcsin(@Const @ByRef Tensor self);

// aten::arcsin_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arcsin_(@ByRef Tensor self);

// aten::arcsin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arcsin_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::arcsin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arcsin_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/arcsinh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/arcsinh_ops.h>


// aten::arcsinh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor arcsinh(@Const @ByRef Tensor self);

// aten::arcsinh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arcsinh_(@ByRef Tensor self);

// aten::arcsinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arcsinh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::arcsinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arcsinh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/arctan.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/arctan_ops.h>


// aten::arctan(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor arctan(@Const @ByRef Tensor self);

// aten::arctan_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arctan_(@ByRef Tensor self);

// aten::arctan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arctan_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::arctan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arctan_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/arctan2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/arctan2_ops.h>


// aten::arctan2(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor arctan2(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::arctan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arctan2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::arctan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arctan2_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/arctanh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/arctanh_ops.h>


// aten::arctanh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor arctanh(@Const @ByRef Tensor self);

// aten::arctanh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arctanh_(@ByRef Tensor self);

// aten::arctanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arctanh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::arctanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor arctanh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/argmax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/argmax_ops.h>


// aten::argmax(Tensor self, int? dim=None, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor argmax(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor argmax(@Const @ByRef Tensor self);

// aten::argmax.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor argmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor argmax_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::argmax.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor argmax_outf(@Const @ByRef Tensor self, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);




// Parsed from ATen/ops/argmin.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/argmin_ops.h>


// aten::argmin(Tensor self, int? dim=None, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor argmin(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor argmin(@Const @ByRef Tensor self);

// aten::argmin.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor argmin_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor argmin_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::argmin.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor argmin_outf(@Const @ByRef Tensor self, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);




// Parsed from ATen/ops/argsort.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/argsort_ops.h>


// aten::argsort(Tensor self, int dim=-1, bool descending=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor argsort(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal Tensor argsort(@Const @ByRef Tensor self);

// aten::argsort.stable(Tensor self, *, bool stable, int dim=-1, bool descending=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor argsort(@Const @ByRef Tensor self, @Cast("bool") boolean stable, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal Tensor argsort(@Const @ByRef Tensor self, @Cast("bool") boolean stable);

// aten::argsort.dimname(Tensor self, Dimname dim, bool descending=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor argsort(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal Tensor argsort(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::argsort.stable_out(Tensor self, *, bool stable, int dim=-1, bool descending=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor argsort_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("bool") boolean stable, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByRef Tensor argsort_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("bool") boolean stable);
// aten::argsort.stable_out(Tensor self, *, bool stable, int dim=-1, bool descending=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor argsort_outf(@Const @ByRef Tensor self, @Cast("bool") boolean stable, @Cast("int64_t") long dim, @Cast("bool") boolean descending, @ByRef Tensor out);




// Parsed from ATen/ops/argwhere.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/argwhere_ops.h>


// aten::argwhere(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor argwhere(@Const @ByRef Tensor self);




// Parsed from ATen/ops/as_strided.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/as_strided_ops.h>


// aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor as_strided(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByVal Tensor as_strided(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor as_strided(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByVal Tensor as_strided(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);


// aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor as_strided_symint(@Const @ByRef Tensor self, @ByVal SymIntRef size, @ByVal SymIntRef stride, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional storage_offset);
@Namespace("at") public static native @ByVal Tensor as_strided_symint(@Const @ByRef Tensor self, @ByVal SymIntRef size, @ByVal SymIntRef stride);


// aten::as_strided_(Tensor(a!) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor as_strided_(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @Const @ByRef Tensor as_strided_(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @Const @ByRef Tensor as_strided_(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @Const @ByRef Tensor as_strided_(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);


// aten::as_strided_(Tensor(a!) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor as_strided__symint(@Const @ByRef Tensor self, @ByVal SymIntRef size, @ByVal SymIntRef stride, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional storage_offset);
@Namespace("at") public static native @Const @ByRef Tensor as_strided__symint(@Const @ByRef Tensor self, @ByVal SymIntRef size, @ByVal SymIntRef stride);





// Parsed from ATen/ops/as_strided_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/as_strided_copy_ops.h>


// aten::as_strided_copy(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor as_strided_copy(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByVal Tensor as_strided_copy(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor as_strided_copy(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByVal Tensor as_strided_copy(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);


// aten::as_strided_copy(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor as_strided_copy_symint(@Const @ByRef Tensor self, @ByVal SymIntRef size, @ByVal SymIntRef stride, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional storage_offset);
@Namespace("at") public static native @ByVal Tensor as_strided_copy_symint(@Const @ByRef Tensor self, @ByVal SymIntRef size, @ByVal SymIntRef stride);


// aten::as_strided_copy.out(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor as_strided_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByRef Tensor as_strided_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor as_strided_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByRef Tensor as_strided_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);


// aten::as_strided_copy.out(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor as_strided_copy_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal LongOptional storage_offset, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor as_strided_copy_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal LongOptional storage_offset, @ByRef Tensor out);


// aten::as_strided_copy.out(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor as_strided_copy_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef size, @ByVal SymIntRef stride, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional storage_offset);
@Namespace("at") public static native @ByRef Tensor as_strided_copy_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef size, @ByVal SymIntRef stride);


// aten::as_strided_copy.out(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor as_strided_copy_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef size, @ByVal SymIntRef stride, @ByVal SymIntOptional storage_offset, @ByRef Tensor out);





// Parsed from ATen/ops/as_strided_scatter.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/as_strided_scatter_ops.h>


// aten::as_strided_scatter(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor as_strided_scatter(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByVal Tensor as_strided_scatter(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor as_strided_scatter(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByVal Tensor as_strided_scatter(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);


// aten::as_strided_scatter(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor as_strided_scatter_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal SymIntRef size, @ByVal SymIntRef stride, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional storage_offset);
@Namespace("at") public static native @ByVal Tensor as_strided_scatter_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal SymIntRef size, @ByVal SymIntRef stride);


// aten::as_strided_scatter.out(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor as_strided_scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByRef Tensor as_strided_scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor as_strided_scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByRef Tensor as_strided_scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);


// aten::as_strided_scatter.out(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor as_strided_scatter_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal LongOptional storage_offset, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor as_strided_scatter_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal LongOptional storage_offset, @ByRef Tensor out);


// aten::as_strided_scatter.out(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor as_strided_scatter_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal SymIntRef size, @ByVal SymIntRef stride, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional storage_offset);
@Namespace("at") public static native @ByRef Tensor as_strided_scatter_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal SymIntRef size, @ByVal SymIntRef stride);


// aten::as_strided_scatter.out(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor as_strided_scatter_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @ByVal SymIntRef size, @ByVal SymIntRef stride, @ByVal SymIntOptional storage_offset, @ByRef Tensor out);





// Parsed from ATen/ops/asin.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/asin_ops.h>


// aten::asin(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor asin(@Const @ByRef Tensor self);

// aten::asin_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor asin_(@ByRef Tensor self);

// aten::asin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor asin_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::asin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor asin_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/asinh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/asinh_ops.h>


// aten::asinh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor asinh(@Const @ByRef Tensor self);

// aten::asinh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor asinh_(@ByRef Tensor self);

// aten::asinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor asinh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::asinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor asinh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/atan.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/atan_ops.h>


// aten::atan(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor atan(@Const @ByRef Tensor self);

// aten::atan_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atan_(@ByRef Tensor self);

// aten::atan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atan_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::atan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atan_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/atan2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/atan2_ops.h>


// aten::atan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atan2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::atan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atan2_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::atan2(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor atan2(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/atanh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/atanh_ops.h>


// aten::atanh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor atanh(@Const @ByRef Tensor self);

// aten::atanh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atanh_(@ByRef Tensor self);

// aten::atanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atanh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::atanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor atanh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/atleast_1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/atleast_1d_ops.h>


// aten::atleast_1d(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor atleast_1d(@Const @ByRef Tensor self);

// aten::atleast_1d.Sequence(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector atleast_1d(@ByVal TensorArrayRef tensors);




// Parsed from ATen/ops/atleast_2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/atleast_2d_ops.h>


// aten::atleast_2d(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor atleast_2d(@Const @ByRef Tensor self);

// aten::atleast_2d.Sequence(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector atleast_2d(@ByVal TensorArrayRef tensors);




// Parsed from ATen/ops/atleast_3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/atleast_3d_ops.h>


// aten::atleast_3d(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor atleast_3d(@Const @ByRef Tensor self);

// aten::atleast_3d.Sequence(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector atleast_3d(@ByVal TensorArrayRef tensors);




// Parsed from ATen/ops/avg_pool1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/avg_pool1d_ops.h>


// aten::avg_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, bool ceil_mode=False, bool count_include_pad=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor avg_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/);
@Namespace("at") public static native @ByVal Tensor avg_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor avg_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/);
@Namespace("at") public static native @ByVal Tensor avg_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);




// Parsed from ATen/ops/avg_pool2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/avg_pool2d_ops.h>


// aten::avg_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
// aten::avg_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor out);

// aten::avg_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);




// Parsed from ATen/ops/avg_pool2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/avg_pool2d_backward_ops.h>


// aten::avg_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
// aten::avg_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor grad_input);

// aten::avg_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor
@Namespace("at") public static native @ByVal Tensor avg_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);




// Parsed from ATen/ops/avg_pool3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/avg_pool3d_ops.h>


// aten::avg_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
// aten::avg_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor out);

// aten::avg_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);




// Parsed from ATen/ops/avg_pool3d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/avg_pool3d_backward_ops.h>


// aten::avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
// aten::avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor avg_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor grad_input);

// aten::avg_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor
@Namespace("at") public static native @ByVal Tensor avg_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);




// Parsed from ATen/ops/baddbmm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/baddbmm_ops.h>


// aten::baddbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor baddbmm(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor baddbmm(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2);

// aten::baddbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor baddbmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor baddbmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2);
// aten::baddbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor baddbmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @Const @ByRef Scalar beta, @Const @ByRef Scalar alpha, @ByRef Tensor out);




// Parsed from ATen/ops/bartlett_window.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bartlett_window_ops.h>


// aten::bartlett_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length);
// aten::bartlett_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::bartlett_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
// aten::bartlett_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::bartlett_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bartlett_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length);
// aten::bartlett_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bartlett_window_outf(@Cast("int64_t") long window_length, @ByRef Tensor out);

// aten::bartlett_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bartlett_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
// aten::bartlett_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bartlett_window_outf(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByRef Tensor out);




// Parsed from ATen/ops/batch_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/batch_norm_ops.h>


// aten::batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -> Tensor
@Namespace("at") public static native @ByVal Tensor batch_norm(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double momentum, double eps, @Cast("bool") boolean cudnn_enabled);




// Parsed from ATen/ops/batch_norm_backward_elemt.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/batch_norm_backward_elemt_ops.h>


// aten::batch_norm_backward_elemt(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, Tensor mean_dy, Tensor mean_dy_xmu, Tensor count) -> Tensor
@Namespace("at") public static native @ByVal Tensor batch_norm_backward_elemt(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional weight, @Const @ByRef Tensor mean_dy, @Const @ByRef Tensor mean_dy_xmu, @Const @ByRef Tensor count);

// aten::batch_norm_backward_elemt.out(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, Tensor mean_dy, Tensor mean_dy_xmu, Tensor count, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor batch_norm_backward_elemt_out(@ByRef Tensor out, @Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional weight, @Const @ByRef Tensor mean_dy, @Const @ByRef Tensor mean_dy_xmu, @Const @ByRef Tensor count);
// aten::batch_norm_backward_elemt.out(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, Tensor mean_dy, Tensor mean_dy_xmu, Tensor count, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor batch_norm_backward_elemt_outf(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional weight, @Const @ByRef Tensor mean_dy, @Const @ByRef Tensor mean_dy_xmu, @Const @ByRef Tensor count, @ByRef Tensor out);




// Parsed from ATen/ops/batch_norm_backward_reduce.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/batch_norm_backward_reduce_ops.h>


// aten::batch_norm_backward_reduce(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g) -> (Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple batch_norm_backward_reduce(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional weight, @Cast("bool") boolean input_g, @Cast("bool") boolean weight_g, @Cast("bool") boolean bias_g);

// aten::batch_norm_backward_reduce.out(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> batch_norm_backward_reduce_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional weight, @Cast("bool") boolean input_g, @Cast("bool") boolean weight_g, @Cast("bool") boolean bias_g);
// aten::batch_norm_backward_reduce.out(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> batch_norm_backward_reduce_outf(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional weight, @Cast("bool") boolean input_g, @Cast("bool") boolean weight_g, @Cast("bool") boolean bias_g, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3);




// Parsed from ATen/ops/batch_norm_elemt.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/batch_norm_elemt_ops.h>


// aten::batch_norm_elemt(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps) -> Tensor
@Namespace("at") public static native @ByVal Tensor batch_norm_elemt(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, double eps);

// aten::batch_norm_elemt.out(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor batch_norm_elemt_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, double eps);
// aten::batch_norm_elemt.out(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor batch_norm_elemt_outf(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, double eps, @ByRef Tensor out);




// Parsed from ATen/ops/batch_norm_gather_stats.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/batch_norm_gather_stats_ops.h>


// aten::batch_norm_gather_stats(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, int count) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple batch_norm_gather_stats(@Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum, double eps, @Cast("int64_t") long count);

// aten::batch_norm_gather_stats.out(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, int count, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> batch_norm_gather_stats_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum, double eps, @Cast("int64_t") long count);
// aten::batch_norm_gather_stats.out(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, int count, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> batch_norm_gather_stats_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum, double eps, @Cast("int64_t") long count, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/batch_norm_gather_stats_with_counts.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/batch_norm_gather_stats_with_counts_ops.h>


// aten::batch_norm_gather_stats_with_counts(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple batch_norm_gather_stats_with_counts(@Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum, double eps, @Const @ByRef Tensor counts);

// aten::batch_norm_gather_stats_with_counts.out(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> batch_norm_gather_stats_with_counts_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum, double eps, @Const @ByRef Tensor counts);
// aten::batch_norm_gather_stats_with_counts.out(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> batch_norm_gather_stats_with_counts_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum, double eps, @Const @ByRef Tensor counts, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/batch_norm_stats.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/batch_norm_stats_ops.h>


// aten::batch_norm_stats(Tensor input, float eps) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple batch_norm_stats(@Const @ByRef Tensor input, double eps);

// aten::batch_norm_stats.out(Tensor input, float eps, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> batch_norm_stats_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor input, double eps);
// aten::batch_norm_stats.out(Tensor input, float eps, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> batch_norm_stats_outf(@Const @ByRef Tensor input, double eps, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/batch_norm_update_stats.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/batch_norm_update_stats_ops.h>


// aten::batch_norm_update_stats(Tensor input, Tensor? running_mean, Tensor? running_var, float momentum) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple batch_norm_update_stats(@Const @ByRef Tensor input, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum);

// aten::batch_norm_update_stats.out(Tensor input, Tensor? running_mean, Tensor? running_var, float momentum, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> batch_norm_update_stats_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor input, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum);
// aten::batch_norm_update_stats.out(Tensor input, Tensor? running_mean, Tensor? running_var, float momentum, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> batch_norm_update_stats_outf(@Const @ByRef Tensor input, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/bernoulli.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bernoulli_ops.h>


// aten::bernoulli(Tensor self, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor bernoulli(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor bernoulli(@Const @ByRef Tensor self);

// aten::bernoulli.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bernoulli_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
// aten::bernoulli.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bernoulli_outf(@Const @ByRef Tensor self, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::bernoulli.p(Tensor self, float p, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor bernoulli(@Const @ByRef Tensor self, double p, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor bernoulli(@Const @ByRef Tensor self, double p);

// aten::bernoulli.Tensor_out(Tensor self, Tensor p, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bernoulli_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor p, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
// aten::bernoulli.Tensor_out(Tensor self, Tensor p, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bernoulli_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor p, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::bernoulli.Tensor(Tensor self, Tensor p, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor bernoulli(@Const @ByRef Tensor self, @Const @ByRef Tensor p, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor bernoulli(@Const @ByRef Tensor self, @Const @ByRef Tensor p);

// aten::bernoulli.float_out(Tensor self, float p=0.5, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bernoulli_out(@ByRef Tensor out, @Const @ByRef Tensor self, double p/*=0.5*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
// aten::bernoulli.float_out(Tensor self, float p=0.5, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bernoulli_outf(@Const @ByRef Tensor self, double p, @ByVal GeneratorOptional generator, @ByRef Tensor out);




// Parsed from ATen/ops/bilinear.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bilinear_ops.h>


// aten::bilinear(Tensor input1, Tensor input2, Tensor weight, Tensor? bias=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor bilinear(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias);
@Namespace("at") public static native @ByVal Tensor bilinear(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor weight);




// Parsed from ATen/ops/binary_cross_entropy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/binary_cross_entropy_ops.h>


// aten::binary_cross_entropy(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy(@Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::binary_cross_entropy.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
// aten::binary_cross_entropy.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByRef Tensor out);




// Parsed from ATen/ops/binary_cross_entropy_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/binary_cross_entropy_backward_ops.h>


// aten::binary_cross_entropy_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::binary_cross_entropy_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
// aten::binary_cross_entropy_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByRef Tensor grad_input);




// Parsed from ATen/ops/binary_cross_entropy_with_logits.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/binary_cross_entropy_with_logits_ops.h>


// aten::binary_cross_entropy_with_logits(Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy_with_logits(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional pos_weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy_with_logits(@Const @ByRef Tensor self, @Const @ByRef Tensor target);

// aten::binary_cross_entropy_with_logits.out(Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_with_logits_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional pos_weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_with_logits_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
// aten::binary_cross_entropy_with_logits.out(Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_with_logits_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional pos_weight, @Cast("int64_t") long reduction, @ByRef Tensor out);




// Parsed from ATen/ops/bincount.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bincount_ops.h>


// aten::bincount(Tensor self, Tensor? weights=None, int minlength=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor bincount(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weights, @Cast("int64_t") long minlength/*=0*/);
@Namespace("at") public static native @ByVal Tensor bincount(@Const @ByRef Tensor self);

// aten::bincount.out(Tensor self, Tensor? weights=None, int minlength=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bincount_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weights, @Cast("int64_t") long minlength/*=0*/);
@Namespace("at") public static native @ByRef Tensor bincount_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::bincount.out(Tensor self, Tensor? weights=None, int minlength=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bincount_outf(@Const @ByRef Tensor self, @Const @ByRef TensorOptional weights, @Cast("int64_t") long minlength, @ByRef Tensor out);




// Parsed from ATen/ops/binomial.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/binomial_ops.h>


// aten::binomial(Tensor count, Tensor prob, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor binomial(@Const @ByRef Tensor count, @Const @ByRef Tensor prob, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor binomial(@Const @ByRef Tensor count, @Const @ByRef Tensor prob);

// aten::binomial.out(Tensor count, Tensor prob, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor binomial_out(@ByRef Tensor out, @Const @ByRef Tensor count, @Const @ByRef Tensor prob, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor binomial_out(@ByRef Tensor out, @Const @ByRef Tensor count, @Const @ByRef Tensor prob);
// aten::binomial.out(Tensor count, Tensor prob, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor binomial_outf(@Const @ByRef Tensor count, @Const @ByRef Tensor prob, @ByVal GeneratorOptional generator, @ByRef Tensor out);




// Parsed from ATen/ops/bitwise_and.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bitwise_and_ops.h>


// aten::bitwise_and.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_and_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::bitwise_and.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_and_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::bitwise_and.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_and_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::bitwise_and.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_and_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::bitwise_and.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_and(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::bitwise_and.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_and(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::bitwise_and.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_and(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::bitwise_and.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_and_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);
// aten::bitwise_and.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_and_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/bitwise_left_shift.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bitwise_left_shift_ops.h>


// aten::bitwise_left_shift.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_left_shift(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::bitwise_left_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_left_shift_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::bitwise_left_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_left_shift_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::bitwise_left_shift.Tensor_Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_left_shift(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::bitwise_left_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_left_shift_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::bitwise_left_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_left_shift_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::bitwise_left_shift.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_left_shift(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::bitwise_left_shift.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_left_shift_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);
// aten::bitwise_left_shift.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_left_shift_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/bitwise_not.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bitwise_not_ops.h>


// aten::bitwise_not(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_not(@Const @ByRef Tensor self);

// aten::bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_not_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_not_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/bitwise_or.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bitwise_or_ops.h>


// aten::bitwise_or.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_or_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::bitwise_or.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_or_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::bitwise_or.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_or_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::bitwise_or.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_or_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::bitwise_or.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_or(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::bitwise_or.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_or(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::bitwise_or.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_or(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::bitwise_or.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_or_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);
// aten::bitwise_or.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_or_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/bitwise_right_shift.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bitwise_right_shift_ops.h>


// aten::bitwise_right_shift.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_right_shift(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::bitwise_right_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_right_shift_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::bitwise_right_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_right_shift_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::bitwise_right_shift.Tensor_Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_right_shift(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::bitwise_right_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_right_shift_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::bitwise_right_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_right_shift_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::bitwise_right_shift.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_right_shift(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::bitwise_right_shift.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_right_shift_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);
// aten::bitwise_right_shift.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_right_shift_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/bitwise_xor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bitwise_xor_ops.h>


// aten::bitwise_xor.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_xor_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::bitwise_xor.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_xor_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::bitwise_xor.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_xor_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::bitwise_xor.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_xor_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::bitwise_xor.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_xor(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::bitwise_xor.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_xor(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::bitwise_xor.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor bitwise_xor(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::bitwise_xor.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_xor_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);
// aten::bitwise_xor.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bitwise_xor_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/blackman_window.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/blackman_window_ops.h>


// aten::blackman_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length);
// aten::blackman_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::blackman_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
// aten::blackman_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::blackman_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor blackman_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length);
// aten::blackman_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor blackman_window_outf(@Cast("int64_t") long window_length, @ByRef Tensor out);

// aten::blackman_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor blackman_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
// aten::blackman_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor blackman_window_outf(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByRef Tensor out);




// Parsed from ATen/ops/block_diag.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/block_diag_ops.h>


// aten::block_diag(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor block_diag(@ByVal TensorArrayRef tensors);

// aten::block_diag.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor block_diag_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
// aten::block_diag.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor block_diag_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);




// Parsed from ATen/ops/bmm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bmm_ops.h>


// aten::bmm(Tensor self, Tensor mat2) -> Tensor
@Namespace("at") public static native @ByVal Tensor bmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat2);

// aten::bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat2);
// aten::bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat2, @ByRef Tensor out);




// Parsed from ATen/ops/broadcast_tensors.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/broadcast_tensors_ops.h>


// aten::broadcast_tensors(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector broadcast_tensors(@ByVal TensorArrayRef tensors);




// Parsed from ATen/ops/broadcast_to.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/broadcast_to_ops.h>


// aten::broadcast_to(Tensor(a) self, SymInt[] size) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor broadcast_to(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor broadcast_to(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::broadcast_to(Tensor(a) self, SymInt[] size) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor broadcast_to_symint(@Const @ByRef Tensor self, @ByVal SymIntRef size);





// Parsed from ATen/ops/bucketize.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/bucketize_ops.h>


// aten::bucketize.Tensor(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor bucketize(@Const @ByRef Tensor self, @Const @ByRef Tensor boundaries, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/);
@Namespace("at") public static native @ByVal Tensor bucketize(@Const @ByRef Tensor self, @Const @ByRef Tensor boundaries);

// aten::bucketize.Tensor_out(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bucketize_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor boundaries, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/);
@Namespace("at") public static native @ByRef Tensor bucketize_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor boundaries);
// aten::bucketize.Tensor_out(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bucketize_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor boundaries, @Cast("bool") boolean out_int32, @Cast("bool") boolean right, @ByRef Tensor out);

// aten::bucketize.Scalar(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor bucketize(@Const @ByRef Scalar self, @Const @ByRef Tensor boundaries, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/);
@Namespace("at") public static native @ByVal Tensor bucketize(@Const @ByRef Scalar self, @Const @ByRef Tensor boundaries);

// aten::bucketize.Scalar_out(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bucketize_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor boundaries, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/);
@Namespace("at") public static native @ByRef Tensor bucketize_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor boundaries);
// aten::bucketize.Scalar_out(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor bucketize_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor boundaries, @Cast("bool") boolean out_int32, @Cast("bool") boolean right, @ByRef Tensor out);




// Parsed from ATen/ops/can_cast.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/can_cast_ops.h>


// aten::can_cast(ScalarType from, ScalarType to) -> bool
@Namespace("at") public static native @Cast("bool") boolean can_cast(ScalarType from, ScalarType to);




// Parsed from ATen/ops/cartesian_prod.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cartesian_prod_ops.h>


// aten::cartesian_prod(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor cartesian_prod(@ByVal TensorArrayRef tensors);




// Parsed from ATen/ops/cat.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cat_ops.h>


// aten::cat(Tensor[] tensors, int dim=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor cat(@Const @ByRef TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor cat(@Const @ByRef TensorArrayRef tensors);

// aten::cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cat_out(@ByRef Tensor out, @Const @ByRef TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByRef Tensor cat_out(@ByRef Tensor out, @Const @ByRef TensorArrayRef tensors);
// aten::cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cat_outf(@Const @ByRef TensorArrayRef tensors, @Cast("int64_t") long dim, @ByRef Tensor out);

// aten::cat.names(Tensor[] tensors, Dimname dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor cat(@ByVal TensorArrayRef tensors, @ByVal Dimname dim);

// aten::cat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cat_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @ByVal Dimname dim);
// aten::cat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cat_outf(@ByVal TensorArrayRef tensors, @ByVal Dimname dim, @ByRef Tensor out);




// Parsed from ATen/ops/cauchy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cauchy_ops.h>


// aten::cauchy.out(Tensor self, float median=0, float sigma=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cauchy_out(@ByRef Tensor out, @Const @ByRef Tensor self, double median/*=0*/, double sigma/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor cauchy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::cauchy.out(Tensor self, float median=0, float sigma=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cauchy_outf(@Const @ByRef Tensor self, double median, double sigma, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::cauchy(Tensor self, float median=0, float sigma=1, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor cauchy(@Const @ByRef Tensor self, double median/*=0*/, double sigma/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor cauchy(@Const @ByRef Tensor self);




// Parsed from ATen/ops/ccol_indices.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ccol_indices_ops.h>






// Parsed from ATen/ops/ccol_indices_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ccol_indices_copy_ops.h>


// aten::ccol_indices_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor ccol_indices_copy(@Const @ByRef Tensor self);

// aten::ccol_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ccol_indices_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::ccol_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ccol_indices_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/cdist.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cdist_ops.h>


// aten::cdist(Tensor x1, Tensor x2, float p=2, int? compute_mode=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor cdist(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2, double p/*=2*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional compute_mode);
@Namespace("at") public static native @ByVal Tensor cdist(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2);




// Parsed from ATen/ops/ceil.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ceil_ops.h>


// aten::ceil(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor ceil(@Const @ByRef Tensor self);

// aten::ceil_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ceil_(@ByRef Tensor self);

// aten::ceil.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ceil_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::ceil.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ceil_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/celu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/celu_ops.h>


// aten::celu(Tensor self, Scalar alpha=1.0) -> Tensor
@Namespace("at") public static native @ByVal Tensor celu(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(1.0)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor celu(@Const @ByRef Tensor self);

// aten::celu_(Tensor(a!) self, Scalar alpha=1.0) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor celu_(@ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(1.0)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor celu_(@ByRef Tensor self);

// aten::celu.out(Tensor self, Scalar alpha=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor celu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(1.0)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor celu_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::celu.out(Tensor self, Scalar alpha=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor celu_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar alpha, @ByRef Tensor out);




// Parsed from ATen/ops/chain_matmul.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/chain_matmul_ops.h>


// aten::chain_matmul(Tensor[] matrices) -> Tensor
@Namespace("at") public static native @ByVal Tensor chain_matmul(@ByVal TensorArrayRef matrices);

// aten::chain_matmul.out(Tensor[] matrices, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor chain_matmul_out(@ByRef Tensor out, @ByVal TensorArrayRef matrices);
// aten::chain_matmul.out(Tensor[] matrices, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor chain_matmul_outf(@ByVal TensorArrayRef matrices, @ByRef Tensor out);




// Parsed from ATen/ops/chalf.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/chalf_ops.h>






// Parsed from ATen/ops/channel_shuffle.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/channel_shuffle_ops.h>


// aten::channel_shuffle(Tensor self, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor channel_shuffle(@Const @ByRef Tensor self, @Cast("int64_t") long groups);

// aten::channel_shuffle.out(Tensor self, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor channel_shuffle_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long groups);
// aten::channel_shuffle.out(Tensor self, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor channel_shuffle_outf(@Const @ByRef Tensor self, @Cast("int64_t") long groups, @ByRef Tensor out);




// Parsed from ATen/ops/cholesky.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cholesky_ops.h>


// aten::cholesky.out(Tensor self, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cholesky_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByRef Tensor cholesky_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::cholesky.out(Tensor self, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cholesky_outf(@Const @ByRef Tensor self, @Cast("bool") boolean upper, @ByRef Tensor out);

// aten::cholesky(Tensor self, bool upper=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor cholesky(@Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByVal Tensor cholesky(@Const @ByRef Tensor self);




// Parsed from ATen/ops/cholesky_inverse.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cholesky_inverse_ops.h>


// aten::cholesky_inverse(Tensor self, bool upper=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor cholesky_inverse(@Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByVal Tensor cholesky_inverse(@Const @ByRef Tensor self);

// aten::cholesky_inverse.out(Tensor self, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cholesky_inverse_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByRef Tensor cholesky_inverse_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::cholesky_inverse.out(Tensor self, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cholesky_inverse_outf(@Const @ByRef Tensor self, @Cast("bool") boolean upper, @ByRef Tensor out);




// Parsed from ATen/ops/cholesky_solve.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cholesky_solve_ops.h>


// aten::cholesky_solve.out(Tensor self, Tensor input2, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cholesky_solve_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByRef Tensor cholesky_solve_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor input2);
// aten::cholesky_solve.out(Tensor self, Tensor input2, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cholesky_solve_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Cast("bool") boolean upper, @ByRef Tensor out);

// aten::cholesky_solve(Tensor self, Tensor input2, bool upper=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor cholesky_solve(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByVal Tensor cholesky_solve(@Const @ByRef Tensor self, @Const @ByRef Tensor input2);




// Parsed from ATen/ops/choose_qparams_optimized.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/choose_qparams_optimized_ops.h>


// aten::choose_qparams_optimized(Tensor input, int numel, int n_bins, float ratio, int bit_width) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple choose_qparams_optimized(@Const @ByRef Tensor input, @Cast("int64_t") long numel, @Cast("int64_t") long n_bins, double ratio, @Cast("int64_t") long bit_width);




// Parsed from ATen/ops/chunk.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/chunk_ops.h>


// aten::chunk(Tensor(a -> *) self, int chunks, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector chunk(@Const @ByRef Tensor self, @Cast("int64_t") long chunks, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector chunk(@Const @ByRef Tensor self, @Cast("int64_t") long chunks);




// Parsed from ATen/ops/clamp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/clamp_ops.h>


// aten::clamp(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor clamp(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByVal Tensor clamp(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional min);

// aten::clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor clamp(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional min, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional max);
@Namespace("at") public static native @ByVal Tensor clamp(@Const @ByRef Tensor self);

// aten::clamp_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_(@ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByRef Tensor clamp_(@ByRef Tensor self, @Const @ByRef ScalarOptional min);

// aten::clamp_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_(@ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional min, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional max);
@Namespace("at") public static native @ByRef Tensor clamp_(@ByRef Tensor self);

// aten::clamp.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByRef Tensor clamp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional min);
// aten::clamp.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef ScalarOptional max, @ByRef Tensor out);

// aten::clamp.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional min, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional max);
@Namespace("at") public static native @ByRef Tensor clamp_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::clamp.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_outf(@Const @ByRef Tensor self, @Const @ByRef TensorOptional min, @Const @ByRef TensorOptional max, @ByRef Tensor out);




// Parsed from ATen/ops/clamp_max.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/clamp_max_ops.h>


// aten::clamp_max(Tensor self, Scalar max) -> Tensor
@Namespace("at") public static native @ByVal Tensor clamp_max(@Const @ByRef Tensor self, @Const @ByRef Scalar max);

// aten::clamp_max.Tensor(Tensor self, Tensor max) -> Tensor
@Namespace("at") public static native @ByVal Tensor clamp_max(@Const @ByRef Tensor self, @Const @ByRef Tensor max);

// aten::clamp_max_(Tensor(a!) self, Scalar max) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_max_(@ByRef Tensor self, @Const @ByRef Scalar max);

// aten::clamp_max_.Tensor(Tensor(a!) self, Tensor max) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_max_(@ByRef Tensor self, @Const @ByRef Tensor max);

// aten::clamp_max.out(Tensor self, Scalar max, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_max_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar max);
// aten::clamp_max.out(Tensor self, Scalar max, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_max_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar max, @ByRef Tensor out);

// aten::clamp_max.Tensor_out(Tensor self, Tensor max, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_max_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor max);
// aten::clamp_max.Tensor_out(Tensor self, Tensor max, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_max_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor max, @ByRef Tensor out);




// Parsed from ATen/ops/clamp_min.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/clamp_min_ops.h>


// aten::clamp_min(Tensor self, Scalar min) -> Tensor
@Namespace("at") public static native @ByVal Tensor clamp_min(@Const @ByRef Tensor self, @Const @ByRef Scalar min);

// aten::clamp_min.Tensor(Tensor self, Tensor min) -> Tensor
@Namespace("at") public static native @ByVal Tensor clamp_min(@Const @ByRef Tensor self, @Const @ByRef Tensor min);

// aten::clamp_min_(Tensor(a!) self, Scalar min) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_min_(@ByRef Tensor self, @Const @ByRef Scalar min);

// aten::clamp_min_.Tensor(Tensor(a!) self, Tensor min) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_min_(@ByRef Tensor self, @Const @ByRef Tensor min);

// aten::clamp_min.out(Tensor self, Scalar min, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_min_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar min);
// aten::clamp_min.out(Tensor self, Scalar min, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_min_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar min, @ByRef Tensor out);

// aten::clamp_min.Tensor_out(Tensor self, Tensor min, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_min_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor min);
// aten::clamp_min.Tensor_out(Tensor self, Tensor min, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clamp_min_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor min, @ByRef Tensor out);




// Parsed from ATen/ops/clip.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/clip_ops.h>


// aten::clip(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor clip(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByVal Tensor clip(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional min);

// aten::clip.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor clip(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional min, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional max);
@Namespace("at") public static native @ByVal Tensor clip(@Const @ByRef Tensor self);

// aten::clip_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clip_(@ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByRef Tensor clip_(@ByRef Tensor self, @Const @ByRef ScalarOptional min);

// aten::clip_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clip_(@ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional min, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional max);
@Namespace("at") public static native @ByRef Tensor clip_(@ByRef Tensor self);

// aten::clip.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clip_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByRef Tensor clip_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional min);
// aten::clip.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clip_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional min, @Const @ByRef ScalarOptional max, @ByRef Tensor out);

// aten::clip.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clip_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional min, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional max);
@Namespace("at") public static native @ByRef Tensor clip_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::clip.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clip_outf(@Const @ByRef Tensor self, @Const @ByRef TensorOptional min, @Const @ByRef TensorOptional max, @ByRef Tensor out);




// Parsed from ATen/ops/clone.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/clone_ops.h>


// aten::clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor clone(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor clone(@Const @ByRef Tensor self);

// aten::clone.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clone_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor clone_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::clone.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor clone_outf(@Const @ByRef Tensor self, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);




// Parsed from ATen/ops/coalesce.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/coalesce_ops.h>






// Parsed from ATen/ops/col2im.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/col2im_ops.h>


// aten::col2im.out(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor col2im_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor col2im_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);


// aten::col2im.out(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor col2im_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor col2im_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByRef Tensor out);


// aten::col2im.out(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor col2im_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor col2im_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);


// aten::col2im.out(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor col2im_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor col2im_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByRef Tensor out);


// aten::col2im(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
@Namespace("at") public static native @ByVal Tensor col2im(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor col2im(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);


// aten::col2im(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
@Namespace("at") public static native @ByVal Tensor col2im_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor col2im_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);





// Parsed from ATen/ops/col_indices.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/col_indices_ops.h>






// Parsed from ATen/ops/col_indices_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/col_indices_copy_ops.h>


// aten::col_indices_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor col_indices_copy(@Const @ByRef Tensor self);

// aten::col_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor col_indices_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::col_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor col_indices_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/column_stack.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/column_stack_ops.h>


// aten::column_stack(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor column_stack(@ByVal TensorArrayRef tensors);

// aten::column_stack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor column_stack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
// aten::column_stack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor column_stack_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);




// Parsed from ATen/ops/combinations.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/combinations_ops.h>


// aten::combinations(Tensor self, int r=2, bool with_replacement=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor combinations(@Const @ByRef Tensor self, @Cast("int64_t") long r/*=2*/, @Cast("bool") boolean with_replacement/*=false*/);
@Namespace("at") public static native @ByVal Tensor combinations(@Const @ByRef Tensor self);




// Parsed from ATen/ops/complex.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/complex_ops.h>


// aten::complex(Tensor real, Tensor imag) -> Tensor


// aten::complex.out(Tensor real, Tensor imag, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor complex_out(@ByRef Tensor out, @Const @ByRef Tensor real, @Const @ByRef Tensor imag);
// aten::complex.out(Tensor real, Tensor imag, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor complex_outf(@Const @ByRef Tensor real, @Const @ByRef Tensor imag, @ByRef Tensor out);




// Parsed from ATen/ops/concat.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/concat_ops.h>


// aten::concat(Tensor[] tensors, int dim=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor concat(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor concat(@ByVal TensorArrayRef tensors);

// aten::concat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor concat_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByRef Tensor concat_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
// aten::concat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor concat_outf(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim, @ByRef Tensor out);

// aten::concat.names(Tensor[] tensors, Dimname dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor concat(@ByVal TensorArrayRef tensors, @ByVal Dimname dim);

// aten::concat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor concat_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @ByVal Dimname dim);
// aten::concat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor concat_outf(@ByVal TensorArrayRef tensors, @ByVal Dimname dim, @ByRef Tensor out);




// Parsed from ATen/ops/concatenate.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/concatenate_ops.h>


// aten::concatenate(Tensor[] tensors, int dim=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor concatenate(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor concatenate(@ByVal TensorArrayRef tensors);

// aten::concatenate.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor concatenate_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByRef Tensor concatenate_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
// aten::concatenate.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor concatenate_outf(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim, @ByRef Tensor out);

// aten::concatenate.names(Tensor[] tensors, Dimname dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor concatenate(@ByVal TensorArrayRef tensors, @ByVal Dimname dim);

// aten::concatenate.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor concatenate_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @ByVal Dimname dim);
// aten::concatenate.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor concatenate_outf(@ByVal TensorArrayRef tensors, @ByVal Dimname dim, @ByRef Tensor out);




// Parsed from ATen/ops/conj.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/conj_ops.h>


// aten::conj(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor __dispatch_conj(@Const @ByRef Tensor self);




// Parsed from ATen/ops/conj_physical.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/conj_physical_ops.h>


// aten::conj_physical(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor conj_physical(@Const @ByRef Tensor self);

// aten::conj_physical.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor conj_physical_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::conj_physical.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor conj_physical_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::conj_physical_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor conj_physical_(@ByRef Tensor self);




// Parsed from ATen/ops/constant_pad_nd.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/constant_pad_nd_ops.h>


// aten::constant_pad_nd(Tensor self, SymInt[] pad, Scalar value=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor constant_pad_nd(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef pad, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar value);
@Namespace("at") public static native @ByVal Tensor constant_pad_nd(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef pad);
@Namespace("at") public static native @ByVal Tensor constant_pad_nd(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] pad, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar value);
@Namespace("at") public static native @ByVal Tensor constant_pad_nd(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... pad);


// aten::constant_pad_nd(Tensor self, SymInt[] pad, Scalar value=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor constant_pad_nd_symint(@Const @ByRef Tensor self, @ByVal SymIntRef pad, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar value);
@Namespace("at") public static native @ByVal Tensor constant_pad_nd_symint(@Const @ByRef Tensor self, @ByVal SymIntRef pad);


// aten::constant_pad_nd.out(Tensor self, SymInt[] pad, Scalar value=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor constant_pad_nd_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef pad, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar value);
@Namespace("at") public static native @ByRef Tensor constant_pad_nd_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef pad);
@Namespace("at") public static native @ByRef Tensor constant_pad_nd_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] pad, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar value);
@Namespace("at") public static native @ByRef Tensor constant_pad_nd_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... pad);


// aten::constant_pad_nd.out(Tensor self, SymInt[] pad, Scalar value=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor constant_pad_nd_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef pad, @Const @ByRef Scalar value, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor constant_pad_nd_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] pad, @Const @ByRef Scalar value, @ByRef Tensor out);


// aten::constant_pad_nd.out(Tensor self, SymInt[] pad, Scalar value=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor constant_pad_nd_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef pad, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar value);
@Namespace("at") public static native @ByRef Tensor constant_pad_nd_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef pad);


// aten::constant_pad_nd.out(Tensor self, SymInt[] pad, Scalar value=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor constant_pad_nd_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef pad, @Const @ByRef Scalar value, @ByRef Tensor out);





// Parsed from ATen/ops/contiguous.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/contiguous_ops.h>






// Parsed from ATen/ops/conv1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/conv1d_ops.h>


// aten::conv1d(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, int[1] padding=0, int[1] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups/*=1*/);

// aten::conv1d.padding(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, str padding="valid", int[1] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::string_view*") Pointer padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::string_view*") Pointer padding);
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast("c10::string_view*") Pointer padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast("c10::string_view*") Pointer padding);




// Parsed from ATen/ops/conv2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/conv2d_ops.h>


// aten::conv2d(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups/*=1*/);

// aten::conv2d.padding(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, str padding="valid", int[2] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::string_view*") Pointer padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::string_view*") Pointer padding);
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast("c10::string_view*") Pointer padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast("c10::string_view*") Pointer padding);




// Parsed from ATen/ops/conv3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/conv3d_ops.h>


// aten::conv3d(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups/*=1*/);

// aten::conv3d.padding(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, str padding="valid", int[3] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::string_view*") Pointer padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::string_view*") Pointer padding);
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast("c10::string_view*") Pointer padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast("c10::string_view*") Pointer padding);




// Parsed from ATen/ops/conv_depthwise3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/conv_depthwise3d_ops.h>


// aten::conv_depthwise3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding, int[3] dilation) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv_depthwise3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor conv_depthwise3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);


// aten::conv_depthwise3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding, int[3] dilation) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv_depthwise3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor conv_depthwise3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);


// aten::conv_depthwise3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding, int[3] dilation, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor conv_depthwise3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor conv_depthwise3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);


// aten::conv_depthwise3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding, int[3] dilation, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor conv_depthwise3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor conv_depthwise3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByRef Tensor out);


// aten::conv_depthwise3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding, int[3] dilation, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor conv_depthwise3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor conv_depthwise3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);


// aten::conv_depthwise3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding, int[3] dilation, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor conv_depthwise3d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor conv_depthwise3d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByRef Tensor out);





// Parsed from ATen/ops/conv_tbc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/conv_tbc_ops.h>


// aten::conv_tbc(Tensor self, Tensor weight, Tensor bias, int pad=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv_tbc(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor bias, @Cast("int64_t") long pad/*=0*/);
@Namespace("at") public static native @ByVal Tensor conv_tbc(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor bias);

// aten::conv_tbc.out(Tensor self, Tensor weight, Tensor bias, int pad=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor conv_tbc_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor bias, @Cast("int64_t") long pad/*=0*/);
@Namespace("at") public static native @ByRef Tensor conv_tbc_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor bias);
// aten::conv_tbc.out(Tensor self, Tensor weight, Tensor bias, int pad=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor conv_tbc_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor bias, @Cast("int64_t") long pad, @ByRef Tensor out);




// Parsed from ATen/ops/conv_tbc_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/conv_tbc_backward_ops.h>


// aten::conv_tbc_backward(Tensor self, Tensor input, Tensor weight, Tensor bias, int pad) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple conv_tbc_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef Tensor bias, @Cast("int64_t") long pad);




// Parsed from ATen/ops/conv_transpose1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/conv_transpose1d_ops.h>


// aten::conv_transpose1d(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, int[1] padding=0, int[1] output_padding=0, int groups=1, int[1] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv_transpose1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor conv_transpose1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv_transpose1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);




// Parsed from ATen/ops/conv_transpose2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/conv_transpose2d_ops.h>


// aten::conv_transpose2d.input(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int groups=1, int[2] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv_transpose2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor conv_transpose2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv_transpose2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);




// Parsed from ATen/ops/conv_transpose3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/conv_transpose3d_ops.h>


// aten::conv_transpose3d.input(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] output_padding=0, int groups=1, int[3] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor conv_transpose3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor conv_transpose3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv_transpose3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);




// Parsed from ATen/ops/convolution.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/convolution_ops.h>


// aten::convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups);


// aten::convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor convolution_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal SymIntRef output_padding, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor convolution_symint(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal SymIntRef output_padding, @Cast("int64_t") long groups);


// aten::convolution.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor convolution_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByRef Tensor convolution_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups);


// aten::convolution.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor convolution_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor convolution_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups, @ByRef Tensor out);


// aten::convolution.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor convolution_symint_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal SymIntRef output_padding, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByRef Tensor convolution_symint_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal SymIntRef output_padding, @Cast("int64_t") long groups);


// aten::convolution.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor convolution_symint_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal SymIntRef output_padding, @Cast("int64_t") long groups, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor convolution_symint_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal SymIntRef output_padding, @Cast("int64_t") long groups, @ByRef Tensor out);





// Parsed from ATen/ops/convolution_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/convolution_backward_ops.h>


// aten::convolution_backward(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple convolution_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal LongArrayRefOptional bias_sizes, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple convolution_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] bias_sizes, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::convolution_backward(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple convolution_backward_symint(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal SymIntArrayRefOptional bias_sizes, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal SymIntRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple convolution_backward_symint(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal SymIntArrayRefOptional bias_sizes, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal SymIntRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::convolution_backward.out(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> convolution_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal LongArrayRefOptional bias_sizes, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> convolution_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] bias_sizes, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::convolution_backward.out(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> convolution_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal LongArrayRefOptional bias_sizes, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> convolution_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] bias_sizes, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);


// aten::convolution_backward.out(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> convolution_backward_symint_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal SymIntArrayRefOptional bias_sizes, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal SymIntRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> convolution_backward_symint_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal SymIntArrayRefOptional bias_sizes, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal SymIntRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::convolution_backward.out(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> convolution_backward_symint_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal SymIntArrayRefOptional bias_sizes, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal SymIntRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> convolution_backward_symint_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal SymIntArrayRefOptional bias_sizes, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal SymIntRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);





// Parsed from ATen/ops/convolution_backward_overrideable.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/convolution_backward_overrideable_ops.h>


// aten::convolution_backward_overrideable(Tensor grad_output, Tensor input, Tensor weight, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple convolution_backward_overrideable(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple convolution_backward_overrideable(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::convolution_backward_overrideable.out(Tensor grad_output, Tensor input, Tensor weight, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> convolution_backward_overrideable_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> convolution_backward_overrideable_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
// aten::convolution_backward_overrideable.out(Tensor grad_output, Tensor input, Tensor weight, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> convolution_backward_overrideable_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> convolution_backward_overrideable_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/convolution_overrideable.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/convolution_overrideable_ops.h>


// aten::convolution_overrideable(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor convolution_overrideable(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor convolution_overrideable(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups);

// aten::convolution_overrideable.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor convolution_overrideable_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByRef Tensor convolution_overrideable_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups);
// aten::convolution_overrideable.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor convolution_overrideable_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor convolution_overrideable_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups, @ByRef Tensor out);




// Parsed from ATen/ops/copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/copy_ops.h>


// aten::copy(Tensor self, Tensor src, bool non_blocking=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor copy(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor copy(@Const @ByRef Tensor self, @Const @ByRef Tensor src);

// aten::copy.out(Tensor self, Tensor src, bool non_blocking=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByRef Tensor copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src);
// aten::copy.out(Tensor self, Tensor src, bool non_blocking=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor copy_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("bool") boolean non_blocking, @ByRef Tensor out);




// Parsed from ATen/ops/copy_sparse_to_sparse.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/copy_sparse_to_sparse_ops.h>


// aten::copy_sparse_to_sparse_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor copy_sparse_to_sparse_(@ByRef Tensor self, @Const @ByRef Tensor src, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByRef Tensor copy_sparse_to_sparse_(@ByRef Tensor self, @Const @ByRef Tensor src);

// aten::copy_sparse_to_sparse.out(Tensor self, Tensor src, bool non_blocking=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor copy_sparse_to_sparse_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByRef Tensor copy_sparse_to_sparse_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src);
// aten::copy_sparse_to_sparse.out(Tensor self, Tensor src, bool non_blocking=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor copy_sparse_to_sparse_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("bool") boolean non_blocking, @ByRef Tensor out);

// aten::copy_sparse_to_sparse(Tensor self, Tensor src, bool non_blocking=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor copy_sparse_to_sparse(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor copy_sparse_to_sparse(@Const @ByRef Tensor self, @Const @ByRef Tensor src);




// Parsed from ATen/ops/copysign.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/copysign_ops.h>


// aten::copysign.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor copysign_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::copysign.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor copysign_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::copysign.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor copysign(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::copysign.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor copysign(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::copysign.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor copysign_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::copysign.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor copysign_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);




// Parsed from ATen/ops/corrcoef.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/corrcoef_ops.h>


// aten::corrcoef(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor corrcoef(@Const @ByRef Tensor self);




// Parsed from ATen/ops/cos.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cos_ops.h>


// aten::cos(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor cos(@Const @ByRef Tensor self);

// aten::cos_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cos_(@ByRef Tensor self);

// aten::cos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cos_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::cos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cos_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/cosh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cosh_ops.h>


// aten::cosh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor cosh(@Const @ByRef Tensor self);

// aten::cosh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cosh_(@ByRef Tensor self);

// aten::cosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cosh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::cosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cosh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/cosine_embedding_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cosine_embedding_loss_ops.h>


// aten::cosine_embedding_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor cosine_embedding_loss(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor target, double margin/*=0.0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor cosine_embedding_loss(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor target);




// Parsed from ATen/ops/cosine_similarity.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cosine_similarity_ops.h>


// aten::cosine_similarity(Tensor x1, Tensor x2, int dim=1, float eps=1e-08) -> Tensor
@Namespace("at") public static native @ByVal Tensor cosine_similarity(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2, @Cast("int64_t") long dim/*=1*/, double eps/*=1e-08*/);
@Namespace("at") public static native @ByVal Tensor cosine_similarity(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2);




// Parsed from ATen/ops/count_nonzero.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/count_nonzero_ops.h>


// aten::count_nonzero.dim_IntList(Tensor self, int[] dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor count_nonzero(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor count_nonzero(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::count_nonzero(Tensor self, int? dim=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor count_nonzero(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByVal Tensor count_nonzero(@Const @ByRef Tensor self);

// aten::count_nonzero.dim_IntList_out(Tensor self, int[] dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor count_nonzero_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor count_nonzero_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);
// aten::count_nonzero.dim_IntList_out(Tensor self, int[] dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor count_nonzero_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor count_nonzero_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByRef Tensor out);

// aten::count_nonzero.out(Tensor self, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor count_nonzero_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByRef Tensor count_nonzero_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::count_nonzero.out(Tensor self, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor count_nonzero_outf(@Const @ByRef Tensor self, @ByVal LongOptional dim, @ByRef Tensor out);




// Parsed from ATen/ops/cov.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cov_ops.h>


// aten::cov(Tensor self, *, int correction=1, Tensor? fweights=None, Tensor? aweights=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor cov(@Const @ByRef Tensor self, @Cast("int64_t") long correction/*=1*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional fweights, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional aweights);
@Namespace("at") public static native @ByVal Tensor cov(@Const @ByRef Tensor self);




// Parsed from ATen/ops/cross.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cross_ops.h>


// aten::cross.out(Tensor self, Tensor other, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cross_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByRef Tensor cross_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::cross.out(Tensor self, Tensor other, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cross_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal LongOptional dim, @ByRef Tensor out);

// aten::cross(Tensor self, Tensor other, int? dim=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor cross(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByVal Tensor cross(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/cross_entropy_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cross_entropy_loss_ops.h>


// aten::cross_entropy_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, float label_smoothing=0.0) -> Tensor
@Namespace("at") public static native @ByVal Tensor cross_entropy_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("int64_t") long ignore_index/*=-100*/, double label_smoothing/*=0.0*/);
@Namespace("at") public static native @ByVal Tensor cross_entropy_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);


// aten::cross_entropy_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, float label_smoothing=0.0) -> Tensor
@Namespace("at") public static native @ByVal Tensor cross_entropy_loss_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @ByVal(nullValue = "c10::SymInt(-100)") SymInt ignore_index, double label_smoothing/*=0.0*/);
@Namespace("at") public static native @ByVal Tensor cross_entropy_loss_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor target);





// Parsed from ATen/ops/crow_indices.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/crow_indices_ops.h>






// Parsed from ATen/ops/crow_indices_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/crow_indices_copy_ops.h>


// aten::crow_indices_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor crow_indices_copy(@Const @ByRef Tensor self);

// aten::crow_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor crow_indices_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::crow_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor crow_indices_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/ctc_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ctc_loss_ops.h>


// aten::ctc_loss.IntList(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Cast("int64_t") long blank/*=0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths);
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Cast("int64_t") long blank/*=0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... target_lengths);

// aten::ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @Const @ByRef Tensor input_lengths, @Const @ByRef Tensor target_lengths, @Cast("int64_t") long blank/*=0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @Const @ByRef Tensor input_lengths, @Const @ByRef Tensor target_lengths);




// Parsed from ATen/ops/cudnn_affine_grid_generator.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cudnn_affine_grid_generator_ops.h>


// aten::cudnn_affine_grid_generator(Tensor theta, int N, int C, int H, int W) -> Tensor grid
@Namespace("at") public static native @ByVal Tensor cudnn_affine_grid_generator(@Const @ByRef Tensor theta, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long H, @Cast("int64_t") long W);

// aten::cudnn_affine_grid_generator.out(Tensor theta, int N, int C, int H, int W, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_affine_grid_generator_out(@ByRef Tensor out, @Const @ByRef Tensor theta, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long H, @Cast("int64_t") long W);
// aten::cudnn_affine_grid_generator.out(Tensor theta, int N, int C, int H, int W, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_affine_grid_generator_outf(@Const @ByRef Tensor theta, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long H, @Cast("int64_t") long W, @ByRef Tensor out);




// Parsed from ATen/ops/cudnn_affine_grid_generator_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cudnn_affine_grid_generator_backward_ops.h>


// aten::cudnn_affine_grid_generator_backward(Tensor grad, int N, int C, int H, int W) -> Tensor grad_theta
@Namespace("at") public static native @ByVal Tensor cudnn_affine_grid_generator_backward(@Const @ByRef Tensor grad, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long H, @Cast("int64_t") long W);

// aten::cudnn_affine_grid_generator_backward.out(Tensor grad, int N, int C, int H, int W, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_affine_grid_generator_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long H, @Cast("int64_t") long W);
// aten::cudnn_affine_grid_generator_backward.out(Tensor grad, int N, int C, int H, int W, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_affine_grid_generator_backward_outf(@Const @ByRef Tensor grad, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long H, @Cast("int64_t") long W, @ByRef Tensor out);




// Parsed from ATen/ops/cudnn_batch_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cudnn_batch_norm_ops.h>


// aten::cudnn_batch_norm(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon) -> (Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple cudnn_batch_norm(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double exponential_average_factor, double epsilon);

// aten::cudnn_batch_norm.out(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cudnn_batch_norm_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double exponential_average_factor, double epsilon);
// aten::cudnn_batch_norm.out(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cudnn_batch_norm_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double exponential_average_factor, double epsilon, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3);




// Parsed from ATen/ops/cudnn_batch_norm_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cudnn_batch_norm_backward_ops.h>


// aten::cudnn_batch_norm_backward(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon, Tensor reserveSpace) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple cudnn_batch_norm_backward(@Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_var, double epsilon, @Const @ByRef Tensor reserveSpace);

// aten::cudnn_batch_norm_backward.out(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon, Tensor reserveSpace, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cudnn_batch_norm_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_var, double epsilon, @Const @ByRef Tensor reserveSpace);
// aten::cudnn_batch_norm_backward.out(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon, Tensor reserveSpace, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cudnn_batch_norm_backward_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_var, double epsilon, @Const @ByRef Tensor reserveSpace, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/cudnn_convolution.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cudnn_convolution_ops.h>


// aten::cudnn_convolution(Tensor self, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
@Namespace("at") public static native @ByVal Tensor cudnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);

// aten::cudnn_convolution.out(Tensor self, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
// aten::cudnn_convolution.out(Tensor self, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32, @ByRef Tensor out);




// Parsed from ATen/ops/cudnn_convolution_add_relu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cudnn_convolution_add_relu_ops.h>


// aten::cudnn_convolution_add_relu(Tensor self, Tensor weight, Tensor z, Scalar? alpha, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_add_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor z, @Const @ByRef ScalarOptional alpha, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_add_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor z, @Const @ByRef ScalarOptional alpha, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups);

// aten::cudnn_convolution_add_relu.out(Tensor self, Tensor weight, Tensor z, Scalar? alpha, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_add_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor z, @Const @ByRef ScalarOptional alpha, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_add_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor z, @Const @ByRef ScalarOptional alpha, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups);
// aten::cudnn_convolution_add_relu.out(Tensor self, Tensor weight, Tensor z, Scalar? alpha, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_add_relu_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor z, @Const @ByRef ScalarOptional alpha, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_add_relu_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor z, @Const @ByRef ScalarOptional alpha, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @ByRef Tensor out);




// Parsed from ATen/ops/cudnn_convolution_relu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cudnn_convolution_relu_ops.h>


// aten::cudnn_convolution_relu(Tensor self, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups);

// aten::cudnn_convolution_relu.out(Tensor self, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups);
// aten::cudnn_convolution_relu.out(Tensor self, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_relu_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_relu_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @ByRef Tensor out);




// Parsed from ATen/ops/cudnn_convolution_transpose.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cudnn_convolution_transpose_ops.h>


// aten::cudnn_convolution_transpose(Tensor self, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);

// aten::cudnn_convolution_transpose.out(Tensor self, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_transpose_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_transpose_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
// aten::cudnn_convolution_transpose.out(Tensor self, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_transpose_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor cudnn_convolution_transpose_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32, @ByRef Tensor out);




// Parsed from ATen/ops/cudnn_grid_sampler.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cudnn_grid_sampler_ops.h>


// aten::cudnn_grid_sampler(Tensor self, Tensor grid) -> Tensor output
@Namespace("at") public static native @ByVal Tensor cudnn_grid_sampler(@Const @ByRef Tensor self, @Const @ByRef Tensor grid);

// aten::cudnn_grid_sampler.out(Tensor self, Tensor grid, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_grid_sampler_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor grid);
// aten::cudnn_grid_sampler.out(Tensor self, Tensor grid, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cudnn_grid_sampler_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor grid, @ByRef Tensor out);




// Parsed from ATen/ops/cudnn_grid_sampler_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cudnn_grid_sampler_backward_ops.h>


// aten::cudnn_grid_sampler_backward(Tensor self, Tensor grid, Tensor grad_output) -> (Tensor grad_self, Tensor grad_grid)
@Namespace("at") public static native @ByVal TensorTensorTuple cudnn_grid_sampler_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grid, @Const @ByRef Tensor grad_output);

// aten::cudnn_grid_sampler_backward.out(Tensor self, Tensor grid, Tensor grad_output, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cudnn_grid_sampler_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, @Const @ByRef Tensor grid, @Const @ByRef Tensor grad_output);
// aten::cudnn_grid_sampler_backward.out(Tensor self, Tensor grid, Tensor grad_output, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cudnn_grid_sampler_backward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor grid, @Const @ByRef Tensor grad_output, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/cudnn_is_acceptable.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cudnn_is_acceptable_ops.h>


// aten::cudnn_is_acceptable(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean cudnn_is_acceptable(@Const @ByRef Tensor self);




// Parsed from ATen/ops/cummax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cummax_ops.h>


// aten::cummax(Tensor self, int dim) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple cummax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::cummax.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummax_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::cummax.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummax_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::cummax.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple cummax(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::cummax.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummax_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::cummax.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummax_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByRef Tensor values, @ByRef Tensor indices);




// Parsed from ATen/ops/cummaxmin_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cummaxmin_backward_ops.h>


// aten::cummaxmin_backward(Tensor grad, Tensor input, Tensor indices, int dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor cummaxmin_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor input, @Const @ByRef Tensor indices, @Cast("int64_t") long dim);




// Parsed from ATen/ops/cummin.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cummin_ops.h>


// aten::cummin(Tensor self, int dim) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple cummin(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::cummin.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummin_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::cummin.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummin_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::cummin.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple cummin(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::cummin.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummin_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::cummin.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummin_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByRef Tensor values, @ByRef Tensor indices);




// Parsed from ATen/ops/cumprod.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cumprod_ops.h>


// aten::cumprod(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor cumprod(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor cumprod(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::cumprod.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumprod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor cumprod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::cumprod.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumprod_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::cumprod.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor cumprod(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor cumprod(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::cumprod.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumprod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor cumprod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::cumprod.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumprod_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/cumprod_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cumprod_backward_ops.h>


// aten::cumprod_backward(Tensor grad, Tensor input, int dim, Tensor output) -> Tensor
@Namespace("at") public static native @ByVal Tensor cumprod_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor input, @Cast("int64_t") long dim, @Const @ByRef Tensor output);




// Parsed from ATen/ops/cumsum.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cumsum_ops.h>


// aten::cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor cumsum(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor cumsum(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::cumsum.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumsum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor cumsum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::cumsum.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumsum_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::cumsum.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor cumsum(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor cumsum(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::cumsum.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumsum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor cumsum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::cumsum.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor cumsum_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/cumulative_trapezoid.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/cumulative_trapezoid_ops.h>


// aten::cumulative_trapezoid.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor cumulative_trapezoid(@Const @ByRef Tensor y, @Const @ByRef Tensor x, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor cumulative_trapezoid(@Const @ByRef Tensor y, @Const @ByRef Tensor x);

// aten::cumulative_trapezoid.dx(Tensor y, *, Scalar dx=1, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor cumulative_trapezoid(@Const @ByRef Tensor y, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar dx, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor cumulative_trapezoid(@Const @ByRef Tensor y);




// Parsed from ATen/ops/data.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/data_ops.h>






// Parsed from ATen/ops/deg2rad.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/deg2rad_ops.h>


// aten::deg2rad(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor deg2rad(@Const @ByRef Tensor self);

// aten::deg2rad_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor deg2rad_(@ByRef Tensor self);

// aten::deg2rad.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor deg2rad_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::deg2rad.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor deg2rad_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/dense_dim.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/dense_dim_ops.h>






// Parsed from ATen/ops/dequantize.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/dequantize_ops.h>


// aten::dequantize.self(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor dequantize(@Const @ByRef Tensor self);

// aten::dequantize.tensors(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector dequantize(@ByVal TensorArrayRef tensors);

// aten::dequantize.self_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor dequantize_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::dequantize.self_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor dequantize_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::dequantize.tensors_out(Tensor[] tensors, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void dequantize_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef tensors);
// aten::dequantize.tensors_out(Tensor[] tensors, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void dequantize_outf(@ByVal TensorArrayRef tensors, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/det.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/det_ops.h>


// aten::det(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor det(@Const @ByRef Tensor self);




// Parsed from ATen/ops/detach.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/detach_ops.h>


// aten::detach(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor detach(@Const @ByRef Tensor self);

// aten::detach_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor detach_(@ByRef Tensor self);




// Parsed from ATen/ops/detach_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/detach_copy_ops.h>


// aten::detach_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor detach_copy(@Const @ByRef Tensor self);

// aten::detach_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor detach_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::detach_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor detach_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/diag.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/diag_ops.h>


// aten::diag.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diag_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByRef Tensor diag_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::diag.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diag_outf(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal, @ByRef Tensor out);

// aten::diag(Tensor self, int diagonal=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor diag(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByVal Tensor diag(@Const @ByRef Tensor self);




// Parsed from ATen/ops/diag_embed.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/diag_embed_ops.h>


// aten::diag_embed(Tensor self, int offset=0, int dim1=-2, int dim2=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor diag_embed(@Const @ByRef Tensor self, @Cast("int64_t") long offset/*=0*/, @Cast("int64_t") long dim1/*=-2*/, @Cast("int64_t") long dim2/*=-1*/);
@Namespace("at") public static native @ByVal Tensor diag_embed(@Const @ByRef Tensor self);

// aten::diag_embed.out(Tensor self, int offset=0, int dim1=-2, int dim2=-1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diag_embed_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long offset/*=0*/, @Cast("int64_t") long dim1/*=-2*/, @Cast("int64_t") long dim2/*=-1*/);
@Namespace("at") public static native @ByRef Tensor diag_embed_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::diag_embed.out(Tensor self, int offset=0, int dim1=-2, int dim2=-1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diag_embed_outf(@Const @ByRef Tensor self, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2, @ByRef Tensor out);




// Parsed from ATen/ops/diagflat.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/diagflat_ops.h>


// aten::diagflat(Tensor self, int offset=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor diagflat(@Const @ByRef Tensor self, @Cast("int64_t") long offset/*=0*/);
@Namespace("at") public static native @ByVal Tensor diagflat(@Const @ByRef Tensor self);




// Parsed from ATen/ops/diagonal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/diagonal_ops.h>


// aten::diagonal(Tensor(a) self, int offset=0, int dim1=0, int dim2=1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor diagonal(@Const @ByRef Tensor self, @Cast("int64_t") long offset/*=0*/, @Cast("int64_t") long dim1/*=0*/, @Cast("int64_t") long dim2/*=1*/);
@Namespace("at") public static native @ByVal Tensor diagonal(@Const @ByRef Tensor self);

// aten::diagonal.Dimname(Tensor(a) self, *, Dimname outdim, Dimname dim1, Dimname dim2, int offset=0) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor diagonal(@Const @ByRef Tensor self, @ByVal Dimname outdim, @ByVal Dimname dim1, @ByVal Dimname dim2, @Cast("int64_t") long offset/*=0*/);
@Namespace("at") public static native @ByVal Tensor diagonal(@Const @ByRef Tensor self, @ByVal Dimname outdim, @ByVal Dimname dim1, @ByVal Dimname dim2);




// Parsed from ATen/ops/diagonal_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/diagonal_backward_ops.h>


// aten::diagonal_backward(Tensor grad_output, SymInt[] input_sizes, int offset, int dim1, int dim2) -> Tensor
@Namespace("at") public static native @ByVal Tensor diagonal_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_sizes, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2);
@Namespace("at") public static native @ByVal Tensor diagonal_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_sizes, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2);


// aten::diagonal_backward(Tensor grad_output, SymInt[] input_sizes, int offset, int dim1, int dim2) -> Tensor
@Namespace("at") public static native @ByVal Tensor diagonal_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef input_sizes, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2);


// aten::diagonal_backward.out(Tensor grad_output, SymInt[] input_sizes, int offset, int dim1, int dim2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diagonal_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_sizes, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2);
@Namespace("at") public static native @ByRef Tensor diagonal_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_sizes, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2);


// aten::diagonal_backward.out(Tensor grad_output, SymInt[] input_sizes, int offset, int dim1, int dim2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diagonal_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_sizes, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor diagonal_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_sizes, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2, @ByRef Tensor out);


// aten::diagonal_backward.out(Tensor grad_output, SymInt[] input_sizes, int offset, int dim1, int dim2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diagonal_backward_symint_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @ByVal SymIntRef input_sizes, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2);


// aten::diagonal_backward.out(Tensor grad_output, SymInt[] input_sizes, int offset, int dim1, int dim2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diagonal_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntRef input_sizes, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2, @ByRef Tensor out);





// Parsed from ATen/ops/diagonal_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/diagonal_copy_ops.h>


// aten::diagonal_copy(Tensor self, int offset=0, int dim1=0, int dim2=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor diagonal_copy(@Const @ByRef Tensor self, @Cast("int64_t") long offset/*=0*/, @Cast("int64_t") long dim1/*=0*/, @Cast("int64_t") long dim2/*=1*/);
@Namespace("at") public static native @ByVal Tensor diagonal_copy(@Const @ByRef Tensor self);

// aten::diagonal_copy.out(Tensor self, int offset=0, int dim1=0, int dim2=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diagonal_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long offset/*=0*/, @Cast("int64_t") long dim1/*=0*/, @Cast("int64_t") long dim2/*=1*/);
@Namespace("at") public static native @ByRef Tensor diagonal_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::diagonal_copy.out(Tensor self, int offset=0, int dim1=0, int dim2=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diagonal_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2, @ByRef Tensor out);




// Parsed from ATen/ops/diagonal_scatter.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/diagonal_scatter_ops.h>


// aten::diagonal_scatter(Tensor self, Tensor src, int offset=0, int dim1=0, int dim2=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor diagonal_scatter(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long offset/*=0*/, @Cast("int64_t") long dim1/*=0*/, @Cast("int64_t") long dim2/*=1*/);
@Namespace("at") public static native @ByVal Tensor diagonal_scatter(@Const @ByRef Tensor self, @Const @ByRef Tensor src);

// aten::diagonal_scatter.out(Tensor self, Tensor src, int offset=0, int dim1=0, int dim2=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diagonal_scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long offset/*=0*/, @Cast("int64_t") long dim1/*=0*/, @Cast("int64_t") long dim2/*=1*/);
@Namespace("at") public static native @ByRef Tensor diagonal_scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src);
// aten::diagonal_scatter.out(Tensor self, Tensor src, int offset=0, int dim1=0, int dim2=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diagonal_scatter_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2, @ByRef Tensor out);




// Parsed from ATen/ops/diff.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/diff_ops.h>


// aten::diff(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor diff(@Const @ByRef Tensor self, @Cast("int64_t") long n/*=1*/, @Cast("int64_t") long dim/*=-1*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional prepend, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional append);
@Namespace("at") public static native @ByVal Tensor diff(@Const @ByRef Tensor self);

// aten::diff.out(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diff_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long n/*=1*/, @Cast("int64_t") long dim/*=-1*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional prepend, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional append);
@Namespace("at") public static native @ByRef Tensor diff_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::diff.out(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor diff_outf(@Const @ByRef Tensor self, @Cast("int64_t") long n, @Cast("int64_t") long dim, @Const @ByRef TensorOptional prepend, @Const @ByRef TensorOptional append, @ByRef Tensor out);




// Parsed from ATen/ops/digamma.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/digamma_ops.h>


// aten::digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor digamma_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor digamma_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::digamma(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor digamma(@Const @ByRef Tensor self);




// Parsed from ATen/ops/dist.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/dist_ops.h>


// aten::dist(Tensor self, Tensor other, Scalar p=2) -> Tensor
@Namespace("at") public static native @ByVal Tensor dist(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar p);
@Namespace("at") public static native @ByVal Tensor dist(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::dist.out(Tensor self, Tensor other, Scalar p=2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor dist_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar p);
@Namespace("at") public static native @ByRef Tensor dist_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::dist.out(Tensor self, Tensor other, Scalar p=2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor dist_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef Scalar p, @ByRef Tensor out);




// Parsed from ATen/ops/div.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/div_ops.h>


// aten::div.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor div(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor div_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor div_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::div.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor
@Namespace("at") public static native @ByVal Tensor div(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer rounding_mode);

// aten::div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor div_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer rounding_mode);
// aten::div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor div_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer rounding_mode, @ByRef Tensor out);

// aten::div.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor div(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::div.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor
@Namespace("at") public static native @ByVal Tensor div(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer rounding_mode);

// aten::div.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor div_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::div.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor div_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::div.Scalar_mode_out(Tensor self, Scalar other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor div_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer rounding_mode);
// aten::div.Scalar_mode_out(Tensor self, Scalar other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor div_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer rounding_mode, @ByRef Tensor out);




// Parsed from ATen/ops/divide.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/divide_ops.h>


// aten::divide.Tensor(Tensor self, Tensor other) -> Tensor

// aten::divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor divide_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor divide_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::divide.Scalar(Tensor self, Scalar other) -> Tensor

// aten::divide.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor
@Namespace("at") public static native @ByVal Tensor divide(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer rounding_mode);

// aten::divide.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor divide_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer rounding_mode);
// aten::divide.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor divide_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer rounding_mode, @ByRef Tensor out);

// aten::divide.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor
@Namespace("at") public static native @ByVal Tensor divide(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer rounding_mode);




// Parsed from ATen/ops/dot.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/dot_ops.h>


// aten::dot(Tensor self, Tensor tensor) -> Tensor
@Namespace("at") public static native @ByVal Tensor dot(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor);

// aten::dot.out(Tensor self, Tensor tensor, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor dot_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor tensor);
// aten::dot.out(Tensor self, Tensor tensor, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor dot_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor, @ByRef Tensor out);




// Parsed from ATen/ops/dropout.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/dropout_ops.h>


// aten::dropout(Tensor input, float p, bool train) -> Tensor
@Namespace("at") public static native @ByVal Tensor dropout(@Const @ByRef Tensor input, double p, @Cast("bool") boolean train);

// aten::dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor dropout_(@ByRef Tensor self, double p, @Cast("bool") boolean train);




// Parsed from ATen/ops/dsplit.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/dsplit_ops.h>


// aten::dsplit.int(Tensor(a -> *) self, int sections) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector dsplit(@Const @ByRef Tensor self, @Cast("int64_t") long sections);

// aten::dsplit.array(Tensor(a -> *) self, int[] indices) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector dsplit(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef indices);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector dsplit(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... indices);




// Parsed from ATen/ops/dstack.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/dstack_ops.h>


// aten::dstack(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor dstack(@ByVal TensorArrayRef tensors);

// aten::dstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor dstack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
// aten::dstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor dstack_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);




// Parsed from ATen/ops/einsum.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/einsum_ops.h>


// aten::einsum(str equation, Tensor[] tensors, *, int[]? path=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor einsum(@ByVal @Cast("c10::string_view*") Pointer equation, @ByVal TensorArrayRef tensors, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional path);
@Namespace("at") public static native @ByVal Tensor einsum(@ByVal @Cast("c10::string_view*") Pointer equation, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor einsum(@ByVal @Cast("c10::string_view*") Pointer equation, @ByVal TensorArrayRef tensors, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... path);




// Parsed from ATen/ops/elu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/elu_ops.h>


// aten::elu.out(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor elu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar scale, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar input_scale);
@Namespace("at") public static native @ByRef Tensor elu_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::elu.out(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor elu_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar alpha, @Const @ByRef Scalar scale, @Const @ByRef Scalar input_scale, @ByRef Tensor out);

// aten::elu(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor elu(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar scale, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar input_scale);
@Namespace("at") public static native @ByVal Tensor elu(@Const @ByRef Tensor self);

// aten::elu_(Tensor(a!) self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor elu_(@ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar scale, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar input_scale);
@Namespace("at") public static native @ByRef Tensor elu_(@ByRef Tensor self);




// Parsed from ATen/ops/elu_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/elu_backward_ops.h>


// aten::elu_backward.grad_input(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor elu_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Scalar alpha, @Const @ByRef Scalar scale, @Const @ByRef Scalar input_scale, @Cast("bool") boolean is_result, @Const @ByRef Tensor self_or_result);
// aten::elu_backward.grad_input(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor elu_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Scalar alpha, @Const @ByRef Scalar scale, @Const @ByRef Scalar input_scale, @Cast("bool") boolean is_result, @Const @ByRef Tensor self_or_result, @ByRef Tensor grad_input);

// aten::elu_backward(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result) -> Tensor
@Namespace("at") public static native @ByVal Tensor elu_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Scalar alpha, @Const @ByRef Scalar scale, @Const @ByRef Scalar input_scale, @Cast("bool") boolean is_result, @Const @ByRef Tensor self_or_result);




// Parsed from ATen/ops/embedding.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/embedding_ops.h>


// aten::embedding(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor embedding(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Cast("int64_t") long padding_idx/*=-1*/, @Cast("bool") boolean scale_grad_by_freq/*=false*/, @Cast("bool") boolean sparse/*=false*/);
@Namespace("at") public static native @ByVal Tensor embedding(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices);


// aten::embedding(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor embedding_symint(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @ByVal(nullValue = "c10::SymInt(-1)") SymInt padding_idx, @Cast("bool") boolean scale_grad_by_freq/*=false*/, @Cast("bool") boolean sparse/*=false*/);
@Namespace("at") public static native @ByVal Tensor embedding_symint(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices);


// aten::embedding.out(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor embedding_out(@ByRef Tensor out, @Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Cast("int64_t") long padding_idx/*=-1*/, @Cast("bool") boolean scale_grad_by_freq/*=false*/, @Cast("bool") boolean sparse/*=false*/);
@Namespace("at") public static native @ByRef Tensor embedding_out(@ByRef Tensor out, @Const @ByRef Tensor weight, @Const @ByRef Tensor indices);


// aten::embedding.out(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor embedding_outf(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Cast("int64_t") long padding_idx, @Cast("bool") boolean scale_grad_by_freq, @Cast("bool") boolean sparse, @ByRef Tensor out);


// aten::embedding.out(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor embedding_symint_out(@ByRef Tensor out, @Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @ByVal(nullValue = "c10::SymInt(-1)") SymInt padding_idx, @Cast("bool") boolean scale_grad_by_freq/*=false*/, @Cast("bool") boolean sparse/*=false*/);
@Namespace("at") public static native @ByRef Tensor embedding_symint_out(@ByRef Tensor out, @Const @ByRef Tensor weight, @Const @ByRef Tensor indices);


// aten::embedding.out(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor embedding_symint_outf(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @ByVal SymInt padding_idx, @Cast("bool") boolean scale_grad_by_freq, @Cast("bool") boolean sparse, @ByRef Tensor out);





// Parsed from ATen/ops/embedding_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/embedding_backward_ops.h>


// aten::embedding_backward(Tensor grad, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq, bool sparse) -> Tensor
@Namespace("at") public static native @ByVal Tensor embedding_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Cast("int64_t") long num_weights, @Cast("int64_t") long padding_idx, @Cast("bool") boolean scale_grad_by_freq, @Cast("bool") boolean sparse);


// aten::embedding_backward(Tensor grad, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq, bool sparse) -> Tensor
@Namespace("at") public static native @ByVal Tensor embedding_backward_symint(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @ByVal SymInt num_weights, @ByVal SymInt padding_idx, @Cast("bool") boolean scale_grad_by_freq, @Cast("bool") boolean sparse);





// Parsed from ATen/ops/embedding_bag.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/embedding_bag_ops.h>


// aten::embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False) -> (Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple embedding_bag(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Cast("bool") boolean scale_grad_by_freq/*=false*/, @Cast("int64_t") long mode/*=0*/, @Cast("bool") boolean sparse/*=false*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional per_sample_weights, @Cast("bool") boolean include_last_offset/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple embedding_bag(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets);

// aten::embedding_bag.padding_idx(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, bool include_last_offset, int? padding_idx) -> (Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple embedding_bag(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Cast("bool") boolean sparse, @Const @ByRef TensorOptional per_sample_weights, @Cast("bool") boolean include_last_offset, @ByVal LongOptional padding_idx);




// Parsed from ATen/ops/embedding_dense_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/embedding_dense_backward_ops.h>


// aten::embedding_dense_backward(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq) -> Tensor
@Namespace("at") public static native @ByVal Tensor embedding_dense_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor indices, @Cast("int64_t") long num_weights, @Cast("int64_t") long padding_idx, @Cast("bool") boolean scale_grad_by_freq);


// aten::embedding_dense_backward(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq) -> Tensor
@Namespace("at") public static native @ByVal Tensor embedding_dense_backward_symint(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor indices, @ByVal SymInt num_weights, @ByVal SymInt padding_idx, @Cast("bool") boolean scale_grad_by_freq);


// aten::embedding_dense_backward.out(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor embedding_dense_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor indices, @Cast("int64_t") long num_weights, @Cast("int64_t") long padding_idx, @Cast("bool") boolean scale_grad_by_freq);


// aten::embedding_dense_backward.out(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor embedding_dense_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor indices, @Cast("int64_t") long num_weights, @Cast("int64_t") long padding_idx, @Cast("bool") boolean scale_grad_by_freq, @ByRef Tensor out);


// aten::embedding_dense_backward.out(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor embedding_dense_backward_symint_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor indices, @ByVal SymInt num_weights, @ByVal SymInt padding_idx, @Cast("bool") boolean scale_grad_by_freq);


// aten::embedding_dense_backward.out(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor embedding_dense_backward_symint_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor indices, @ByVal SymInt num_weights, @ByVal SymInt padding_idx, @Cast("bool") boolean scale_grad_by_freq, @ByRef Tensor out);





// Parsed from ATen/ops/embedding_renorm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/embedding_renorm_ops.h>


// aten::embedding_renorm_(Tensor(a!) self, Tensor indices, float max_norm, float norm_type) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor embedding_renorm_(@ByRef Tensor self, @Const @ByRef Tensor indices, double max_norm, double norm_type);

// aten::embedding_renorm.out(Tensor self, Tensor indices, float max_norm, float norm_type, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor embedding_renorm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, double max_norm, double norm_type);
// aten::embedding_renorm.out(Tensor self, Tensor indices, float max_norm, float norm_type, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor embedding_renorm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, double max_norm, double norm_type, @ByRef Tensor out);

// aten::embedding_renorm(Tensor self, Tensor indices, float max_norm, float norm_type) -> Tensor
@Namespace("at") public static native @ByVal Tensor embedding_renorm(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, double max_norm, double norm_type);




// Parsed from ATen/ops/embedding_sparse_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/embedding_sparse_backward_ops.h>


// aten::embedding_sparse_backward(Tensor grad, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq) -> Tensor
@Namespace("at") public static native @ByVal Tensor embedding_sparse_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Cast("int64_t") long num_weights, @Cast("int64_t") long padding_idx, @Cast("bool") boolean scale_grad_by_freq);




// Parsed from ATen/ops/empty.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/empty_ops.h>


// aten::empty.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);
// aten::empty.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);


// aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_symint(@ByVal SymIntRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty_symint(@ByVal SymIntRef size);


// aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_symint(@ByVal SymIntRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);


// aten::empty.out(SymInt[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::empty.out(SymInt[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor empty_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);


// aten::empty.out(SymInt[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_symint_out(@ByRef Tensor out, @ByVal SymIntRef size, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor empty_symint_out(@ByRef Tensor out, @ByVal SymIntRef size);


// aten::empty.out(SymInt[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_symint_outf(@ByVal SymIntRef size, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);


// aten::empty.names_out(int[] size, *, Dimname[]? names, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);
// aten::empty.names_out(int[] size, *, Dimname[]? names, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor empty_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);




// Parsed from ATen/ops/empty_like.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/empty_like_ops.h>


// aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty_like(@Const @ByRef Tensor self);
// aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_like(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::empty_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_like_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor empty_like_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::empty_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_like_outf(@Const @ByRef Tensor self, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);




// Parsed from ATen/ops/empty_quantized.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/empty_quantized_ops.h>


// aten::empty_quantized(int[] size, Tensor qtensor, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor qtensor, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor qtensor);
@Namespace("at") public static native @ByVal Tensor empty_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor qtensor, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor qtensor);
// aten::empty_quantized(int[] size, Tensor qtensor, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor qtensor, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor qtensor, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::empty_quantized.out(int[] size, Tensor qtensor, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_quantized_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor qtensor, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor empty_quantized_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor qtensor);
@Namespace("at") public static native @ByRef Tensor empty_quantized_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor qtensor, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor empty_quantized_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor qtensor);
// aten::empty_quantized.out(int[] size, Tensor qtensor, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_quantized_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor qtensor, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor empty_quantized_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor qtensor, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);




// Parsed from ATen/ops/empty_strided.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/empty_strided_ops.h>


// aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);


// aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_strided_symint(@ByVal SymIntRef size, @ByVal SymIntRef stride, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor empty_strided_symint(@ByVal SymIntRef size, @ByVal SymIntRef stride);


// aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor empty_strided_symint(@ByVal SymIntRef size, @ByVal SymIntRef stride, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::empty_strided.out(SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_strided_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor empty_strided_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);


// aten::empty_strided.out(SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_strided_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor empty_strided_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByRef Tensor out);


// aten::empty_strided.out(SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_strided_symint_out(@ByRef Tensor out, @ByVal SymIntRef size, @ByVal SymIntRef stride);


// aten::empty_strided.out(SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor empty_strided_symint_outf(@ByVal SymIntRef size, @ByVal SymIntRef stride, @ByRef Tensor out);





// Parsed from ATen/ops/eq.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/eq_ops.h>


// aten::eq.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eq_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::eq.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eq_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::eq.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor eq(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::eq.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eq_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::eq.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eq_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::eq.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor eq(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/equal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/equal_ops.h>


// aten::equal(Tensor self, Tensor other) -> bool
@Namespace("at") public static native @Cast("bool") boolean equal(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/erf.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/erf_ops.h>


// aten::erf(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor erf(@Const @ByRef Tensor self);

// aten::erf_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erf_(@ByRef Tensor self);

// aten::erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erf_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erf_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/erfc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/erfc_ops.h>


// aten::erfc(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor erfc(@Const @ByRef Tensor self);

// aten::erfc_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erfc_(@ByRef Tensor self);

// aten::erfc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erfc_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::erfc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erfc_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/erfinv.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/erfinv_ops.h>


// aten::erfinv(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor erfinv(@Const @ByRef Tensor self);

// aten::erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erfinv_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor erfinv_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/exp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/exp_ops.h>


// aten::exp(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor exp(@Const @ByRef Tensor self);

// aten::exp_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor exp_(@ByRef Tensor self);

// aten::exp.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor exp_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::exp.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor exp_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/exp2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/exp2_ops.h>


// aten::exp2(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor exp2(@Const @ByRef Tensor self);

// aten::exp2_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor exp2_(@ByRef Tensor self);

// aten::exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor exp2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor exp2_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/expand.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/expand_ops.h>






// Parsed from ATen/ops/expand_as.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/expand_as_ops.h>






// Parsed from ATen/ops/expand_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/expand_copy_ops.h>


// aten::expand_copy(Tensor self, SymInt[] size, *, bool implicit=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor expand_copy(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Cast("bool") boolean implicit/*=false*/);
@Namespace("at") public static native @ByVal Tensor expand_copy(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor expand_copy(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Cast("bool") boolean implicit/*=false*/);
@Namespace("at") public static native @ByVal Tensor expand_copy(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::expand_copy(Tensor self, SymInt[] size, *, bool implicit=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor expand_copy_symint(@Const @ByRef Tensor self, @ByVal SymIntRef size, @Cast("bool") boolean implicit/*=false*/);
@Namespace("at") public static native @ByVal Tensor expand_copy_symint(@Const @ByRef Tensor self, @ByVal SymIntRef size);


// aten::expand_copy.out(Tensor self, SymInt[] size, *, bool implicit=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor expand_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Cast("bool") boolean implicit/*=false*/);
@Namespace("at") public static native @ByRef Tensor expand_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor expand_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Cast("bool") boolean implicit/*=false*/);
@Namespace("at") public static native @ByRef Tensor expand_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::expand_copy.out(Tensor self, SymInt[] size, *, bool implicit=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor expand_copy_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Cast("bool") boolean implicit, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor expand_copy_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Cast("bool") boolean implicit, @ByRef Tensor out);


// aten::expand_copy.out(Tensor self, SymInt[] size, *, bool implicit=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor expand_copy_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef size, @Cast("bool") boolean implicit/*=false*/);
@Namespace("at") public static native @ByRef Tensor expand_copy_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef size);


// aten::expand_copy.out(Tensor self, SymInt[] size, *, bool implicit=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor expand_copy_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef size, @Cast("bool") boolean implicit, @ByRef Tensor out);





// Parsed from ATen/ops/expm1.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/expm1_ops.h>


// aten::expm1(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor expm1(@Const @ByRef Tensor self);

// aten::expm1_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor expm1_(@ByRef Tensor self);

// aten::expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor expm1_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor expm1_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/exponential.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/exponential_ops.h>


// aten::exponential.out(Tensor self, float lambd=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor exponential_out(@ByRef Tensor out, @Const @ByRef Tensor self, double lambd/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor exponential_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::exponential.out(Tensor self, float lambd=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor exponential_outf(@Const @ByRef Tensor self, double lambd, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::exponential(Tensor self, float lambd=1, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor exponential(@Const @ByRef Tensor self, double lambd/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor exponential(@Const @ByRef Tensor self);




// Parsed from ATen/ops/eye.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/eye_ops.h>


// aten::eye(int n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n);
// aten::eye(int n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::eye.m(int n, int m, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n, @Cast("int64_t") long m, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n, @Cast("int64_t") long m);
// aten::eye.m(int n, int m, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n, @Cast("int64_t") long m, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::eye.out(int n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eye_out(@ByRef Tensor out, @Cast("int64_t") long n);
// aten::eye.out(int n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eye_outf(@Cast("int64_t") long n, @ByRef Tensor out);

// aten::eye.m_out(int n, int m, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eye_out(@ByRef Tensor out, @Cast("int64_t") long n, @Cast("int64_t") long m);
// aten::eye.m_out(int n, int m, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor eye_outf(@Cast("int64_t") long n, @Cast("int64_t") long m, @ByRef Tensor out);




// Parsed from ATen/ops/fake_quantize_per_channel_affine.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fake_quantize_per_channel_affine_ops.h>


// aten::fake_quantize_per_channel_affine(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max) -> Tensor
@Namespace("at") public static native @ByVal Tensor fake_quantize_per_channel_affine(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);




// Parsed from ATen/ops/fake_quantize_per_channel_affine_cachemask.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fake_quantize_per_channel_affine_cachemask_ops.h>


// aten::fake_quantize_per_channel_affine_cachemask(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max) -> (Tensor output, Tensor mask)
@Namespace("at") public static native @ByVal TensorTensorTuple fake_quantize_per_channel_affine_cachemask(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);

// aten::fake_quantize_per_channel_affine_cachemask.out(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fake_quantize_per_channel_affine_cachemask_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);
// aten::fake_quantize_per_channel_affine_cachemask.out(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fake_quantize_per_channel_affine_cachemask_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/fake_quantize_per_channel_affine_cachemask_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fake_quantize_per_channel_affine_cachemask_backward_ops.h>


// aten::fake_quantize_per_channel_affine_cachemask_backward(Tensor grad, Tensor mask) -> Tensor
@Namespace("at") public static native @ByVal Tensor fake_quantize_per_channel_affine_cachemask_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor mask);




// Parsed from ATen/ops/fake_quantize_per_tensor_affine.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fake_quantize_per_tensor_affine_ops.h>


// aten::fake_quantize_per_tensor_affine(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -> Tensor
@Namespace("at") public static native @ByVal Tensor fake_quantize_per_tensor_affine(@Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);

// aten::fake_quantize_per_tensor_affine.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max) -> Tensor
@Namespace("at") public static native @ByVal Tensor fake_quantize_per_tensor_affine(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);




// Parsed from ATen/ops/fake_quantize_per_tensor_affine_cachemask.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fake_quantize_per_tensor_affine_cachemask_ops.h>


// aten::fake_quantize_per_tensor_affine_cachemask(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -> (Tensor output, Tensor mask)
@Namespace("at") public static native @ByVal TensorTensorTuple fake_quantize_per_tensor_affine_cachemask(@Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);

// aten::fake_quantize_per_tensor_affine_cachemask.out(Tensor self, float scale, int zero_point, int quant_min, int quant_max, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fake_quantize_per_tensor_affine_cachemask_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);
// aten::fake_quantize_per_tensor_affine_cachemask.out(Tensor self, float scale, int zero_point, int quant_min, int quant_max, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fake_quantize_per_tensor_affine_cachemask_outf(@Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/fake_quantize_per_tensor_affine_cachemask_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fake_quantize_per_tensor_affine_cachemask_backward_ops.h>


// aten::fake_quantize_per_tensor_affine_cachemask_backward(Tensor grad, Tensor mask) -> Tensor
@Namespace("at") public static native @ByVal Tensor fake_quantize_per_tensor_affine_cachemask_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor mask);




// Parsed from ATen/ops/fbgemm_linear_fp16_weight.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fbgemm_linear_fp16_weight_ops.h>


// aten::fbgemm_linear_fp16_weight(Tensor input, Tensor packed_weight, Tensor bias) -> Tensor
@Namespace("at") public static native @ByVal Tensor fbgemm_linear_fp16_weight(@Const @ByRef Tensor input, @Const @ByRef Tensor packed_weight, @Const @ByRef Tensor bias);




// Parsed from ATen/ops/fbgemm_linear_fp16_weight_fp32_activation.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fbgemm_linear_fp16_weight_fp32_activation_ops.h>


// aten::fbgemm_linear_fp16_weight_fp32_activation(Tensor input, Tensor packed_weight, Tensor bias) -> Tensor
@Namespace("at") public static native @ByVal Tensor fbgemm_linear_fp16_weight_fp32_activation(@Const @ByRef Tensor input, @Const @ByRef Tensor packed_weight, @Const @ByRef Tensor bias);




// Parsed from ATen/ops/fbgemm_linear_int8_weight.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fbgemm_linear_int8_weight_ops.h>


// aten::fbgemm_linear_int8_weight(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -> Tensor
@Namespace("at") public static native @ByVal Tensor fbgemm_linear_int8_weight(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef Tensor packed, @Const @ByRef Tensor col_offsets, @Const @ByRef Scalar weight_scale, @Const @ByRef Scalar weight_zero_point, @Const @ByRef Tensor bias);




// Parsed from ATen/ops/fbgemm_linear_int8_weight_fp32_activation.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fbgemm_linear_int8_weight_fp32_activation_ops.h>


// aten::fbgemm_linear_int8_weight_fp32_activation(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -> Tensor
@Namespace("at") public static native @ByVal Tensor fbgemm_linear_int8_weight_fp32_activation(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef Tensor packed, @Const @ByRef Tensor col_offsets, @Const @ByRef Scalar weight_scale, @Const @ByRef Scalar weight_zero_point, @Const @ByRef Tensor bias);




// Parsed from ATen/ops/fbgemm_linear_quantize_weight.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fbgemm_linear_quantize_weight_ops.h>


// aten::fbgemm_linear_quantize_weight(Tensor input) -> (Tensor, Tensor, float, int)
@Namespace("at") public static native @ByVal TensorTensorDoubleLongTuple fbgemm_linear_quantize_weight(@Const @ByRef Tensor input);




// Parsed from ATen/ops/fbgemm_pack_gemm_matrix_fp16.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fbgemm_pack_gemm_matrix_fp16_ops.h>


// aten::fbgemm_pack_gemm_matrix_fp16(Tensor input) -> Tensor
@Namespace("at") public static native @ByVal Tensor fbgemm_pack_gemm_matrix_fp16(@Const @ByRef Tensor input);




// Parsed from ATen/ops/fbgemm_pack_quantized_matrix.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fbgemm_pack_quantized_matrix_ops.h>


// aten::fbgemm_pack_quantized_matrix(Tensor input) -> Tensor
@Namespace("at") public static native @ByVal Tensor fbgemm_pack_quantized_matrix(@Const @ByRef Tensor input);

// aten::fbgemm_pack_quantized_matrix.KN(Tensor input, int K, int N) -> Tensor
@Namespace("at") public static native @ByVal Tensor fbgemm_pack_quantized_matrix(@Const @ByRef Tensor input, @Cast("int64_t") long K, @Cast("int64_t") long N);




// Parsed from ATen/ops/feature_alpha_dropout.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/feature_alpha_dropout_ops.h>


// aten::feature_alpha_dropout(Tensor input, float p, bool train) -> Tensor
@Namespace("at") public static native @ByVal Tensor feature_alpha_dropout(@Const @ByRef Tensor input, double p, @Cast("bool") boolean train);

// aten::feature_alpha_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor feature_alpha_dropout_(@ByRef Tensor self, double p, @Cast("bool") boolean train);




// Parsed from ATen/ops/feature_dropout.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/feature_dropout_ops.h>


// aten::feature_dropout(Tensor input, float p, bool train) -> Tensor
@Namespace("at") public static native @ByVal Tensor feature_dropout(@Const @ByRef Tensor input, double p, @Cast("bool") boolean train);

// aten::feature_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor feature_dropout_(@ByRef Tensor self, double p, @Cast("bool") boolean train);




// Parsed from ATen/ops/fft_fft.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_fft_ops.h>


// aten::fft_fft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_fft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_fft(@Const @ByRef Tensor self);

// aten::fft_fft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_fft_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::fft_fft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);




// Parsed from ATen/ops/fft_fft2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_fft2_ops.h>


// aten::fft_fft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_fft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_fft2(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_fft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);

// aten::fft_fft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_fft2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_fft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
// aten::fft_fft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_fft2_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);




// Parsed from ATen/ops/fft_fftfreq.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_fftfreq_ops.h>


// aten::fft_fftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_fftfreq(@Cast("int64_t") long n, double d/*=1.0*/, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor fft_fftfreq(@Cast("int64_t") long n);
// aten::fft_fftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_fftfreq(@Cast("int64_t") long n, double d, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::fft_fftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fftfreq_out(@ByRef Tensor out, @Cast("int64_t") long n, double d/*=1.0*/);
@Namespace("at") public static native @ByRef Tensor fft_fftfreq_out(@ByRef Tensor out, @Cast("int64_t") long n);
// aten::fft_fftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fftfreq_outf(@Cast("int64_t") long n, double d, @ByRef Tensor out);




// Parsed from ATen/ops/fft_fftn.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_fftn_ops.h>


// aten::fft_fftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_fftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_fftn(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_fftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);

// aten::fft_fftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_fftn_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_fftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
// aten::fft_fftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_fftn_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_fftn_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);




// Parsed from ATen/ops/fft_fftshift.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_fftshift_ops.h>


// aten::fft_fftshift(Tensor self, int[1]? dim=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_fftshift(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim);
@Namespace("at") public static native @ByVal Tensor fft_fftshift(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_fftshift(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);




// Parsed from ATen/ops/fft_hfft.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_hfft_ops.h>


// aten::fft_hfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_hfft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_hfft(@Const @ByRef Tensor self);

// aten::fft_hfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_hfft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_hfft_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::fft_hfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_hfft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);




// Parsed from ATen/ops/fft_hfft2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_hfft2_ops.h>


// aten::fft_hfft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_hfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_hfft2(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_hfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);

// aten::fft_hfft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_hfft2_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @Const @ByRef Tensor fft_hfft2_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @Const @ByRef Tensor fft_hfft2_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
// aten::fft_hfft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_hfft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @Const @ByRef Tensor out);
@Namespace("at") public static native @Const @ByRef Tensor fft_hfft2_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @Const @ByRef Tensor out);




// Parsed from ATen/ops/fft_hfftn.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_hfftn_ops.h>


// aten::fft_hfftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_hfftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_hfftn(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_hfftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);

// aten::fft_hfftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_hfftn_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @Const @ByRef Tensor fft_hfftn_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @Const @ByRef Tensor fft_hfftn_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
// aten::fft_hfftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_hfftn_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @Const @ByRef Tensor out);
@Namespace("at") public static native @Const @ByRef Tensor fft_hfftn_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @Const @ByRef Tensor out);




// Parsed from ATen/ops/fft_ifft.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_ifft_ops.h>


// aten::fft_ifft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ifft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_ifft(@Const @ByRef Tensor self);

// aten::fft_ifft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_ifft_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::fft_ifft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);




// Parsed from ATen/ops/fft_ifft2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_ifft2_ops.h>


// aten::fft_ifft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ifft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_ifft2(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_ifft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);

// aten::fft_ifft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_ifft2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_ifft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
// aten::fft_ifft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_ifft2_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);




// Parsed from ATen/ops/fft_ifftn.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_ifftn_ops.h>


// aten::fft_ifftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ifftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_ifftn(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_ifftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);

// aten::fft_ifftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_ifftn_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_ifftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
// aten::fft_ifftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ifftn_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_ifftn_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);




// Parsed from ATen/ops/fft_ifftshift.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_ifftshift_ops.h>


// aten::fft_ifftshift(Tensor self, int[1]? dim=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ifftshift(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim);
@Namespace("at") public static native @ByVal Tensor fft_ifftshift(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_ifftshift(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);




// Parsed from ATen/ops/fft_ihfft.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_ihfft_ops.h>


// aten::fft_ihfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ihfft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_ihfft(@Const @ByRef Tensor self);

// aten::fft_ihfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ihfft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_ihfft_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::fft_ihfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_ihfft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);




// Parsed from ATen/ops/fft_ihfft2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_ihfft2_ops.h>


// aten::fft_ihfft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ihfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_ihfft2(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_ihfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);

// aten::fft_ihfft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfft2_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfft2_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfft2_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
// aten::fft_ihfft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @Const @ByRef Tensor out);
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfft2_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @Const @ByRef Tensor out);




// Parsed from ATen/ops/fft_ihfftn.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_ihfftn_ops.h>


// aten::fft_ihfftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_ihfftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_ihfftn(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_ihfftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);

// aten::fft_ihfftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfftn_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfftn_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfftn_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
// aten::fft_ihfftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfftn_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @Const @ByRef Tensor out);
@Namespace("at") public static native @Const @ByRef Tensor fft_ihfftn_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @Const @ByRef Tensor out);




// Parsed from ATen/ops/fft_irfft.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_irfft_ops.h>


// aten::fft_irfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_irfft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_irfft(@Const @ByRef Tensor self);

// aten::fft_irfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_irfft_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::fft_irfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);




// Parsed from ATen/ops/fft_irfft2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_irfft2_ops.h>


// aten::fft_irfft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_irfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_irfft2(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_irfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);

// aten::fft_irfft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_irfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_irfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
// aten::fft_irfft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_irfft2_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);




// Parsed from ATen/ops/fft_irfftn.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_irfftn_ops.h>


// aten::fft_irfftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_irfftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_irfftn(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_irfftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);

// aten::fft_irfftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_irfftn_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_irfftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
// aten::fft_irfftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_irfftn_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_irfftn_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);




// Parsed from ATen/ops/fft_rfft.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_rfft_ops.h>


// aten::fft_rfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_rfft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_rfft(@Const @ByRef Tensor self);

// aten::fft_rfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_rfft_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::fft_rfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);




// Parsed from ATen/ops/fft_rfft2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_rfft2_ops.h>


// aten::fft_rfft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_rfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_rfft2(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_rfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);

// aten::fft_rfft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_rfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_rfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
// aten::fft_rfft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_rfft2_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);




// Parsed from ATen/ops/fft_rfftfreq.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_rfftfreq_ops.h>


// aten::fft_rfftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_rfftfreq(@Cast("int64_t") long n, double d/*=1.0*/, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor fft_rfftfreq(@Cast("int64_t") long n);
// aten::fft_rfftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_rfftfreq(@Cast("int64_t") long n, double d, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::fft_rfftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfftfreq_out(@ByRef Tensor out, @Cast("int64_t") long n, double d/*=1.0*/);
@Namespace("at") public static native @ByRef Tensor fft_rfftfreq_out(@ByRef Tensor out, @Cast("int64_t") long n);
// aten::fft_rfftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfftfreq_outf(@Cast("int64_t") long n, double d, @ByRef Tensor out);




// Parsed from ATen/ops/fft_rfftn.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fft_rfftn_ops.h>


// aten::fft_rfftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor fft_rfftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByVal Tensor fft_rfftn(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_rfftn(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);

// aten::fft_rfftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
@Namespace("at") public static native @ByRef Tensor fft_rfftn_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_rfftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer norm);
// aten::fft_rfftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fft_rfftn_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_rfftn_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] s, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer norm, @ByRef Tensor out);




// Parsed from ATen/ops/fill.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fill_ops.h>


// aten::fill.Scalar(Tensor self, Scalar value) -> Tensor
@Namespace("at") public static native @ByVal @Name("fill") Tensor _fill(@Const @ByRef Tensor self, @Const @ByRef Scalar value);

// aten::fill.Tensor(Tensor self, Tensor value) -> Tensor
@Namespace("at") public static native @ByVal @Name("fill") Tensor _fill(@Const @ByRef Tensor self, @Const @ByRef Tensor value);

// aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fill_(@ByRef Tensor self, @Const @ByRef Scalar value);

// aten::fill_.Tensor(Tensor(a!) self, Tensor value) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fill_(@ByRef Tensor self, @Const @ByRef Tensor value);

// aten::fill.Scalar_out(Tensor self, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fill_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar value);
// aten::fill.Scalar_out(Tensor self, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fill_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar value, @ByRef Tensor out);

// aten::fill.Tensor_out(Tensor self, Tensor value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fill_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor value);
// aten::fill.Tensor_out(Tensor self, Tensor value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fill_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor value, @ByRef Tensor out);




// Parsed from ATen/ops/fill_diagonal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fill_diagonal_ops.h>






// Parsed from ATen/ops/fix.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fix_ops.h>


// aten::fix(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor fix(@Const @ByRef Tensor self);

// aten::fix_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fix_(@ByRef Tensor self);

// aten::fix.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fix_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::fix.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fix_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/flatten.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/flatten_ops.h>


// aten::flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor flatten(@Const @ByRef Tensor self, @Cast("int64_t") long start_dim/*=0*/, @Cast("int64_t") long end_dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor flatten(@Const @ByRef Tensor self);

// aten::flatten.named_out_dim(Tensor(a) self, int start_dim, int end_dim, Dimname out_dim) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor flatten(@Const @ByRef Tensor self, @Cast("int64_t") long start_dim, @Cast("int64_t") long end_dim, @ByVal Dimname out_dim);

// aten::flatten.using_names(Tensor(a) self, Dimname start_dim, Dimname end_dim, Dimname out_dim) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor flatten(@Const @ByRef Tensor self, @ByVal Dimname start_dim, @ByVal Dimname end_dim, @ByVal Dimname out_dim);

// aten::flatten.DimnameList(Tensor(a) self, Dimname[] dims, Dimname out_dim) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor flatten(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dims, @ByVal Dimname out_dim);




// Parsed from ATen/ops/flatten_dense_tensors.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/flatten_dense_tensors_ops.h>


// aten::flatten_dense_tensors(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor flatten_dense_tensors(@ByVal TensorArrayRef tensors);




// Parsed from ATen/ops/flip.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/flip_ops.h>


// aten::flip(Tensor self, int[] dims) -> Tensor
@Namespace("at") public static native @ByVal Tensor flip(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("at") public static native @ByVal Tensor flip(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);

// aten::flip.out(Tensor self, int[] dims, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor flip_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("at") public static native @ByRef Tensor flip_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);
// aten::flip.out(Tensor self, int[] dims, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor flip_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor flip_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dims, @ByRef Tensor out);




// Parsed from ATen/ops/fliplr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fliplr_ops.h>


// aten::fliplr(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor fliplr(@Const @ByRef Tensor self);




// Parsed from ATen/ops/flipud.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/flipud_ops.h>


// aten::flipud(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor flipud(@Const @ByRef Tensor self);




// Parsed from ATen/ops/float_power.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/float_power_ops.h>


// aten::float_power.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor float_power_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor exponent);
// aten::float_power.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor float_power_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor exponent, @ByRef Tensor out);

// aten::float_power.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor
@Namespace("at") public static native @ByVal Tensor float_power(@Const @ByRef Tensor self, @Const @ByRef Tensor exponent);

// aten::float_power.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor float_power_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor exponent);
// aten::float_power.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor float_power_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor exponent, @ByRef Tensor out);

// aten::float_power.Scalar(Scalar self, Tensor exponent) -> Tensor
@Namespace("at") public static native @ByVal Tensor float_power(@Const @ByRef Scalar self, @Const @ByRef Tensor exponent);

// aten::float_power.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor float_power_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar exponent);
// aten::float_power.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor float_power_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar exponent, @ByRef Tensor out);

// aten::float_power.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor
@Namespace("at") public static native @ByVal Tensor float_power(@Const @ByRef Tensor self, @Const @ByRef Scalar exponent);




// Parsed from ATen/ops/floor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/floor_ops.h>


// aten::floor(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor floor(@Const @ByRef Tensor self);

// aten::floor_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor floor_(@ByRef Tensor self);

// aten::floor.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor floor_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::floor.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor floor_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/floor_divide.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/floor_divide_ops.h>


// aten::floor_divide(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor floor_divide(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::floor_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor floor_divide_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::floor_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor floor_divide_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::floor_divide.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor floor_divide(@Const @ByRef Tensor self, @Const @ByRef Scalar other);




// Parsed from ATen/ops/fmax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fmax_ops.h>


// aten::fmax(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor fmax(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::fmax.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::fmax.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmax_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/fmin.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fmin_ops.h>


// aten::fmin(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor fmin(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::fmin.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmin_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::fmin.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmin_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/fmod.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fmod_ops.h>


// aten::fmod.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::fmod.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmod_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::fmod.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor fmod(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::fmod.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::fmod.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fmod_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::fmod.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor fmod(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/frac.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/frac_ops.h>


// aten::frac(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor frac(@Const @ByRef Tensor self);

// aten::frac_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor frac_(@ByRef Tensor self);

// aten::frac.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor frac_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::frac.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor frac_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/fractional_max_pool2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fractional_max_pool2d_ops.h>


// aten::fractional_max_pool2d.output(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool2d_out(@ByRef Tensor output, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor random_samples);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool2d_out(@ByRef Tensor output, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor random_samples);
// aten::fractional_max_pool2d.output(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor random_samples, @ByRef Tensor output, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor random_samples, @ByRef Tensor output, @ByRef Tensor indices);

// aten::fractional_max_pool2d(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple fractional_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor random_samples);
@Namespace("at") public static native @ByVal TensorTensorTuple fractional_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor random_samples);




// Parsed from ATen/ops/fractional_max_pool2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fractional_max_pool2d_backward_ops.h>


// aten::fractional_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] output_size, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fractional_max_pool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor fractional_max_pool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor indices);
// aten::fractional_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] output_size, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fractional_max_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor fractional_max_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);

// aten::fractional_max_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] output_size, Tensor indices) -> Tensor
@Namespace("at") public static native @ByVal Tensor fractional_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByVal Tensor fractional_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor indices);




// Parsed from ATen/ops/fractional_max_pool3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fractional_max_pool3d_ops.h>


// aten::fractional_max_pool3d.output(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool3d_out(@ByRef Tensor output, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor random_samples);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool3d_out(@ByRef Tensor output, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor random_samples);
// aten::fractional_max_pool3d.output(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor random_samples, @ByRef Tensor output, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor random_samples, @ByRef Tensor output, @ByRef Tensor indices);

// aten::fractional_max_pool3d(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple fractional_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor random_samples);
@Namespace("at") public static native @ByVal TensorTensorTuple fractional_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor random_samples);




// Parsed from ATen/ops/fractional_max_pool3d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fractional_max_pool3d_backward_ops.h>


// aten::fractional_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] output_size, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fractional_max_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor fractional_max_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor indices);
// aten::fractional_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] output_size, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor fractional_max_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor fractional_max_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);

// aten::fractional_max_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] output_size, Tensor indices) -> Tensor
@Namespace("at") public static native @ByVal Tensor fractional_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByVal Tensor fractional_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor indices);




// Parsed from ATen/ops/frexp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/frexp_ops.h>


// aten::frexp.Tensor(Tensor self) -> (Tensor mantissa, Tensor exponent)
@Namespace("at") public static native @ByVal TensorTensorTuple frexp(@Const @ByRef Tensor self);

// aten::frexp.Tensor_out(Tensor self, *, Tensor(a!) mantissa, Tensor(b!) exponent) -> (Tensor(a!) mantissa, Tensor(b!) exponent)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> frexp_out(@ByRef Tensor mantissa, @ByRef Tensor exponent, @Const @ByRef Tensor self);
// aten::frexp.Tensor_out(Tensor self, *, Tensor(a!) mantissa, Tensor(b!) exponent) -> (Tensor(a!) mantissa, Tensor(b!) exponent)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> frexp_outf(@Const @ByRef Tensor self, @ByRef Tensor mantissa, @ByRef Tensor exponent);




// Parsed from ATen/ops/frobenius_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/frobenius_norm_ops.h>


// aten::frobenius_norm.dim(Tensor self, int[1] dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor frobenius_norm(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor frobenius_norm(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor frobenius_norm(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor frobenius_norm(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::frobenius_norm.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor frobenius_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor frobenius_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor frobenius_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor frobenius_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);
// aten::frobenius_norm.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor frobenius_norm_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor frobenius_norm_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);




// Parsed from ATen/ops/from_file.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/from_file_ops.h>


// aten::from_file(str filename, bool? shared=None, int? size=0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor from_file(@ByVal @Cast("c10::string_view*") Pointer filename, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional shared, @ByVal(nullValue = "c10::optional<int64_t>(0)") LongOptional size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_file(@ByVal @Cast("c10::string_view*") Pointer filename);
// aten::from_file(str filename, bool? shared=None, int? size=0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor from_file(@ByVal @Cast("c10::string_view*") Pointer filename, @ByVal BoolOptional shared, @ByVal LongOptional size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::from_file.out(str filename, bool? shared=None, int? size=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor from_file_out(@ByRef Tensor out, @ByVal @Cast("c10::string_view*") Pointer filename, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional shared, @ByVal(nullValue = "c10::optional<int64_t>(0)") LongOptional size);
@Namespace("at") public static native @ByRef Tensor from_file_out(@ByRef Tensor out, @ByVal @Cast("c10::string_view*") Pointer filename);
// aten::from_file.out(str filename, bool? shared=None, int? size=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor from_file_outf(@ByVal @Cast("c10::string_view*") Pointer filename, @ByVal BoolOptional shared, @ByVal LongOptional size, @ByRef Tensor out);




// Parsed from ATen/ops/full.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/full_ops.h>


// aten::full.names(int[] size, Scalar fill_value, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names);
// aten::full.names(int[] size, Scalar fill_value, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::full(SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value);


// aten::full(SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::full(SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor full_symint(@ByVal SymIntRef size, @Const @ByRef Scalar fill_value, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor full_symint(@ByVal SymIntRef size, @Const @ByRef Scalar fill_value);


// aten::full(SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor full_symint(@ByVal SymIntRef size, @Const @ByRef Scalar fill_value, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::full.out(SymInt[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor full_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value);
@Namespace("at") public static native @ByRef Tensor full_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value);


// aten::full.out(SymInt[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor full_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor full_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value, @ByRef Tensor out);


// aten::full.out(SymInt[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor full_symint_out(@ByRef Tensor out, @ByVal SymIntRef size, @Const @ByRef Scalar fill_value);


// aten::full.out(SymInt[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor full_symint_outf(@ByVal SymIntRef size, @Const @ByRef Scalar fill_value, @ByRef Tensor out);


// aten::full.names_out(int[] size, Scalar fill_value, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor full_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByRef Tensor full_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names);
// aten::full.names_out(int[] size, Scalar fill_value, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor full_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor full_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names, @ByRef Tensor out);




// Parsed from ATen/ops/full_like.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/full_like_ops.h>


// aten::full_like(Tensor self, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor full_like(@Const @ByRef Tensor self, @Const @ByRef Scalar fill_value, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor full_like(@Const @ByRef Tensor self, @Const @ByRef Scalar fill_value);
// aten::full_like(Tensor self, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor full_like(@Const @ByRef Tensor self, @Const @ByRef Scalar fill_value, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::full_like.out(Tensor self, Scalar fill_value, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor full_like_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar fill_value, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor full_like_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar fill_value);
// aten::full_like.out(Tensor self, Scalar fill_value, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor full_like_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar fill_value, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);




// Parsed from ATen/ops/fused_moving_avg_obs_fake_quant.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/fused_moving_avg_obs_fake_quant_ops.h>


// aten::fused_moving_avg_obs_fake_quant(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor fused_moving_avg_obs_fake_quant(@Const @ByRef Tensor self, @Const @ByRef Tensor observer_on, @Const @ByRef Tensor fake_quant_on, @ByRef Tensor running_min, @ByRef Tensor running_max, @ByRef Tensor scale, @ByRef Tensor zero_point, double averaging_const, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, @Cast("int64_t") long ch_axis, @Cast("bool") boolean per_row_fake_quant/*=false*/, @Cast("bool") boolean symmetric_quant/*=false*/);
@Namespace("at") public static native @ByVal Tensor fused_moving_avg_obs_fake_quant(@Const @ByRef Tensor self, @Const @ByRef Tensor observer_on, @Const @ByRef Tensor fake_quant_on, @ByRef Tensor running_min, @ByRef Tensor running_max, @ByRef Tensor scale, @ByRef Tensor zero_point, double averaging_const, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, @Cast("int64_t") long ch_axis);




// Parsed from ATen/ops/gather.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/gather_ops.h>


// aten::gather.out(Tensor self, int dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gather_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad/*=false*/);
@Namespace("at") public static native @ByRef Tensor gather_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index);
// aten::gather.out(Tensor self, int dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gather_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad, @ByRef Tensor out);

// aten::gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor gather(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad/*=false*/);
@Namespace("at") public static native @ByVal Tensor gather(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index);

// aten::gather.dimname_out(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gather_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad/*=false*/);
@Namespace("at") public static native @ByRef Tensor gather_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index);
// aten::gather.dimname_out(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gather_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad, @ByRef Tensor out);

// aten::gather.dimname(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor gather(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad/*=false*/);
@Namespace("at") public static native @ByVal Tensor gather(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index);




// Parsed from ATen/ops/gather_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/gather_backward_ops.h>


// aten::gather_backward(Tensor grad, Tensor self, int dim, Tensor index, bool sparse_grad) -> Tensor
@Namespace("at") public static native @ByVal Tensor gather_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad);




// Parsed from ATen/ops/gcd.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/gcd_ops.h>


// aten::gcd.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gcd_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::gcd.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gcd_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::gcd(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor gcd(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::gcd_(Tensor(a!) self, Tensor other) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gcd_(@ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/ge.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ge_ops.h>


// aten::ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ge_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ge_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::ge.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor ge(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ge_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ge_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::ge.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor ge(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/gelu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/gelu_ops.h>


// aten::gelu.out(Tensor self, *, str approximate='none', Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gelu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::string_view(\"none\")") @Cast("c10::string_view*") Pointer approximate);
@Namespace("at") public static native @ByRef Tensor gelu_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::gelu.out(Tensor self, *, str approximate='none', Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gelu_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer approximate, @ByRef Tensor out);

// aten::gelu_(Tensor(a!) self, *, str approximate='none') -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gelu_(@ByRef Tensor self, @ByVal(nullValue = "c10::string_view(\"none\")") @Cast("c10::string_view*") Pointer approximate);
@Namespace("at") public static native @ByRef Tensor gelu_(@ByRef Tensor self);

// aten::gelu(Tensor self, *, str approximate='none') -> Tensor
@Namespace("at") public static native @ByVal Tensor gelu(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::string_view(\"none\")") @Cast("c10::string_view*") Pointer approximate);
@Namespace("at") public static native @ByVal Tensor gelu(@Const @ByRef Tensor self);




// Parsed from ATen/ops/gelu_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/gelu_backward_ops.h>


// aten::gelu_backward.grad_input(Tensor grad_output, Tensor self, *, str approximate='none', Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gelu_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::string_view(\"none\")") @Cast("c10::string_view*") Pointer approximate);
@Namespace("at") public static native @ByRef Tensor gelu_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
// aten::gelu_backward.grad_input(Tensor grad_output, Tensor self, *, str approximate='none', Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gelu_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer approximate, @ByRef Tensor grad_input);

// aten::gelu_backward(Tensor grad_output, Tensor self, *, str approximate='none') -> Tensor
@Namespace("at") public static native @ByVal Tensor gelu_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::string_view(\"none\")") @Cast("c10::string_view*") Pointer approximate);
@Namespace("at") public static native @ByVal Tensor gelu_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);




// Parsed from ATen/ops/geometric.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/geometric_ops.h>


// aten::geometric.out(Tensor self, float p, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor geometric_out(@ByRef Tensor out, @Const @ByRef Tensor self, double p, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor geometric_out(@ByRef Tensor out, @Const @ByRef Tensor self, double p);
// aten::geometric.out(Tensor self, float p, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor geometric_outf(@Const @ByRef Tensor self, double p, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::geometric(Tensor self, float p, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor geometric(@Const @ByRef Tensor self, double p, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor geometric(@Const @ByRef Tensor self, double p);




// Parsed from ATen/ops/geqrf.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/geqrf_ops.h>


// aten::geqrf.a(Tensor self, *, Tensor(a!) a, Tensor(b!) tau) -> (Tensor(a!) a, Tensor(b!) tau)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> geqrf_out(@ByRef Tensor a, @ByRef Tensor tau, @Const @ByRef Tensor self);
// aten::geqrf.a(Tensor self, *, Tensor(a!) a, Tensor(b!) tau) -> (Tensor(a!) a, Tensor(b!) tau)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> geqrf_outf(@Const @ByRef Tensor self, @ByRef Tensor a, @ByRef Tensor tau);

// aten::geqrf(Tensor self) -> (Tensor a, Tensor tau)
@Namespace("at") public static native @ByVal TensorTensorTuple geqrf(@Const @ByRef Tensor self);




// Parsed from ATen/ops/ger.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ger_ops.h>


// aten::ger(Tensor self, Tensor vec2) -> Tensor
@Namespace("at") public static native @ByVal Tensor ger(@Const @ByRef Tensor self, @Const @ByRef Tensor vec2);

// aten::ger.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ger_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor vec2);
// aten::ger.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ger_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor vec2, @ByRef Tensor out);




// Parsed from ATen/ops/glu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/glu_ops.h>


// aten::glu.out(Tensor self, int dim=-1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor glu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByRef Tensor glu_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::glu.out(Tensor self, int dim=-1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor glu_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor out);

// aten::glu(Tensor self, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor glu(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor glu(@Const @ByRef Tensor self);




// Parsed from ATen/ops/glu_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/glu_backward_ops.h>


// aten::glu_backward.grad_input(Tensor grad_output, Tensor self, int dim, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor glu_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::glu_backward.grad_input(Tensor grad_output, Tensor self, int dim, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor glu_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor grad_input);

// aten::glu_backward(Tensor grad_output, Tensor self, int dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor glu_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Cast("int64_t") long dim);




// Parsed from ATen/ops/glu_backward_jvp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/glu_backward_jvp_ops.h>


// aten::glu_backward_jvp(Tensor grad_x, Tensor grad_glu, Tensor x, Tensor dgrad_glu, Tensor dx, int dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor glu_backward_jvp(@Const @ByRef Tensor grad_x, @Const @ByRef Tensor grad_glu, @Const @ByRef Tensor x, @Const @ByRef Tensor dgrad_glu, @Const @ByRef Tensor dx, @Cast("int64_t") long dim);

// aten::glu_backward_jvp.out(Tensor grad_x, Tensor grad_glu, Tensor x, Tensor dgrad_glu, Tensor dx, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor glu_backward_jvp_out(@ByRef Tensor out, @Const @ByRef Tensor grad_x, @Const @ByRef Tensor grad_glu, @Const @ByRef Tensor x, @Const @ByRef Tensor dgrad_glu, @Const @ByRef Tensor dx, @Cast("int64_t") long dim);
// aten::glu_backward_jvp.out(Tensor grad_x, Tensor grad_glu, Tensor x, Tensor dgrad_glu, Tensor dx, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor glu_backward_jvp_outf(@Const @ByRef Tensor grad_x, @Const @ByRef Tensor grad_glu, @Const @ByRef Tensor x, @Const @ByRef Tensor dgrad_glu, @Const @ByRef Tensor dx, @Cast("int64_t") long dim, @ByRef Tensor out);




// Parsed from ATen/ops/glu_jvp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/glu_jvp_ops.h>


// aten::glu_jvp(Tensor glu, Tensor x, Tensor dx, int dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor glu_jvp(@Const @ByRef Tensor glu, @Const @ByRef Tensor x, @Const @ByRef Tensor dx, @Cast("int64_t") long dim);

// aten::glu_jvp.out(Tensor glu, Tensor x, Tensor dx, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor glu_jvp_out(@ByRef Tensor out, @Const @ByRef Tensor glu, @Const @ByRef Tensor x, @Const @ByRef Tensor dx, @Cast("int64_t") long dim);
// aten::glu_jvp.out(Tensor glu, Tensor x, Tensor dx, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor glu_jvp_outf(@Const @ByRef Tensor glu, @Const @ByRef Tensor x, @Const @ByRef Tensor dx, @Cast("int64_t") long dim, @ByRef Tensor out);




// Parsed from ATen/ops/gradient.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/gradient_ops.h>


// aten::gradient.scalarint(Tensor self, *, Scalar? spacing=None, int? dim=None, int edge_order=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional spacing, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self);

// aten::gradient.scalararray(Tensor self, *, Scalar spacing, int[] dim, int edge_order=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @Const @ByRef Scalar spacing, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @Const @ByRef Scalar spacing, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @Const @ByRef Scalar spacing, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @Const @ByRef Scalar spacing, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::gradient.array(Tensor self, *, int[] dim, int edge_order=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::gradient.scalarrayint(Tensor self, *, Scalar[] spacing, int? dim=None, int edge_order=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal ScalarArrayRef spacing, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal ScalarArrayRef spacing);

// aten::gradient.scalarrayarray(Tensor self, *, Scalar[] spacing, int[] dim, int edge_order=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal ScalarArrayRef spacing, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal ScalarArrayRef spacing, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal ScalarArrayRef spacing, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal ScalarArrayRef spacing, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::gradient.tensorarrayint(Tensor self, *, Tensor[] spacing, int? dim=None, int edge_order=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal TensorArrayRef spacing, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal TensorArrayRef spacing);

// aten::gradient.tensorarray(Tensor self, *, Tensor[] spacing, int[] dim, int edge_order=1) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal TensorArrayRef spacing, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal TensorArrayRef spacing, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal TensorArrayRef spacing, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long edge_order/*=1*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector gradient(@Const @ByRef Tensor self, @ByVal TensorArrayRef spacing, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);




// Parsed from ATen/ops/greater.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/greater_ops.h>


// aten::greater.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::greater.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::greater.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor greater(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::greater.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::greater.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::greater.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor greater(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/greater_equal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/greater_equal_ops.h>


// aten::greater_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::greater_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_equal_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::greater_equal.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor greater_equal(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::greater_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::greater_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor greater_equal_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::greater_equal.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor greater_equal(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/grid_sampler.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/grid_sampler_ops.h>


// aten::grid_sampler(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
@Namespace("at") public static native @ByVal Tensor grid_sampler(@Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);




// Parsed from ATen/ops/grid_sampler_2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/grid_sampler_2d_ops.h>


// aten::grid_sampler_2d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
@Namespace("at") public static native @ByVal Tensor grid_sampler_2d(@Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);

// aten::grid_sampler_2d.out(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor grid_sampler_2d_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);
// aten::grid_sampler_2d.out(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor grid_sampler_2d_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners, @ByRef Tensor out);




// Parsed from ATen/ops/grid_sampler_2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/grid_sampler_2d_backward_ops.h>


// aten::grid_sampler_2d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple grid_sampler_2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);

// aten::grid_sampler_2d_backward.out(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> grid_sampler_2d_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);
// aten::grid_sampler_2d_backward.out(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> grid_sampler_2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/grid_sampler_3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/grid_sampler_3d_ops.h>


// aten::grid_sampler_3d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
@Namespace("at") public static native @ByVal Tensor grid_sampler_3d(@Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);

// aten::grid_sampler_3d.out(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor grid_sampler_3d_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);
// aten::grid_sampler_3d.out(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor grid_sampler_3d_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners, @ByRef Tensor out);




// Parsed from ATen/ops/grid_sampler_3d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/grid_sampler_3d_backward_ops.h>


// aten::grid_sampler_3d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple grid_sampler_3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);

// aten::grid_sampler_3d_backward.out(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> grid_sampler_3d_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);
// aten::grid_sampler_3d_backward.out(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> grid_sampler_3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/group_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/group_norm_ops.h>


// aten::group_norm(Tensor input, int num_groups, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enabled=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor group_norm(@Const @ByRef Tensor input, @Cast("int64_t") long num_groups, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, double eps/*=1e-05*/, @Cast("bool") boolean cudnn_enabled/*=true*/);
@Namespace("at") public static native @ByVal Tensor group_norm(@Const @ByRef Tensor input, @Cast("int64_t") long num_groups);




// Parsed from ATen/ops/gru.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/gru_ops.h>


// aten::gru.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple gru(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);

// aten::gru.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple gru(@Const @ByRef Tensor data, @Const @ByRef Tensor batch_sizes, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional);




// Parsed from ATen/ops/gru_cell.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/gru_cell_ops.h>


// aten::gru_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor gru_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_ih, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_hh);
@Namespace("at") public static native @ByVal Tensor gru_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh);




// Parsed from ATen/ops/gt.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/gt_ops.h>


// aten::gt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gt_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::gt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gt_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::gt.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor gt(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::gt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gt_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::gt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor gt_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::gt.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor gt(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/hamming_window.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hamming_window_ops.h>


// aten::hamming_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length);
// aten::hamming_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::hamming_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
// aten::hamming_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::hamming_window.periodic_alpha(int window_length, bool periodic, float alpha, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha);
// aten::hamming_window.periodic_alpha(int window_length, bool periodic, float alpha, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::hamming_window.periodic_alpha_beta(int window_length, bool periodic, float alpha, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, double beta, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, double beta);
// aten::hamming_window.periodic_alpha_beta(int window_length, bool periodic, float alpha, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, double beta, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::hamming_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hamming_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length);
// aten::hamming_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hamming_window_outf(@Cast("int64_t") long window_length, @ByRef Tensor out);

// aten::hamming_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hamming_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
// aten::hamming_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hamming_window_outf(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByRef Tensor out);

// aten::hamming_window.periodic_alpha_out(int window_length, bool periodic, float alpha, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hamming_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha);
// aten::hamming_window.periodic_alpha_out(int window_length, bool periodic, float alpha, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hamming_window_outf(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, @ByRef Tensor out);

// aten::hamming_window.periodic_alpha_beta_out(int window_length, bool periodic, float alpha, float beta, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hamming_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, double beta);
// aten::hamming_window.periodic_alpha_beta_out(int window_length, bool periodic, float alpha, float beta, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hamming_window_outf(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, double beta, @ByRef Tensor out);




// Parsed from ATen/ops/hann_window.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hann_window_ops.h>


// aten::hann_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length);
// aten::hann_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::hann_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
// aten::hann_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::hann_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hann_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length);
// aten::hann_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hann_window_outf(@Cast("int64_t") long window_length, @ByRef Tensor out);

// aten::hann_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hann_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
// aten::hann_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hann_window_outf(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByRef Tensor out);




// Parsed from ATen/ops/hardshrink.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hardshrink_ops.h>


// aten::hardshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardshrink_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.5)") Scalar lambd);
@Namespace("at") public static native @ByRef Tensor hardshrink_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::hardshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardshrink_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar lambd, @ByRef Tensor out);

// aten::hardshrink(Tensor self, Scalar lambd=0.5) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardshrink(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.5)") Scalar lambd);
@Namespace("at") public static native @ByVal Tensor hardshrink(@Const @ByRef Tensor self);




// Parsed from ATen/ops/hardshrink_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hardshrink_backward_ops.h>


// aten::hardshrink_backward.grad_input(Tensor grad_out, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardshrink_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_out, @Const @ByRef Tensor self, @Const @ByRef Scalar lambd);
// aten::hardshrink_backward.grad_input(Tensor grad_out, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardshrink_backward_outf(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor self, @Const @ByRef Scalar lambd, @ByRef Tensor grad_input);

// aten::hardshrink_backward(Tensor grad_out, Tensor self, Scalar lambd) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardshrink_backward(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor self, @Const @ByRef Scalar lambd);




// Parsed from ATen/ops/hardsigmoid.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hardsigmoid_ops.h>


// aten::hardsigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardsigmoid_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::hardsigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardsigmoid_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::hardsigmoid(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardsigmoid(@Const @ByRef Tensor self);

// aten::hardsigmoid_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardsigmoid_(@ByRef Tensor self);




// Parsed from ATen/ops/hardsigmoid_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hardsigmoid_backward_ops.h>


// aten::hardsigmoid_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardsigmoid_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
// aten::hardsigmoid_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardsigmoid_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByRef Tensor grad_input);

// aten::hardsigmoid_backward(Tensor grad_output, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardsigmoid_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);




// Parsed from ATen/ops/hardswish.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hardswish_ops.h>


// aten::hardswish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardswish_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::hardswish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardswish_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::hardswish(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardswish(@Const @ByRef Tensor self);

// aten::hardswish_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardswish_(@ByRef Tensor self);




// Parsed from ATen/ops/hardswish_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hardswish_backward_ops.h>


// aten::hardswish_backward(Tensor grad_output, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardswish_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);

// aten::hardswish_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardswish_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
// aten::hardswish_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardswish_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/hardtanh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hardtanh_ops.h>


// aten::hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardtanh_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(-1)") Scalar min_val, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar max_val);
@Namespace("at") public static native @ByRef Tensor hardtanh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardtanh_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar min_val, @Const @ByRef Scalar max_val, @ByRef Tensor out);

// aten::hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardtanh(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(-1)") Scalar min_val, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar max_val);
@Namespace("at") public static native @ByVal Tensor hardtanh(@Const @ByRef Tensor self);

// aten::hardtanh_(Tensor(a!) self, Scalar min_val=-1, Scalar max_val=1) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardtanh_(@ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(-1)") Scalar min_val, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar max_val);
@Namespace("at") public static native @ByRef Tensor hardtanh_(@ByRef Tensor self);




// Parsed from ATen/ops/hardtanh_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hardtanh_backward_ops.h>


// aten::hardtanh_backward.grad_input(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardtanh_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar min_val, @Const @ByRef Scalar max_val);
// aten::hardtanh_backward.grad_input(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hardtanh_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar min_val, @Const @ByRef Scalar max_val, @ByRef Tensor grad_input);

// aten::hardtanh_backward(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val) -> Tensor
@Namespace("at") public static native @ByVal Tensor hardtanh_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar min_val, @Const @ByRef Scalar max_val);




// Parsed from ATen/ops/heaviside.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/heaviside_ops.h>


// aten::heaviside.out(Tensor self, Tensor values, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor heaviside_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor values);
// aten::heaviside.out(Tensor self, Tensor values, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor heaviside_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor values, @ByRef Tensor out);

// aten::heaviside(Tensor self, Tensor values) -> Tensor
@Namespace("at") public static native @ByVal Tensor heaviside(@Const @ByRef Tensor self, @Const @ByRef Tensor values);




// Parsed from ATen/ops/hinge_embedding_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hinge_embedding_loss_ops.h>


// aten::hinge_embedding_loss(Tensor self, Tensor target, float margin=1.0, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor hinge_embedding_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, double margin/*=1.0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor hinge_embedding_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);




// Parsed from ATen/ops/histc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/histc_ops.h>


// aten::histc.out(Tensor self, int bins=100, Scalar min=0, Scalar max=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor histc_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long bins/*=100*/, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar min, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar max);
@Namespace("at") public static native @ByRef Tensor histc_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::histc.out(Tensor self, int bins=100, Scalar min=0, Scalar max=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor histc_outf(@Const @ByRef Tensor self, @Cast("int64_t") long bins, @Const @ByRef Scalar min, @Const @ByRef Scalar max, @ByRef Tensor out);

// aten::histc(Tensor self, int bins=100, Scalar min=0, Scalar max=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor histc(@Const @ByRef Tensor self, @Cast("int64_t") long bins/*=100*/, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar min, @Const @ByRef(nullValue = "at::Scalar(0)") Scalar max);
@Namespace("at") public static native @ByVal Tensor histc(@Const @ByRef Tensor self);




// Parsed from ATen/ops/histogram.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/histogram_ops.h>


// aten::histogram.bins_tensor_out(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -> (Tensor(a!) hist, Tensor(b!) bin_edges)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> histogram_out(@ByRef Tensor hist, @ByRef Tensor bin_edges, @Const @ByRef Tensor self, @Const @ByRef Tensor bins, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> histogram_out(@ByRef Tensor hist, @ByRef Tensor bin_edges, @Const @ByRef Tensor self, @Const @ByRef Tensor bins);
// aten::histogram.bins_tensor_out(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -> (Tensor(a!) hist, Tensor(b!) bin_edges)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> histogram_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor bins, @Const @ByRef TensorOptional weight, @Cast("bool") boolean density, @ByRef Tensor hist, @ByRef Tensor bin_edges);

// aten::histogram.bins_tensor(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor bin_edges)
@Namespace("at") public static native @ByVal TensorTensorTuple histogram(@Const @ByRef Tensor self, @Const @ByRef Tensor bins, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple histogram(@Const @ByRef Tensor self, @Const @ByRef Tensor bins);

// aten::histogram.bin_ct_out(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -> (Tensor(a!) hist, Tensor(b!) bin_edges)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> histogram_out(@ByRef Tensor hist, @ByRef Tensor bin_edges, @Const @ByRef Tensor self, @Cast("int64_t") long bins/*=100*/, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") DoubleArrayRefOptional range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> histogram_out(@ByRef Tensor hist, @ByRef Tensor bin_edges, @Const @ByRef Tensor self);
// aten::histogram.bin_ct_out(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -> (Tensor(a!) hist, Tensor(b!) bin_edges)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> histogram_outf(@Const @ByRef Tensor self, @Cast("int64_t") long bins, @ByVal DoubleArrayRefOptional range, @Const @ByRef TensorOptional weight, @Cast("bool") boolean density, @ByRef Tensor hist, @ByRef Tensor bin_edges);

// aten::histogram.bin_ct(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor bin_edges)
@Namespace("at") public static native @ByVal TensorTensorTuple histogram(@Const @ByRef Tensor self, @Cast("int64_t") long bins/*=100*/, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") DoubleArrayRefOptional range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple histogram(@Const @ByRef Tensor self);




// Parsed from ATen/ops/histogramdd.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/histogramdd_ops.h>


// aten::histogramdd(Tensor self, int[] bins, float[]? range=None, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor[] bin_edges)
@Namespace("at") public static native @ByVal TensorTensorVectorTuple histogramdd(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef bins, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") DoubleArrayRefOptional range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorVectorTuple histogramdd(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef bins);
@Namespace("at") public static native @ByVal TensorTensorVectorTuple histogramdd(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] bins, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") DoubleArrayRefOptional range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorVectorTuple histogramdd(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... bins);

// aten::histogramdd.int_bins(Tensor self, int bins, float[]? range=None, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor[] bin_edges)
@Namespace("at") public static native @ByVal TensorTensorVectorTuple histogramdd(@Const @ByRef Tensor self, @Cast("int64_t") long bins, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") DoubleArrayRefOptional range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorVectorTuple histogramdd(@Const @ByRef Tensor self, @Cast("int64_t") long bins);

// aten::histogramdd.TensorList_bins(Tensor self, Tensor[] bins, float[]? range=None, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor[] bin_edges)
@Namespace("at") public static native @ByVal TensorTensorVectorTuple histogramdd(@Const @ByRef Tensor self, @ByVal TensorArrayRef bins, @ByVal(nullValue = "c10::optional<at::ArrayRef<double> >(c10::nullopt)") DoubleArrayRefOptional range, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("bool") boolean density/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorVectorTuple histogramdd(@Const @ByRef Tensor self, @ByVal TensorArrayRef bins);




// Parsed from ATen/ops/hsplit.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hsplit_ops.h>


// aten::hsplit.int(Tensor(a -> *) self, int sections) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector hsplit(@Const @ByRef Tensor self, @Cast("int64_t") long sections);

// aten::hsplit.array(Tensor(a -> *) self, int[] indices) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector hsplit(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef indices);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector hsplit(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... indices);




// Parsed from ATen/ops/hspmm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hspmm_ops.h>


// aten::hspmm.out(Tensor mat1, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hspmm_out(@ByRef Tensor out, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);
// aten::hspmm.out(Tensor mat1, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hspmm_outf(@Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @ByRef Tensor out);

// aten::hspmm(Tensor mat1, Tensor mat2) -> Tensor
@Namespace("at") public static native @ByVal Tensor hspmm(@Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);




// Parsed from ATen/ops/hstack.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hstack_ops.h>


// aten::hstack(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor hstack(@ByVal TensorArrayRef tensors);

// aten::hstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hstack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
// aten::hstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hstack_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);




// Parsed from ATen/ops/huber_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/huber_loss_ops.h>


// aten::huber_loss.out(Tensor self, Tensor target, int reduction=Mean, float delta=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor huber_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, double delta/*=1.0*/);
@Namespace("at") public static native @ByRef Tensor huber_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
// aten::huber_loss.out(Tensor self, Tensor target, int reduction=Mean, float delta=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor huber_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double delta, @ByRef Tensor out);

// aten::huber_loss(Tensor self, Tensor target, int reduction=Mean, float delta=1.0) -> Tensor
@Namespace("at") public static native @ByVal Tensor huber_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, double delta/*=1.0*/);
@Namespace("at") public static native @ByVal Tensor huber_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);




// Parsed from ATen/ops/huber_loss_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/huber_loss_backward_ops.h>


// aten::huber_loss_backward.out(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor huber_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double delta);
// aten::huber_loss_backward.out(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor huber_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double delta, @ByRef Tensor grad_input);

// aten::huber_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta) -> Tensor
@Namespace("at") public static native @ByVal Tensor huber_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double delta);




// Parsed from ATen/ops/hypot.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/hypot_ops.h>


// aten::hypot.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hypot_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::hypot.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor hypot_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::hypot(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor hypot(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/i0.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/i0_ops.h>


// aten::i0(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor i0(@Const @ByRef Tensor self);

// aten::i0_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor i0_(@ByRef Tensor self);

// aten::i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor i0_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor i0_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/igamma.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/igamma_ops.h>


// aten::igamma.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor igamma_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::igamma.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor igamma_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::igamma(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor igamma(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/igammac.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/igammac_ops.h>


// aten::igammac.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor igammac_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::igammac.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor igammac_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::igammac(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor igammac(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/im2col.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/im2col_ops.h>


// aten::im2col.out(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor im2col_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor im2col_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);
// aten::im2col.out(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor im2col_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor im2col_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByRef Tensor out);

// aten::im2col(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
@Namespace("at") public static native @ByVal Tensor im2col(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor im2col(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);




// Parsed from ATen/ops/imag.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/imag_ops.h>


// aten::imag(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor imag(@Const @ByRef Tensor self);




// Parsed from ATen/ops/index.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/index_ops.h>


// aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor

// aten::index.Tensor_out(Tensor self, Tensor?[] indices, *, Tensor(a!) out) -> Tensor(a!)
// aten::index.Tensor_out(Tensor self, Tensor?[] indices, *, Tensor(a!) out) -> Tensor(a!)




// Parsed from ATen/ops/index_add.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/index_add_ops.h>


// aten::index_add.out(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_add_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor index_add_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source);
// aten::index_add.out(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_add_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::index_add(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_add(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor index_add(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source);

// aten::index_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_add(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor index_add(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source);




// Parsed from ATen/ops/index_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/index_copy_ops.h>


// aten::index_copy.out(Tensor self, int dim, Tensor index, Tensor source, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source);
// aten::index_copy.out(Tensor self, int dim, Tensor index, Tensor source, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @ByRef Tensor out);

// aten::index_copy(Tensor self, int dim, Tensor index, Tensor source) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_copy(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source);

// aten::index_copy.dimname(Tensor self, Dimname dim, Tensor index, Tensor source) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_copy(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source);




// Parsed from ATen/ops/index_fill.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/index_fill_ops.h>


// aten::index_fill.int_Scalar(Tensor self, int dim, Tensor index, Scalar value) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_fill(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value);

// aten::index_fill.int_Tensor(Tensor self, int dim, Tensor index, Tensor value) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_fill(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor value);

// aten::index_fill.Dimname_Scalar(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_fill(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value);

// aten::index_fill.Dimname_Tensor(Tensor self, Dimname dim, Tensor index, Tensor value) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_fill(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Tensor value);

// aten::index_fill.int_Scalar_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_fill_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value);
// aten::index_fill.int_Scalar_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_fill_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value, @ByRef Tensor out);

// aten::index_fill.int_Tensor_out(Tensor self, int dim, Tensor index, Tensor value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_fill_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor value);
// aten::index_fill.int_Tensor_out(Tensor self, int dim, Tensor index, Tensor value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_fill_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor value, @ByRef Tensor out);




// Parsed from ATen/ops/index_put.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/index_put_ops.h>


// aten::index_put_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor(a!)

// aten::index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor

// aten::index_put.out(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False, *, Tensor(a!) out) -> Tensor(a!)
// aten::index_put.out(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False, *, Tensor(a!) out) -> Tensor(a!)




// Parsed from ATen/ops/index_reduce.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/index_reduce_ops.h>


// aten::index_reduce.out(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_reduce_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @ByVal @Cast("c10::string_view*") Pointer reduce, @Cast("bool") boolean include_self/*=true*/);
@Namespace("at") public static native @ByRef Tensor index_reduce_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @ByVal @Cast("c10::string_view*") Pointer reduce);
// aten::index_reduce.out(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_reduce_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @ByVal @Cast("c10::string_view*") Pointer reduce, @Cast("bool") boolean include_self, @ByRef Tensor out);

// aten::index_reduce(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_reduce(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @ByVal @Cast("c10::string_view*") Pointer reduce, @Cast("bool") boolean include_self/*=true*/);
@Namespace("at") public static native @ByVal Tensor index_reduce(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @ByVal @Cast("c10::string_view*") Pointer reduce);




// Parsed from ATen/ops/index_select.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/index_select_ops.h>


// aten::index_select.out(Tensor self, int dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_select_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index);
// aten::index_select.out(Tensor self, int dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_select_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @ByRef Tensor out);

// aten::index_select(Tensor self, int dim, Tensor index) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_select(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index);

// aten::index_select.dimname_out(Tensor self, Dimname dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_select_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index);
// aten::index_select.dimname_out(Tensor self, Dimname dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor index_select_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @ByRef Tensor out);

// aten::index_select.dimname(Tensor self, Dimname dim, Tensor index) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_select(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index);




// Parsed from ATen/ops/index_select_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/index_select_backward_ops.h>


// aten::index_select_backward(Tensor grad, SymInt[] self_sizes, int dim, Tensor index) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_select_backward(@Const @ByRef Tensor grad, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef self_sizes, @Cast("int64_t") long dim, @Const @ByRef Tensor index);
@Namespace("at") public static native @ByVal Tensor index_select_backward(@Const @ByRef Tensor grad, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] self_sizes, @Cast("int64_t") long dim, @Const @ByRef Tensor index);


// aten::index_select_backward(Tensor grad, SymInt[] self_sizes, int dim, Tensor index) -> Tensor
@Namespace("at") public static native @ByVal Tensor index_select_backward_symint(@Const @ByRef Tensor grad, @ByVal SymIntRef self_sizes, @Cast("int64_t") long dim, @Const @ByRef Tensor index);





// Parsed from ATen/ops/indices.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/indices_ops.h>






// Parsed from ATen/ops/indices_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/indices_copy_ops.h>


// aten::indices_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor indices_copy(@Const @ByRef Tensor self);

// aten::indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor indices_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor indices_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/infinitely_differentiable_gelu_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/infinitely_differentiable_gelu_backward_ops.h>


// aten::infinitely_differentiable_gelu_backward(Tensor grad, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor infinitely_differentiable_gelu_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self);




// Parsed from ATen/ops/inner.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/inner_ops.h>


// aten::inner(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor inner(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::inner.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor inner_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::inner.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor inner_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/instance_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/instance_norm_ops.h>


// aten::instance_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool use_input_stats, float momentum, float eps, bool cudnn_enabled) -> Tensor
@Namespace("at") public static native @ByVal Tensor instance_norm(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean use_input_stats, double momentum, double eps, @Cast("bool") boolean cudnn_enabled);




// Parsed from ATen/ops/int_repr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/int_repr_ops.h>


// aten::int_repr(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor int_repr(@Const @ByRef Tensor self);

// aten::int_repr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor int_repr_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::int_repr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor int_repr_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/inverse.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/inverse_ops.h>


// aten::inverse(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor inverse(@Const @ByRef Tensor self);

// aten::inverse.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor inverse_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::inverse.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor inverse_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/is_coalesced.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_coalesced_ops.h>






// Parsed from ATen/ops/is_complex.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_complex_ops.h>


// aten::is_complex(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean __dispatch_is_complex(@Const @ByRef Tensor self);




// Parsed from ATen/ops/is_conj.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_conj_ops.h>


// aten::is_conj(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean __dispatch_is_conj(@Const @ByRef Tensor self);




// Parsed from ATen/ops/is_distributed.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_distributed_ops.h>


// aten::is_distributed(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean is_distributed(@Const @ByRef Tensor self);




// Parsed from ATen/ops/is_floating_point.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_floating_point_ops.h>


// aten::is_floating_point(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean __dispatch_is_floating_point(@Const @ByRef Tensor self);




// Parsed from ATen/ops/is_inference.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_inference_ops.h>


// aten::is_inference(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean __dispatch_is_inference(@Const @ByRef Tensor self);




// Parsed from ATen/ops/is_leaf.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_leaf_ops.h>






// Parsed from ATen/ops/is_neg.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_neg_ops.h>


// aten::is_neg(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean __dispatch_is_neg(@Const @ByRef Tensor self);




// Parsed from ATen/ops/is_nonzero.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_nonzero_ops.h>


// aten::is_nonzero(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean is_nonzero(@Const @ByRef Tensor self);




// Parsed from ATen/ops/is_pinned.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_pinned_ops.h>






// Parsed from ATen/ops/is_same_size.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_same_size_ops.h>


// aten::is_same_size(Tensor self, Tensor other) -> bool
@Namespace("at") public static native @Cast("bool") boolean is_same_size(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/is_set_to.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_set_to_ops.h>






// Parsed from ATen/ops/is_signed.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/is_signed_ops.h>


// aten::is_signed(Tensor self) -> bool
@Namespace("at") public static native @Cast("bool") boolean __dispatch_is_signed(@Const @ByRef Tensor self);




// Parsed from ATen/ops/isclose.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/isclose_ops.h>


// aten::isclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor isclose(@Const @ByRef Tensor self, @Const @ByRef Tensor other, double rtol/*=1e-05*/, double atol/*=1e-08*/, @Cast("bool") boolean equal_nan/*=false*/);
@Namespace("at") public static native @ByVal Tensor isclose(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/isfinite.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/isfinite_ops.h>


// aten::isfinite(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor isfinite(@Const @ByRef Tensor self);




// Parsed from ATen/ops/isin.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/isin_ops.h>


// aten::isin.Tensor_Tensor_out(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isin_out(@ByRef Tensor out, @Const @ByRef Tensor elements, @Const @ByRef Tensor test_elements, @Cast("bool") boolean assume_unique/*=false*/, @Cast("bool") boolean invert/*=false*/);
@Namespace("at") public static native @ByRef Tensor isin_out(@ByRef Tensor out, @Const @ByRef Tensor elements, @Const @ByRef Tensor test_elements);
// aten::isin.Tensor_Tensor_out(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isin_outf(@Const @ByRef Tensor elements, @Const @ByRef Tensor test_elements, @Cast("bool") boolean assume_unique, @Cast("bool") boolean invert, @ByRef Tensor out);

// aten::isin.Tensor_Tensor(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor isin(@Const @ByRef Tensor elements, @Const @ByRef Tensor test_elements, @Cast("bool") boolean assume_unique/*=false*/, @Cast("bool") boolean invert/*=false*/);
@Namespace("at") public static native @ByVal Tensor isin(@Const @ByRef Tensor elements, @Const @ByRef Tensor test_elements);

// aten::isin.Tensor_Scalar_out(Tensor elements, Scalar test_element, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isin_out(@ByRef Tensor out, @Const @ByRef Tensor elements, @Const @ByRef Scalar test_element, @Cast("bool") boolean assume_unique/*=false*/, @Cast("bool") boolean invert/*=false*/);
@Namespace("at") public static native @ByRef Tensor isin_out(@ByRef Tensor out, @Const @ByRef Tensor elements, @Const @ByRef Scalar test_element);
// aten::isin.Tensor_Scalar_out(Tensor elements, Scalar test_element, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isin_outf(@Const @ByRef Tensor elements, @Const @ByRef Scalar test_element, @Cast("bool") boolean assume_unique, @Cast("bool") boolean invert, @ByRef Tensor out);

// aten::isin.Tensor_Scalar(Tensor elements, Scalar test_element, *, bool assume_unique=False, bool invert=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor isin(@Const @ByRef Tensor elements, @Const @ByRef Scalar test_element, @Cast("bool") boolean assume_unique/*=false*/, @Cast("bool") boolean invert/*=false*/);
@Namespace("at") public static native @ByVal Tensor isin(@Const @ByRef Tensor elements, @Const @ByRef Scalar test_element);

// aten::isin.Scalar_Tensor_out(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isin_out(@ByRef Tensor out, @Const @ByRef Scalar element, @Const @ByRef Tensor test_elements, @Cast("bool") boolean assume_unique/*=false*/, @Cast("bool") boolean invert/*=false*/);
@Namespace("at") public static native @ByRef Tensor isin_out(@ByRef Tensor out, @Const @ByRef Scalar element, @Const @ByRef Tensor test_elements);
// aten::isin.Scalar_Tensor_out(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isin_outf(@Const @ByRef Scalar element, @Const @ByRef Tensor test_elements, @Cast("bool") boolean assume_unique, @Cast("bool") boolean invert, @ByRef Tensor out);

// aten::isin.Scalar_Tensor(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor isin(@Const @ByRef Scalar element, @Const @ByRef Tensor test_elements, @Cast("bool") boolean assume_unique/*=false*/, @Cast("bool") boolean invert/*=false*/);
@Namespace("at") public static native @ByVal Tensor isin(@Const @ByRef Scalar element, @Const @ByRef Tensor test_elements);




// Parsed from ATen/ops/isinf.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/isinf_ops.h>


// aten::isinf(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor isinf(@Const @ByRef Tensor self);

// aten::isinf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isinf_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::isinf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isinf_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/isnan.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/isnan_ops.h>


// aten::isnan(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor isnan(@Const @ByRef Tensor self);

// aten::isnan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isnan_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::isnan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isnan_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/isneginf.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/isneginf_ops.h>


// aten::isneginf(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor isneginf(@Const @ByRef Tensor self);

// aten::isneginf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isneginf_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::isneginf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isneginf_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/isposinf.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/isposinf_ops.h>


// aten::isposinf(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor isposinf(@Const @ByRef Tensor self);

// aten::isposinf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isposinf_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::isposinf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor isposinf_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/isreal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/isreal_ops.h>


// aten::isreal(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor isreal(@Const @ByRef Tensor self);




// Parsed from ATen/ops/istft.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/istft_ops.h>


// aten::istft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, bool normalized=False, bool? onesided=None, int? length=None, bool return_complex=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor istft(@Const @ByRef Tensor self, @Cast("int64_t") long n_fft, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional hop_length, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional win_length, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional window, @Cast("bool") boolean center/*=true*/, @Cast("bool") boolean normalized/*=false*/, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional onesided, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional length, @Cast("bool") boolean return_complex/*=false*/);
@Namespace("at") public static native @ByVal Tensor istft(@Const @ByRef Tensor self, @Cast("int64_t") long n_fft);




// Parsed from ATen/ops/item.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/item_ops.h>






// Parsed from ATen/ops/kaiser_window.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/kaiser_window_ops.h>


// aten::kaiser_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length);
// aten::kaiser_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::kaiser_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
// aten::kaiser_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::kaiser_window.beta(int window_length, bool periodic, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double beta, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double beta);
// aten::kaiser_window.beta(int window_length, bool periodic, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double beta, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::kaiser_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor kaiser_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length);
// aten::kaiser_window.out(int window_length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor kaiser_window_outf(@Cast("int64_t") long window_length, @ByRef Tensor out);

// aten::kaiser_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor kaiser_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
// aten::kaiser_window.periodic_out(int window_length, bool periodic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor kaiser_window_outf(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByRef Tensor out);

// aten::kaiser_window.beta_out(int window_length, bool periodic, float beta, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor kaiser_window_out(@ByRef Tensor out, @Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double beta);
// aten::kaiser_window.beta_out(int window_length, bool periodic, float beta, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor kaiser_window_outf(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double beta, @ByRef Tensor out);




// Parsed from ATen/ops/kl_div.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/kl_div_ops.h>


// aten::kl_div(Tensor self, Tensor target, int reduction=Mean, *, bool log_target=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor kl_div(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("bool") boolean log_target/*=false*/);
@Namespace("at") public static native @ByVal Tensor kl_div(@Const @ByRef Tensor self, @Const @ByRef Tensor target);




// Parsed from ATen/ops/kron.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/kron_ops.h>


// aten::kron(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor kron(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::kron.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor kron_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::kron.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor kron_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/kthvalue.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/kthvalue_ops.h>


// aten::kthvalue(Tensor self, int k, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple kthvalue(@Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple kthvalue(@Const @ByRef Tensor self, @Cast("int64_t") long k);

// aten::kthvalue.values(Tensor self, int k, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> kthvalue_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> kthvalue_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k);
// aten::kthvalue.values(Tensor self, int k, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> kthvalue_outf(@Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::kthvalue.dimname(Tensor self, int k, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple kthvalue(@Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple kthvalue(@Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal Dimname dim);

// aten::kthvalue.dimname_out(Tensor self, int k, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> kthvalue_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> kthvalue_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal Dimname dim);
// aten::kthvalue.dimname_out(Tensor self, int k, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> kthvalue_outf(@Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);




// Parsed from ATen/ops/l1_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/l1_loss_ops.h>


// aten::l1_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor l1_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor l1_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);




// Parsed from ATen/ops/layer_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/layer_norm_ops.h>


// aten::layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enable=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor layer_norm(@Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef normalized_shape, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, double eps/*=1e-05*/, @Cast("bool") boolean cudnn_enable/*=true*/);
@Namespace("at") public static native @ByVal Tensor layer_norm(@Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef normalized_shape);
@Namespace("at") public static native @ByVal Tensor layer_norm(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] normalized_shape, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, double eps/*=1e-05*/, @Cast("bool") boolean cudnn_enable/*=true*/);
@Namespace("at") public static native @ByVal Tensor layer_norm(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... normalized_shape);


// aten::layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enable=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor layer_norm_symint(@Const @ByRef Tensor input, @ByVal SymIntRef normalized_shape, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, double eps/*=1e-05*/, @Cast("bool") boolean cudnn_enable/*=true*/);
@Namespace("at") public static native @ByVal Tensor layer_norm_symint(@Const @ByRef Tensor input, @ByVal SymIntRef normalized_shape);





// Parsed from ATen/ops/lcm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lcm_ops.h>


// aten::lcm.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lcm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::lcm.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lcm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::lcm(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor lcm(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::lcm_(Tensor(a!) self, Tensor other) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lcm_(@ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/ldexp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ldexp_ops.h>


// aten::ldexp.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor ldexp(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::ldexp_(Tensor(a!) self, Tensor other) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ldexp_(@ByRef Tensor self, @Const @ByRef Tensor other);

// aten::ldexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ldexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::ldexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ldexp_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/le.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/le_ops.h>


// aten::le.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor le_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::le.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor le_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::le.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor le(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::le.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor le_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::le.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor le_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::le.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor le(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/leaky_relu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/leaky_relu_ops.h>


// aten::leaky_relu.out(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor leaky_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.01)") Scalar negative_slope);
@Namespace("at") public static native @ByRef Tensor leaky_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::leaky_relu.out(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor leaky_relu_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar negative_slope, @ByRef Tensor out);

// aten::leaky_relu(Tensor self, Scalar negative_slope=0.01) -> Tensor
@Namespace("at") public static native @ByVal Tensor leaky_relu(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.01)") Scalar negative_slope);
@Namespace("at") public static native @ByVal Tensor leaky_relu(@Const @ByRef Tensor self);

// aten::leaky_relu_(Tensor(a!) self, Scalar negative_slope=0.01) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor leaky_relu_(@ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.01)") Scalar negative_slope);
@Namespace("at") public static native @ByRef Tensor leaky_relu_(@ByRef Tensor self);




// Parsed from ATen/ops/leaky_relu_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/leaky_relu_backward_ops.h>


// aten::leaky_relu_backward.grad_input(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor leaky_relu_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar negative_slope, @Cast("bool") boolean self_is_result);
// aten::leaky_relu_backward.grad_input(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor leaky_relu_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar negative_slope, @Cast("bool") boolean self_is_result, @ByRef Tensor grad_input);

// aten::leaky_relu_backward(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result) -> Tensor
@Namespace("at") public static native @ByVal Tensor leaky_relu_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar negative_slope, @Cast("bool") boolean self_is_result);




// Parsed from ATen/ops/lerp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lerp_ops.h>


// aten::lerp.Scalar_out(Tensor self, Tensor end, Scalar weight, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lerp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor end, @Const @ByRef Scalar weight);
// aten::lerp.Scalar_out(Tensor self, Tensor end, Scalar weight, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lerp_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor end, @Const @ByRef Scalar weight, @ByRef Tensor out);

// aten::lerp.Tensor_out(Tensor self, Tensor end, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lerp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor end, @Const @ByRef Tensor weight);
// aten::lerp.Tensor_out(Tensor self, Tensor end, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lerp_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor end, @Const @ByRef Tensor weight, @ByRef Tensor out);

// aten::lerp.Scalar(Tensor self, Tensor end, Scalar weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor lerp(@Const @ByRef Tensor self, @Const @ByRef Tensor end, @Const @ByRef Scalar weight);

// aten::lerp.Tensor(Tensor self, Tensor end, Tensor weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor lerp(@Const @ByRef Tensor self, @Const @ByRef Tensor end, @Const @ByRef Tensor weight);




// Parsed from ATen/ops/less.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/less_ops.h>


// aten::less.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::less.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::less.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor less(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::less.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::less.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::less.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor less(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/less_equal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/less_equal_ops.h>


// aten::less_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::less_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_equal_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::less_equal.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor less_equal(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::less_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::less_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor less_equal_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::less_equal.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor less_equal(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/lgamma.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lgamma_ops.h>


// aten::lgamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lgamma_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::lgamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lgamma_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::lgamma(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor lgamma(@Const @ByRef Tensor self);




// Parsed from ATen/ops/lift.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lift_ops.h>


// aten::lift(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor lift(@Const @ByRef Tensor self);

// aten::lift.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lift_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::lift.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lift_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/lift_fresh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lift_fresh_ops.h>


// aten::lift_fresh(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor lift_fresh(@Const @ByRef Tensor self);




// Parsed from ATen/ops/lift_fresh_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lift_fresh_copy_ops.h>


// aten::lift_fresh_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor lift_fresh_copy(@Const @ByRef Tensor self);

// aten::lift_fresh_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lift_fresh_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::lift_fresh_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lift_fresh_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_cholesky.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_cholesky_ops.h>


// aten::linalg_cholesky(Tensor self, *, bool upper=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_cholesky(@Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_cholesky(@Const @ByRef Tensor self);

// aten::linalg_cholesky.out(Tensor self, *, bool upper=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_cholesky_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_cholesky_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::linalg_cholesky.out(Tensor self, *, bool upper=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_cholesky_outf(@Const @ByRef Tensor self, @Cast("bool") boolean upper, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_cholesky_ex.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_cholesky_ex_ops.h>


// aten::linalg_cholesky_ex(Tensor self, *, bool upper=False, bool check_errors=False) -> (Tensor L, Tensor info)
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_cholesky_ex(@Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_cholesky_ex(@Const @ByRef Tensor self);

// aten::linalg_cholesky_ex.L(Tensor self, *, bool upper=False, bool check_errors=False, Tensor(a!) L, Tensor(b!) info) -> (Tensor(a!) L, Tensor(b!) info)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_cholesky_ex_out(@ByRef Tensor L, @ByRef Tensor info, @Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_cholesky_ex_out(@ByRef Tensor L, @ByRef Tensor info, @Const @ByRef Tensor self);
// aten::linalg_cholesky_ex.L(Tensor self, *, bool upper=False, bool check_errors=False, Tensor(a!) L, Tensor(b!) info) -> (Tensor(a!) L, Tensor(b!) info)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_cholesky_ex_outf(@Const @ByRef Tensor self, @Cast("bool") boolean upper, @Cast("bool") boolean check_errors, @ByRef Tensor L, @ByRef Tensor info);




// Parsed from ATen/ops/linalg_cond.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_cond_ops.h>


// aten::linalg_cond(Tensor self, Scalar? p=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_cond(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional p);
@Namespace("at") public static native @ByVal Tensor linalg_cond(@Const @ByRef Tensor self);

// aten::linalg_cond.out(Tensor self, Scalar? p=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_cond_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional p);
@Namespace("at") public static native @ByRef Tensor linalg_cond_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::linalg_cond.out(Tensor self, Scalar? p=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_cond_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByRef Tensor out);

// aten::linalg_cond.p_str(Tensor self, str p) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_cond(@Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer p);

// aten::linalg_cond.p_str_out(Tensor self, str p, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_cond_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer p);
// aten::linalg_cond.p_str_out(Tensor self, str p, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_cond_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer p, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_cross.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_cross_ops.h>


// aten::linalg_cross(Tensor self, Tensor other, *, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_cross(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor linalg_cross(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::linalg_cross.out(Tensor self, Tensor other, *, int dim=-1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_cross_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByRef Tensor linalg_cross_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::linalg_cross.out(Tensor self, Tensor other, *, int dim=-1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_cross_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Cast("int64_t") long dim, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_det.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_det_ops.h>


// aten::linalg_det(Tensor A) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_det(@Const @ByRef Tensor A);

// aten::linalg_det.out(Tensor A, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_det_out(@ByRef Tensor out, @Const @ByRef Tensor A);
// aten::linalg_det.out(Tensor A, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_det_outf(@Const @ByRef Tensor A, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_diagonal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_diagonal_ops.h>


// aten::linalg_diagonal(Tensor(a) A, *, int offset=0, int dim1=-2, int dim2=-1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor linalg_diagonal(@Const @ByRef Tensor A, @Cast("int64_t") long offset/*=0*/, @Cast("int64_t") long dim1/*=-2*/, @Cast("int64_t") long dim2/*=-1*/);
@Namespace("at") public static native @ByVal Tensor linalg_diagonal(@Const @ByRef Tensor A);




// Parsed from ATen/ops/linalg_eig.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_eig_ops.h>


// aten::linalg_eig(Tensor self) -> (Tensor eigenvalues, Tensor eigenvectors)
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_eig(@Const @ByRef Tensor self);

// aten::linalg_eig.out(Tensor self, *, Tensor(a!) eigenvalues, Tensor(b!) eigenvectors) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_eig_out(@ByRef Tensor eigenvalues, @ByRef Tensor eigenvectors, @Const @ByRef Tensor self);
// aten::linalg_eig.out(Tensor self, *, Tensor(a!) eigenvalues, Tensor(b!) eigenvectors) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_eig_outf(@Const @ByRef Tensor self, @ByRef Tensor eigenvalues, @ByRef Tensor eigenvectors);




// Parsed from ATen/ops/linalg_eigh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_eigh_ops.h>


// aten::linalg_eigh(Tensor self, str UPLO="L") -> (Tensor eigenvalues, Tensor eigenvectors)
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_eigh(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::string_view(\"L\")") @Cast("c10::string_view*") Pointer UPLO);
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_eigh(@Const @ByRef Tensor self);

// aten::linalg_eigh.eigvals(Tensor self, str UPLO="L", *, Tensor(a!) eigvals, Tensor(b!) eigvecs) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_eigh_out(@ByRef Tensor eigvals, @ByRef Tensor eigvecs, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::string_view(\"L\")") @Cast("c10::string_view*") Pointer UPLO);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_eigh_out(@ByRef Tensor eigvals, @ByRef Tensor eigvecs, @Const @ByRef Tensor self);
// aten::linalg_eigh.eigvals(Tensor self, str UPLO="L", *, Tensor(a!) eigvals, Tensor(b!) eigvecs) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_eigh_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer UPLO, @ByRef Tensor eigvals, @ByRef Tensor eigvecs);




// Parsed from ATen/ops/linalg_eigvals.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_eigvals_ops.h>


// aten::linalg_eigvals(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_eigvals(@Const @ByRef Tensor self);

// aten::linalg_eigvals.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_eigvals_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::linalg_eigvals.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_eigvals_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_eigvalsh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_eigvalsh_ops.h>


// aten::linalg_eigvalsh(Tensor self, str UPLO="L") -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_eigvalsh(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::string_view(\"L\")") @Cast("c10::string_view*") Pointer UPLO);
@Namespace("at") public static native @ByVal Tensor linalg_eigvalsh(@Const @ByRef Tensor self);

// aten::linalg_eigvalsh.out(Tensor self, str UPLO="L", *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_eigvalsh_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::string_view(\"L\")") @Cast("c10::string_view*") Pointer UPLO);
@Namespace("at") public static native @ByRef Tensor linalg_eigvalsh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::linalg_eigvalsh.out(Tensor self, str UPLO="L", *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_eigvalsh_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer UPLO, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_householder_product.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_householder_product_ops.h>


// aten::linalg_householder_product(Tensor input, Tensor tau) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_householder_product(@Const @ByRef Tensor input, @Const @ByRef Tensor tau);

// aten::linalg_householder_product.out(Tensor input, Tensor tau, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_householder_product_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor tau);
// aten::linalg_householder_product.out(Tensor input, Tensor tau, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_householder_product_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor tau, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_inv.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_inv_ops.h>


// aten::linalg_inv(Tensor A) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_inv(@Const @ByRef Tensor A);

// aten::linalg_inv.out(Tensor A, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_inv_out(@ByRef Tensor out, @Const @ByRef Tensor A);
// aten::linalg_inv.out(Tensor A, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_inv_outf(@Const @ByRef Tensor A, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_inv_ex.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_inv_ex_ops.h>


// aten::linalg_inv_ex(Tensor A, *, bool check_errors=False) -> (Tensor inverse, Tensor info)
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_inv_ex(@Const @ByRef Tensor A, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_inv_ex(@Const @ByRef Tensor A);

// aten::linalg_inv_ex.inverse(Tensor A, *, bool check_errors=False, Tensor(a!) inverse, Tensor(b!) info) -> (Tensor(a!) inverse, Tensor(b!) info)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_inv_ex_out(@ByRef Tensor inverse, @ByRef Tensor info, @Const @ByRef Tensor A, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_inv_ex_out(@ByRef Tensor inverse, @ByRef Tensor info, @Const @ByRef Tensor A);
// aten::linalg_inv_ex.inverse(Tensor A, *, bool check_errors=False, Tensor(a!) inverse, Tensor(b!) info) -> (Tensor(a!) inverse, Tensor(b!) info)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_inv_ex_outf(@Const @ByRef Tensor A, @Cast("bool") boolean check_errors, @ByRef Tensor inverse, @ByRef Tensor info);




// Parsed from ATen/ops/linalg_ldl_factor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_ldl_factor_ops.h>


// aten::linalg_ldl_factor(Tensor self, *, bool hermitian=False) -> (Tensor LD, Tensor pivots)
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_ldl_factor(@Const @ByRef Tensor self, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_ldl_factor(@Const @ByRef Tensor self);

// aten::linalg_ldl_factor.out(Tensor self, *, bool hermitian=False, Tensor(a!) LD, Tensor(b!) pivots) -> (Tensor(a!) LD, Tensor(b!) pivots)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_ldl_factor_out(@ByRef Tensor LD, @ByRef Tensor pivots, @Const @ByRef Tensor self, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_ldl_factor_out(@ByRef Tensor LD, @ByRef Tensor pivots, @Const @ByRef Tensor self);
// aten::linalg_ldl_factor.out(Tensor self, *, bool hermitian=False, Tensor(a!) LD, Tensor(b!) pivots) -> (Tensor(a!) LD, Tensor(b!) pivots)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_ldl_factor_outf(@Const @ByRef Tensor self, @Cast("bool") boolean hermitian, @ByRef Tensor LD, @ByRef Tensor pivots);




// Parsed from ATen/ops/linalg_ldl_factor_ex.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_ldl_factor_ex_ops.h>


// aten::linalg_ldl_factor_ex(Tensor self, *, bool hermitian=False, bool check_errors=False) -> (Tensor LD, Tensor pivots, Tensor info)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple linalg_ldl_factor_ex(@Const @ByRef Tensor self, @Cast("bool") boolean hermitian/*=false*/, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple linalg_ldl_factor_ex(@Const @ByRef Tensor self);

// aten::linalg_ldl_factor_ex.out(Tensor self, *, bool hermitian=False, bool check_errors=False, Tensor(a!) LD, Tensor(b!) pivots, Tensor(c!) info) -> (Tensor(a!) LD, Tensor(b!) pivots, Tensor(c!) info)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_ldl_factor_ex_out(@ByRef Tensor LD, @ByRef Tensor pivots, @ByRef Tensor info, @Const @ByRef Tensor self, @Cast("bool") boolean hermitian/*=false*/, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_ldl_factor_ex_out(@ByRef Tensor LD, @ByRef Tensor pivots, @ByRef Tensor info, @Const @ByRef Tensor self);
// aten::linalg_ldl_factor_ex.out(Tensor self, *, bool hermitian=False, bool check_errors=False, Tensor(a!) LD, Tensor(b!) pivots, Tensor(c!) info) -> (Tensor(a!) LD, Tensor(b!) pivots, Tensor(c!) info)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_ldl_factor_ex_outf(@Const @ByRef Tensor self, @Cast("bool") boolean hermitian, @Cast("bool") boolean check_errors, @ByRef Tensor LD, @ByRef Tensor pivots, @ByRef Tensor info);




// Parsed from ATen/ops/linalg_ldl_solve.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_ldl_solve_ops.h>


// aten::linalg_ldl_solve(Tensor LD, Tensor pivots, Tensor B, *, bool hermitian=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_ldl_solve(@Const @ByRef Tensor LD, @Const @ByRef Tensor pivots, @Const @ByRef Tensor B, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_ldl_solve(@Const @ByRef Tensor LD, @Const @ByRef Tensor pivots, @Const @ByRef Tensor B);

// aten::linalg_ldl_solve.out(Tensor LD, Tensor pivots, Tensor B, *, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_ldl_solve_out(@ByRef Tensor out, @Const @ByRef Tensor LD, @Const @ByRef Tensor pivots, @Const @ByRef Tensor B, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_ldl_solve_out(@ByRef Tensor out, @Const @ByRef Tensor LD, @Const @ByRef Tensor pivots, @Const @ByRef Tensor B);
// aten::linalg_ldl_solve.out(Tensor LD, Tensor pivots, Tensor B, *, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_ldl_solve_outf(@Const @ByRef Tensor LD, @Const @ByRef Tensor pivots, @Const @ByRef Tensor B, @Cast("bool") boolean hermitian, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_lstsq.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_lstsq_ops.h>


// aten::linalg_lstsq(Tensor self, Tensor b, float? rcond=None, *, str? driver=None) -> (Tensor solution, Tensor residuals, Tensor rank, Tensor singular_values)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple linalg_lstsq(@Const @ByRef Tensor self, @Const @ByRef Tensor b, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional rcond, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer driver);
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple linalg_lstsq(@Const @ByRef Tensor self, @Const @ByRef Tensor b);

// aten::linalg_lstsq.out(Tensor self, Tensor b, float? rcond=None, *, str? driver=None, Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values) -> (Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_lstsq_out(@ByRef Tensor solution, @ByRef Tensor residuals, @ByRef Tensor rank, @ByRef Tensor singular_values, @Const @ByRef Tensor self, @Const @ByRef Tensor b, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional rcond, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer driver);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_lstsq_out(@ByRef Tensor solution, @ByRef Tensor residuals, @ByRef Tensor rank, @ByRef Tensor singular_values, @Const @ByRef Tensor self, @Const @ByRef Tensor b);
// aten::linalg_lstsq.out(Tensor self, Tensor b, float? rcond=None, *, str? driver=None, Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values) -> (Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_lstsq_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor b, @ByVal DoubleOptional rcond, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer driver, @ByRef Tensor solution, @ByRef Tensor residuals, @ByRef Tensor rank, @ByRef Tensor singular_values);




// Parsed from ATen/ops/linalg_lu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_lu_ops.h>


// aten::linalg_lu(Tensor A, *, bool pivot=True) -> (Tensor P, Tensor L, Tensor U)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple linalg_lu(@Const @ByRef Tensor A, @Cast("bool") boolean pivot/*=true*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple linalg_lu(@Const @ByRef Tensor A);

// aten::linalg_lu.out(Tensor A, *, bool pivot=True, Tensor(a!) P, Tensor(b!) L, Tensor(c!) U) -> (Tensor(a!) P, Tensor(b!) L, Tensor(c!) U)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_lu_out(@ByRef Tensor P, @ByRef Tensor L, @ByRef Tensor U, @Const @ByRef Tensor A, @Cast("bool") boolean pivot/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_lu_out(@ByRef Tensor P, @ByRef Tensor L, @ByRef Tensor U, @Const @ByRef Tensor A);
// aten::linalg_lu.out(Tensor A, *, bool pivot=True, Tensor(a!) P, Tensor(b!) L, Tensor(c!) U) -> (Tensor(a!) P, Tensor(b!) L, Tensor(c!) U)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_lu_outf(@Const @ByRef Tensor A, @Cast("bool") boolean pivot, @ByRef Tensor P, @ByRef Tensor L, @ByRef Tensor U);




// Parsed from ATen/ops/linalg_lu_factor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_lu_factor_ops.h>


// aten::linalg_lu_factor(Tensor A, *, bool pivot=True) -> (Tensor LU, Tensor pivots)
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_lu_factor(@Const @ByRef Tensor A, @Cast("bool") boolean pivot/*=true*/);
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_lu_factor(@Const @ByRef Tensor A);

// aten::linalg_lu_factor.out(Tensor A, *, bool pivot=True, Tensor(a!) LU, Tensor(b!) pivots) -> (Tensor(a!) LU, Tensor(b!) pivots)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_lu_factor_out(@ByRef Tensor LU, @ByRef Tensor pivots, @Const @ByRef Tensor A, @Cast("bool") boolean pivot/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_lu_factor_out(@ByRef Tensor LU, @ByRef Tensor pivots, @Const @ByRef Tensor A);
// aten::linalg_lu_factor.out(Tensor A, *, bool pivot=True, Tensor(a!) LU, Tensor(b!) pivots) -> (Tensor(a!) LU, Tensor(b!) pivots)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_lu_factor_outf(@Const @ByRef Tensor A, @Cast("bool") boolean pivot, @ByRef Tensor LU, @ByRef Tensor pivots);




// Parsed from ATen/ops/linalg_lu_factor_ex.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_lu_factor_ex_ops.h>


// aten::linalg_lu_factor_ex(Tensor A, *, bool pivot=True, bool check_errors=False) -> (Tensor LU, Tensor pivots, Tensor info)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple linalg_lu_factor_ex(@Const @ByRef Tensor A, @Cast("bool") boolean pivot/*=true*/, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple linalg_lu_factor_ex(@Const @ByRef Tensor A);

// aten::linalg_lu_factor_ex.out(Tensor A, *, bool pivot=True, bool check_errors=False, Tensor(a!) LU, Tensor(b!) pivots, Tensor(c!) info) -> (Tensor(a!) LU, Tensor(b!) pivots, Tensor(c!) info)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_lu_factor_ex_out(@ByRef Tensor LU, @ByRef Tensor pivots, @ByRef Tensor info, @Const @ByRef Tensor A, @Cast("bool") boolean pivot/*=true*/, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_lu_factor_ex_out(@ByRef Tensor LU, @ByRef Tensor pivots, @ByRef Tensor info, @Const @ByRef Tensor A);
// aten::linalg_lu_factor_ex.out(Tensor A, *, bool pivot=True, bool check_errors=False, Tensor(a!) LU, Tensor(b!) pivots, Tensor(c!) info) -> (Tensor(a!) LU, Tensor(b!) pivots, Tensor(c!) info)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_lu_factor_ex_outf(@Const @ByRef Tensor A, @Cast("bool") boolean pivot, @Cast("bool") boolean check_errors, @ByRef Tensor LU, @ByRef Tensor pivots, @ByRef Tensor info);




// Parsed from ATen/ops/linalg_lu_solve.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_lu_solve_ops.h>


// aten::linalg_lu_solve(Tensor LU, Tensor pivots, Tensor B, *, bool left=True, bool adjoint=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_lu_solve(@Const @ByRef Tensor LU, @Const @ByRef Tensor pivots, @Const @ByRef Tensor B, @Cast("bool") boolean left/*=true*/, @Cast("bool") boolean adjoint/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_lu_solve(@Const @ByRef Tensor LU, @Const @ByRef Tensor pivots, @Const @ByRef Tensor B);

// aten::linalg_lu_solve.out(Tensor LU, Tensor pivots, Tensor B, *, bool left=True, bool adjoint=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_lu_solve_out(@ByRef Tensor out, @Const @ByRef Tensor LU, @Const @ByRef Tensor pivots, @Const @ByRef Tensor B, @Cast("bool") boolean left/*=true*/, @Cast("bool") boolean adjoint/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_lu_solve_out(@ByRef Tensor out, @Const @ByRef Tensor LU, @Const @ByRef Tensor pivots, @Const @ByRef Tensor B);
// aten::linalg_lu_solve.out(Tensor LU, Tensor pivots, Tensor B, *, bool left=True, bool adjoint=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_lu_solve_outf(@Const @ByRef Tensor LU, @Const @ByRef Tensor pivots, @Const @ByRef Tensor B, @Cast("bool") boolean left, @Cast("bool") boolean adjoint, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_matmul.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_matmul_ops.h>


// aten::linalg_matmul(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matmul(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::linalg_matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matmul_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::linalg_matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matmul_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_matrix_exp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_matrix_exp_ops.h>


// aten::linalg_matrix_exp(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matrix_exp(@Const @ByRef Tensor self);

// aten::linalg_matrix_exp.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_exp_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::linalg_matrix_exp.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_exp_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_matrix_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_matrix_norm_ops.h>


// aten::linalg_matrix_norm(Tensor self, Scalar ord, int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matrix_norm(@Const @ByRef Tensor self, @Const @ByRef Scalar ord, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_norm(@Const @ByRef Tensor self, @Const @ByRef Scalar ord);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_norm(@Const @ByRef Tensor self, @Const @ByRef Scalar ord, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);

// aten::linalg_matrix_norm.out(Tensor self, Scalar ord, int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar ord, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar ord);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar ord, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
// aten::linalg_matrix_norm.out(Tensor self, Scalar ord, int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar ord, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar ord, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::linalg_matrix_norm.str_ord(Tensor self, str ord='fro', int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matrix_norm(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::string_view(\"fro\")") @Cast("c10::string_view*") Pointer ord, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_norm(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_norm(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::string_view(\"fro\")") @Cast("c10::string_view*") Pointer ord, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);

// aten::linalg_matrix_norm.str_ord_out(Tensor self, str ord='fro', int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::string_view(\"fro\")") @Cast("c10::string_view*") Pointer ord, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::string_view(\"fro\")") @Cast("c10::string_view*") Pointer ord, @ByVal(nullValue = "at::IntArrayRef({-2,-1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
// aten::linalg_matrix_norm.str_ord_out(Tensor self, str ord='fro', int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer ord, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_norm_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer ord, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_matrix_power.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_matrix_power_ops.h>


// aten::linalg_matrix_power(Tensor self, int n) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matrix_power(@Const @ByRef Tensor self, @Cast("int64_t") long n);

// aten::linalg_matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_power_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long n);
// aten::linalg_matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_power_outf(@Const @ByRef Tensor self, @Cast("int64_t") long n, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_matrix_rank.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_matrix_rank_ops.h>


// aten::linalg_matrix_rank.atol_rtol_tensor(Tensor input, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matrix_rank(@Const @ByRef Tensor input, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional atol, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional rtol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_rank(@Const @ByRef Tensor input);

// aten::linalg_matrix_rank.atol_rtol_tensor_out(Tensor input, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional atol, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional rtol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_out(@ByRef Tensor out, @Const @ByRef Tensor input);
// aten::linalg_matrix_rank.atol_rtol_tensor_out(Tensor input, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_outf(@Const @ByRef Tensor input, @Const @ByRef TensorOptional atol, @Const @ByRef TensorOptional rtol, @Cast("bool") boolean hermitian, @ByRef Tensor out);

// aten::linalg_matrix_rank.atol_rtol_float(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matrix_rank(@Const @ByRef Tensor self, @ByVal DoubleOptional atol, @ByVal DoubleOptional rtol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_rank(@Const @ByRef Tensor self, @ByVal DoubleOptional atol, @ByVal DoubleOptional rtol);

// aten::linalg_matrix_rank.atol_rtol_float_out(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DoubleOptional atol, @ByVal DoubleOptional rtol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DoubleOptional atol, @ByVal DoubleOptional rtol);
// aten::linalg_matrix_rank.atol_rtol_float_out(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_outf(@Const @ByRef Tensor self, @ByVal DoubleOptional atol, @ByVal DoubleOptional rtol, @Cast("bool") boolean hermitian, @ByRef Tensor out);

// aten::linalg_matrix_rank(Tensor self, float tol, bool hermitian=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matrix_rank(@Const @ByRef Tensor self, double tol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_rank(@Const @ByRef Tensor self, double tol);

// aten::linalg_matrix_rank.out(Tensor self, float tol, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_out(@ByRef Tensor out, @Const @ByRef Tensor self, double tol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_out(@ByRef Tensor out, @Const @ByRef Tensor self, double tol);
// aten::linalg_matrix_rank.out(Tensor self, float tol, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_outf(@Const @ByRef Tensor self, double tol, @Cast("bool") boolean hermitian, @ByRef Tensor out);

// aten::linalg_matrix_rank.tol_tensor(Tensor input, Tensor tol, bool hermitian=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_matrix_rank(@Const @ByRef Tensor input, @Const @ByRef Tensor tol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_rank(@Const @ByRef Tensor input, @Const @ByRef Tensor tol);

// aten::linalg_matrix_rank.out_tol_tensor(Tensor input, Tensor tol, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor tol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor tol);
// aten::linalg_matrix_rank.out_tol_tensor(Tensor input, Tensor tol, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor tol, @Cast("bool") boolean hermitian, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_multi_dot.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_multi_dot_ops.h>


// aten::linalg_multi_dot(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_multi_dot(@ByVal TensorArrayRef tensors);

// aten::linalg_multi_dot.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_multi_dot_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
// aten::linalg_multi_dot.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_multi_dot_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_norm_ops.h>


// aten::linalg_norm(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_norm(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor linalg_norm(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor linalg_norm(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);

// aten::linalg_norm.ord_str(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_norm(@Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor linalg_norm(@Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer ord);
@Namespace("at") public static native @ByVal Tensor linalg_norm(@Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);

// aten::linalg_norm.out(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor linalg_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor linalg_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
// aten::linalg_norm.out(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional ord, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor linalg_norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional ord, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::linalg_norm.ord_str_out(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor linalg_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer ord);
@Namespace("at") public static native @ByRef Tensor linalg_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
// aten::linalg_norm.ord_str_out(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_norm_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer ord, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor linalg_norm_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::string_view*") Pointer ord, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_pinv.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_pinv_ops.h>


// aten::linalg_pinv.atol_rtol_tensor(Tensor self, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional atol, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional rtol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self);

// aten::linalg_pinv.atol_rtol_tensor_out(Tensor self, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional atol, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional rtol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::linalg_pinv.atol_rtol_tensor_out(Tensor self, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_pinv_outf(@Const @ByRef Tensor self, @Const @ByRef TensorOptional atol, @Const @ByRef TensorOptional rtol, @Cast("bool") boolean hermitian, @ByRef Tensor out);

// aten::linalg_pinv.atol_rtol_float(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self, @ByVal DoubleOptional atol, @ByVal DoubleOptional rtol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self, @ByVal DoubleOptional atol, @ByVal DoubleOptional rtol);

// aten::linalg_pinv.atol_rtol_float_out(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DoubleOptional atol, @ByVal DoubleOptional rtol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DoubleOptional atol, @ByVal DoubleOptional rtol);
// aten::linalg_pinv.atol_rtol_float_out(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_pinv_outf(@Const @ByRef Tensor self, @ByVal DoubleOptional atol, @ByVal DoubleOptional rtol, @Cast("bool") boolean hermitian, @ByRef Tensor out);

// aten::linalg_pinv(Tensor self, float rcond, bool hermitian=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self, double rcond, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self, double rcond);

// aten::linalg_pinv.rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self, @Const @ByRef Tensor rcond, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self, @Const @ByRef Tensor rcond);

// aten::linalg_pinv.out(Tensor self, float rcond, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, double rcond, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, double rcond);
// aten::linalg_pinv.out(Tensor self, float rcond, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_pinv_outf(@Const @ByRef Tensor self, double rcond, @Cast("bool") boolean hermitian, @ByRef Tensor out);

// aten::linalg_pinv.out_rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor rcond, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor rcond);
// aten::linalg_pinv.out_rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_pinv_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor rcond, @Cast("bool") boolean hermitian, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_qr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_qr_ops.h>


// aten::linalg_qr(Tensor A, str mode='reduced') -> (Tensor Q, Tensor R)
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_qr(@Const @ByRef Tensor A, @ByVal(nullValue = "c10::string_view(\"reduced\")") @Cast("c10::string_view*") Pointer mode);
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_qr(@Const @ByRef Tensor A);

// aten::linalg_qr.out(Tensor A, str mode='reduced', *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_qr_out(@ByRef Tensor Q, @ByRef Tensor R, @Const @ByRef Tensor A, @ByVal(nullValue = "c10::string_view(\"reduced\")") @Cast("c10::string_view*") Pointer mode);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_qr_out(@ByRef Tensor Q, @ByRef Tensor R, @Const @ByRef Tensor A);
// aten::linalg_qr.out(Tensor A, str mode='reduced', *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_qr_outf(@Const @ByRef Tensor A, @ByVal @Cast("c10::string_view*") Pointer mode, @ByRef Tensor Q, @ByRef Tensor R);




// Parsed from ATen/ops/linalg_slogdet.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_slogdet_ops.h>


// aten::linalg_slogdet(Tensor A) -> (Tensor sign, Tensor logabsdet)
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_slogdet(@Const @ByRef Tensor A);

// aten::linalg_slogdet.out(Tensor A, *, Tensor(a!) sign, Tensor(b!) logabsdet) -> (Tensor(a!) sign, Tensor(b!) logabsdet)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_slogdet_out(@ByRef Tensor sign, @ByRef Tensor logabsdet, @Const @ByRef Tensor A);
// aten::linalg_slogdet.out(Tensor A, *, Tensor(a!) sign, Tensor(b!) logabsdet) -> (Tensor(a!) sign, Tensor(b!) logabsdet)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_slogdet_outf(@Const @ByRef Tensor A, @ByRef Tensor sign, @ByRef Tensor logabsdet);




// Parsed from ATen/ops/linalg_solve.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_solve_ops.h>


// aten::linalg_solve(Tensor A, Tensor B, *, bool left=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_solve(@Const @ByRef Tensor A, @Const @ByRef Tensor B, @Cast("bool") boolean left/*=true*/);
@Namespace("at") public static native @ByVal Tensor linalg_solve(@Const @ByRef Tensor A, @Const @ByRef Tensor B);

// aten::linalg_solve.out(Tensor A, Tensor B, *, bool left=True, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_solve_out(@ByRef Tensor out, @Const @ByRef Tensor A, @Const @ByRef Tensor B, @Cast("bool") boolean left/*=true*/);
@Namespace("at") public static native @ByRef Tensor linalg_solve_out(@ByRef Tensor out, @Const @ByRef Tensor A, @Const @ByRef Tensor B);
// aten::linalg_solve.out(Tensor A, Tensor B, *, bool left=True, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_solve_outf(@Const @ByRef Tensor A, @Const @ByRef Tensor B, @Cast("bool") boolean left, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_solve_ex.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_solve_ex_ops.h>


// aten::linalg_solve_ex(Tensor A, Tensor B, *, bool left=True, bool check_errors=False) -> (Tensor result, Tensor info)
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_solve_ex(@Const @ByRef Tensor A, @Const @ByRef Tensor B, @Cast("bool") boolean left/*=true*/, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple linalg_solve_ex(@Const @ByRef Tensor A, @Const @ByRef Tensor B);

// aten::linalg_solve_ex.out(Tensor A, Tensor B, *, bool left=True, bool check_errors=False, Tensor(a!) result, Tensor(b!) info) -> (Tensor(a!) result, Tensor(b!) info)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_solve_ex_out(@ByRef Tensor result, @ByRef Tensor info, @Const @ByRef Tensor A, @Const @ByRef Tensor B, @Cast("bool") boolean left/*=true*/, @Cast("bool") boolean check_errors/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_solve_ex_out(@ByRef Tensor result, @ByRef Tensor info, @Const @ByRef Tensor A, @Const @ByRef Tensor B);
// aten::linalg_solve_ex.out(Tensor A, Tensor B, *, bool left=True, bool check_errors=False, Tensor(a!) result, Tensor(b!) info) -> (Tensor(a!) result, Tensor(b!) info)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_solve_ex_outf(@Const @ByRef Tensor A, @Const @ByRef Tensor B, @Cast("bool") boolean left, @Cast("bool") boolean check_errors, @ByRef Tensor result, @ByRef Tensor info);




// Parsed from ATen/ops/linalg_solve_triangular.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_solve_triangular_ops.h>


// aten::linalg_solve_triangular.out(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_solve_triangular_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor B, @Cast("bool") boolean upper, @Cast("bool") boolean left/*=true*/, @Cast("bool") boolean unitriangular/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_solve_triangular_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor B, @Cast("bool") boolean upper);
// aten::linalg_solve_triangular.out(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_solve_triangular_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor B, @Cast("bool") boolean upper, @Cast("bool") boolean left, @Cast("bool") boolean unitriangular, @ByRef Tensor out);

// aten::linalg_solve_triangular(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_solve_triangular(@Const @ByRef Tensor self, @Const @ByRef Tensor B, @Cast("bool") boolean upper, @Cast("bool") boolean left/*=true*/, @Cast("bool") boolean unitriangular/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_solve_triangular(@Const @ByRef Tensor self, @Const @ByRef Tensor B, @Cast("bool") boolean upper);




// Parsed from ATen/ops/linalg_svd.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_svd_ops.h>


// aten::linalg_svd(Tensor A, bool full_matrices=True, *, str? driver=None) -> (Tensor U, Tensor S, Tensor Vh)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple linalg_svd(@Const @ByRef Tensor A, @Cast("bool") boolean full_matrices/*=true*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer driver);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple linalg_svd(@Const @ByRef Tensor A);

// aten::linalg_svd.U(Tensor A, bool full_matrices=True, *, str? driver=None, Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_svd_out(@ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor Vh, @Const @ByRef Tensor A, @Cast("bool") boolean full_matrices/*=true*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer driver);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_svd_out(@ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor Vh, @Const @ByRef Tensor A);
// aten::linalg_svd.U(Tensor A, bool full_matrices=True, *, str? driver=None, Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_svd_outf(@Const @ByRef Tensor A, @Cast("bool") boolean full_matrices, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer driver, @ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor Vh);




// Parsed from ATen/ops/linalg_svdvals.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_svdvals_ops.h>


// aten::linalg_svdvals(Tensor A, *, str? driver=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_svdvals(@Const @ByRef Tensor A, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer driver);
@Namespace("at") public static native @ByVal Tensor linalg_svdvals(@Const @ByRef Tensor A);

// aten::linalg_svdvals.out(Tensor A, *, str? driver=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_svdvals_out(@ByRef Tensor out, @Const @ByRef Tensor A, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer driver);
@Namespace("at") public static native @ByRef Tensor linalg_svdvals_out(@ByRef Tensor out, @Const @ByRef Tensor A);
// aten::linalg_svdvals.out(Tensor A, *, str? driver=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_svdvals_outf(@Const @ByRef Tensor A, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer driver, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_tensorinv.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_tensorinv_ops.h>


// aten::linalg_tensorinv(Tensor self, int ind=2) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_tensorinv(@Const @ByRef Tensor self, @Cast("int64_t") long ind/*=2*/);
@Namespace("at") public static native @ByVal Tensor linalg_tensorinv(@Const @ByRef Tensor self);

// aten::linalg_tensorinv.out(Tensor self, int ind=2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_tensorinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long ind/*=2*/);
@Namespace("at") public static native @ByRef Tensor linalg_tensorinv_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::linalg_tensorinv.out(Tensor self, int ind=2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_tensorinv_outf(@Const @ByRef Tensor self, @Cast("int64_t") long ind, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_tensorsolve.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_tensorsolve_ops.h>


// aten::linalg_tensorsolve(Tensor self, Tensor other, int[]? dims=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_tensorsolve(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dims);
@Namespace("at") public static native @ByVal Tensor linalg_tensorsolve(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByVal Tensor linalg_tensorsolve(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);

// aten::linalg_tensorsolve.out(Tensor self, Tensor other, int[]? dims=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_tensorsolve_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dims);
@Namespace("at") public static native @ByRef Tensor linalg_tensorsolve_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor linalg_tensorsolve_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);
// aten::linalg_tensorsolve.out(Tensor self, Tensor other, int[]? dims=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_tensorsolve_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal LongArrayRefOptional dims, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor linalg_tensorsolve_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dims, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_vander.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_vander_ops.h>


// aten::linalg_vander(Tensor x, *, int? N=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_vander(@Const @ByRef Tensor x, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional N);
@Namespace("at") public static native @ByVal Tensor linalg_vander(@Const @ByRef Tensor x);




// Parsed from ATen/ops/linalg_vecdot.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_vecdot_ops.h>


// aten::linalg_vecdot(Tensor x, Tensor y, *, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_vecdot(@Const @ByRef Tensor x, @Const @ByRef Tensor y, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor linalg_vecdot(@Const @ByRef Tensor x, @Const @ByRef Tensor y);

// aten::linalg_vecdot.out(Tensor x, Tensor y, *, int dim=-1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_vecdot_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor y, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByRef Tensor linalg_vecdot_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor y);
// aten::linalg_vecdot.out(Tensor x, Tensor y, *, int dim=-1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_vecdot_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor y, @Cast("int64_t") long dim, @ByRef Tensor out);




// Parsed from ATen/ops/linalg_vector_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linalg_vector_norm_ops.h>


// aten::linalg_vector_norm(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linalg_vector_norm(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor linalg_vector_norm(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor linalg_vector_norm(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);

// aten::linalg_vector_norm.out(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_vector_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor linalg_vector_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor linalg_vector_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar ord, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
// aten::linalg_vector_norm.out(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linalg_vector_norm_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar ord, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor linalg_vector_norm_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar ord, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/linear.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linear_ops.h>


// aten::linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linear(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias);
@Namespace("at") public static native @ByVal Tensor linear(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);

// aten::linear.out(Tensor input, Tensor weight, Tensor? bias=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linear_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias);
@Namespace("at") public static native @ByRef Tensor linear_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor weight);
// aten::linear.out(Tensor input, Tensor weight, Tensor? bias=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linear_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByRef Tensor out);




// Parsed from ATen/ops/linear_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linear_backward_ops.h>


// aten::linear_backward(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple linear_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::linear_backward.out(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linear_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
// aten::linear_backward.out(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linear_backward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/linspace.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/linspace_ops.h>


// aten::linspace(Scalar start, Scalar end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor linspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps);
// aten::linspace(Scalar start, Scalar end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor linspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::linspace.out(Scalar start, Scalar end, int steps, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linspace_out(@ByRef Tensor out, @Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps);
// aten::linspace.out(Scalar start, Scalar end, int steps, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor linspace_outf(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps, @ByRef Tensor out);




// Parsed from ATen/ops/log.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/log_ops.h>


// aten::log(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor log(@Const @ByRef Tensor self);

// aten::log_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_(@ByRef Tensor self);

// aten::log.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::log.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/log10.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/log10_ops.h>


// aten::log10(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor log10(@Const @ByRef Tensor self);

// aten::log10_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log10_(@ByRef Tensor self);

// aten::log10.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log10_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::log10.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log10_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/log1p.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/log1p_ops.h>


// aten::log1p(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor log1p(@Const @ByRef Tensor self);

// aten::log1p_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log1p_(@ByRef Tensor self);

// aten::log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log1p_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log1p_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/log2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/log2_ops.h>


// aten::log2(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor log2(@Const @ByRef Tensor self);

// aten::log2_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log2_(@ByRef Tensor self);

// aten::log2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::log2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log2_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/log_normal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/log_normal_ops.h>


// aten::log_normal.out(Tensor self, float mean=1, float std=2, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_normal_out(@ByRef Tensor out, @Const @ByRef Tensor self, double mean/*=1*/, double std/*=2*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor log_normal_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::log_normal.out(Tensor self, float mean=1, float std=2, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_normal_outf(@Const @ByRef Tensor self, double mean, double std, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::log_normal(Tensor self, float mean=1, float std=2, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor log_normal(@Const @ByRef Tensor self, double mean/*=1*/, double std/*=2*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor log_normal(@Const @ByRef Tensor self);




// Parsed from ATen/ops/log_sigmoid.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/log_sigmoid_ops.h>


// aten::log_sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_sigmoid_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::log_sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_sigmoid_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::log_sigmoid(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor log_sigmoid(@Const @ByRef Tensor self);




// Parsed from ATen/ops/log_sigmoid_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/log_sigmoid_backward_ops.h>


// aten::log_sigmoid_backward.grad_input(Tensor grad_output, Tensor self, Tensor buffer, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_sigmoid_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor buffer);
// aten::log_sigmoid_backward.grad_input(Tensor grad_output, Tensor self, Tensor buffer, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_sigmoid_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor buffer, @ByRef Tensor grad_input);

// aten::log_sigmoid_backward(Tensor grad_output, Tensor self, Tensor buffer) -> Tensor
@Namespace("at") public static native @ByVal Tensor log_sigmoid_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor buffer);




// Parsed from ATen/ops/log_sigmoid_forward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/log_sigmoid_forward_ops.h>


// aten::log_sigmoid_forward.output(Tensor self, *, Tensor(a!) output, Tensor(b!) buffer) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> log_sigmoid_forward_out(@ByRef Tensor output, @ByRef Tensor buffer, @Const @ByRef Tensor self);
// aten::log_sigmoid_forward.output(Tensor self, *, Tensor(a!) output, Tensor(b!) buffer) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> log_sigmoid_forward_outf(@Const @ByRef Tensor self, @ByRef Tensor output, @ByRef Tensor buffer);

// aten::log_sigmoid_forward(Tensor self) -> (Tensor output, Tensor buffer)
@Namespace("at") public static native @ByVal TensorTensorTuple log_sigmoid_forward(@Const @ByRef Tensor self);




// Parsed from ATen/ops/log_softmax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/log_softmax_ops.h>


// aten::log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::log_softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_softmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor log_softmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::log_softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor log_softmax_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor log_softmax(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor log_softmax(@Const @ByRef Tensor self, @ByVal Dimname dim);




// Parsed from ATen/ops/logaddexp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logaddexp_ops.h>


// aten::logaddexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logaddexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::logaddexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logaddexp_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::logaddexp(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor logaddexp(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/logaddexp2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logaddexp2_ops.h>


// aten::logaddexp2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logaddexp2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::logaddexp2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logaddexp2_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::logaddexp2(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor logaddexp2(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/logcumsumexp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logcumsumexp_ops.h>


// aten::logcumsumexp(Tensor self, int dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor logcumsumexp(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logcumsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logcumsumexp_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor out);

// aten::logcumsumexp.dimname(Tensor self, Dimname dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor logcumsumexp(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::logcumsumexp.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logcumsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::logcumsumexp.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logcumsumexp_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByRef Tensor out);




// Parsed from ATen/ops/logdet.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logdet_ops.h>


// aten::logdet(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor logdet(@Const @ByRef Tensor self);




// Parsed from ATen/ops/logical_and.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logical_and_ops.h>


// aten::logical_and(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor logical_and(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::logical_and.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_and_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::logical_and.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_and_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/logical_not.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logical_not_ops.h>


// aten::logical_not(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor logical_not(@Const @ByRef Tensor self);

// aten::logical_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_not_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::logical_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_not_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/logical_or.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logical_or_ops.h>


// aten::logical_or(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor logical_or(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::logical_or.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_or_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::logical_or.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_or_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/logical_xor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logical_xor_ops.h>


// aten::logical_xor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor logical_xor(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::logical_xor.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_xor_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::logical_xor.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logical_xor_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/logit.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logit_ops.h>


// aten::logit(Tensor self, float? eps=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor logit(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByVal Tensor logit(@Const @ByRef Tensor self);

// aten::logit_(Tensor(a!) self, float? eps=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logit_(@ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByRef Tensor logit_(@ByRef Tensor self);

// aten::logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logit_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByRef Tensor logit_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logit_outf(@Const @ByRef Tensor self, @ByVal DoubleOptional eps, @ByRef Tensor out);




// Parsed from ATen/ops/logit_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logit_backward_ops.h>


// aten::logit_backward.grad_input(Tensor grad_output, Tensor self, float? eps=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logit_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByRef Tensor logit_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
// aten::logit_backward.grad_input(Tensor grad_output, Tensor self, float? eps=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logit_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal DoubleOptional eps, @ByRef Tensor grad_input);

// aten::logit_backward(Tensor grad_output, Tensor self, float? eps=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor logit_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByVal Tensor logit_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);




// Parsed from ATen/ops/logspace.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logspace_ops.h>


// aten::logspace(Scalar start, Scalar end, int steps, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor logspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps, double base/*=10.0*/, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor logspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps);
// aten::logspace(Scalar start, Scalar end, int steps, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor logspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps, double base, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::logspace.out(Scalar start, Scalar end, int steps, float base=10.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logspace_out(@ByRef Tensor out, @Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps, double base/*=10.0*/);
@Namespace("at") public static native @ByRef Tensor logspace_out(@ByRef Tensor out, @Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps);
// aten::logspace.out(Scalar start, Scalar end, int steps, float base=10.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logspace_outf(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps, double base, @ByRef Tensor out);




// Parsed from ATen/ops/logsumexp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/logsumexp_ops.h>


// aten::logsumexp(Tensor self, int[1] dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);
// aten::logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logsumexp_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor logsumexp_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::logsumexp.names(Tensor self, Dimname[1] dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);

// aten::logsumexp.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
// aten::logsumexp.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor logsumexp_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);




// Parsed from ATen/ops/lshift.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lshift_ops.h>


// aten::__lshift__.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __lshift__(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::__lshift__.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __lshift__(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::__lshift__.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor __lshift___out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::__lshift__.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor __lshift___outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::__lshift__.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor __lshift___out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::__lshift__.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor __lshift___outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/lstm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lstm_ops.h>


// aten::lstm.input(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple lstm(@Const @ByRef Tensor input, @ByVal TensorArrayRef hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);

// aten::lstm.data(Tensor data, Tensor batch_sizes, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple lstm(@Const @ByRef Tensor data, @Const @ByRef Tensor batch_sizes, @ByVal TensorArrayRef hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional);




// Parsed from ATen/ops/lstm_cell.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lstm_cell_ops.h>


// aten::lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple lstm_cell(@Const @ByRef Tensor input, @ByVal TensorArrayRef hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_ih, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_hh);
@Namespace("at") public static native @ByVal TensorTensorTuple lstm_cell(@Const @ByRef Tensor input, @ByVal TensorArrayRef hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh);




// Parsed from ATen/ops/lstm_mps_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lstm_mps_backward_ops.h>


// aten::lstm_mps_backward(Tensor grad_y, Tensor? grad_hy, Tensor? grad_cy, Tensor z_state, Tensor cell_state_fwd, Tensor input, Tensor layersOutputs, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor[], Tensor[])
@Namespace("at") public static native @ByVal TensorTensorVectorTensorVectorTuple lstm_mps_backward(@Const @ByRef Tensor grad_y, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Const @ByRef Tensor z_state, @Const @ByRef Tensor cell_state_fwd, @Const @ByRef Tensor input, @Const @ByRef Tensor layersOutputs, @ByVal TensorArrayRef hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);

// aten::lstm_mps_backward.out(Tensor grad_y, Tensor? grad_hy, Tensor? grad_cy, Tensor z_state, Tensor cell_state_fwd, Tensor input, Tensor layersOutputs, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, *, Tensor(a!) out0, Tensor(b!)[] out1, Tensor(c!)[] out2) -> ()
@Namespace("at") public static native void lstm_mps_backward_out(@ByRef Tensor out0, @ByVal TensorArrayRef out1, @ByVal TensorArrayRef out2, @Const @ByRef Tensor grad_y, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Const @ByRef Tensor z_state, @Const @ByRef Tensor cell_state_fwd, @Const @ByRef Tensor input, @Const @ByRef Tensor layersOutputs, @ByVal TensorArrayRef hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);
// aten::lstm_mps_backward.out(Tensor grad_y, Tensor? grad_hy, Tensor? grad_cy, Tensor z_state, Tensor cell_state_fwd, Tensor input, Tensor layersOutputs, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, *, Tensor(a!) out0, Tensor(b!)[] out1, Tensor(c!)[] out2) -> ()
@Namespace("at") public static native void lstm_mps_backward_outf(@Const @ByRef Tensor grad_y, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Const @ByRef Tensor z_state, @Const @ByRef Tensor cell_state_fwd, @Const @ByRef Tensor input, @Const @ByRef Tensor layersOutputs, @ByVal TensorArrayRef hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first, @ByRef Tensor out0, @ByVal TensorArrayRef out1, @ByVal TensorArrayRef out2);




// Parsed from ATen/ops/lt.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lt_ops.h>


// aten::lt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lt_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::lt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lt_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::lt.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor lt(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::lt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lt_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::lt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lt_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::lt.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor lt(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/lu_solve.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lu_solve_ops.h>


// aten::lu_solve.out(Tensor self, Tensor LU_data, Tensor LU_pivots, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lu_solve_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots);
// aten::lu_solve.out(Tensor self, Tensor LU_data, Tensor LU_pivots, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor lu_solve_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots, @ByRef Tensor out);

// aten::lu_solve(Tensor self, Tensor LU_data, Tensor LU_pivots) -> Tensor
@Namespace("at") public static native @ByVal Tensor lu_solve(@Const @ByRef Tensor self, @Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots);




// Parsed from ATen/ops/lu_unpack.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/lu_unpack_ops.h>


// aten::lu_unpack(Tensor LU_data, Tensor LU_pivots, bool unpack_data=True, bool unpack_pivots=True) -> (Tensor P, Tensor L, Tensor U)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple lu_unpack(@Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots, @Cast("bool") boolean unpack_data/*=true*/, @Cast("bool") boolean unpack_pivots/*=true*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple lu_unpack(@Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots);

// aten::lu_unpack.out(Tensor LU_data, Tensor LU_pivots, bool unpack_data=True, bool unpack_pivots=True, *, Tensor(a!) P, Tensor(b!) L, Tensor(c!) U) -> (Tensor(a!) P, Tensor(b!) L, Tensor(c!) U)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> lu_unpack_out(@ByRef Tensor P, @ByRef Tensor L, @ByRef Tensor U, @Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots, @Cast("bool") boolean unpack_data/*=true*/, @Cast("bool") boolean unpack_pivots/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> lu_unpack_out(@ByRef Tensor P, @ByRef Tensor L, @ByRef Tensor U, @Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots);
// aten::lu_unpack.out(Tensor LU_data, Tensor LU_pivots, bool unpack_data=True, bool unpack_pivots=True, *, Tensor(a!) P, Tensor(b!) L, Tensor(c!) U) -> (Tensor(a!) P, Tensor(b!) L, Tensor(c!) U)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> lu_unpack_outf(@Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots, @Cast("bool") boolean unpack_data, @Cast("bool") boolean unpack_pivots, @ByRef Tensor P, @ByRef Tensor L, @ByRef Tensor U);




// Parsed from ATen/ops/mH.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mH_ops.h>






// Parsed from ATen/ops/mT.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mT_ops.h>






// Parsed from ATen/ops/margin_ranking_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/margin_ranking_loss_ops.h>


// aten::margin_ranking_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor margin_ranking_loss(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor target, double margin/*=0.0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor margin_ranking_loss(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor target);




// Parsed from ATen/ops/masked_fill.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/masked_fill_ops.h>


// aten::masked_fill.Scalar(Tensor self, Tensor mask, Scalar value) -> Tensor
@Namespace("at") public static native @ByVal Tensor masked_fill(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @Const @ByRef Scalar value);

// aten::masked_fill.Tensor(Tensor self, Tensor mask, Tensor value) -> Tensor
@Namespace("at") public static native @ByVal Tensor masked_fill(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @Const @ByRef Tensor value);

// aten::masked_fill.Scalar_out(Tensor self, Tensor mask, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor masked_fill_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mask, @Const @ByRef Scalar value);
// aten::masked_fill.Scalar_out(Tensor self, Tensor mask, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor masked_fill_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @Const @ByRef Scalar value, @ByRef Tensor out);

// aten::masked_fill.Tensor_out(Tensor self, Tensor mask, Tensor value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor masked_fill_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mask, @Const @ByRef Tensor value);
// aten::masked_fill.Tensor_out(Tensor self, Tensor mask, Tensor value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor masked_fill_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @Const @ByRef Tensor value, @ByRef Tensor out);




// Parsed from ATen/ops/masked_scatter.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/masked_scatter_ops.h>


// aten::masked_scatter(Tensor self, Tensor mask, Tensor source) -> Tensor
@Namespace("at") public static native @ByVal Tensor masked_scatter(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @Const @ByRef Tensor source);

// aten::masked_scatter.out(Tensor self, Tensor mask, Tensor source, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor masked_scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mask, @Const @ByRef Tensor source);
// aten::masked_scatter.out(Tensor self, Tensor mask, Tensor source, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor masked_scatter_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @Const @ByRef Tensor source, @ByRef Tensor out);




// Parsed from ATen/ops/masked_select.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/masked_select_ops.h>


// aten::masked_select.out(Tensor self, Tensor mask, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor masked_select_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mask);
// aten::masked_select.out(Tensor self, Tensor mask, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor masked_select_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @ByRef Tensor out);

// aten::masked_select(Tensor self, Tensor mask) -> Tensor
@Namespace("at") public static native @ByVal Tensor masked_select(@Const @ByRef Tensor self, @Const @ByRef Tensor mask);




// Parsed from ATen/ops/masked_select_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/masked_select_backward_ops.h>


// aten::masked_select_backward(Tensor grad, Tensor input, Tensor mask) -> Tensor
@Namespace("at") public static native @ByVal Tensor masked_select_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor input, @Const @ByRef Tensor mask);




// Parsed from ATen/ops/matmul.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/matmul_ops.h>


// aten::matmul(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor matmul(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor matmul_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor matmul_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/matmul_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/matmul_backward_ops.h>


// aten::matmul_backward(Tensor grad, Tensor self, Tensor other, bool[2] mask) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple matmul_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("std::array<bool,2>*") BoolPointer mask);

// aten::matmul_backward.out(Tensor grad, Tensor self, Tensor other, bool[2] mask, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> matmul_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor grad, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("std::array<bool,2>*") BoolPointer mask);
// aten::matmul_backward.out(Tensor grad, Tensor self, Tensor other, bool[2] mask, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> matmul_backward_outf(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("std::array<bool,2>*") BoolPointer mask, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/matrix_H.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/matrix_H_ops.h>






// Parsed from ATen/ops/matrix_exp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/matrix_exp_ops.h>


// aten::matrix_exp(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor matrix_exp(@Const @ByRef Tensor self);




// Parsed from ATen/ops/matrix_exp_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/matrix_exp_backward_ops.h>


// aten::matrix_exp_backward(Tensor self, Tensor grad) -> Tensor
@Namespace("at") public static native @ByVal Tensor matrix_exp_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad);




// Parsed from ATen/ops/matrix_power.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/matrix_power_ops.h>


// aten::matrix_power(Tensor self, int n) -> Tensor
@Namespace("at") public static native @ByVal Tensor matrix_power(@Const @ByRef Tensor self, @Cast("int64_t") long n);

// aten::matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor matrix_power_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long n);
// aten::matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor matrix_power_outf(@Const @ByRef Tensor self, @Cast("int64_t") long n, @ByRef Tensor out);




// Parsed from ATen/ops/max.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_ops.h>


// aten::max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple max(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple max(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::max.dim_max(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_out(@ByRef Tensor max, @ByRef Tensor max_values, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_out(@ByRef Tensor max, @ByRef Tensor max_values, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::max.dim_max(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor max, @ByRef Tensor max_values);

// aten::max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple max(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple max(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::max.names_dim_max(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_out(@ByRef Tensor max, @ByRef Tensor max_values, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_out(@ByRef Tensor max, @ByRef Tensor max_values, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::max.names_dim_max(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor max, @ByRef Tensor max_values);

// aten::max(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor max(@Const @ByRef Tensor self);

// aten::max.other(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor max(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::max.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::max.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::max.unary_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::max.unary_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/max_pool1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_pool1d_ops.h>


// aten::max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);




// Parsed from ATen/ops/max_pool1d_with_indices.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_pool1d_with_indices_ops.h>


// aten::max_pool1d_with_indices(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool1d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool1d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool1d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool1d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);




// Parsed from ATen/ops/max_pool2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_pool2d_ops.h>


// aten::max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);




// Parsed from ATen/ops/max_pool2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_pool2d_backward_ops.h>


// aten::max_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::max_pool2d_backward.out(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_pool2d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor max_pool2d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor max_pool2d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor max_pool2d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
// aten::max_pool2d_backward.out(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor max_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);




// Parsed from ATen/ops/max_pool2d_with_indices.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_pool2d_with_indices_ops.h>


// aten::max_pool2d_with_indices.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool2d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool2d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool2d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool2d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
// aten::max_pool2d_with_indices.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool2d_with_indices_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool2d_with_indices_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out, @ByRef Tensor indices);

// aten::max_pool2d_with_indices(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool2d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool2d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool2d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool2d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);




// Parsed from ATen/ops/max_pool2d_with_indices_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_pool2d_with_indices_backward_ops.h>


// aten::max_pool2d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_pool2d_with_indices_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor max_pool2d_with_indices_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
// aten::max_pool2d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_pool2d_with_indices_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor max_pool2d_with_indices_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);

// aten::max_pool2d_with_indices_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_pool2d_with_indices_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByVal Tensor max_pool2d_with_indices_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);




// Parsed from ATen/ops/max_pool3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_pool3d_ops.h>


// aten::max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);




// Parsed from ATen/ops/max_pool3d_with_indices.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_pool3d_with_indices_ops.h>


// aten::max_pool3d_with_indices.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool3d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool3d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool3d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool3d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
// aten::max_pool3d_with_indices.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool3d_with_indices_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool3d_with_indices_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out, @ByRef Tensor indices);

// aten::max_pool3d_with_indices(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool3d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool3d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool3d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple max_pool3d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);




// Parsed from ATen/ops/max_pool3d_with_indices_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_pool3d_with_indices_backward_ops.h>


// aten::max_pool3d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_pool3d_with_indices_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor max_pool3d_with_indices_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
// aten::max_pool3d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_pool3d_with_indices_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor max_pool3d_with_indices_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);

// aten::max_pool3d_with_indices_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_pool3d_with_indices_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByVal Tensor max_pool3d_with_indices_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);




// Parsed from ATen/ops/max_unpool2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_unpool2d_ops.h>


// aten::max_unpool2d.out(Tensor self, Tensor indices, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_unpool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor max_unpool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);
// aten::max_unpool2d.out(Tensor self, Tensor indices, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_unpool2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor max_unpool2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByRef Tensor out);

// aten::max_unpool2d(Tensor self, Tensor indices, int[2] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_unpool2d(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor max_unpool2d(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);




// Parsed from ATen/ops/max_unpool3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/max_unpool3d_ops.h>


// aten::max_unpool3d.out(Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_unpool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor max_unpool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);
// aten::max_unpool3d.out(Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor max_unpool3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor max_unpool3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);

// aten::max_unpool3d(Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor max_unpool3d(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor max_unpool3d(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);




// Parsed from ATen/ops/maximum.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/maximum_ops.h>


// aten::maximum(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor maximum(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::maximum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor maximum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::maximum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor maximum_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/mean.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mean_ops.h>


// aten::mean(Tensor self, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self);

// aten::mean.dim(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::mean.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim);
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);
// aten::mean.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mean_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mean_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::mean.names_dim(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);

// aten::mean.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
// aten::mean.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mean_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/median.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/median_ops.h>


// aten::median(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor median(@Const @ByRef Tensor self);

// aten::median.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple median(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple median(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::median.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> median_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> median_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::median.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> median_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::median.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple median(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple median(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::median.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> median_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> median_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::median.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> median_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::median.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor median_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::median.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor median_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/meshgrid.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/meshgrid_ops.h>


// aten::meshgrid(Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector meshgrid(@ByVal TensorArrayRef tensors);

// aten::meshgrid.indexing(Tensor[] tensors, *, str indexing) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector meshgrid(@ByVal TensorArrayRef tensors, @ByVal @Cast("c10::string_view*") Pointer indexing);




// Parsed from ATen/ops/min.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/min_ops.h>


// aten::min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple min(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple min(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::min.dim_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> min_out(@ByRef Tensor min, @ByRef Tensor min_indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> min_out(@ByRef Tensor min, @ByRef Tensor min_indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::min.dim_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> min_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor min, @ByRef Tensor min_indices);

// aten::min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple min(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple min(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::min.names_dim_min(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> min_out(@ByRef Tensor min, @ByRef Tensor min_indices, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> min_out(@ByRef Tensor min, @ByRef Tensor min_indices, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::min.names_dim_min(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> min_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor min, @ByRef Tensor min_indices);

// aten::min(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor min(@Const @ByRef Tensor self);

// aten::min.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor min_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::min.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor min_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::min.other(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor min(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/minimum.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/minimum_ops.h>


// aten::minimum(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor minimum(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::minimum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor minimum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::minimum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor minimum_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/miopen_batch_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/miopen_batch_norm_ops.h>


// aten::miopen_batch_norm(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple miopen_batch_norm(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double exponential_average_factor, double epsilon);

// aten::miopen_batch_norm.out(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> miopen_batch_norm_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double exponential_average_factor, double epsilon);
// aten::miopen_batch_norm.out(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> miopen_batch_norm_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double exponential_average_factor, double epsilon, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/miopen_batch_norm_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/miopen_batch_norm_backward_ops.h>


// aten::miopen_batch_norm_backward(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple miopen_batch_norm_backward(@Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_var, double epsilon);

// aten::miopen_batch_norm_backward.out(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> miopen_batch_norm_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_var, double epsilon);
// aten::miopen_batch_norm_backward.out(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> miopen_batch_norm_backward_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_var, double epsilon, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/miopen_convolution.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/miopen_convolution_ops.h>


// aten::miopen_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_convolution_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_convolution_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByRef Tensor miopen_convolution_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_convolution_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor miopen_convolution_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);


// aten::miopen_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_convolution_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByRef Tensor miopen_convolution_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_convolution_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor miopen_convolution_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);





// Parsed from ATen/ops/miopen_convolution_add_relu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/miopen_convolution_add_relu_ops.h>


// aten::miopen_convolution_add_relu(Tensor self, Tensor weight, Tensor z, Scalar? alpha, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_convolution_add_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor z, @Const @ByRef ScalarOptional alpha, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_add_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor z, @Const @ByRef ScalarOptional alpha, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups);




// Parsed from ATen/ops/miopen_convolution_relu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/miopen_convolution_relu_ops.h>


// aten::miopen_convolution_relu(Tensor self, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_convolution_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups);




// Parsed from ATen/ops/miopen_convolution_transpose.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/miopen_convolution_transpose_ops.h>


// aten::miopen_convolution_transpose(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_convolution_transpose(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_convolution_transpose_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal SymIntRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_transpose_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal SymIntRef output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_convolution_transpose.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_convolution_transpose_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByRef Tensor miopen_convolution_transpose_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_convolution_transpose.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_convolution_transpose_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor miopen_convolution_transpose_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);


// aten::miopen_convolution_transpose.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_convolution_transpose_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal SymIntRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByRef Tensor miopen_convolution_transpose_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal SymIntRef output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_convolution_transpose.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_convolution_transpose_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal SymIntRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor miopen_convolution_transpose_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal SymIntRef output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);





// Parsed from ATen/ops/miopen_depthwise_convolution.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/miopen_depthwise_convolution_ops.h>


// aten::miopen_depthwise_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_depthwise_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_depthwise_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_depthwise_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
@Namespace("at") public static native @ByVal Tensor miopen_depthwise_convolution_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_depthwise_convolution_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_depthwise_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_depthwise_convolution_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByRef Tensor miopen_depthwise_convolution_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_depthwise_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_depthwise_convolution_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor miopen_depthwise_convolution_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);


// aten::miopen_depthwise_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_depthwise_convolution_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByRef Tensor miopen_depthwise_convolution_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);


// aten::miopen_depthwise_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor miopen_depthwise_convolution_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor miopen_depthwise_convolution_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByRef Tensor out);





// Parsed from ATen/ops/miopen_rnn.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/miopen_rnn_ops.h>


// aten::miopen_rnn(Tensor input, Tensor[] weight, int weight_stride0, Tensor hx, Tensor? cx, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTensorTuple miopen_rnn(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state);
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTensorTuple miopen_rnn(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Const @ByRef TensorOptional dropout_state);

// aten::miopen_rnn.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor hx, Tensor? cx, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> miopen_rnn_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4, @Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> miopen_rnn_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4, @Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Const @ByRef TensorOptional dropout_state);
// aten::miopen_rnn.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor hx, Tensor? cx, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> miopen_rnn_outf(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> miopen_rnn_outf(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Const @ByRef TensorOptional dropout_state, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4);




// Parsed from ATen/ops/miopen_rnn_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/miopen_rnn_backward_ops.h>


// aten::miopen_rnn_backward(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask) -> (Tensor, Tensor, Tensor, Tensor[])
@Namespace("at") public static native @ByVal TensorTensorTensorTensorVectorTuple miopen_rnn_backward(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTensorVectorTuple miopen_rnn_backward(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask);

// aten::miopen_rnn_backward.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!)[] out3) -> ()
@Namespace("at") public static native void miopen_rnn_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByVal TensorArrayRef out3, @Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask);
@Namespace("at") public static native void miopen_rnn_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByVal TensorArrayRef out3, @Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask);
// aten::miopen_rnn_backward.out(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!)[] out3) -> ()
@Namespace("at") public static native void miopen_rnn_backward_outf(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByVal TensorArrayRef out3);
@Namespace("at") public static native void miopen_rnn_backward_outf(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByVal TensorArrayRef out3);




// Parsed from ATen/ops/mish.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mish_ops.h>


// aten::mish(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor mish(@Const @ByRef Tensor self);

// aten::mish_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mish_(@ByRef Tensor self);

// aten::mish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mish_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::mish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mish_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/mish_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mish_backward_ops.h>


// aten::mish_backward(Tensor grad_output, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor mish_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);




// Parsed from ATen/ops/mkldnn_adaptive_avg_pool2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_adaptive_avg_pool2d_ops.h>


// aten::mkldnn_adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_adaptive_avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor mkldnn_adaptive_avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);

// aten::mkldnn_adaptive_avg_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_adaptive_avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor mkldnn_adaptive_avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);
// aten::mkldnn_adaptive_avg_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_adaptive_avg_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mkldnn_adaptive_avg_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByRef Tensor out);




// Parsed from ATen/ops/mkldnn_adaptive_avg_pool2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_adaptive_avg_pool2d_backward_ops.h>


// aten::mkldnn_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_adaptive_avg_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);

// aten::mkldnn_adaptive_avg_pool2d_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_adaptive_avg_pool2d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
// aten::mkldnn_adaptive_avg_pool2d_backward.out(Tensor grad_output, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_adaptive_avg_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/mkldnn_convolution.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_convolution_ops.h>


// aten::mkldnn_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor mkldnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups);


// aten::mkldnn_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_convolution_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor mkldnn_convolution_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups);


// aten::mkldnn_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_convolution_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByRef Tensor mkldnn_convolution_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups);


// aten::mkldnn_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_convolution_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mkldnn_convolution_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @ByRef Tensor out);


// aten::mkldnn_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_convolution_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByRef Tensor mkldnn_convolution_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups);


// aten::mkldnn_convolution.out(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, int[] stride, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_convolution_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mkldnn_convolution_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @ByRef Tensor out);





// Parsed from ATen/ops/mkldnn_linear.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_linear_ops.h>


// aten::mkldnn_linear(Tensor self, Tensor weight, Tensor? bias=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_linear(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias);
@Namespace("at") public static native @ByVal Tensor mkldnn_linear(@Const @ByRef Tensor self, @Const @ByRef Tensor weight);

// aten::mkldnn_linear.out(Tensor self, Tensor weight, Tensor? bias=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_linear_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias);
@Namespace("at") public static native @ByRef Tensor mkldnn_linear_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight);
// aten::mkldnn_linear.out(Tensor self, Tensor weight, Tensor? bias=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_linear_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByRef Tensor out);




// Parsed from ATen/ops/mkldnn_linear_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_linear_backward_ops.h>


// aten::mkldnn_linear_backward(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple mkldnn_linear_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::mkldnn_linear_backward.out(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mkldnn_linear_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
// aten::mkldnn_linear_backward.out(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mkldnn_linear_backward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/mkldnn_linear_backward_input.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_linear_backward_input_ops.h>


// aten::mkldnn_linear_backward_input(int[] input_size, Tensor grad_output, Tensor weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_linear_backward_input(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor mkldnn_linear_backward_input(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight);

// aten::mkldnn_linear_backward_input.out(int[] input_size, Tensor grad_output, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_linear_backward_input_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByRef Tensor mkldnn_linear_backward_input_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight);
// aten::mkldnn_linear_backward_input.out(int[] input_size, Tensor grad_output, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_linear_backward_input_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mkldnn_linear_backward_input_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByRef Tensor out);




// Parsed from ATen/ops/mkldnn_linear_backward_weights.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_linear_backward_weights_ops.h>


// aten::mkldnn_linear_backward_weights(Tensor grad_output, Tensor input, Tensor weight, bool bias_defined) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple mkldnn_linear_backward_weights(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Cast("bool") boolean bias_defined);

// aten::mkldnn_linear_backward_weights.out(Tensor grad_output, Tensor input, Tensor weight, bool bias_defined, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mkldnn_linear_backward_weights_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Cast("bool") boolean bias_defined);
// aten::mkldnn_linear_backward_weights.out(Tensor grad_output, Tensor input, Tensor weight, bool bias_defined, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mkldnn_linear_backward_weights_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Cast("bool") boolean bias_defined, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/mkldnn_max_pool2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_max_pool2d_ops.h>


// aten::mkldnn_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::mkldnn_max_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
// aten::mkldnn_max_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);




// Parsed from ATen/ops/mkldnn_max_pool2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_max_pool2d_backward_ops.h>


// aten::mkldnn_max_pool2d_backward(Tensor grad_output, Tensor output, Tensor input, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::mkldnn_max_pool2d_backward.out(Tensor grad_output, Tensor output, Tensor input, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
// aten::mkldnn_max_pool2d_backward.out(Tensor grad_output, Tensor output, Tensor input, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);




// Parsed from ATen/ops/mkldnn_max_pool3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_max_pool3d_ops.h>


// aten::mkldnn_max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::mkldnn_max_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
// aten::mkldnn_max_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);




// Parsed from ATen/ops/mkldnn_max_pool3d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_max_pool3d_backward_ops.h>


// aten::mkldnn_max_pool3d_backward(Tensor grad_output, Tensor output, Tensor input, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::mkldnn_max_pool3d_backward.out(Tensor grad_output, Tensor output, Tensor input, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
// aten::mkldnn_max_pool3d_backward.out(Tensor grad_output, Tensor output, Tensor input, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mkldnn_max_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);




// Parsed from ATen/ops/mkldnn_reorder_conv2d_weight.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_reorder_conv2d_weight_ops.h>


// aten::mkldnn_reorder_conv2d_weight(Tensor self, int[2] padding=0, int[2] stride=1, int[2] dilation=1, int groups=1, int[]? input_size=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv2d_weight(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional input_size);
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv2d_weight(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv2d_weight(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... input_size);

// aten::mkldnn_reorder_conv2d_weight.out(Tensor self, int[2] padding=0, int[2] stride=1, int[2] dilation=1, int groups=1, int[]? input_size=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_reorder_conv2d_weight_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional input_size);
@Namespace("at") public static native @ByRef Tensor mkldnn_reorder_conv2d_weight_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor mkldnn_reorder_conv2d_weight_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... input_size);
// aten::mkldnn_reorder_conv2d_weight.out(Tensor self, int[2] padding=0, int[2] stride=1, int[2] dilation=1, int groups=1, int[]? input_size=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_reorder_conv2d_weight_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @ByVal LongArrayRefOptional input_size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mkldnn_reorder_conv2d_weight_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByRef Tensor out);




// Parsed from ATen/ops/mkldnn_reorder_conv3d_weight.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_reorder_conv3d_weight_ops.h>


// aten::mkldnn_reorder_conv3d_weight(Tensor self, int[3] padding=0, int[3] stride=1, int[3] dilation=1, int groups=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv3d_weight(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv3d_weight(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv3d_weight(@Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups/*=1*/);

// aten::mkldnn_reorder_conv3d_weight.out(Tensor self, int[3] padding=0, int[3] stride=1, int[3] dilation=1, int groups=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_reorder_conv3d_weight_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByRef Tensor mkldnn_reorder_conv3d_weight_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor mkldnn_reorder_conv3d_weight_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups/*=1*/);
// aten::mkldnn_reorder_conv3d_weight.out(Tensor self, int[3] padding=0, int[3] stride=1, int[3] dilation=1, int groups=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mkldnn_reorder_conv3d_weight_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mkldnn_reorder_conv3d_weight_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @ByRef Tensor out);




// Parsed from ATen/ops/mkldnn_rnn_layer.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_rnn_layer_ops.h>


// aten::mkldnn_rnn_layer(Tensor input, Tensor weight0, Tensor weight1, Tensor weight2, Tensor weight3, Tensor hx_, Tensor cx_, bool reverse, int[] batch_sizes, int mode, int hidden_size, int num_layers, bool has_biases, bool bidirectional, bool batch_first, bool train) -> (Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple mkldnn_rnn_layer(@Const @ByRef Tensor input, @Const @ByRef Tensor weight0, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_, @Cast("bool") boolean reverse, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first, @Cast("bool") boolean train);
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTuple mkldnn_rnn_layer(@Const @ByRef Tensor input, @Const @ByRef Tensor weight0, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_, @Cast("bool") boolean reverse, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first, @Cast("bool") boolean train);

// aten::mkldnn_rnn_layer.out(Tensor input, Tensor weight0, Tensor weight1, Tensor weight2, Tensor weight3, Tensor hx_, Tensor cx_, bool reverse, int[] batch_sizes, int mode, int hidden_size, int num_layers, bool has_biases, bool bidirectional, bool batch_first, bool train, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mkldnn_rnn_layer_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @Const @ByRef Tensor input, @Const @ByRef Tensor weight0, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_, @Cast("bool") boolean reverse, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first, @Cast("bool") boolean train);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mkldnn_rnn_layer_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @Const @ByRef Tensor input, @Const @ByRef Tensor weight0, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_, @Cast("bool") boolean reverse, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first, @Cast("bool") boolean train);
// aten::mkldnn_rnn_layer.out(Tensor input, Tensor weight0, Tensor weight1, Tensor weight2, Tensor weight3, Tensor hx_, Tensor cx_, bool reverse, int[] batch_sizes, int mode, int hidden_size, int num_layers, bool has_biases, bool bidirectional, bool batch_first, bool train, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mkldnn_rnn_layer_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight0, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_, @Cast("bool") boolean reverse, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first, @Cast("bool") boolean train, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mkldnn_rnn_layer_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight0, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_, @Cast("bool") boolean reverse, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first, @Cast("bool") boolean train, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3);




// Parsed from ATen/ops/mkldnn_rnn_layer_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mkldnn_rnn_layer_backward_ops.h>


// aten::mkldnn_rnn_layer_backward(Tensor input, Tensor weight1, Tensor weight2, Tensor weight3, Tensor weight4, Tensor hx_, Tensor cx_tmp, Tensor output, Tensor hy_, Tensor cy_, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, bool reverse, int mode, int hidden_size, int num_layers, bool has_biases, bool train, bool bidirectional, int[] batch_sizes, bool batch_first, Tensor workspace) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTensorTensorTensorTuple mkldnn_rnn_layer_backward(@Const @ByRef Tensor input, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor weight4, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_tmp, @Const @ByRef Tensor output, @Const @ByRef Tensor hy_, @Const @ByRef Tensor cy_, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("bool") boolean reverse, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Cast("bool") boolean batch_first, @Const @ByRef Tensor workspace);
@Namespace("at") public static native @ByVal TensorTensorTensorTensorTensorTensorTensorTuple mkldnn_rnn_layer_backward(@Const @ByRef Tensor input, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor weight4, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_tmp, @Const @ByRef Tensor output, @Const @ByRef Tensor hy_, @Const @ByRef Tensor cy_, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("bool") boolean reverse, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Cast("bool") boolean batch_first, @Const @ByRef Tensor workspace);

// aten::mkldnn_rnn_layer_backward.out(Tensor input, Tensor weight1, Tensor weight2, Tensor weight3, Tensor weight4, Tensor hx_, Tensor cx_tmp, Tensor output, Tensor hy_, Tensor cy_, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, bool reverse, int mode, int hidden_size, int num_layers, bool has_biases, bool train, bool bidirectional, int[] batch_sizes, bool batch_first, Tensor workspace, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4, Tensor(f!) out5, Tensor(g!) out6) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!), Tensor(f!), Tensor(g!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mkldnn_rnn_layer_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4, @ByRef Tensor out5, @ByRef Tensor out6, @Const @ByRef Tensor input, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor weight4, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_tmp, @Const @ByRef Tensor output, @Const @ByRef Tensor hy_, @Const @ByRef Tensor cy_, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("bool") boolean reverse, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Cast("bool") boolean batch_first, @Const @ByRef Tensor workspace);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mkldnn_rnn_layer_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4, @ByRef Tensor out5, @ByRef Tensor out6, @Const @ByRef Tensor input, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor weight4, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_tmp, @Const @ByRef Tensor output, @Const @ByRef Tensor hy_, @Const @ByRef Tensor cy_, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("bool") boolean reverse, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Cast("bool") boolean batch_first, @Const @ByRef Tensor workspace);
// aten::mkldnn_rnn_layer_backward.out(Tensor input, Tensor weight1, Tensor weight2, Tensor weight3, Tensor weight4, Tensor hx_, Tensor cx_tmp, Tensor output, Tensor hy_, Tensor cy_, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, bool reverse, int mode, int hidden_size, int num_layers, bool has_biases, bool train, bool bidirectional, int[] batch_sizes, bool batch_first, Tensor workspace, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3, Tensor(e!) out4, Tensor(f!) out5, Tensor(g!) out6) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!), Tensor(e!), Tensor(f!), Tensor(g!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mkldnn_rnn_layer_backward_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor weight4, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_tmp, @Const @ByRef Tensor output, @Const @ByRef Tensor hy_, @Const @ByRef Tensor cy_, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("bool") boolean reverse, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Cast("bool") boolean batch_first, @Const @ByRef Tensor workspace, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4, @ByRef Tensor out5, @ByRef Tensor out6);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mkldnn_rnn_layer_backward_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor weight1, @Const @ByRef Tensor weight2, @Const @ByRef Tensor weight3, @Const @ByRef Tensor weight4, @Const @ByRef Tensor hx_, @Const @ByRef Tensor cx_tmp, @Const @ByRef Tensor output, @Const @ByRef Tensor hy_, @Const @ByRef Tensor cy_, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("bool") boolean reverse, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean has_biases, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Cast("bool") boolean batch_first, @Const @ByRef Tensor workspace, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @ByRef Tensor out3, @ByRef Tensor out4, @ByRef Tensor out5, @ByRef Tensor out6);




// Parsed from ATen/ops/mm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mm_ops.h>


// aten::mm(Tensor self, Tensor mat2) -> Tensor
@Namespace("at") public static native @ByVal Tensor mm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat2);

// aten::mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat2);
// aten::mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat2, @ByRef Tensor out);




// Parsed from ATen/ops/mode.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mode_ops.h>


// aten::mode(Tensor self, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple mode(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple mode(@Const @ByRef Tensor self);

// aten::mode.values(Tensor self, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mode_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mode_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self);
// aten::mode.values(Tensor self, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mode_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::mode.dimname(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple mode(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple mode(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::mode.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mode_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mode_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::mode.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mode_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);




// Parsed from ATen/ops/moveaxis.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/moveaxis_ops.h>


// aten::moveaxis.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor moveaxis(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef source, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef destination);
@Namespace("at") public static native @ByVal Tensor moveaxis(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] source, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... destination);

// aten::moveaxis.int(Tensor(a) self, int source, int destination) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor moveaxis(@Const @ByRef Tensor self, @Cast("int64_t") long source, @Cast("int64_t") long destination);




// Parsed from ATen/ops/movedim.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/movedim_ops.h>


// aten::movedim.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor movedim(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef source, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef destination);
@Namespace("at") public static native @ByVal Tensor movedim(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] source, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... destination);

// aten::movedim.int(Tensor(a) self, int source, int destination) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor movedim(@Const @ByRef Tensor self, @Cast("int64_t") long source, @Cast("int64_t") long destination);




// Parsed from ATen/ops/mps_convolution_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mps_convolution_backward_ops.h>


// aten::mps_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple mps_convolution_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple mps_convolution_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::mps_convolution_backward.out(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mps_convolution_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mps_convolution_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
// aten::mps_convolution_backward.out(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mps_convolution_backward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mps_convolution_backward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/mps_convolution_transpose_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mps_convolution_transpose_backward_ops.h>


// aten::mps_convolution_transpose_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[2] output_mask) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple mps_convolution_transpose_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTuple mps_convolution_transpose_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);

// aten::mps_convolution_transpose_backward.out(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[2] output_mask, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mps_convolution_transpose_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mps_convolution_transpose_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);
// aten::mps_convolution_transpose_backward.out(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[2] output_mask, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mps_convolution_transpose_backward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mps_convolution_transpose_backward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/mse_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mse_loss_ops.h>


// aten::mse_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mse_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor mse_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
// aten::mse_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mse_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor out);

// aten::mse_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor mse_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor mse_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);




// Parsed from ATen/ops/mse_loss_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mse_loss_backward_ops.h>


// aten::mse_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mse_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);
// aten::mse_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mse_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor grad_input);

// aten::mse_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor
@Namespace("at") public static native @ByVal Tensor mse_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);




// Parsed from ATen/ops/msort.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/msort_ops.h>


// aten::msort.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor msort_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::msort.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor msort_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::msort(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor msort(@Const @ByRef Tensor self);




// Parsed from ATen/ops/mul.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mul_ops.h>


// aten::mul.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor mul(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mul_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mul_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::mul.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor mul(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::mul.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mul_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::mul.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mul_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);




// Parsed from ATen/ops/multi_margin_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/multi_margin_loss_ops.h>


// aten::multi_margin_loss.out(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar p, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar margin, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
// aten::multi_margin_loss.out(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef Scalar p, @Const @ByRef Scalar margin, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByRef Tensor out);

// aten::multi_margin_loss(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor multi_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar p, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar margin, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor multi_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);




// Parsed from ATen/ops/multi_margin_loss_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/multi_margin_loss_backward_ops.h>


// aten::multi_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Scalar p, Scalar margin, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef Scalar p, @Const @ByRef Scalar margin, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef Scalar p, @Const @ByRef Scalar margin);
// aten::multi_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Scalar p, Scalar margin, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef Scalar p, @Const @ByRef Scalar margin, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByRef Tensor grad_input);

// aten::multi_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, Scalar p, Scalar margin, Tensor? weight=None, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor multi_margin_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef Scalar p, @Const @ByRef Scalar margin, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor multi_margin_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef Scalar p, @Const @ByRef Scalar margin);




// Parsed from ATen/ops/multilabel_margin_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/multilabel_margin_loss_ops.h>


// aten::multilabel_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multilabel_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor multilabel_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
// aten::multilabel_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multilabel_margin_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor out);

// aten::multilabel_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor multilabel_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor multilabel_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);




// Parsed from ATen/ops/multilabel_margin_loss_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/multilabel_margin_loss_backward_ops.h>


// aten::multilabel_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, Tensor is_target, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multilabel_margin_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @Const @ByRef Tensor is_target);
// aten::multilabel_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, Tensor is_target, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multilabel_margin_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @Const @ByRef Tensor is_target, @ByRef Tensor grad_input);

// aten::multilabel_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, Tensor is_target) -> Tensor
@Namespace("at") public static native @ByVal Tensor multilabel_margin_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @Const @ByRef Tensor is_target);




// Parsed from ATen/ops/multilabel_margin_loss_forward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/multilabel_margin_loss_forward_ops.h>


// aten::multilabel_margin_loss_forward.output(Tensor self, Tensor target, int reduction, *, Tensor(a!) output, Tensor(b!) is_target) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> multilabel_margin_loss_forward_out(@ByRef Tensor output, @ByRef Tensor is_target, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);
// aten::multilabel_margin_loss_forward.output(Tensor self, Tensor target, int reduction, *, Tensor(a!) output, Tensor(b!) is_target) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> multilabel_margin_loss_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor output, @ByRef Tensor is_target);

// aten::multilabel_margin_loss_forward(Tensor self, Tensor target, int reduction) -> (Tensor output, Tensor is_target)
@Namespace("at") public static native @ByVal TensorTensorTuple multilabel_margin_loss_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);




// Parsed from ATen/ops/multinomial.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/multinomial_ops.h>


// aten::multinomial.out(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multinomial_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long num_samples, @Cast("bool") boolean replacement/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor multinomial_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long num_samples);
// aten::multinomial.out(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multinomial_outf(@Const @ByRef Tensor self, @Cast("int64_t") long num_samples, @Cast("bool") boolean replacement, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::multinomial(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor multinomial(@Const @ByRef Tensor self, @Cast("int64_t") long num_samples, @Cast("bool") boolean replacement/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor multinomial(@Const @ByRef Tensor self, @Cast("int64_t") long num_samples);




// Parsed from ATen/ops/multiply.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/multiply_ops.h>


// aten::multiply.Tensor(Tensor self, Tensor other) -> Tensor

// aten::multiply.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multiply_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::multiply.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor multiply_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::multiply.Scalar(Tensor self, Scalar other) -> Tensor




// Parsed from ATen/ops/mv.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mv_ops.h>


// aten::mv(Tensor self, Tensor vec) -> Tensor
@Namespace("at") public static native @ByVal Tensor mv(@Const @ByRef Tensor self, @Const @ByRef Tensor vec);

// aten::mv.out(Tensor self, Tensor vec, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor vec);
// aten::mv.out(Tensor self, Tensor vec, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mv_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor vec, @ByRef Tensor out);




// Parsed from ATen/ops/mvlgamma.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/mvlgamma_ops.h>


// aten::mvlgamma.out(Tensor self, int p, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mvlgamma_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long p);
// aten::mvlgamma.out(Tensor self, int p, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor mvlgamma_outf(@Const @ByRef Tensor self, @Cast("int64_t") long p, @ByRef Tensor out);

// aten::mvlgamma(Tensor self, int p) -> Tensor
@Namespace("at") public static native @ByVal Tensor mvlgamma(@Const @ByRef Tensor self, @Cast("int64_t") long p);




// Parsed from ATen/ops/nan_to_num.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nan_to_num_ops.h>


// aten::nan_to_num(Tensor self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor nan_to_num(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional nan, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional posinf, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional neginf);
@Namespace("at") public static native @ByVal Tensor nan_to_num(@Const @ByRef Tensor self);

// aten::nan_to_num_(Tensor(a!) self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nan_to_num_(@ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional nan, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional posinf, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional neginf);
@Namespace("at") public static native @ByRef Tensor nan_to_num_(@ByRef Tensor self);

// aten::nan_to_num.out(Tensor self, float? nan=None, float? posinf=None, float? neginf=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nan_to_num_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional nan, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional posinf, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional neginf);
@Namespace("at") public static native @ByRef Tensor nan_to_num_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::nan_to_num.out(Tensor self, float? nan=None, float? posinf=None, float? neginf=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nan_to_num_outf(@Const @ByRef Tensor self, @ByVal DoubleOptional nan, @ByVal DoubleOptional posinf, @ByVal DoubleOptional neginf, @ByRef Tensor out);




// Parsed from ATen/ops/nanmean.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nanmean_ops.h>


// aten::nanmean(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor nanmean(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor nanmean(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor nanmean(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);

// aten::nanmean.out(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanmean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor nanmean_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor nanmean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
// aten::nanmean.out(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanmean_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor nanmean_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/nanmedian.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nanmedian_ops.h>


// aten::nanmedian(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor nanmedian(@Const @ByRef Tensor self);

// aten::nanmedian.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple nanmedian(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple nanmedian(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::nanmedian.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nanmedian_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nanmedian_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::nanmedian.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nanmedian_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::nanmedian.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple nanmedian(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple nanmedian(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::nanmedian.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nanmedian_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nanmedian_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::nanmedian.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nanmedian_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);

// aten::nanmedian.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanmedian_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::nanmedian.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanmedian_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/nanquantile.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nanquantile_ops.h>


// aten::nanquantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor
@Namespace("at") public static native @ByVal Tensor nanquantile(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::string_view(\"linear\")") @Cast("c10::string_view*") Pointer interpolation);
@Namespace("at") public static native @ByVal Tensor nanquantile(@Const @ByRef Tensor self, @Const @ByRef Tensor q);

// aten::nanquantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanquantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::string_view(\"linear\")") @Cast("c10::string_view*") Pointer interpolation);
@Namespace("at") public static native @ByRef Tensor nanquantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor q);
// aten::nanquantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanquantile_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByVal @Cast("c10::string_view*") Pointer interpolation, @ByRef Tensor out);

// aten::nanquantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor
@Namespace("at") public static native @ByVal Tensor nanquantile(@Const @ByRef Tensor self, double q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::string_view(\"linear\")") @Cast("c10::string_view*") Pointer interpolation);
@Namespace("at") public static native @ByVal Tensor nanquantile(@Const @ByRef Tensor self, double q);

// aten::nanquantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanquantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, double q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::string_view(\"linear\")") @Cast("c10::string_view*") Pointer interpolation);
@Namespace("at") public static native @ByRef Tensor nanquantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, double q);
// aten::nanquantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nanquantile_outf(@Const @ByRef Tensor self, double q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByVal @Cast("c10::string_view*") Pointer interpolation, @ByRef Tensor out);




// Parsed from ATen/ops/nansum.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nansum_ops.h>


// aten::nansum(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor nansum(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor nansum(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor nansum(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);

// aten::nansum.out(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nansum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor nansum_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor nansum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
// aten::nansum.out(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nansum_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor nansum_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/narrow.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/narrow_ops.h>


// aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor narrow(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long length);


// aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor narrow_symint(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal SymInt start, @ByVal SymInt length);


// aten::narrow.Tensor(Tensor(a) self, int dim, Tensor start, SymInt length) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor narrow(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor start, @Cast("int64_t") long length);


// aten::narrow.Tensor(Tensor(a) self, int dim, Tensor start, SymInt length) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor narrow_symint(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor start, @ByVal SymInt length);





// Parsed from ATen/ops/narrow_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/narrow_copy_ops.h>


// aten::narrow_copy(Tensor self, int dim, SymInt start, SymInt length) -> Tensor
@Namespace("at") public static native @ByVal Tensor narrow_copy(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long length);


// aten::narrow_copy(Tensor self, int dim, SymInt start, SymInt length) -> Tensor
@Namespace("at") public static native @ByVal Tensor narrow_copy_symint(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal SymInt start, @ByVal SymInt length);


// aten::narrow_copy.out(Tensor self, int dim, SymInt start, SymInt length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor narrow_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long length);


// aten::narrow_copy.out(Tensor self, int dim, SymInt start, SymInt length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor narrow_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long length, @ByRef Tensor out);


// aten::narrow_copy.out(Tensor self, int dim, SymInt start, SymInt length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor narrow_copy_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal SymInt start, @ByVal SymInt length);


// aten::narrow_copy.out(Tensor self, int dim, SymInt start, SymInt length, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor narrow_copy_symint_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal SymInt start, @ByVal SymInt length, @ByRef Tensor out);





// Parsed from ATen/ops/native_batch_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/native_batch_norm_ops.h>


// aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple native_batch_norm(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double momentum, double eps);

// aten::native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_batch_norm_out(@ByRef Tensor out, @ByRef Tensor save_mean, @ByRef Tensor save_invstd, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double momentum, double eps);
// aten::native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_batch_norm_outf(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double momentum, double eps, @ByRef Tensor out, @ByRef Tensor save_mean, @ByRef Tensor save_invstd);




// Parsed from ATen/ops/native_batch_norm_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/native_batch_norm_backward_ops.h>


// aten::native_batch_norm_backward(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple native_batch_norm_backward(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_invstd, @Cast("bool") boolean train, double eps, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);

// aten::native_batch_norm_backward.out(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_batch_norm_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_invstd, @Cast("bool") boolean train, double eps, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
// aten::native_batch_norm_backward.out(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_batch_norm_backward_outf(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_invstd, @Cast("bool") boolean train, double eps, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/native_channel_shuffle.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/native_channel_shuffle_ops.h>


// aten::native_channel_shuffle(Tensor self, int groups) -> Tensor
@Namespace("at") public static native @ByVal Tensor native_channel_shuffle(@Const @ByRef Tensor self, @Cast("int64_t") long groups);




// Parsed from ATen/ops/native_dropout.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/native_dropout_ops.h>


// aten::native_dropout(Tensor input, float p, bool? train) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple native_dropout(@Const @ByRef Tensor input, double p, @ByVal BoolOptional train);

// aten::native_dropout.out(Tensor input, float p, bool? train, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_dropout_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor input, double p, @ByVal BoolOptional train);
// aten::native_dropout.out(Tensor input, float p, bool? train, *, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_dropout_outf(@Const @ByRef Tensor input, double p, @ByVal BoolOptional train, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/native_dropout_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/native_dropout_backward_ops.h>


// aten::native_dropout_backward(Tensor grad_output, Tensor mask, float scale) -> Tensor
@Namespace("at") public static native @ByVal Tensor native_dropout_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor mask, double scale);

// aten::native_dropout_backward.out(Tensor grad_output, Tensor mask, float scale, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor native_dropout_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor mask, double scale);
// aten::native_dropout_backward.out(Tensor grad_output, Tensor mask, float scale, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor native_dropout_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor mask, double scale, @ByRef Tensor out);




// Parsed from ATen/ops/native_group_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/native_group_norm_ops.h>


// aten::native_group_norm(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple native_group_norm(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long HxW, @Cast("int64_t") long group, double eps);


// aten::native_group_norm(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple native_group_norm_symint(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal SymInt N, @ByVal SymInt C, @ByVal SymInt HxW, @Cast("int64_t") long group, double eps);


// aten::native_group_norm.out(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_group_norm_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long HxW, @Cast("int64_t") long group, double eps);


// aten::native_group_norm.out(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_group_norm_outf(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long HxW, @Cast("int64_t") long group, double eps, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);


// aten::native_group_norm.out(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_group_norm_symint_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal SymInt N, @ByVal SymInt C, @ByVal SymInt HxW, @Cast("int64_t") long group, double eps);


// aten::native_group_norm.out(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_group_norm_symint_outf(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal SymInt N, @ByVal SymInt C, @ByVal SymInt HxW, @Cast("int64_t") long group, double eps, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);





// Parsed from ATen/ops/native_group_norm_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/native_group_norm_backward_ops.h>


// aten::native_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple native_group_norm_backward(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long HxW, @Cast("int64_t") long group, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::native_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple native_group_norm_backward_symint(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @ByVal SymInt N, @ByVal SymInt C, @ByVal SymInt HxW, @Cast("int64_t") long group, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::native_group_norm_backward.out(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_group_norm_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long HxW, @Cast("int64_t") long group, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::native_group_norm_backward.out(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_group_norm_backward_outf(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long HxW, @Cast("int64_t") long group, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);


// aten::native_group_norm_backward.out(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_group_norm_backward_symint_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @ByVal SymInt N, @ByVal SymInt C, @ByVal SymInt HxW, @Cast("int64_t") long group, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::native_group_norm_backward.out(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_group_norm_backward_symint_outf(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @ByVal SymInt N, @ByVal SymInt C, @ByVal SymInt HxW, @Cast("int64_t") long group, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);





// Parsed from ATen/ops/native_layer_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/native_layer_norm_ops.h>


// aten::native_layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple native_layer_norm(@Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef normalized_shape, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, double eps);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple native_layer_norm(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] normalized_shape, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, double eps);


// aten::native_layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple native_layer_norm_symint(@Const @ByRef Tensor input, @ByVal SymIntRef normalized_shape, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, double eps);


// aten::native_layer_norm.out(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_layer_norm_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef normalized_shape, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, double eps);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_layer_norm_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] normalized_shape, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, double eps);


// aten::native_layer_norm.out(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_layer_norm_outf(@Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef normalized_shape, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, double eps, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_layer_norm_outf(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] normalized_shape, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, double eps, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);


// aten::native_layer_norm.out(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_layer_norm_symint_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor input, @ByVal SymIntRef normalized_shape, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, double eps);


// aten::native_layer_norm.out(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_layer_norm_symint_outf(@Const @ByRef Tensor input, @ByVal SymIntRef normalized_shape, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, double eps, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);





// Parsed from ATen/ops/native_layer_norm_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/native_layer_norm_backward_ops.h>


// aten::native_layer_norm_backward(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple native_layer_norm_backward(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef normalized_shape, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple native_layer_norm_backward(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] normalized_shape, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::native_layer_norm_backward(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple native_layer_norm_backward_symint(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @ByVal SymIntRef normalized_shape, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::native_layer_norm_backward.out(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_layer_norm_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef normalized_shape, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_layer_norm_backward_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] normalized_shape, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::native_layer_norm_backward.out(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_layer_norm_backward_outf(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef normalized_shape, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_layer_norm_backward_outf(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] normalized_shape, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);


// aten::native_layer_norm_backward.out(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_layer_norm_backward_symint_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @ByVal SymIntRef normalized_shape, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);


// aten::native_layer_norm_backward.out(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_layer_norm_backward_symint_outf(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @ByVal SymIntRef normalized_shape, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);





// Parsed from ATen/ops/native_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/native_norm_ops.h>


// aten::native_norm(Tensor self, Scalar p=2) -> Tensor
@Namespace("at") public static native @ByVal Tensor native_norm(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar p);
@Namespace("at") public static native @ByVal Tensor native_norm(@Const @ByRef Tensor self);

// aten::native_norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, ScalarType? dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor native_norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor native_norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype);

// aten::native_norm.out(Tensor self, Scalar p=2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor native_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar p);
@Namespace("at") public static native @ByRef Tensor native_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::native_norm.out(Tensor self, Scalar p=2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor native_norm_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar p, @ByRef Tensor out);

// aten::native_norm.ScalarOpt_dim_dtype_out(Tensor self, Scalar? p, int[1] dim, bool keepdim, ScalarType? dtype, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor native_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor native_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype);
// aten::native_norm.ScalarOpt_dim_dtype_out(Tensor self, Scalar? p, int[1] dim, bool keepdim, ScalarType? dtype, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor native_norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor native_norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/ne.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ne_ops.h>


// aten::ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ne_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ne_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::ne.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor ne(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ne_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ne_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::ne.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor ne(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/neg.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/neg_ops.h>


// aten::neg(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor neg(@Const @ByRef Tensor self);

// aten::neg_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor neg_(@ByRef Tensor self);

// aten::neg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor neg_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::neg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor neg_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/negative.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/negative_ops.h>


// aten::negative(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor negative(@Const @ByRef Tensor self);

// aten::negative_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor negative_(@ByRef Tensor self);

// aten::negative.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor negative_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::negative.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor negative_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/nested_to_padded_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nested_to_padded_tensor_ops.h>


// aten::nested_to_padded_tensor(Tensor self, float padding, int[]? output_size=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor nested_to_padded_tensor(@Const @ByRef Tensor self, double padding, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional output_size);
@Namespace("at") public static native @ByVal Tensor nested_to_padded_tensor(@Const @ByRef Tensor self, double padding);
@Namespace("at") public static native @ByVal Tensor nested_to_padded_tensor(@Const @ByRef Tensor self, double padding, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);




// Parsed from ATen/ops/new_empty.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/new_empty_ops.h>





// aten::new_empty.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_empty_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor new_empty_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::new_empty.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_empty_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor new_empty_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);


// aten::new_empty.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_empty_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef size);


// aten::new_empty.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_empty_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef size, @ByRef Tensor out);





// Parsed from ATen/ops/new_empty_strided.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/new_empty_strided_ops.h>





// aten::new_empty_strided.out(Tensor self, SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_empty_strided_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor new_empty_strided_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);


// aten::new_empty_strided.out(Tensor self, SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_empty_strided_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor new_empty_strided_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByRef Tensor out);


// aten::new_empty_strided.out(Tensor self, SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_empty_strided_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef size, @ByVal SymIntRef stride);


// aten::new_empty_strided.out(Tensor self, SymInt[] size, SymInt[] stride, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_empty_strided_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef size, @ByVal SymIntRef stride, @ByRef Tensor out);





// Parsed from ATen/ops/new_full.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/new_full_ops.h>





// aten::new_full.out(Tensor self, SymInt[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_full_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value);
@Namespace("at") public static native @ByRef Tensor new_full_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value);


// aten::new_full.out(Tensor self, SymInt[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_full_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor new_full_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value, @ByRef Tensor out);


// aten::new_full.out(Tensor self, SymInt[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_full_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef size, @Const @ByRef Scalar fill_value);


// aten::new_full.out(Tensor self, SymInt[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_full_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef size, @Const @ByRef Scalar fill_value, @ByRef Tensor out);





// Parsed from ATen/ops/new_ones.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/new_ones_ops.h>





// aten::new_ones.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_ones_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor new_ones_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::new_ones.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_ones_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor new_ones_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);


// aten::new_ones.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_ones_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef size);


// aten::new_ones.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_ones_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef size, @ByRef Tensor out);





// Parsed from ATen/ops/new_zeros.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/new_zeros_ops.h>





// aten::new_zeros.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_zeros_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor new_zeros_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::new_zeros.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_zeros_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor new_zeros_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);


// aten::new_zeros.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_zeros_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef size);


// aten::new_zeros.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor new_zeros_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef size, @ByRef Tensor out);





// Parsed from ATen/ops/nextafter.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nextafter_ops.h>


// aten::nextafter.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nextafter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::nextafter.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nextafter_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::nextafter(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor nextafter(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/nll_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nll_loss_ops.h>


// aten::nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("int64_t") long ignore_index/*=-100*/);
@Namespace("at") public static native @ByRef Tensor nll_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);


// aten::nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @ByRef Tensor out);


// aten::nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @ByVal(nullValue = "c10::SymInt(-100)") SymInt ignore_index);
@Namespace("at") public static native @ByRef Tensor nll_loss_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);


// aten::nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index, @ByRef Tensor out);


// aten::nll_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("int64_t") long ignore_index/*=-100*/);
@Namespace("at") public static native @ByVal Tensor nll_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);


// aten::nll_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @ByVal(nullValue = "c10::SymInt(-100)") SymInt ignore_index);
@Namespace("at") public static native @ByVal Tensor nll_loss_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor target);





// Parsed from ATen/ops/nll_loss2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nll_loss2d_ops.h>


// aten::nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("int64_t") long ignore_index/*=-100*/);
@Namespace("at") public static native @ByRef Tensor nll_loss2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);


// aten::nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @ByRef Tensor out);


// aten::nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @ByVal(nullValue = "c10::SymInt(-100)") SymInt ignore_index);
@Namespace("at") public static native @ByRef Tensor nll_loss2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);


// aten::nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss2d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index, @ByRef Tensor out);


// aten::nll_loss2d(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss2d(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("int64_t") long ignore_index/*=-100*/);
@Namespace("at") public static native @ByVal Tensor nll_loss2d(@Const @ByRef Tensor self, @Const @ByRef Tensor target);


// aten::nll_loss2d(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss2d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @ByVal(nullValue = "c10::SymInt(-100)") SymInt ignore_index);
@Namespace("at") public static native @ByVal Tensor nll_loss2d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor target);





// Parsed from ATen/ops/nll_loss2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nll_loss2d_backward_ops.h>


// aten::nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight);


// aten::nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight, @ByRef Tensor grad_input);


// aten::nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss2d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index, @Const @ByRef Tensor total_weight);


// aten::nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss2d_backward_symint_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index, @Const @ByRef Tensor total_weight, @ByRef Tensor grad_input);


// aten::nll_loss2d_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight);


// aten::nll_loss2d_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss2d_backward_symint(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index, @Const @ByRef Tensor total_weight);





// Parsed from ATen/ops/nll_loss2d_forward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nll_loss2d_forward_ops.h>


// aten::nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nll_loss2d_forward_out(@ByRef Tensor output, @ByRef Tensor total_weight, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index);


// aten::nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nll_loss2d_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @ByRef Tensor output, @ByRef Tensor total_weight);


// aten::nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nll_loss2d_forward_symint_out(@ByRef Tensor output, @ByRef Tensor total_weight, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index);


// aten::nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nll_loss2d_forward_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index, @ByRef Tensor output, @ByRef Tensor total_weight);


// aten::nll_loss2d_forward(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -> (Tensor output, Tensor total_weight)
@Namespace("at") public static native @ByVal TensorTensorTuple nll_loss2d_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index);


// aten::nll_loss2d_forward(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -> (Tensor output, Tensor total_weight)
@Namespace("at") public static native @ByVal TensorTensorTuple nll_loss2d_forward_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index);





// Parsed from ATen/ops/nll_loss_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nll_loss_backward_ops.h>


// aten::nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight);


// aten::nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight, @ByRef Tensor grad_input);


// aten::nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index, @Const @ByRef Tensor total_weight);


// aten::nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nll_loss_backward_symint_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index, @Const @ByRef Tensor total_weight, @ByRef Tensor grad_input);


// aten::nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight);


// aten::nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss_backward_symint(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index, @Const @ByRef Tensor total_weight);





// Parsed from ATen/ops/nll_loss_forward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nll_loss_forward_ops.h>


// aten::nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nll_loss_forward_out(@ByRef Tensor output, @ByRef Tensor total_weight, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index);


// aten::nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nll_loss_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @ByRef Tensor output, @ByRef Tensor total_weight);


// aten::nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nll_loss_forward_symint_out(@ByRef Tensor output, @ByRef Tensor total_weight, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index);


// aten::nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nll_loss_forward_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index, @ByRef Tensor output, @ByRef Tensor total_weight);


// aten::nll_loss_forward(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -> (Tensor output, Tensor total_weight)
@Namespace("at") public static native @ByVal TensorTensorTuple nll_loss_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index);


// aten::nll_loss_forward(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -> (Tensor output, Tensor total_weight)
@Namespace("at") public static native @ByVal TensorTensorTuple nll_loss_forward_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByVal SymInt ignore_index);





// Parsed from ATen/ops/nll_loss_nd.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nll_loss_nd_ops.h>


// aten::nll_loss_nd(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss_nd(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("int64_t") long ignore_index/*=-100*/);
@Namespace("at") public static native @ByVal Tensor nll_loss_nd(@Const @ByRef Tensor self, @Const @ByRef Tensor target);


// aten::nll_loss_nd(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor
@Namespace("at") public static native @ByVal Tensor nll_loss_nd_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @ByVal(nullValue = "c10::SymInt(-100)") SymInt ignore_index);
@Namespace("at") public static native @ByVal Tensor nll_loss_nd_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor target);





// Parsed from ATen/ops/nonzero.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nonzero_ops.h>


// aten::nonzero.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nonzero_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::nonzero.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nonzero_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::nonzero(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor nonzero(@Const @ByRef Tensor self);




// Parsed from ATen/ops/nonzero_numpy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nonzero_numpy_ops.h>


// aten::nonzero_numpy(Tensor self) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector nonzero_numpy(@Const @ByRef Tensor self);




// Parsed from ATen/ops/norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/norm_ops.h>


// aten::norm.ScalarOpt_dtype(Tensor self, Scalar? p, *, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, ScalarType dtype);

// aten::norm.Scalar(Tensor self, Scalar p=2) -> Tensor
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar p);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self);

// aten::norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, ScalarType dtype);

// aten::norm.ScalarOpt_dim(Tensor self, Scalar? p, int[1] dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::norm.dtype_out(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, ScalarType dtype);
// aten::norm.dtype_out(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, ScalarType dtype, @ByRef Tensor out);

// aten::norm.out(Tensor self, Scalar? p, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);
// aten::norm.out(Tensor self, Scalar? p, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::norm.names_ScalarOpt_dim_dtype(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype);

// aten::norm.names_ScalarOpt_dim(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim);

// aten::norm.names_dtype_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype);
// aten::norm.names_dtype_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype, @ByRef Tensor out);

// aten::norm.names_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim);
// aten::norm.names_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::norm.ScalarOpt_dtype_out(Tensor self, Scalar? p, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, ScalarType dtype);
// aten::norm.ScalarOpt_dtype_out(Tensor self, Scalar? p, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef ScalarOptional p, ScalarType dtype, @ByRef Tensor out);

// aten::norm.Scalar_out(Tensor self, Scalar p=2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(2)") Scalar p);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::norm.Scalar_out(Tensor self, Scalar p=2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar p, @ByRef Tensor out);




// Parsed from ATen/ops/norm_except_dim.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/norm_except_dim_ops.h>


// aten::norm_except_dim(Tensor v, int pow=2, int dim=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor norm_except_dim(@Const @ByRef Tensor v, @Cast("int64_t") long pow/*=2*/, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor norm_except_dim(@Const @ByRef Tensor v);




// Parsed from ATen/ops/normal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/normal_ops.h>


// aten::normal_functional(Tensor self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor normal_functional(@Const @ByRef Tensor self, double mean/*=0*/, double std/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor normal_functional(@Const @ByRef Tensor self);

// aten::normal.Tensor_float_out(Tensor mean, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, @Const @ByRef Tensor mean, double std/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
// aten::normal.Tensor_float_out(Tensor mean, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_outf(@Const @ByRef Tensor mean, double std, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::normal.Tensor_float(Tensor mean, float std=1, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor normal(@Const @ByRef Tensor mean, double std/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor normal(@Const @ByRef Tensor mean);

// aten::normal.float_Tensor_out(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, double mean, @Const @ByRef Tensor std, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
// aten::normal.float_Tensor_out(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_outf(double mean, @Const @ByRef Tensor std, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::normal.float_Tensor(float mean, Tensor std, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor normal(double mean, @Const @ByRef Tensor std, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor normal(double mean, @Const @ByRef Tensor std);

// aten::normal.Tensor_Tensor_out(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, @Const @ByRef Tensor mean, @Const @ByRef Tensor std, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
// aten::normal.Tensor_Tensor_out(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_outf(@Const @ByRef Tensor mean, @Const @ByRef Tensor std, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::normal.Tensor_Tensor(Tensor mean, Tensor std, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor normal(@Const @ByRef Tensor mean, @Const @ByRef Tensor std, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor normal(@Const @ByRef Tensor mean, @Const @ByRef Tensor std);

// aten::normal.float_float(float mean, float std, SymInt[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::normal.float_float(float mean, float std, SymInt[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::normal.float_float(float mean, float std, SymInt[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor normal_symint(double mean, double std, @ByVal SymIntRef size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor normal_symint(double mean, double std, @ByVal SymIntRef size);


// aten::normal.float_float(float mean, float std, SymInt[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor normal_symint(double mean, double std, @ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::normal.float_float_out(float mean, float std, SymInt[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, double mean, double std, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, double mean, double std, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
  


// aten::normal.float_float_out(float mean, float std, SymInt[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_outf(double mean, double std, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor normal_outf(double mean, double std, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByRef Tensor out);


// aten::normal.float_float_out(float mean, float std, SymInt[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_symint_out(@ByRef Tensor out, double mean, double std, @ByVal SymIntRef size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor normal_symint_out(@ByRef Tensor out, double mean, double std, @ByVal SymIntRef size);
  


// aten::normal.float_float_out(float mean, float std, SymInt[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_symint_outf(double mean, double std, @ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);


// aten::normal.out(Tensor self, float mean=0, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, @Const @ByRef Tensor self, double mean/*=0*/, double std/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
// aten::normal.out(Tensor self, float mean=0, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor normal_outf(@Const @ByRef Tensor self, double mean, double std, @ByVal GeneratorOptional generator, @ByRef Tensor out);




// Parsed from ATen/ops/not_equal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/not_equal_ops.h>


// aten::not_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor not_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::not_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor not_equal_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::not_equal.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor not_equal(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::not_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor not_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::not_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor not_equal_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::not_equal.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor not_equal(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/nuclear_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/nuclear_norm_ops.h>


// aten::nuclear_norm(Tensor self, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self);

// aten::nuclear_norm.out(Tensor self, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::nuclear_norm.out(Tensor self, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nuclear_norm_outf(@Const @ByRef Tensor self, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::nuclear_norm.dim(Tensor self, int[2] dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::nuclear_norm.dim_out(Tensor self, int[2] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);
// aten::nuclear_norm.dim_out(Tensor self, int[2] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor nuclear_norm_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);




// Parsed from ATen/ops/numpy_T.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/numpy_T_ops.h>






// Parsed from ATen/ops/one_hot.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/one_hot_ops.h>


// aten::one_hot(Tensor self, int num_classes=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor one_hot(@Const @ByRef Tensor self, @Cast("int64_t") long num_classes/*=-1*/);
@Namespace("at") public static native @ByVal Tensor one_hot(@Const @ByRef Tensor self);




// Parsed from ATen/ops/ones.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ones_ops.h>


// aten::ones.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);
// aten::ones.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::ones(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::ones(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::ones(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor ones_symint(@ByVal SymIntRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor ones_symint(@ByVal SymIntRef size);


// aten::ones(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor ones_symint(@ByVal SymIntRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::ones.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ones_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor ones_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::ones.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ones_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor ones_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);


// aten::ones.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ones_symint_out(@ByRef Tensor out, @ByVal SymIntRef size);


// aten::ones.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ones_symint_outf(@ByVal SymIntRef size, @ByRef Tensor out);


// aten::ones.names_out(int[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ones_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByRef Tensor ones_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);
// aten::ones.names_out(int[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ones_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor ones_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByRef Tensor out);




// Parsed from ATen/ops/ones_like.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ones_like_ops.h>


// aten::ones_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor ones_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor ones_like(@Const @ByRef Tensor self);
// aten::ones_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor ones_like(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::ones_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ones_like_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor ones_like_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::ones_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ones_like_outf(@Const @ByRef Tensor self, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);




// Parsed from ATen/ops/or.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/or_ops.h>


// aten::__or__.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __or__(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::__or__.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __or__(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/orgqr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/orgqr_ops.h>


// aten::orgqr(Tensor self, Tensor input2) -> Tensor
@Namespace("at") public static native @ByVal Tensor orgqr(@Const @ByRef Tensor self, @Const @ByRef Tensor input2);

// aten::orgqr.out(Tensor self, Tensor input2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor orgqr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor input2);
// aten::orgqr.out(Tensor self, Tensor input2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor orgqr_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @ByRef Tensor out);




// Parsed from ATen/ops/ormqr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ormqr_ops.h>


// aten::ormqr.out(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ormqr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Const @ByRef Tensor input3, @Cast("bool") boolean left/*=true*/, @Cast("bool") boolean transpose/*=false*/);
@Namespace("at") public static native @ByRef Tensor ormqr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Const @ByRef Tensor input3);
// aten::ormqr.out(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor ormqr_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Const @ByRef Tensor input3, @Cast("bool") boolean left, @Cast("bool") boolean transpose, @ByRef Tensor out);

// aten::ormqr(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor ormqr(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Const @ByRef Tensor input3, @Cast("bool") boolean left/*=true*/, @Cast("bool") boolean transpose/*=false*/);
@Namespace("at") public static native @ByVal Tensor ormqr(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Const @ByRef Tensor input3);




// Parsed from ATen/ops/outer.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/outer_ops.h>


// aten::outer(Tensor self, Tensor vec2) -> Tensor
@Namespace("at") public static native @ByVal Tensor outer(@Const @ByRef Tensor self, @Const @ByRef Tensor vec2);

// aten::outer.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor outer_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor vec2);
// aten::outer.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor outer_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor vec2, @ByRef Tensor out);




// Parsed from ATen/ops/output_nr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/output_nr_ops.h>






// Parsed from ATen/ops/pad.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/pad_ops.h>


// aten::pad(Tensor self, SymInt[] pad, str mode="constant", float? value=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor pad(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef pad, @ByVal(nullValue = "c10::string_view(\"constant\")") @Cast("c10::string_view*") Pointer mode, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional value);
@Namespace("at") public static native @ByVal Tensor pad(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef pad);
@Namespace("at") public static native @ByVal Tensor pad(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] pad, @ByVal(nullValue = "c10::string_view(\"constant\")") @Cast("c10::string_view*") Pointer mode, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional value);
@Namespace("at") public static native @ByVal Tensor pad(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... pad);


// aten::pad(Tensor self, SymInt[] pad, str mode="constant", float? value=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor pad_symint(@Const @ByRef Tensor self, @ByVal SymIntRef pad, @ByVal(nullValue = "c10::string_view(\"constant\")") @Cast("c10::string_view*") Pointer mode, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional value);
@Namespace("at") public static native @ByVal Tensor pad_symint(@Const @ByRef Tensor self, @ByVal SymIntRef pad);





// Parsed from ATen/ops/pad_sequence.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/pad_sequence_ops.h>


// aten::pad_sequence(Tensor[] sequences, bool batch_first=False, float padding_value=0.0) -> Tensor
@Namespace("at") public static native @ByVal Tensor pad_sequence(@ByVal TensorArrayRef sequences, @Cast("bool") boolean batch_first/*=false*/, double padding_value/*=0.0*/);
@Namespace("at") public static native @ByVal Tensor pad_sequence(@ByVal TensorArrayRef sequences);




// Parsed from ATen/ops/pairwise_distance.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/pairwise_distance_ops.h>


// aten::pairwise_distance(Tensor x1, Tensor x2, float p=2, float eps=1e-06, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor pairwise_distance(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2, double p/*=2*/, double eps/*=1e-06*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor pairwise_distance(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2);




// Parsed from ATen/ops/pdist.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/pdist_ops.h>


// aten::pdist(Tensor self, float p=2) -> Tensor
@Namespace("at") public static native @ByVal Tensor pdist(@Const @ByRef Tensor self, double p/*=2*/);
@Namespace("at") public static native @ByVal Tensor pdist(@Const @ByRef Tensor self);




// Parsed from ATen/ops/permute.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/permute_ops.h>


// aten::permute(Tensor(a) self, int[] dims) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor permute(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("at") public static native @ByVal Tensor permute(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);




// Parsed from ATen/ops/permute_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/permute_copy_ops.h>


// aten::permute_copy(Tensor self, int[] dims) -> Tensor
@Namespace("at") public static native @ByVal Tensor permute_copy(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("at") public static native @ByVal Tensor permute_copy(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);

// aten::permute_copy.out(Tensor self, int[] dims, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor permute_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("at") public static native @ByRef Tensor permute_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);
// aten::permute_copy.out(Tensor self, int[] dims, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor permute_copy_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor permute_copy_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dims, @ByRef Tensor out);




// Parsed from ATen/ops/pin_memory.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/pin_memory_ops.h>






// Parsed from ATen/ops/pinverse.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/pinverse_ops.h>


// aten::pinverse(Tensor self, float rcond=1e-15) -> Tensor
@Namespace("at") public static native @ByVal Tensor pinverse(@Const @ByRef Tensor self, double rcond/*=1e-15*/);
@Namespace("at") public static native @ByVal Tensor pinverse(@Const @ByRef Tensor self);




// Parsed from ATen/ops/pixel_shuffle.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/pixel_shuffle_ops.h>


// aten::pixel_shuffle(Tensor self, int upscale_factor) -> Tensor
@Namespace("at") public static native @ByVal Tensor pixel_shuffle(@Const @ByRef Tensor self, @Cast("int64_t") long upscale_factor);

// aten::pixel_shuffle.out(Tensor self, int upscale_factor, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pixel_shuffle_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long upscale_factor);
// aten::pixel_shuffle.out(Tensor self, int upscale_factor, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pixel_shuffle_outf(@Const @ByRef Tensor self, @Cast("int64_t") long upscale_factor, @ByRef Tensor out);




// Parsed from ATen/ops/pixel_unshuffle.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/pixel_unshuffle_ops.h>


// aten::pixel_unshuffle(Tensor self, int downscale_factor) -> Tensor
@Namespace("at") public static native @ByVal Tensor pixel_unshuffle(@Const @ByRef Tensor self, @Cast("int64_t") long downscale_factor);

// aten::pixel_unshuffle.out(Tensor self, int downscale_factor, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pixel_unshuffle_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long downscale_factor);
// aten::pixel_unshuffle.out(Tensor self, int downscale_factor, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pixel_unshuffle_outf(@Const @ByRef Tensor self, @Cast("int64_t") long downscale_factor, @ByRef Tensor out);




// Parsed from ATen/ops/poisson.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/poisson_ops.h>


// aten::poisson(Tensor self, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor poisson(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor poisson(@Const @ByRef Tensor self);

// aten::poisson.out(Tensor self, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor poisson_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor poisson_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::poisson.out(Tensor self, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor poisson_outf(@Const @ByRef Tensor self, @ByVal GeneratorOptional generator, @ByRef Tensor out);




// Parsed from ATen/ops/poisson_nll_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/poisson_nll_loss_ops.h>


// aten::poisson_nll_loss(Tensor input, Tensor target, bool log_input, bool full, float eps, int reduction) -> Tensor
@Namespace("at") public static native @ByVal Tensor poisson_nll_loss(@Const @ByRef Tensor input, @Const @ByRef Tensor target, @Cast("bool") boolean log_input, @Cast("bool") boolean full, double eps, @Cast("int64_t") long reduction);




// Parsed from ATen/ops/polar.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/polar_ops.h>


// aten::polar(Tensor abs, Tensor angle) -> Tensor
@Namespace("at") public static native @ByVal Tensor polar(@Const @ByRef Tensor abs, @Const @ByRef Tensor angle);

// aten::polar.out(Tensor abs, Tensor angle, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor polar_out(@ByRef Tensor out, @Const @ByRef Tensor abs, @Const @ByRef Tensor angle);
// aten::polar.out(Tensor abs, Tensor angle, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor polar_outf(@Const @ByRef Tensor abs, @Const @ByRef Tensor angle, @ByRef Tensor out);




// Parsed from ATen/ops/polygamma.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/polygamma_ops.h>


// aten::polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor polygamma_out(@ByRef Tensor out, @Cast("int64_t") long n, @Const @ByRef Tensor self);
// aten::polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor polygamma_outf(@Cast("int64_t") long n, @Const @ByRef Tensor self, @ByRef Tensor out);

// aten::polygamma(int n, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor polygamma(@Cast("int64_t") long n, @Const @ByRef Tensor self);




// Parsed from ATen/ops/positive.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/positive_ops.h>


// aten::positive(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor positive(@Const @ByRef Tensor self);




// Parsed from ATen/ops/pow.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/pow_ops.h>


// aten::pow.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pow_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor exponent);
// aten::pow.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pow_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor exponent, @ByRef Tensor out);

// aten::pow.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor
@Namespace("at") public static native @ByVal Tensor pow(@Const @ByRef Tensor self, @Const @ByRef Tensor exponent);

// aten::pow.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pow_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor exponent);
// aten::pow.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pow_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor exponent, @ByRef Tensor out);

// aten::pow.Scalar(Scalar self, Tensor exponent) -> Tensor
@Namespace("at") public static native @ByVal Tensor pow(@Const @ByRef Scalar self, @Const @ByRef Tensor exponent);

// aten::pow.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pow_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar exponent);
// aten::pow.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor pow_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar exponent, @ByRef Tensor out);

// aten::pow.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor
@Namespace("at") public static native @ByVal Tensor pow(@Const @ByRef Tensor self, @Const @ByRef Scalar exponent);




// Parsed from ATen/ops/prelu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/prelu_ops.h>


// aten::prelu(Tensor self, Tensor weight) -> Tensor
@Namespace("at") public static native @ByVal Tensor prelu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight);




// Parsed from ATen/ops/prod.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/prod_ops.h>


// aten::prod(Tensor self, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self);

// aten::prod.dim_int(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::prod.int_out(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor prod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor prod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::prod.int_out(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor prod_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::prod.dim_Dimname(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::prod.Dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor prod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor prod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::prod.Dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor prod_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::prod.out(Tensor self, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor prod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor prod_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::prod.out(Tensor self, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor prod_outf(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/promote_types.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/promote_types_ops.h>


// aten::promote_types(ScalarType type1, ScalarType type2) -> ScalarType
@Namespace("at") public static native ScalarType promote_types(ScalarType type1, ScalarType type2);




// Parsed from ATen/ops/put.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/put_ops.h>


// aten::put(Tensor self, Tensor index, Tensor source, bool accumulate=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor put(@Const @ByRef Tensor self, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @Cast("bool") boolean accumulate/*=false*/);
@Namespace("at") public static native @ByVal Tensor put(@Const @ByRef Tensor self, @Const @ByRef Tensor index, @Const @ByRef Tensor source);

// aten::put.out(Tensor self, Tensor index, Tensor source, bool accumulate=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor put_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @Cast("bool") boolean accumulate/*=false*/);
@Namespace("at") public static native @ByRef Tensor put_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor index, @Const @ByRef Tensor source);
// aten::put.out(Tensor self, Tensor index, Tensor source, bool accumulate=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor put_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor index, @Const @ByRef Tensor source, @Cast("bool") boolean accumulate, @ByRef Tensor out);




// Parsed from ATen/ops/q_per_channel_axis.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/q_per_channel_axis_ops.h>


// aten::q_per_channel_axis(Tensor self) -> int
@Namespace("at") public static native @Cast("int64_t") long q_per_channel_axis(@Const @ByRef Tensor self);




// Parsed from ATen/ops/q_per_channel_scales.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/q_per_channel_scales_ops.h>


// aten::q_per_channel_scales(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor q_per_channel_scales(@Const @ByRef Tensor self);

// aten::q_per_channel_scales.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor q_per_channel_scales_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::q_per_channel_scales.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor q_per_channel_scales_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/q_per_channel_zero_points.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/q_per_channel_zero_points_ops.h>


// aten::q_per_channel_zero_points(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor q_per_channel_zero_points(@Const @ByRef Tensor self);

// aten::q_per_channel_zero_points.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor q_per_channel_zero_points_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::q_per_channel_zero_points.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor q_per_channel_zero_points_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/q_scale.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/q_scale_ops.h>


// aten::q_scale(Tensor self) -> float
@Namespace("at") public static native double q_scale(@Const @ByRef Tensor self);




// Parsed from ATen/ops/q_zero_point.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/q_zero_point_ops.h>


// aten::q_zero_point(Tensor self) -> int
@Namespace("at") public static native @Cast("int64_t") long q_zero_point(@Const @ByRef Tensor self);




// Parsed from ATen/ops/qr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/qr_ops.h>


// aten::qr.Q(Tensor self, bool some=True, *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> qr_out(@ByRef Tensor Q, @ByRef Tensor R, @Const @ByRef Tensor self, @Cast("bool") boolean some/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> qr_out(@ByRef Tensor Q, @ByRef Tensor R, @Const @ByRef Tensor self);
// aten::qr.Q(Tensor self, bool some=True, *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> qr_outf(@Const @ByRef Tensor self, @Cast("bool") boolean some, @ByRef Tensor Q, @ByRef Tensor R);

// aten::qr(Tensor self, bool some=True) -> (Tensor Q, Tensor R)
@Namespace("at") public static native @ByVal TensorTensorTuple qr(@Const @ByRef Tensor self, @Cast("bool") boolean some/*=true*/);
@Namespace("at") public static native @ByVal TensorTensorTuple qr(@Const @ByRef Tensor self);




// Parsed from ATen/ops/qscheme.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/qscheme_ops.h>






// Parsed from ATen/ops/quantile.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/quantile_ops.h>


// aten::quantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor
@Namespace("at") public static native @ByVal Tensor quantile(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::string_view(\"linear\")") @Cast("c10::string_view*") Pointer interpolation);
@Namespace("at") public static native @ByVal Tensor quantile(@Const @ByRef Tensor self, @Const @ByRef Tensor q);

// aten::quantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::string_view(\"linear\")") @Cast("c10::string_view*") Pointer interpolation);
@Namespace("at") public static native @ByRef Tensor quantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor q);
// aten::quantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantile_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByVal @Cast("c10::string_view*") Pointer interpolation, @ByRef Tensor out);

// aten::quantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -> Tensor
@Namespace("at") public static native @ByVal Tensor quantile(@Const @ByRef Tensor self, double q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::string_view(\"linear\")") @Cast("c10::string_view*") Pointer interpolation);
@Namespace("at") public static native @ByVal Tensor quantile(@Const @ByRef Tensor self, double q);

// aten::quantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, double q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::string_view(\"linear\")") @Cast("c10::string_view*") Pointer interpolation);
@Namespace("at") public static native @ByRef Tensor quantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, double q);
// aten::quantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantile_outf(@Const @ByRef Tensor self, double q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByVal @Cast("c10::string_view*") Pointer interpolation, @ByRef Tensor out);




// Parsed from ATen/ops/quantize_per_channel.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/quantize_per_channel_ops.h>


// aten::quantize_per_channel(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantize_per_channel(@Const @ByRef Tensor self, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, ScalarType dtype);

// aten::quantize_per_channel.out(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantize_per_channel_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, ScalarType dtype);
// aten::quantize_per_channel.out(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantize_per_channel_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, ScalarType dtype, @ByRef Tensor out);




// Parsed from ATen/ops/quantize_per_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/quantize_per_tensor_ops.h>


// aten::quantize_per_tensor(Tensor self, float scale, int zero_point, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantize_per_tensor(@Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point, ScalarType dtype);

// aten::quantize_per_tensor.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantize_per_tensor(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, ScalarType dtype);

// aten::quantize_per_tensor.tensors(Tensor[] tensors, Tensor scales, Tensor zero_points, ScalarType dtype) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector quantize_per_tensor(@ByVal TensorArrayRef tensors, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, ScalarType dtype);

// aten::quantize_per_tensor.out(Tensor self, float scale, int zero_point, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantize_per_tensor_out(@ByRef Tensor out, @Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point, ScalarType dtype);
// aten::quantize_per_tensor.out(Tensor self, float scale, int zero_point, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantize_per_tensor_outf(@Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point, ScalarType dtype, @ByRef Tensor out);

// aten::quantize_per_tensor.tensor_qparams_out(Tensor self, Tensor scale, Tensor zero_point, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantize_per_tensor_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, ScalarType dtype);
// aten::quantize_per_tensor.tensor_qparams_out(Tensor self, Tensor scale, Tensor zero_point, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantize_per_tensor_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, ScalarType dtype, @ByRef Tensor out);

// aten::quantize_per_tensor.tensors_out(Tensor[] tensors, Tensor scales, Tensor zero_points, ScalarType dtype, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void quantize_per_tensor_out(@ByVal TensorArrayRef out, @ByVal TensorArrayRef tensors, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, ScalarType dtype);
// aten::quantize_per_tensor.tensors_out(Tensor[] tensors, Tensor scales, Tensor zero_points, ScalarType dtype, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void quantize_per_tensor_outf(@ByVal TensorArrayRef tensors, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, ScalarType dtype, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/quantize_per_tensor_dynamic.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/quantize_per_tensor_dynamic_ops.h>


// aten::quantize_per_tensor_dynamic(Tensor self, ScalarType dtype, bool reduce_range) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantize_per_tensor_dynamic(@Const @ByRef Tensor self, ScalarType dtype, @Cast("bool") boolean reduce_range);

// aten::quantize_per_tensor_dynamic.out(Tensor self, ScalarType dtype, bool reduce_range, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantize_per_tensor_dynamic_out(@ByRef Tensor out, @Const @ByRef Tensor self, ScalarType dtype, @Cast("bool") boolean reduce_range);
// aten::quantize_per_tensor_dynamic.out(Tensor self, ScalarType dtype, bool reduce_range, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantize_per_tensor_dynamic_outf(@Const @ByRef Tensor self, ScalarType dtype, @Cast("bool") boolean reduce_range, @ByRef Tensor out);




// Parsed from ATen/ops/quantized_batch_norm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/quantized_batch_norm_ops.h>


// aten::quantized_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantized_batch_norm(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef Tensor mean, @Const @ByRef Tensor var, double eps, double output_scale, @Cast("int64_t") long output_zero_point);

// aten::quantized_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantized_batch_norm_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef Tensor mean, @Const @ByRef Tensor var, double eps, double output_scale, @Cast("int64_t") long output_zero_point);
// aten::quantized_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantized_batch_norm_outf(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef Tensor mean, @Const @ByRef Tensor var, double eps, double output_scale, @Cast("int64_t") long output_zero_point, @ByRef Tensor out);




// Parsed from ATen/ops/quantized_gru_cell.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/quantized_gru_cell_ops.h>


// aten::quantized_gru_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantized_gru_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef Tensor b_ih, @Const @ByRef Tensor b_hh, @Const @ByRef Tensor packed_ih, @Const @ByRef Tensor packed_hh, @Const @ByRef Tensor col_offsets_ih, @Const @ByRef Tensor col_offsets_hh, @Const @ByRef Scalar scale_ih, @Const @ByRef Scalar scale_hh, @Const @ByRef Scalar zero_point_ih, @Const @ByRef Scalar zero_point_hh);




// Parsed from ATen/ops/quantized_lstm_cell.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/quantized_lstm_cell_ops.h>


// aten::quantized_lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple quantized_lstm_cell(@Const @ByRef Tensor input, @ByVal TensorArrayRef hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef Tensor b_ih, @Const @ByRef Tensor b_hh, @Const @ByRef Tensor packed_ih, @Const @ByRef Tensor packed_hh, @Const @ByRef Tensor col_offsets_ih, @Const @ByRef Tensor col_offsets_hh, @Const @ByRef Scalar scale_ih, @Const @ByRef Scalar scale_hh, @Const @ByRef Scalar zero_point_ih, @Const @ByRef Scalar zero_point_hh);




// Parsed from ATen/ops/quantized_max_pool1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/quantized_max_pool1d_ops.h>


// aten::quantized_max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantized_max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::quantized_max_pool1d.out(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantized_max_pool1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor quantized_max_pool1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor quantized_max_pool1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor quantized_max_pool1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
// aten::quantized_max_pool1d.out(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantized_max_pool1d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor quantized_max_pool1d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);




// Parsed from ATen/ops/quantized_max_pool2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/quantized_max_pool2d_ops.h>


// aten::quantized_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantized_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);

// aten::quantized_max_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantized_max_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor quantized_max_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor quantized_max_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByRef Tensor quantized_max_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
// aten::quantized_max_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor quantized_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor quantized_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out);




// Parsed from ATen/ops/quantized_rnn_relu_cell.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/quantized_rnn_relu_cell_ops.h>


// aten::quantized_rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantized_rnn_relu_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef Tensor b_ih, @Const @ByRef Tensor b_hh, @Const @ByRef Tensor packed_ih, @Const @ByRef Tensor packed_hh, @Const @ByRef Tensor col_offsets_ih, @Const @ByRef Tensor col_offsets_hh, @Const @ByRef Scalar scale_ih, @Const @ByRef Scalar scale_hh, @Const @ByRef Scalar zero_point_ih, @Const @ByRef Scalar zero_point_hh);




// Parsed from ATen/ops/quantized_rnn_tanh_cell.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/quantized_rnn_tanh_cell_ops.h>


// aten::quantized_rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor
@Namespace("at") public static native @ByVal Tensor quantized_rnn_tanh_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef Tensor b_ih, @Const @ByRef Tensor b_hh, @Const @ByRef Tensor packed_ih, @Const @ByRef Tensor packed_hh, @Const @ByRef Tensor col_offsets_ih, @Const @ByRef Tensor col_offsets_hh, @Const @ByRef Scalar scale_ih, @Const @ByRef Scalar scale_hh, @Const @ByRef Scalar zero_point_ih, @Const @ByRef Scalar zero_point_hh);




// Parsed from ATen/ops/rad2deg.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rad2deg_ops.h>


// aten::rad2deg(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor rad2deg(@Const @ByRef Tensor self);

// aten::rad2deg_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rad2deg_(@ByRef Tensor self);

// aten::rad2deg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rad2deg_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::rad2deg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rad2deg_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/rand.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rand_ops.h>


// aten::rand.names(SymInt[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);


// aten::rand.names(SymInt[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::rand.names(SymInt[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntRef size, @ByVal DimnameListOptional names);


// aten::rand.names(SymInt[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::rand.generator_with_names(SymInt[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);


// aten::rand.generator_with_names(SymInt[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::rand.generator_with_names(SymInt[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);


// aten::rand.generator_with_names(SymInt[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::rand(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::rand(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::rand(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntRef size);


// aten::rand(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::rand.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);


// aten::rand.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::rand.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntRef size, @ByVal GeneratorOptional generator);


// aten::rand.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand_symint(@ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::rand.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::rand.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);


// aten::rand.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_symint_out(@ByRef Tensor out, @ByVal SymIntRef size);


// aten::rand.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_symint_outf(@ByVal SymIntRef size, @ByRef Tensor out);


// aten::rand.generator_out(SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);


// aten::rand.generator_out(SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByRef Tensor out);


// aten::rand.generator_out(SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_symint_out(@ByRef Tensor out, @ByVal SymIntRef size, @ByVal GeneratorOptional generator);


// aten::rand.generator_out(SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_symint_outf(@ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);


// aten::rand.names_out(SymInt[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);


// aten::rand.names_out(SymInt[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByRef Tensor out);


// aten::rand.names_out(SymInt[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_symint_out(@ByRef Tensor out, @ByVal SymIntRef size, @ByVal DimnameListOptional names);


// aten::rand.names_out(SymInt[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_symint_outf(@ByVal SymIntRef size, @ByVal DimnameListOptional names, @ByRef Tensor out);


// aten::rand.generator_with_names_out(SymInt[] size, *, Generator? generator, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);


// aten::rand.generator_with_names_out(SymInt[] size, *, Generator? generator, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByRef Tensor out);


// aten::rand.generator_with_names_out(SymInt[] size, *, Generator? generator, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_symint_out(@ByRef Tensor out, @ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);


// aten::rand.generator_with_names_out(SymInt[] size, *, Generator? generator, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_symint_outf(@ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByRef Tensor out);





// Parsed from ATen/ops/rand_like.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rand_like_ops.h>


// aten::rand_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor rand_like(@Const @ByRef Tensor self);
// aten::rand_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rand_like(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::rand_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_like_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor rand_like_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::rand_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rand_like_outf(@Const @ByRef Tensor self, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);




// Parsed from ATen/ops/randint.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/randint_ops.h>


// aten::randint(int high, SymInt[] size, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::randint(int high, SymInt[] size, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randint(int high, SymInt[] size, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_symint(@Cast("int64_t") long high, @ByVal SymIntRef size, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint_symint(@Cast("int64_t") long high, @ByVal SymIntRef size);


// aten::randint(int high, SymInt[] size, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_symint(@Cast("int64_t") long high, @ByVal SymIntRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randint.generator(int high, SymInt[] size, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);


// aten::randint.generator(int high, SymInt[] size, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randint.generator(int high, SymInt[] size, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_symint(@Cast("int64_t") long high, @ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint_symint(@Cast("int64_t") long high, @ByVal SymIntRef size, @ByVal GeneratorOptional generator);


// aten::randint.generator(int high, SymInt[] size, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_symint(@Cast("int64_t") long high, @ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randint.low(int low, int high, SymInt[] size, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::randint.low(int low, int high, SymInt[] size, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randint.low(int low, int high, SymInt[] size, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_symint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal SymIntRef size, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint_symint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal SymIntRef size);


// aten::randint.low(int low, int high, SymInt[] size, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_symint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal SymIntRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randint.low_generator(int low, int high, SymInt[] size, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);


// aten::randint.low_generator(int low, int high, SymInt[] size, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randint.low_generator(int low, int high, SymInt[] size, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_symint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint_symint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal SymIntRef size, @ByVal GeneratorOptional generator);


// aten::randint.low_generator(int low, int high, SymInt[] size, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_symint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randint.out(int high, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::randint.out(int high, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);


// aten::randint.out(int high, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_symint_out(@ByRef Tensor out, @Cast("int64_t") long high, @ByVal SymIntRef size);


// aten::randint.out(int high, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_symint_outf(@Cast("int64_t") long high, @ByVal SymIntRef size, @ByRef Tensor out);


// aten::randint.generator_out(int high, SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);


// aten::randint.generator_out(int high, SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByRef Tensor out);


// aten::randint.generator_out(int high, SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_symint_out(@ByRef Tensor out, @Cast("int64_t") long high, @ByVal SymIntRef size, @ByVal GeneratorOptional generator);


// aten::randint.generator_out(int high, SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_symint_outf(@Cast("int64_t") long high, @ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);


// aten::randint.low_out(int low, int high, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::randint.low_out(int low, int high, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);


// aten::randint.low_out(int low, int high, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_symint_out(@ByRef Tensor out, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal SymIntRef size);


// aten::randint.low_out(int low, int high, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_symint_outf(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal SymIntRef size, @ByRef Tensor out);


// aten::randint.low_generator_out(int low, int high, SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);


// aten::randint.low_generator_out(int low, int high, SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByRef Tensor out);


// aten::randint.low_generator_out(int low, int high, SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_symint_out(@ByRef Tensor out, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal SymIntRef size, @ByVal GeneratorOptional generator);


// aten::randint.low_generator_out(int low, int high, SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_symint_outf(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);





// Parsed from ATen/ops/randint_like.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/randint_like_ops.h>


// aten::randint_like(Tensor self, int high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long high, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long high);
// aten::randint_like(Tensor self, int high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long high, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::randint_like.low_dtype(Tensor self, int low, int high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long low, @Cast("int64_t") long high);
// aten::randint_like.low_dtype(Tensor self, int low, int high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::randint_like.out(Tensor self, int high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_like_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long high, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor randint_like_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long high);
// aten::randint_like.out(Tensor self, int high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_like_outf(@Const @ByRef Tensor self, @Cast("int64_t") long high, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);

// aten::randint_like.low_dtype_out(Tensor self, int low, int high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_like_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor randint_like_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long low, @Cast("int64_t") long high);
// aten::randint_like.low_dtype_out(Tensor self, int low, int high, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randint_like_outf(@Const @ByRef Tensor self, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);




// Parsed from ATen/ops/randn.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/randn_ops.h>


// aten::randn(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::randn(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randn(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntRef size);


// aten::randn(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randn.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);


// aten::randn.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randn.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntRef size, @ByVal GeneratorOptional generator);


// aten::randn.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randn.names(SymInt[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);


// aten::randn.names(SymInt[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randn.names(SymInt[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntRef size, @ByVal DimnameListOptional names);


// aten::randn.names(SymInt[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randn.generator_with_names(SymInt[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);


// aten::randn.generator_with_names(SymInt[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randn.generator_with_names(SymInt[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);


// aten::randn.generator_with_names(SymInt[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn_symint(@ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::randn.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::randn.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);


// aten::randn.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_symint_out(@ByRef Tensor out, @ByVal SymIntRef size);


// aten::randn.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_symint_outf(@ByVal SymIntRef size, @ByRef Tensor out);


// aten::randn.generator_out(SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);


// aten::randn.generator_out(SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByRef Tensor out);


// aten::randn.generator_out(SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_symint_out(@ByRef Tensor out, @ByVal SymIntRef size, @ByVal GeneratorOptional generator);


// aten::randn.generator_out(SymInt[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_symint_outf(@ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);


// aten::randn.names_out(SymInt[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);


// aten::randn.names_out(SymInt[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByRef Tensor out);


// aten::randn.names_out(SymInt[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_symint_out(@ByRef Tensor out, @ByVal SymIntRef size, @ByVal DimnameListOptional names);


// aten::randn.names_out(SymInt[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_symint_outf(@ByVal SymIntRef size, @ByVal DimnameListOptional names, @ByRef Tensor out);


// aten::randn.generator_with_names_out(SymInt[] size, *, Generator? generator, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);


// aten::randn.generator_with_names_out(SymInt[] size, *, Generator? generator, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByRef Tensor out);


// aten::randn.generator_with_names_out(SymInt[] size, *, Generator? generator, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_symint_out(@ByRef Tensor out, @ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);


// aten::randn.generator_with_names_out(SymInt[] size, *, Generator? generator, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_symint_outf(@ByVal SymIntRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByRef Tensor out);





// Parsed from ATen/ops/randn_like.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/randn_like_ops.h>


// aten::randn_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor randn_like(@Const @ByRef Tensor self);
// aten::randn_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randn_like(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::randn_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_like_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor randn_like_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::randn_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randn_like_outf(@Const @ByRef Tensor self, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);




// Parsed from ATen/ops/random.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/random_ops.h>


// aten::random.from_out(Tensor self, int from, int? to, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor random_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long from, @ByVal LongOptional to, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor random_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long from, @ByVal LongOptional to);
// aten::random.from_out(Tensor self, int from, int? to, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor random_outf(@Const @ByRef Tensor self, @Cast("int64_t") long from, @ByVal LongOptional to, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::random.from(Tensor self, int from, int? to, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor random(@Const @ByRef Tensor self, @Cast("int64_t") long from, @ByVal LongOptional to, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor random(@Const @ByRef Tensor self, @Cast("int64_t") long from, @ByVal LongOptional to);

// aten::random.to_out(Tensor self, int to, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor random_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long to, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor random_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long to);
// aten::random.to_out(Tensor self, int to, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor random_outf(@Const @ByRef Tensor self, @Cast("int64_t") long to, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::random.to(Tensor self, int to, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor random(@Const @ByRef Tensor self, @Cast("int64_t") long to, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor random(@Const @ByRef Tensor self, @Cast("int64_t") long to);

// aten::random.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor random_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor random_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::random.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor random_outf(@Const @ByRef Tensor self, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::random(Tensor self, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor random(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor random(@Const @ByRef Tensor self);




// Parsed from ATen/ops/randperm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/randperm_ops.h>


// aten::randperm(int n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n);
// aten::randperm(int n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::randperm.generator(int n, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n, @ByVal GeneratorOptional generator);
// aten::randperm.generator(int n, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::randperm.out(int n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randperm_out(@ByRef Tensor out, @Cast("int64_t") long n);
// aten::randperm.out(int n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randperm_outf(@Cast("int64_t") long n, @ByRef Tensor out);

// aten::randperm.generator_out(int n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randperm_out(@ByRef Tensor out, @Cast("int64_t") long n, @ByVal GeneratorOptional generator);
// aten::randperm.generator_out(int n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor randperm_outf(@Cast("int64_t") long n, @ByVal GeneratorOptional generator, @ByRef Tensor out);




// Parsed from ATen/ops/range.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/range_ops.h>


// aten::range.step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor range(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar step, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
// aten::range.step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor range(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::range(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor range(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
// aten::range(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor range(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::range.out_(Scalar start, Scalar end, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor range_out(@ByRef Tensor out, @Const @ByRef Scalar start, @Const @ByRef Scalar end);
// aten::range.out_(Scalar start, Scalar end, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor range_outf(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByRef Tensor out);

// aten::range.out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor range_out(@ByRef Tensor out, @Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step);
// aten::range.out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor range_outf(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step, @ByRef Tensor out);




// Parsed from ATen/ops/ravel.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/ravel_ops.h>


// aten::ravel(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor ravel(@Const @ByRef Tensor self);




// Parsed from ATen/ops/real.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/real_ops.h>


// aten::real(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor real(@Const @ByRef Tensor self);




// Parsed from ATen/ops/reciprocal.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/reciprocal_ops.h>


// aten::reciprocal(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor reciprocal(@Const @ByRef Tensor self);

// aten::reciprocal_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reciprocal_(@ByRef Tensor self);

// aten::reciprocal.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reciprocal_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::reciprocal.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reciprocal_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/record_stream.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/record_stream_ops.h>






// Parsed from ATen/ops/refine_names.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/refine_names_ops.h>






// Parsed from ATen/ops/reflection_pad1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/reflection_pad1d_ops.h>


// aten::reflection_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::reflection_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);


// aten::reflection_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef padding);


// aten::reflection_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef padding, @ByRef Tensor out);


// aten::reflection_pad1d(Tensor self, SymInt[2] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor reflection_pad1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::reflection_pad1d(Tensor self, SymInt[2] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad1d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef padding);





// Parsed from ATen/ops/reflection_pad1d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/reflection_pad1d_backward_ops.h>


// aten::reflection_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::reflection_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor grad_input);


// aten::reflection_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntRef padding);


// aten::reflection_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_backward_symint_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntRef padding, @ByRef Tensor grad_input);


// aten::reflection_pad1d_backward(Tensor grad_output, Tensor self, SymInt[2] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad1d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor reflection_pad1d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::reflection_pad1d_backward(Tensor grad_output, Tensor self, SymInt[2] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad1d_backward_symint(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntRef padding);





// Parsed from ATen/ops/reflection_pad2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/reflection_pad2d_ops.h>


// aten::reflection_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::reflection_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);


// aten::reflection_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef padding);


// aten::reflection_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef padding, @ByRef Tensor out);


// aten::reflection_pad2d(Tensor self, SymInt[4] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor reflection_pad2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::reflection_pad2d(Tensor self, SymInt[4] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad2d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef padding);





// Parsed from ATen/ops/reflection_pad2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/reflection_pad2d_backward_ops.h>


// aten::reflection_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::reflection_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor grad_input);


// aten::reflection_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntRef padding);


// aten::reflection_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_backward_symint_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntRef padding, @ByRef Tensor grad_input);


// aten::reflection_pad2d_backward(Tensor grad_output, Tensor self, SymInt[4] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor reflection_pad2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::reflection_pad2d_backward(Tensor grad_output, Tensor self, SymInt[4] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad2d_backward_symint(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntRef padding);





// Parsed from ATen/ops/reflection_pad3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/reflection_pad3d_ops.h>


// aten::reflection_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::reflection_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);


// aten::reflection_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef padding);


// aten::reflection_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef padding, @ByRef Tensor out);


// aten::reflection_pad3d(Tensor self, SymInt[6] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor reflection_pad3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::reflection_pad3d(Tensor self, SymInt[6] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad3d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef padding);





// Parsed from ATen/ops/reflection_pad3d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/reflection_pad3d_backward_ops.h>


// aten::reflection_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::reflection_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor grad_input);


// aten::reflection_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntRef padding);


// aten::reflection_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor reflection_pad3d_backward_symint_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntRef padding, @ByRef Tensor grad_input);


// aten::reflection_pad3d_backward(Tensor grad_output, Tensor self, SymInt[6] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor reflection_pad3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::reflection_pad3d_backward(Tensor grad_output, Tensor self, SymInt[6] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor reflection_pad3d_backward_symint(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntRef padding);





// Parsed from ATen/ops/relu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/relu_ops.h>


// aten::relu(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor relu(@Const @ByRef Tensor self);

// aten::relu_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor relu_(@ByRef Tensor self);

// aten::relu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor relu_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::relu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor relu_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/relu6.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/relu6_ops.h>


// aten::relu6(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor relu6(@Const @ByRef Tensor self);

// aten::relu6_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor relu6_(@ByRef Tensor self);




// Parsed from ATen/ops/remainder.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/remainder_ops.h>


// aten::remainder.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor remainder_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::remainder.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor remainder_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::remainder.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor remainder(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::remainder.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor remainder_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::remainder.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor remainder_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::remainder.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor remainder(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::remainder.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor remainder(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::remainder.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor remainder_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);
// aten::remainder.Scalar_Tensor_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor remainder_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/rename.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rename_ops.h>






// Parsed from ATen/ops/renorm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/renorm_ops.h>


// aten::renorm.out(Tensor self, Scalar p, int dim, Scalar maxnorm, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor renorm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar p, @Cast("int64_t") long dim, @Const @ByRef Scalar maxnorm);
// aten::renorm.out(Tensor self, Scalar p, int dim, Scalar maxnorm, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor renorm_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar p, @Cast("int64_t") long dim, @Const @ByRef Scalar maxnorm, @ByRef Tensor out);

// aten::renorm(Tensor self, Scalar p, int dim, Scalar maxnorm) -> Tensor
@Namespace("at") public static native @ByVal Tensor renorm(@Const @ByRef Tensor self, @Const @ByRef Scalar p, @Cast("int64_t") long dim, @Const @ByRef Scalar maxnorm);




// Parsed from ATen/ops/repeat.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/repeat_ops.h>



// aten::repeat.out(Tensor self, SymInt[] repeats, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor repeat_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef repeats);
@Namespace("at") public static native @ByRef Tensor repeat_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... repeats);


// aten::repeat.out(Tensor self, SymInt[] repeats, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor repeat_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef repeats, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor repeat_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] repeats, @ByRef Tensor out);


// aten::repeat.out(Tensor self, SymInt[] repeats, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor repeat_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef repeats);


// aten::repeat.out(Tensor self, SymInt[] repeats, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor repeat_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef repeats, @ByRef Tensor out);





// Parsed from ATen/ops/repeat_interleave.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/repeat_interleave_ops.h>


// aten::repeat_interleave.Tensor(Tensor repeats, *, int? output_size=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor repeat_interleave(@Const @ByRef Tensor repeats, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional output_size);
@Namespace("at") public static native @ByVal Tensor repeat_interleave(@Const @ByRef Tensor repeats);

// aten::repeat_interleave.self_Tensor(Tensor self, Tensor repeats, int? dim=None, *, int? output_size=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor repeat_interleave(@Const @ByRef Tensor self, @Const @ByRef Tensor repeats, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional output_size);
@Namespace("at") public static native @ByVal Tensor repeat_interleave(@Const @ByRef Tensor self, @Const @ByRef Tensor repeats);

// aten::repeat_interleave.self_int(Tensor self, SymInt repeats, int? dim=None, *, int? output_size=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor repeat_interleave(@Const @ByRef Tensor self, @Cast("int64_t") long repeats, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional output_size);
@Namespace("at") public static native @ByVal Tensor repeat_interleave(@Const @ByRef Tensor self, @Cast("int64_t") long repeats);


// aten::repeat_interleave.self_int(Tensor self, SymInt repeats, int? dim=None, *, int? output_size=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor repeat_interleave_symint(@Const @ByRef Tensor self, @ByVal SymInt repeats, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional output_size);
@Namespace("at") public static native @ByVal Tensor repeat_interleave_symint(@Const @ByRef Tensor self, @ByVal SymInt repeats);


// aten::repeat_interleave.Tensor_out(Tensor repeats, *, int? output_size=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor repeat_interleave_out(@ByRef Tensor out, @Const @ByRef Tensor repeats, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional output_size);
@Namespace("at") public static native @ByRef Tensor repeat_interleave_out(@ByRef Tensor out, @Const @ByRef Tensor repeats);
// aten::repeat_interleave.Tensor_out(Tensor repeats, *, int? output_size=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor repeat_interleave_outf(@Const @ByRef Tensor repeats, @ByVal LongOptional output_size, @ByRef Tensor out);




// Parsed from ATen/ops/replication_pad1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/replication_pad1d_ops.h>


// aten::replication_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::replication_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad1d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor replication_pad1d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);


// aten::replication_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad1d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef padding);


// aten::replication_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad1d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef padding, @ByRef Tensor out);


// aten::replication_pad1d(Tensor self, SymInt[2] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::replication_pad1d(Tensor self, SymInt[2] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad1d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef padding);





// Parsed from ATen/ops/replication_pad1d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/replication_pad1d_backward_ops.h>


// aten::replication_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::replication_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad1d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor replication_pad1d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor grad_input);


// aten::replication_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad1d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntRef padding);


// aten::replication_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad1d_backward_symint_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntRef padding, @ByRef Tensor grad_input);


// aten::replication_pad1d_backward(Tensor grad_output, Tensor self, SymInt[2] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad1d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad1d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::replication_pad1d_backward(Tensor grad_output, Tensor self, SymInt[2] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad1d_backward_symint(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntRef padding);





// Parsed from ATen/ops/replication_pad2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/replication_pad2d_ops.h>


// aten::replication_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::replication_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor replication_pad2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);


// aten::replication_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef padding);


// aten::replication_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad2d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef padding, @ByRef Tensor out);


// aten::replication_pad2d(Tensor self, SymInt[4] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::replication_pad2d(Tensor self, SymInt[4] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad2d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef padding);





// Parsed from ATen/ops/replication_pad2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/replication_pad2d_backward_ops.h>


// aten::replication_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::replication_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor replication_pad2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor grad_input);


// aten::replication_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad2d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntRef padding);


// aten::replication_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad2d_backward_symint_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntRef padding, @ByRef Tensor grad_input);


// aten::replication_pad2d_backward(Tensor grad_output, Tensor self, SymInt[4] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::replication_pad2d_backward(Tensor grad_output, Tensor self, SymInt[4] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad2d_backward_symint(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntRef padding);





// Parsed from ATen/ops/replication_pad3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/replication_pad3d_ops.h>


// aten::replication_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::replication_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor replication_pad3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);


// aten::replication_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef padding);


// aten::replication_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad3d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef padding, @ByRef Tensor out);


// aten::replication_pad3d(Tensor self, SymInt[6] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::replication_pad3d(Tensor self, SymInt[6] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad3d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef padding);





// Parsed from ATen/ops/replication_pad3d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/replication_pad3d_backward_ops.h>


// aten::replication_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::replication_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor replication_pad3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor grad_input);


// aten::replication_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad3d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntRef padding);


// aten::replication_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor replication_pad3d_backward_symint_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntRef padding, @ByRef Tensor grad_input);


// aten::replication_pad3d_backward(Tensor grad_output, Tensor self, SymInt[6] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::replication_pad3d_backward(Tensor grad_output, Tensor self, SymInt[6] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor replication_pad3d_backward_symint(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal SymIntRef padding);





// Parsed from ATen/ops/requires_grad.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/requires_grad_ops.h>






// Parsed from ATen/ops/reshape.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/reshape_ops.h>


// aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor reshape(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape);
@Namespace("at") public static native @ByVal Tensor reshape(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... shape);


// aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor reshape_symint(@Const @ByRef Tensor self, @ByVal SymIntRef shape);





// Parsed from ATen/ops/reshape_as.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/reshape_as_ops.h>






// Parsed from ATen/ops/resize.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/resize_ops.h>



// aten::resize.out(Tensor self, SymInt[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor resize_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @Const @ByRef Tensor resize_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @Const @ByRef Tensor resize_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @Const @ByRef Tensor resize_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::resize.out(Tensor self, SymInt[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor resize_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal MemoryFormatOptional memory_format, @Const @ByRef Tensor out);
@Namespace("at") public static native @Const @ByRef Tensor resize_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal MemoryFormatOptional memory_format, @Const @ByRef Tensor out);


// aten::resize.out(Tensor self, SymInt[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor resize_symint_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef size, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @Const @ByRef Tensor resize_symint_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef size);


// aten::resize.out(Tensor self, SymInt[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor resize_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef size, @ByVal MemoryFormatOptional memory_format, @Const @ByRef Tensor out);


// aten::resize(Tensor self, SymInt[] size, *, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor resize(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor resize(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor resize(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor resize(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::resize(Tensor self, SymInt[] size, *, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor resize_symint(@Const @ByRef Tensor self, @ByVal SymIntRef size, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor resize_symint(@Const @ByRef Tensor self, @ByVal SymIntRef size);





// Parsed from ATen/ops/resize_as.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/resize_as_ops.h>


// aten::resize_as_(Tensor(a!) self, Tensor the_template, *, MemoryFormat? memory_format=None) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor resize_as_(@Const @ByRef Tensor self, @Const @ByRef Tensor the_template, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @Const @ByRef Tensor resize_as_(@Const @ByRef Tensor self, @Const @ByRef Tensor the_template);

// aten::resize_as.out(Tensor self, Tensor the_template, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor resize_as_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor the_template, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @Const @ByRef Tensor resize_as_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor the_template);
// aten::resize_as.out(Tensor self, Tensor the_template, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor resize_as_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor the_template, @ByVal MemoryFormatOptional memory_format, @Const @ByRef Tensor out);

// aten::resize_as(Tensor self, Tensor the_template, *, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor resize_as(@Const @ByRef Tensor self, @Const @ByRef Tensor the_template, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor resize_as(@Const @ByRef Tensor self, @Const @ByRef Tensor the_template);




// Parsed from ATen/ops/resize_as_sparse.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/resize_as_sparse_ops.h>


// aten::resize_as_sparse_(Tensor(a!) self, Tensor the_template) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor resize_as_sparse_(@Const @ByRef Tensor self, @Const @ByRef Tensor the_template);

// aten::resize_as_sparse.out(Tensor self, Tensor the_template, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor resize_as_sparse_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor the_template);
// aten::resize_as_sparse.out(Tensor self, Tensor the_template, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor resize_as_sparse_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor the_template, @Const @ByRef Tensor out);

// aten::resize_as_sparse(Tensor self, Tensor the_template) -> Tensor
@Namespace("at") public static native @ByVal Tensor resize_as_sparse(@Const @ByRef Tensor self, @Const @ByRef Tensor the_template);




// Parsed from ATen/ops/resolve_conj.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/resolve_conj_ops.h>


// aten::resolve_conj(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor resolve_conj(@Const @ByRef Tensor self);




// Parsed from ATen/ops/resolve_neg.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/resolve_neg_ops.h>


// aten::resolve_neg(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor resolve_neg(@Const @ByRef Tensor self);




// Parsed from ATen/ops/result_type.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/result_type_ops.h>


// aten::result_type.Tensor(Tensor tensor, Tensor other) -> ScalarType
@Namespace("at") public static native ScalarType result_type(@Const @ByRef Tensor tensor, @Const @ByRef Tensor other);

// aten::result_type.Scalar(Tensor tensor, Scalar other) -> ScalarType
@Namespace("at") public static native ScalarType result_type(@Const @ByRef Tensor tensor, @Const @ByRef Scalar other);

// aten::result_type.Scalar_Tensor(Scalar scalar, Tensor tensor) -> ScalarType
@Namespace("at") public static native ScalarType result_type(@Const @ByRef Scalar scalar, @Const @ByRef Tensor tensor);

// aten::result_type.Scalar_Scalar(Scalar scalar1, Scalar scalar2) -> ScalarType
@Namespace("at") public static native ScalarType result_type(@Const @ByRef Scalar scalar1, @Const @ByRef Scalar scalar2);




// Parsed from ATen/ops/retain_grad.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/retain_grad_ops.h>






// Parsed from ATen/ops/retains_grad.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/retains_grad_ops.h>






// Parsed from ATen/ops/rnn_relu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rnn_relu_ops.h>


// aten::rnn_relu.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple rnn_relu(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);

// aten::rnn_relu.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple rnn_relu(@Const @ByRef Tensor data, @Const @ByRef Tensor batch_sizes, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional);




// Parsed from ATen/ops/rnn_relu_cell.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rnn_relu_cell_ops.h>


// aten::rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rnn_relu_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_ih, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_hh);
@Namespace("at") public static native @ByVal Tensor rnn_relu_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh);




// Parsed from ATen/ops/rnn_tanh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rnn_tanh_ops.h>


// aten::rnn_tanh.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple rnn_tanh(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);

// aten::rnn_tanh.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple rnn_tanh(@Const @ByRef Tensor data, @Const @ByRef Tensor batch_sizes, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional);




// Parsed from ATen/ops/rnn_tanh_cell.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rnn_tanh_cell_ops.h>


// aten::rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rnn_tanh_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_ih, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_hh);
@Namespace("at") public static native @ByVal Tensor rnn_tanh_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh);




// Parsed from ATen/ops/roll.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/roll_ops.h>


// aten::roll(Tensor self, int[1] shifts, int[1] dims=[]) -> Tensor
@Namespace("at") public static native @ByVal Tensor roll(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shifts, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("at") public static native @ByVal Tensor roll(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shifts);
@Namespace("at") public static native @ByVal Tensor roll(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] shifts, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);
@Namespace("at") public static native @ByVal Tensor roll(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... shifts);

// aten::roll.out(Tensor self, int[1] shifts, int[1] dims=[], *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor roll_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shifts, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("at") public static native @ByRef Tensor roll_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shifts);
@Namespace("at") public static native @ByRef Tensor roll_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] shifts, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);
@Namespace("at") public static native @ByRef Tensor roll_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... shifts);
// aten::roll.out(Tensor self, int[1] shifts, int[1] dims=[], *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor roll_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shifts, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor roll_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] shifts, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dims, @ByRef Tensor out);




// Parsed from ATen/ops/rot90.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rot90_ops.h>


// aten::rot90(Tensor self, int k=1, int[] dims=[0,1]) -> Tensor
@Namespace("at") public static native @ByVal Tensor rot90(@Const @ByRef Tensor self, @Cast("int64_t") long k/*=1*/, @ByVal(nullValue = "at::IntArrayRef({0,1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("at") public static native @ByVal Tensor rot90(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor rot90(@Const @ByRef Tensor self, @Cast("int64_t") long k/*=1*/, @ByVal(nullValue = "at::IntArrayRef({0,1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);

// aten::rot90.out(Tensor self, int k=1, int[] dims=[0,1], *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rot90_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long k/*=1*/, @ByVal(nullValue = "at::IntArrayRef({0,1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("at") public static native @ByRef Tensor rot90_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor rot90_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long k/*=1*/, @ByVal(nullValue = "at::IntArrayRef({0,1})") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);
// aten::rot90.out(Tensor self, int k=1, int[] dims=[0,1], *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rot90_outf(@Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor rot90_outf(@Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dims, @ByRef Tensor out);




// Parsed from ATen/ops/round.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/round_ops.h>


// aten::round(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor round(@Const @ByRef Tensor self);

// aten::round_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor round_(@ByRef Tensor self);

// aten::round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor round_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor round_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::round.decimals(Tensor self, *, int decimals) -> Tensor
@Namespace("at") public static native @ByVal Tensor round(@Const @ByRef Tensor self, @Cast("int64_t") long decimals);

// aten::round_.decimals(Tensor(a!) self, *, int decimals) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor round_(@ByRef Tensor self, @Cast("int64_t") long decimals);

// aten::round.decimals_out(Tensor self, *, int decimals, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor round_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long decimals);
// aten::round.decimals_out(Tensor self, *, int decimals, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor round_outf(@Const @ByRef Tensor self, @Cast("int64_t") long decimals, @ByRef Tensor out);




// Parsed from ATen/ops/row_indices.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/row_indices_ops.h>






// Parsed from ATen/ops/row_indices_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/row_indices_copy_ops.h>


// aten::row_indices_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor row_indices_copy(@Const @ByRef Tensor self);

// aten::row_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor row_indices_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::row_indices_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor row_indices_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/row_stack.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/row_stack_ops.h>


// aten::row_stack(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor row_stack(@ByVal TensorArrayRef tensors);

// aten::row_stack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor row_stack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
// aten::row_stack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor row_stack_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);




// Parsed from ATen/ops/rrelu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rrelu_ops.h>


// aten::rrelu(Tensor self, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rrelu(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.125)") Scalar lower, @Const @ByRef(nullValue = "at::Scalar(0.3333333333333333)") Scalar upper, @Cast("bool") boolean training/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor rrelu(@Const @ByRef Tensor self);

// aten::rrelu_(Tensor(a!) self, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rrelu_(@ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.125)") Scalar lower, @Const @ByRef(nullValue = "at::Scalar(0.3333333333333333)") Scalar upper, @Cast("bool") boolean training/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor rrelu_(@ByRef Tensor self);




// Parsed from ATen/ops/rrelu_with_noise.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rrelu_with_noise_ops.h>


// aten::rrelu_with_noise.out(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor noise, @Const @ByRef(nullValue = "at::Scalar(0.125)") Scalar lower, @Const @ByRef(nullValue = "at::Scalar(0.3333333333333333)") Scalar upper, @Cast("bool") boolean training/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor noise);
// aten::rrelu_with_noise.out(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor noise, @Const @ByRef Scalar lower, @Const @ByRef Scalar upper, @Cast("bool") boolean training, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::rrelu_with_noise(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor rrelu_with_noise(@Const @ByRef Tensor self, @Const @ByRef Tensor noise, @Const @ByRef(nullValue = "at::Scalar(0.125)") Scalar lower, @Const @ByRef(nullValue = "at::Scalar(0.3333333333333333)") Scalar upper, @Cast("bool") boolean training/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor rrelu_with_noise(@Const @ByRef Tensor self, @Const @ByRef Tensor noise);

// aten::rrelu_with_noise_(Tensor(a!) self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_(@ByRef Tensor self, @Const @ByRef Tensor noise, @Const @ByRef(nullValue = "at::Scalar(0.125)") Scalar lower, @Const @ByRef(nullValue = "at::Scalar(0.3333333333333333)") Scalar upper, @Cast("bool") boolean training/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_(@ByRef Tensor self, @Const @ByRef Tensor noise);




// Parsed from ATen/ops/rrelu_with_noise_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rrelu_with_noise_backward_ops.h>


// aten::rrelu_with_noise_backward(Tensor grad_output, Tensor self, Tensor noise, Scalar lower, Scalar upper, bool training, bool self_is_result) -> Tensor
@Namespace("at") public static native @ByVal Tensor rrelu_with_noise_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor noise, @Const @ByRef Scalar lower, @Const @ByRef Scalar upper, @Cast("bool") boolean training, @Cast("bool") boolean self_is_result);

// aten::rrelu_with_noise_backward.out(Tensor grad_output, Tensor self, Tensor noise, Scalar lower, Scalar upper, bool training, bool self_is_result, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor noise, @Const @ByRef Scalar lower, @Const @ByRef Scalar upper, @Cast("bool") boolean training, @Cast("bool") boolean self_is_result);
// aten::rrelu_with_noise_backward.out(Tensor grad_output, Tensor self, Tensor noise, Scalar lower, Scalar upper, bool training, bool self_is_result, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor noise, @Const @ByRef Scalar lower, @Const @ByRef Scalar upper, @Cast("bool") boolean training, @Cast("bool") boolean self_is_result, @ByRef Tensor out);




// Parsed from ATen/ops/rshift.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rshift_ops.h>


// aten::__rshift__.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __rshift__(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::__rshift__.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __rshift__(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::__rshift__.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor __rshift___out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::__rshift__.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor __rshift___outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);

// aten::__rshift__.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor __rshift___out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::__rshift__.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor __rshift___outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/rsqrt.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rsqrt_ops.h>


// aten::rsqrt(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor rsqrt(@Const @ByRef Tensor self);

// aten::rsqrt_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rsqrt_(@ByRef Tensor self);

// aten::rsqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rsqrt_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::rsqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rsqrt_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/rsub.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/rsub_ops.h>


// aten::rsub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor rsub(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor rsub(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::rsub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor rsub(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor rsub(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::rsub.Tensor_out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rsub_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor rsub_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::rsub.Tensor_out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rsub_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::rsub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rsub_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor rsub_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::rsub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor rsub_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef Scalar alpha, @ByRef Tensor out);




// Parsed from ATen/ops/scalar_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/scalar_tensor_ops.h>


// aten::scalar_tensor(Scalar s, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor scalar_tensor(@Const @ByRef Scalar s, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor scalar_tensor(@Const @ByRef Scalar s);
// aten::scalar_tensor(Scalar s, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor scalar_tensor(@Const @ByRef Scalar s, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::scalar_tensor.out(Scalar s, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scalar_tensor_out(@ByRef Tensor out, @Const @ByRef Scalar s);
// aten::scalar_tensor.out(Scalar s, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scalar_tensor_outf(@Const @ByRef Scalar s, @ByRef Tensor out);




// Parsed from ATen/ops/scaled_dot_product_attention.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/scaled_dot_product_attention_ops.h>


// aten::scaled_dot_product_attention(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor scaled_dot_product_attention(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional attn_mask, double dropout_p/*=0.0*/, @Cast("bool") boolean is_causal/*=false*/);
@Namespace("at") public static native @ByVal Tensor scaled_dot_product_attention(@Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value);




// Parsed from ATen/ops/scatter.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/scatter_ops.h>


// aten::scatter.src(Tensor self, int dim, Tensor index, Tensor src) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src);

// aten::scatter.src_out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src);
// aten::scatter.src_out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @ByRef Tensor out);

// aten::scatter.value(Tensor self, int dim, Tensor index, Scalar value) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value);

// aten::scatter.value_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value);
// aten::scatter.value_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value, @ByRef Tensor out);

// aten::scatter.reduce(Tensor self, int dim, Tensor index, Tensor src, *, str reduce) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @ByVal @Cast("c10::string_view*") Pointer reduce);

// aten::scatter.reduce_out(Tensor self, int dim, Tensor index, Tensor src, *, str reduce, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @ByVal @Cast("c10::string_view*") Pointer reduce);
// aten::scatter.reduce_out(Tensor self, int dim, Tensor index, Tensor src, *, str reduce, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @ByVal @Cast("c10::string_view*") Pointer reduce, @ByRef Tensor out);

// aten::scatter.value_reduce(Tensor self, int dim, Tensor index, Scalar value, *, str reduce) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value, @ByVal @Cast("c10::string_view*") Pointer reduce);

// aten::scatter.value_reduce_out(Tensor self, int dim, Tensor index, Scalar value, *, str reduce, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value, @ByVal @Cast("c10::string_view*") Pointer reduce);
// aten::scatter.value_reduce_out(Tensor self, int dim, Tensor index, Scalar value, *, str reduce, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value, @ByVal @Cast("c10::string_view*") Pointer reduce, @ByRef Tensor out);

// aten::scatter.dimname_src(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src);

// aten::scatter.dimname_value(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Scalar value);




// Parsed from ATen/ops/scatter_add.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/scatter_add_ops.h>


// aten::scatter_add(Tensor self, int dim, Tensor index, Tensor src) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter_add(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src);

// aten::scatter_add.out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_add_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src);
// aten::scatter_add.out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_add_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @ByRef Tensor out);

// aten::scatter_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter_add(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src);




// Parsed from ATen/ops/scatter_reduce.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/scatter_reduce_ops.h>


// aten::scatter_reduce.two(Tensor self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor scatter_reduce(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @ByVal @Cast("c10::string_view*") Pointer reduce, @Cast("bool") boolean include_self/*=true*/);
@Namespace("at") public static native @ByVal Tensor scatter_reduce(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @ByVal @Cast("c10::string_view*") Pointer reduce);

// aten::scatter_reduce.two_out(Tensor self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_reduce_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @ByVal @Cast("c10::string_view*") Pointer reduce, @Cast("bool") boolean include_self/*=true*/);
@Namespace("at") public static native @ByRef Tensor scatter_reduce_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @ByVal @Cast("c10::string_view*") Pointer reduce);
// aten::scatter_reduce.two_out(Tensor self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor scatter_reduce_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src, @ByVal @Cast("c10::string_view*") Pointer reduce, @Cast("bool") boolean include_self, @ByRef Tensor out);




// Parsed from ATen/ops/searchsorted.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/searchsorted_ops.h>


// aten::searchsorted.Tensor(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor searchsorted(@Const @ByRef Tensor sorted_sequence, @Const @ByRef Tensor self, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer side, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional sorter);
@Namespace("at") public static native @ByVal Tensor searchsorted(@Const @ByRef Tensor sorted_sequence, @Const @ByRef Tensor self);

// aten::searchsorted.Tensor_out(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor searchsorted_out(@ByRef Tensor out, @Const @ByRef Tensor sorted_sequence, @Const @ByRef Tensor self, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer side, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional sorter);
@Namespace("at") public static native @ByRef Tensor searchsorted_out(@ByRef Tensor out, @Const @ByRef Tensor sorted_sequence, @Const @ByRef Tensor self);
// aten::searchsorted.Tensor_out(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor searchsorted_outf(@Const @ByRef Tensor sorted_sequence, @Const @ByRef Tensor self, @Cast("bool") boolean out_int32, @Cast("bool") boolean right, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer side, @Const @ByRef TensorOptional sorter, @ByRef Tensor out);

// aten::searchsorted.Scalar(Tensor sorted_sequence, Scalar self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor searchsorted(@Const @ByRef Tensor sorted_sequence, @Const @ByRef Scalar self, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer side, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional sorter);
@Namespace("at") public static native @ByVal Tensor searchsorted(@Const @ByRef Tensor sorted_sequence, @Const @ByRef Scalar self);

// aten::searchsorted.Scalar_out(Tensor sorted_sequence, Scalar self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor searchsorted_out(@ByRef Tensor out, @Const @ByRef Tensor sorted_sequence, @Const @ByRef Scalar self, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/, @ByVal(nullValue = "c10::optional<c10::string_view>(c10::nullopt)") @Cast("c10::optional<c10::string_view>*") Pointer side, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional sorter);
@Namespace("at") public static native @ByRef Tensor searchsorted_out(@ByRef Tensor out, @Const @ByRef Tensor sorted_sequence, @Const @ByRef Scalar self);
// aten::searchsorted.Scalar_out(Tensor sorted_sequence, Scalar self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor searchsorted_outf(@Const @ByRef Tensor sorted_sequence, @Const @ByRef Scalar self, @Cast("bool") boolean out_int32, @Cast("bool") boolean right, @ByVal @Cast("c10::optional<c10::string_view>*") Pointer side, @Const @ByRef TensorOptional sorter, @ByRef Tensor out);




// Parsed from ATen/ops/segment_reduce.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/segment_reduce_ops.h>


// aten::segment_reduce(Tensor data, str reduce, *, Tensor? lengths=None, Tensor? indices=None, Tensor? offsets=None, int axis=0, bool unsafe=False, Scalar? initial=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor segment_reduce(@Const @ByRef Tensor data, @ByVal @Cast("c10::string_view*") Pointer reduce, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional lengths, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional indices, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional offsets, @Cast("int64_t") long axis/*=0*/, @Cast("bool") boolean unsafe/*=false*/, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional initial);
@Namespace("at") public static native @ByVal Tensor segment_reduce(@Const @ByRef Tensor data, @ByVal @Cast("c10::string_view*") Pointer reduce);

// aten::segment_reduce.out(Tensor data, str reduce, *, Tensor? lengths=None, Tensor? indices=None, Tensor? offsets=None, int axis=0, bool unsafe=False, Scalar? initial=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor segment_reduce_out(@ByRef Tensor out, @Const @ByRef Tensor data, @ByVal @Cast("c10::string_view*") Pointer reduce, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional lengths, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional indices, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional offsets, @Cast("int64_t") long axis/*=0*/, @Cast("bool") boolean unsafe/*=false*/, @Const @ByRef(nullValue = "c10::optional<at::Scalar>(c10::nullopt)") ScalarOptional initial);
@Namespace("at") public static native @ByRef Tensor segment_reduce_out(@ByRef Tensor out, @Const @ByRef Tensor data, @ByVal @Cast("c10::string_view*") Pointer reduce);
// aten::segment_reduce.out(Tensor data, str reduce, *, Tensor? lengths=None, Tensor? indices=None, Tensor? offsets=None, int axis=0, bool unsafe=False, Scalar? initial=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor segment_reduce_outf(@Const @ByRef Tensor data, @ByVal @Cast("c10::string_view*") Pointer reduce, @Const @ByRef TensorOptional lengths, @Const @ByRef TensorOptional indices, @Const @ByRef TensorOptional offsets, @Cast("int64_t") long axis, @Cast("bool") boolean unsafe, @Const @ByRef ScalarOptional initial, @ByRef Tensor out);




// Parsed from ATen/ops/select.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/select_ops.h>


// aten::select.Dimname(Tensor(a) self, Dimname dim, int index) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor select(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("int64_t") long index);

// aten::select.int(Tensor(a) self, int dim, SymInt index) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor select(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long index);


// aten::select.int(Tensor(a) self, int dim, SymInt index) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor select_symint(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal SymInt index);





// Parsed from ATen/ops/select_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/select_backward_ops.h>


// aten::select_backward(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index) -> Tensor
@Namespace("at") public static native @ByVal Tensor select_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long index);
@Namespace("at") public static native @ByVal Tensor select_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long index);


// aten::select_backward(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index) -> Tensor
@Namespace("at") public static native @ByVal Tensor select_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef input_sizes, @Cast("int64_t") long dim, @ByVal SymInt index);


// aten::select_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long index);
@Namespace("at") public static native @ByRef Tensor select_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long index);


// aten::select_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long index, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor select_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long index, @ByRef Tensor out);


// aten::select_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_backward_symint_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @ByVal SymIntRef input_sizes, @Cast("int64_t") long dim, @ByVal SymInt index);


// aten::select_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntRef input_sizes, @Cast("int64_t") long dim, @ByVal SymInt index, @ByRef Tensor out);





// Parsed from ATen/ops/select_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/select_copy_ops.h>


// aten::select_copy.int(Tensor self, int dim, SymInt index) -> Tensor
@Namespace("at") public static native @ByVal Tensor select_copy(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long index);


// aten::select_copy.int(Tensor self, int dim, SymInt index) -> Tensor
@Namespace("at") public static native @ByVal Tensor select_copy_symint(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal SymInt index);


// aten::select_copy.int_out(Tensor self, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long index);


// aten::select_copy.int_out(Tensor self, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long index, @ByRef Tensor out);


// aten::select_copy.int_out(Tensor self, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_copy_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal SymInt index);


// aten::select_copy.int_out(Tensor self, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_copy_symint_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal SymInt index, @ByRef Tensor out);





// Parsed from ATen/ops/select_scatter.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/select_scatter_ops.h>


// aten::select_scatter(Tensor self, Tensor src, int dim, SymInt index) -> Tensor
@Namespace("at") public static native @ByVal Tensor select_scatter(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim, @Cast("int64_t") long index);


// aten::select_scatter(Tensor self, Tensor src, int dim, SymInt index) -> Tensor
@Namespace("at") public static native @ByVal Tensor select_scatter_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim, @ByVal SymInt index);


// aten::select_scatter.out(Tensor self, Tensor src, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim, @Cast("int64_t") long index);


// aten::select_scatter.out(Tensor self, Tensor src, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_scatter_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim, @Cast("int64_t") long index, @ByRef Tensor out);


// aten::select_scatter.out(Tensor self, Tensor src, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_scatter_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim, @ByVal SymInt index);


// aten::select_scatter.out(Tensor self, Tensor src, int dim, SymInt index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor select_scatter_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim, @ByVal SymInt index, @ByRef Tensor out);





// Parsed from ATen/ops/selu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/selu_ops.h>


// aten::selu(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor selu(@Const @ByRef Tensor self);

// aten::selu_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor selu_(@ByRef Tensor self);




// Parsed from ATen/ops/set.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/set_ops.h>





// aten::set.source_Storage_out(Tensor self, Storage source, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor set_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source);
// aten::set.source_Storage_out(Tensor self, Storage source, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor set_outf(@Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @ByRef Tensor out);

// aten::set.source_Storage(Tensor self, Storage source) -> Tensor
@Namespace("at") public static native @ByVal Tensor set(@Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source);

// aten::set.source_Storage_storage_offset_out(Tensor self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[], *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor set_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @Cast("int64_t") long storage_offset, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor set_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @Cast("int64_t") long storage_offset, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor set_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @Cast("int64_t") long storage_offset, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);
@Namespace("at") public static native @ByRef Tensor set_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @Cast("int64_t") long storage_offset, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::set.source_Storage_storage_offset_out(Tensor self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[], *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor set_outf(@Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @Cast("int64_t") long storage_offset, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor set_outf(@Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @Cast("int64_t") long storage_offset, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByRef Tensor out);


// aten::set.source_Storage_storage_offset_out(Tensor self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[], *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor set_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @ByVal SymInt storage_offset, @ByVal SymIntRef size, @ByVal(nullValue = "c10::SymIntArrayRef{}") SymIntRef stride);
@Namespace("at") public static native @ByRef Tensor set_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @ByVal SymInt storage_offset, @ByVal SymIntRef size);


// aten::set.source_Storage_storage_offset_out(Tensor self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[], *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor set_symint_outf(@Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @ByVal SymInt storage_offset, @ByVal SymIntRef size, @ByVal SymIntRef stride, @ByRef Tensor out);


// aten::set.source_Storage_storage_offset(Tensor self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -> Tensor
@Namespace("at") public static native @ByVal Tensor set(@Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @Cast("int64_t") long storage_offset, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor set(@Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @Cast("int64_t") long storage_offset, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor set(@Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @Cast("int64_t") long storage_offset, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::IntArrayRef{}") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);
@Namespace("at") public static native @ByVal Tensor set(@Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @Cast("int64_t") long storage_offset, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::set.source_Storage_storage_offset(Tensor self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -> Tensor
@Namespace("at") public static native @ByVal Tensor set_symint(@Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @ByVal SymInt storage_offset, @ByVal SymIntRef size, @ByVal(nullValue = "c10::SymIntArrayRef{}") SymIntRef stride);
@Namespace("at") public static native @ByVal Tensor set_symint(@Const @ByRef Tensor self, @Cast({"", "c10::Storage&&"}) @StdMove Storage source, @ByVal SymInt storage_offset, @ByVal SymIntRef size);


// aten::set.source_Tensor_out(Tensor self, Tensor source, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor set_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor source);
// aten::set.source_Tensor_out(Tensor self, Tensor source, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor set_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor source, @ByRef Tensor out);

// aten::set.source_Tensor(Tensor self, Tensor source) -> Tensor
@Namespace("at") public static native @ByVal Tensor set(@Const @ByRef Tensor self, @Const @ByRef Tensor source);

// aten::set.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor set_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::set.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor set_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::set(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor set(@Const @ByRef Tensor self);




// Parsed from ATen/ops/set_data.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/set_data_ops.h>






// Parsed from ATen/ops/sgn.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sgn_ops.h>


// aten::sgn(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor sgn(@Const @ByRef Tensor self);

// aten::sgn.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sgn_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::sgn.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sgn_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/sigmoid.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sigmoid_ops.h>


// aten::sigmoid(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor sigmoid(@Const @ByRef Tensor self);

// aten::sigmoid_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sigmoid_(@ByRef Tensor self);

// aten::sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sigmoid_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sigmoid_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/sigmoid_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sigmoid_backward_ops.h>


// aten::sigmoid_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sigmoid_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output);
// aten::sigmoid_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sigmoid_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @ByRef Tensor grad_input);

// aten::sigmoid_backward(Tensor grad_output, Tensor output) -> Tensor
@Namespace("at") public static native @ByVal Tensor sigmoid_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output);




// Parsed from ATen/ops/sign.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sign_ops.h>


// aten::sign(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor sign(@Const @ByRef Tensor self);

// aten::sign.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sign_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::sign.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sign_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/signbit.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/signbit_ops.h>


// aten::signbit(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor signbit(@Const @ByRef Tensor self);

// aten::signbit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor signbit_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::signbit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor signbit_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/silu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/silu_ops.h>


// aten::silu(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor silu(@Const @ByRef Tensor self);

// aten::silu_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor silu_(@ByRef Tensor self);

// aten::silu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor silu_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::silu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor silu_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/silu_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/silu_backward_ops.h>


// aten::silu_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor silu_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
// aten::silu_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor silu_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByRef Tensor grad_input);

// aten::silu_backward(Tensor grad_output, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor silu_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);




// Parsed from ATen/ops/sin.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sin_ops.h>


// aten::sin(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor sin(@Const @ByRef Tensor self);

// aten::sin_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sin_(@ByRef Tensor self);

// aten::sin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sin_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::sin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sin_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/sinc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sinc_ops.h>


// aten::sinc(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor sinc(@Const @ByRef Tensor self);

// aten::sinc_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sinc_(@ByRef Tensor self);

// aten::sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sinc_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sinc_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/sinh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sinh_ops.h>


// aten::sinh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor sinh(@Const @ByRef Tensor self);

// aten::sinh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sinh_(@ByRef Tensor self);

// aten::sinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sinh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::sinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sinh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/size.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/size_ops.h>


// aten::size.int(Tensor self, int dim) -> int
@Namespace("at") public static native @Cast("int64_t") long __dispatch_size(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::size.Dimname(Tensor self, Dimname dim) -> int
@Namespace("at") public static native @Cast("int64_t") long size(@Const @ByRef Tensor self, @ByVal Dimname dim);




// Parsed from ATen/ops/slice.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/slice_ops.h>


// aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor slice(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=0*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional start, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional end, @Cast("int64_t") long step/*=1*/);
@Namespace("at") public static native @ByVal Tensor slice(@Const @ByRef Tensor self);


// aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor slice_symint(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=0*/, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional start, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional end, @ByVal(nullValue = "c10::SymInt(1)") SymInt step);
@Namespace("at") public static native @ByVal Tensor slice_symint(@Const @ByRef Tensor self);





// Parsed from ATen/ops/slice_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/slice_backward_ops.h>


// aten::slice_backward(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt start, SymInt end, SymInt step) -> Tensor
@Namespace("at") public static native @ByVal Tensor slice_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long end, @Cast("int64_t") long step);
@Namespace("at") public static native @ByVal Tensor slice_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long end, @Cast("int64_t") long step);


// aten::slice_backward(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt start, SymInt end, SymInt step) -> Tensor
@Namespace("at") public static native @ByVal Tensor slice_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef input_sizes, @Cast("int64_t") long dim, @ByVal SymInt start, @ByVal SymInt end, @ByVal SymInt step);


// aten::slice_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt start, SymInt end, SymInt step, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long end, @Cast("int64_t") long step);
@Namespace("at") public static native @ByRef Tensor slice_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long end, @Cast("int64_t") long step);


// aten::slice_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt start, SymInt end, SymInt step, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long end, @Cast("int64_t") long step, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slice_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long end, @Cast("int64_t") long step, @ByRef Tensor out);


// aten::slice_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt start, SymInt end, SymInt step, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_backward_symint_out(@ByRef Tensor out, @Const @ByRef Tensor grad_output, @ByVal SymIntRef input_sizes, @Cast("int64_t") long dim, @ByVal SymInt start, @ByVal SymInt end, @ByVal SymInt step);


// aten::slice_backward.out(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt start, SymInt end, SymInt step, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntRef input_sizes, @Cast("int64_t") long dim, @ByVal SymInt start, @ByVal SymInt end, @ByVal SymInt step, @ByRef Tensor out);





// Parsed from ATen/ops/slice_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/slice_copy_ops.h>


// aten::slice_copy.Tensor(Tensor self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slice_copy(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=0*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional start, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional end, @Cast("int64_t") long step/*=1*/);
@Namespace("at") public static native @ByVal Tensor slice_copy(@Const @ByRef Tensor self);


// aten::slice_copy.Tensor(Tensor self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slice_copy_symint(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=0*/, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional start, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional end, @ByVal(nullValue = "c10::SymInt(1)") SymInt step);
@Namespace("at") public static native @ByVal Tensor slice_copy_symint(@Const @ByRef Tensor self);


// aten::slice_copy.Tensor_out(Tensor self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim/*=0*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional start, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional end, @Cast("int64_t") long step/*=1*/);
@Namespace("at") public static native @ByRef Tensor slice_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);


// aten::slice_copy.Tensor_out(Tensor self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal LongOptional start, @ByVal LongOptional end, @Cast("int64_t") long step, @ByRef Tensor out);


// aten::slice_copy.Tensor_out(Tensor self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_copy_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim/*=0*/, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional start, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional end, @ByVal(nullValue = "c10::SymInt(1)") SymInt step);
@Namespace("at") public static native @ByRef Tensor slice_copy_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self);


// aten::slice_copy.Tensor_out(Tensor self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_copy_symint_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal SymIntOptional start, @ByVal SymIntOptional end, @ByVal SymInt step, @ByRef Tensor out);





// Parsed from ATen/ops/slice_scatter.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/slice_scatter_ops.h>


// aten::slice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slice_scatter(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim/*=0*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional start, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional end, @Cast("int64_t") long step/*=1*/);
@Namespace("at") public static native @ByVal Tensor slice_scatter(@Const @ByRef Tensor self, @Const @ByRef Tensor src);


// aten::slice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slice_scatter_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim/*=0*/, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional start, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional end, @ByVal(nullValue = "c10::SymInt(1)") SymInt step);
@Namespace("at") public static native @ByVal Tensor slice_scatter_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor src);


// aten::slice_scatter.out(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim/*=0*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional start, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional end, @Cast("int64_t") long step/*=1*/);
@Namespace("at") public static native @ByRef Tensor slice_scatter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src);


// aten::slice_scatter.out(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_scatter_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim, @ByVal LongOptional start, @ByVal LongOptional end, @Cast("int64_t") long step, @ByRef Tensor out);


// aten::slice_scatter.out(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_scatter_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim/*=0*/, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional start, @ByVal(nullValue = "c10::optional<c10::SymInt>(c10::nullopt)") SymIntOptional end, @ByVal(nullValue = "c10::SymInt(1)") SymInt step);
@Namespace("at") public static native @ByRef Tensor slice_scatter_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor src);


// aten::slice_scatter.out(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slice_scatter_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor src, @Cast("int64_t") long dim, @ByVal SymIntOptional start, @ByVal SymIntOptional end, @ByVal SymInt step, @ByRef Tensor out);





// Parsed from ATen/ops/slogdet.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/slogdet_ops.h>


// aten::slogdet(Tensor self) -> (Tensor sign, Tensor logabsdet)
@Namespace("at") public static native @ByVal TensorTensorTuple slogdet(@Const @ByRef Tensor self);

// aten::slogdet.out(Tensor self, *, Tensor(a!) sign, Tensor(b!) logabsdet) -> (Tensor(a!) sign, Tensor(b!) logabsdet)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slogdet_out(@ByRef Tensor sign, @ByRef Tensor logabsdet, @Const @ByRef Tensor self);
// aten::slogdet.out(Tensor self, *, Tensor(a!) sign, Tensor(b!) logabsdet) -> (Tensor(a!) sign, Tensor(b!) logabsdet)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slogdet_outf(@Const @ByRef Tensor self, @ByRef Tensor sign, @ByRef Tensor logabsdet);




// Parsed from ATen/ops/slow_conv3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/slow_conv3d_ops.h>


// aten::slow_conv3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);


// aten::slow_conv3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);


// aten::slow_conv3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef padding);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef padding);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);


// aten::slow_conv3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv3d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding, @ByRef Tensor out);


// aten::slow_conv3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor slow_conv3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByVal Tensor slow_conv3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);


// aten::slow_conv3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef padding);
@Namespace("at") public static native @ByVal Tensor slow_conv3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef padding);
@Namespace("at") public static native @ByVal Tensor slow_conv3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);





// Parsed from ATen/ops/slow_conv3d_forward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/slow_conv3d_forward_ops.h>


// aten::slow_conv3d_forward.output(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding, *, Tensor(a!) output) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv3d_forward_out(@ByRef Tensor output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_forward_out(@ByRef Tensor output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::slow_conv3d_forward.output(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding, *, Tensor(a!) output) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv3d_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor output);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor output);


// aten::slow_conv3d_forward.output(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding, *, Tensor(a!) output) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv3d_forward_symint_out(@ByRef Tensor output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_forward_symint_out(@ByRef Tensor output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding);


// aten::slow_conv3d_forward.output(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding, *, Tensor(a!) output) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv3d_forward_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding, @ByRef Tensor output);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_forward_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding, @ByRef Tensor output);


// aten::slow_conv3d_forward(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv3d_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor slow_conv3d_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);


// aten::slow_conv3d_forward(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv3d_forward_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding);
@Namespace("at") public static native @ByVal Tensor slow_conv3d_forward_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding);





// Parsed from ATen/ops/slow_conv_dilated2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/slow_conv_dilated2d_ops.h>


// aten::slow_conv_dilated2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, int[2] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);


// aten::slow_conv_dilated2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, int[2] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);


// aten::slow_conv_dilated2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);


// aten::slow_conv_dilated2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByRef Tensor out);


// aten::slow_conv_dilated2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);


// aten::slow_conv_dilated2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated2d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByRef Tensor out);





// Parsed from ATen/ops/slow_conv_dilated3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/slow_conv_dilated3d_ops.h>


// aten::slow_conv_dilated3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, int[3] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);


// aten::slow_conv_dilated3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, int[3] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);


// aten::slow_conv_dilated3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);


// aten::slow_conv_dilated3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByRef Tensor out);


// aten::slow_conv_dilated3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);


// aten::slow_conv_dilated3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv_dilated3d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByRef Tensor out);





// Parsed from ATen/ops/slow_conv_transpose2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/slow_conv_transpose2d_ops.h>


// aten::slow_conv_transpose2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);


// aten::slow_conv_transpose2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByRef Tensor out);


// aten::slow_conv_transpose2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef padding, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef padding, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);


// aten::slow_conv_transpose2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding, @ByVal SymIntRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding, @ByVal SymIntRef output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByRef Tensor out);


// aten::slow_conv_transpose2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, int[2] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);


// aten::slow_conv_transpose2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, int[2] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef padding, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef padding, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);





// Parsed from ATen/ops/slow_conv_transpose3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/slow_conv_transpose3d_ops.h>


// aten::slow_conv_transpose3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);


// aten::slow_conv_transpose3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByRef Tensor out);


// aten::slow_conv_transpose3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef padding, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef padding, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);


// aten::slow_conv_transpose3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal SymIntRef padding, @ByVal SymIntRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_symint_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal SymIntRef padding, @ByVal SymIntRef output_padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByRef Tensor out);


// aten::slow_conv_transpose3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, int[3] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);


// aten::slow_conv_transpose3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, int[3] dilation=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef padding, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef padding, @ByVal(nullValue = "c10::SymIntArrayRef(c10::SymInt(0))") SymIntRef output_padding, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d_symint(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);





// Parsed from ATen/ops/smm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/smm_ops.h>


// aten::smm(Tensor self, Tensor mat2) -> Tensor
@Namespace("at") public static native @ByVal Tensor smm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat2);




// Parsed from ATen/ops/smooth_l1_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/smooth_l1_loss_ops.h>


// aten::smooth_l1_loss.out(Tensor self, Tensor target, int reduction=Mean, float beta=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor smooth_l1_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, double beta/*=1.0*/);
@Namespace("at") public static native @ByRef Tensor smooth_l1_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
// aten::smooth_l1_loss.out(Tensor self, Tensor target, int reduction=Mean, float beta=1.0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor smooth_l1_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double beta, @ByRef Tensor out);

// aten::smooth_l1_loss(Tensor self, Tensor target, int reduction=Mean, float beta=1.0) -> Tensor
@Namespace("at") public static native @ByVal Tensor smooth_l1_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, double beta/*=1.0*/);
@Namespace("at") public static native @ByVal Tensor smooth_l1_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);




// Parsed from ATen/ops/smooth_l1_loss_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/smooth_l1_loss_backward_ops.h>


// aten::smooth_l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor smooth_l1_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double beta);
// aten::smooth_l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor smooth_l1_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double beta, @ByRef Tensor grad_input);

// aten::smooth_l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta) -> Tensor
@Namespace("at") public static native @ByVal Tensor smooth_l1_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double beta);




// Parsed from ATen/ops/soft_margin_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/soft_margin_loss_ops.h>


// aten::soft_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor soft_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor soft_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
// aten::soft_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor soft_margin_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor out);

// aten::soft_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor soft_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor soft_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);




// Parsed from ATen/ops/soft_margin_loss_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/soft_margin_loss_backward_ops.h>


// aten::soft_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor soft_margin_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);
// aten::soft_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor soft_margin_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor grad_input);

// aten::soft_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor
@Namespace("at") public static native @ByVal Tensor soft_margin_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);




// Parsed from ATen/ops/softmax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/softmax_ops.h>


// aten::softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor softmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softmax_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor softmax(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor softmax(@Const @ByRef Tensor self, @ByVal Dimname dim);




// Parsed from ATen/ops/softplus.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/softplus_ops.h>


// aten::softplus.out(Tensor self, Scalar beta=1, Scalar threshold=20, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softplus_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(20)") Scalar threshold);
@Namespace("at") public static native @ByRef Tensor softplus_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::softplus.out(Tensor self, Scalar beta=1, Scalar threshold=20, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softplus_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar beta, @Const @ByRef Scalar threshold, @ByRef Tensor out);

// aten::softplus(Tensor self, Scalar beta=1, Scalar threshold=20) -> Tensor
@Namespace("at") public static native @ByVal Tensor softplus(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(20)") Scalar threshold);
@Namespace("at") public static native @ByVal Tensor softplus(@Const @ByRef Tensor self);




// Parsed from ATen/ops/softplus_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/softplus_backward_ops.h>


// aten::softplus_backward.grad_input(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softplus_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar beta, @Const @ByRef Scalar threshold);
// aten::softplus_backward.grad_input(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softplus_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar beta, @Const @ByRef Scalar threshold, @ByRef Tensor grad_input);

// aten::softplus_backward(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold) -> Tensor
@Namespace("at") public static native @ByVal Tensor softplus_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar beta, @Const @ByRef Scalar threshold);




// Parsed from ATen/ops/softshrink.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/softshrink_ops.h>


// aten::softshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softshrink_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.5)") Scalar lambd);
@Namespace("at") public static native @ByRef Tensor softshrink_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::softshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softshrink_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar lambd, @ByRef Tensor out);

// aten::softshrink(Tensor self, Scalar lambd=0.5) -> Tensor
@Namespace("at") public static native @ByVal Tensor softshrink(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "at::Scalar(0.5)") Scalar lambd);
@Namespace("at") public static native @ByVal Tensor softshrink(@Const @ByRef Tensor self);




// Parsed from ATen/ops/softshrink_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/softshrink_backward_ops.h>


// aten::softshrink_backward.grad_input(Tensor grad_output, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softshrink_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar lambd);
// aten::softshrink_backward.grad_input(Tensor grad_output, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor softshrink_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar lambd, @ByRef Tensor grad_input);

// aten::softshrink_backward(Tensor grad_output, Tensor self, Scalar lambd) -> Tensor
@Namespace("at") public static native @ByVal Tensor softshrink_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar lambd);




// Parsed from ATen/ops/sort.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sort_ops.h>


// aten::sort.values(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self);
// aten::sort.values(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean descending, @ByRef Tensor values, @ByRef Tensor indices);

// aten::sort.values_stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal BoolOptional stable, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal BoolOptional stable);
// aten::sort.values_stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_outf(@Const @ByRef Tensor self, @ByVal BoolOptional stable, @Cast("int64_t") long dim, @Cast("bool") boolean descending, @ByRef Tensor values, @ByRef Tensor indices);

// aten::sort(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple sort(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple sort(@Const @ByRef Tensor self);

// aten::sort.stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple sort(@Const @ByRef Tensor self, @ByVal BoolOptional stable, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple sort(@Const @ByRef Tensor self, @ByVal BoolOptional stable);

// aten::sort.dimname_values(Tensor self, Dimname dim, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);
// aten::sort.dimname_values(Tensor self, Dimname dim, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean descending, @ByRef Tensor values, @ByRef Tensor indices);

// aten::sort.dimname_values_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal BoolOptional stable, @ByVal Dimname dim, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal BoolOptional stable, @ByVal Dimname dim);
// aten::sort.dimname_values_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_outf(@Const @ByRef Tensor self, @ByVal BoolOptional stable, @ByVal Dimname dim, @Cast("bool") boolean descending, @ByRef Tensor values, @ByRef Tensor indices);

// aten::sort.dimname(Tensor self, Dimname dim, bool descending=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple sort(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple sort(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::sort.dimname_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple sort(@Const @ByRef Tensor self, @ByVal BoolOptional stable, @ByVal Dimname dim, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple sort(@Const @ByRef Tensor self, @ByVal BoolOptional stable, @ByVal Dimname dim);




// Parsed from ATen/ops/sparse_bsc_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sparse_bsc_tensor_ops.h>


// aten::sparse_bsc_tensor.ccol_row_value_size(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_bsc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_bsc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal TensorOptions options);
// aten::sparse_bsc_tensor.ccol_row_value_size(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_bsc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor sparse_bsc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::sparse_bsc_tensor.ccol_row_value(Tensor ccol_indices, Tensor row_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_bsc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
// aten::sparse_bsc_tensor.ccol_row_value(Tensor ccol_indices, Tensor row_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_bsc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);




// Parsed from ATen/ops/sparse_bsr_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sparse_bsr_tensor_ops.h>


// aten::sparse_bsr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_bsr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_bsr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal TensorOptions options);
// aten::sparse_bsr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_bsr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor sparse_bsr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::sparse_bsr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_bsr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
// aten::sparse_bsr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_bsr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);




// Parsed from ATen/ops/sparse_compressed_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sparse_compressed_tensor_ops.h>


// aten::sparse_compressed_tensor.comp_plain_value_size(Tensor compressed_indices, Tensor plain_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_compressed_tensor(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_compressed_tensor(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal TensorOptions options);
// aten::sparse_compressed_tensor.comp_plain_value_size(Tensor compressed_indices, Tensor plain_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_compressed_tensor(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor sparse_compressed_tensor(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::sparse_compressed_tensor.comp_plain_value(Tensor compressed_indices, Tensor plain_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_compressed_tensor(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
// aten::sparse_compressed_tensor.comp_plain_value(Tensor compressed_indices, Tensor plain_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_compressed_tensor(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);




// Parsed from ATen/ops/sparse_coo_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sparse_coo_tensor_ops.h>


// aten::sparse_coo_tensor.size(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal TensorOptions options);
// aten::sparse_coo_tensor.size(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::sparse_coo_tensor.indices(Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values);
// aten::sparse_coo_tensor.indices(Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::sparse_coo_tensor.indices_size(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
// aten::sparse_coo_tensor.indices_size(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::sparse_coo_tensor.size_out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sparse_coo_tensor_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor sparse_coo_tensor_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
// aten::sparse_coo_tensor.size_out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sparse_coo_tensor_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor sparse_coo_tensor_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);




// Parsed from ATen/ops/sparse_csc_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sparse_csc_tensor_ops.h>


// aten::sparse_csc_tensor.ccol_row_value_size(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_csc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_csc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal TensorOptions options);
// aten::sparse_csc_tensor.ccol_row_value_size(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_csc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor sparse_csc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::sparse_csc_tensor.ccol_row_value(Tensor ccol_indices, Tensor row_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_csc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
// aten::sparse_csc_tensor.ccol_row_value(Tensor ccol_indices, Tensor row_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_csc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);




// Parsed from ATen/ops/sparse_csr_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sparse_csr_tensor_ops.h>


// aten::sparse_csr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal TensorOptions options);
// aten::sparse_csr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::sparse_csr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
// aten::sparse_csr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);




// Parsed from ATen/ops/sparse_dim.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sparse_dim_ops.h>






// Parsed from ATen/ops/sparse_mask.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sparse_mask_ops.h>


// aten::sparse_mask.out(Tensor self, Tensor mask, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sparse_mask_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mask);
// aten::sparse_mask.out(Tensor self, Tensor mask, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sparse_mask_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @ByRef Tensor out);




// Parsed from ATen/ops/sparse_resize.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sparse_resize_ops.h>


// aten::sparse_resize.out(Tensor self, int[] size, int sparse_dim, int dense_dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor sparse_resize_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim);
@Namespace("at") public static native @Const @ByRef Tensor sparse_resize_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim);
// aten::sparse_resize.out(Tensor self, int[] size, int sparse_dim, int dense_dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor sparse_resize_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @Const @ByRef Tensor out);
@Namespace("at") public static native @Const @ByRef Tensor sparse_resize_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @Const @ByRef Tensor out);

// aten::sparse_resize(Tensor self, int[] size, int sparse_dim, int dense_dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_resize(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim);
@Namespace("at") public static native @ByVal Tensor sparse_resize(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim);




// Parsed from ATen/ops/sparse_resize_and_clear.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sparse_resize_and_clear_ops.h>


// aten::sparse_resize_and_clear.out(Tensor self, int[] size, int sparse_dim, int dense_dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor sparse_resize_and_clear_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim);
@Namespace("at") public static native @Const @ByRef Tensor sparse_resize_and_clear_out(@Const @ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim);
// aten::sparse_resize_and_clear.out(Tensor self, int[] size, int sparse_dim, int dense_dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @Const @ByRef Tensor sparse_resize_and_clear_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @Const @ByRef Tensor out);
@Namespace("at") public static native @Const @ByRef Tensor sparse_resize_and_clear_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @Const @ByRef Tensor out);

// aten::sparse_resize_and_clear(Tensor self, int[] size, int sparse_dim, int dense_dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_resize_and_clear(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim);
@Namespace("at") public static native @ByVal Tensor sparse_resize_and_clear(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim);




// Parsed from ATen/ops/sparse_sampled_addmm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sparse_sampled_addmm_ops.h>


// aten::sparse_sampled_addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sparse_sampled_addmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor sparse_sampled_addmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);
// aten::sparse_sampled_addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sparse_sampled_addmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef Scalar beta, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::sparse_sampled_addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor sparse_sampled_addmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor sparse_sampled_addmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);




// Parsed from ATen/ops/special_airy_ai.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_airy_ai_ops.h>


// aten::special_airy_ai(Tensor x) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_airy_ai(@Const @ByRef Tensor x);

// aten::special_airy_ai.out(Tensor x, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_airy_ai_out(@ByRef Tensor out, @Const @ByRef Tensor x);
// aten::special_airy_ai.out(Tensor x, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_airy_ai_outf(@Const @ByRef Tensor x, @ByRef Tensor out);




// Parsed from ATen/ops/special_bessel_j0.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_bessel_j0_ops.h>


// aten::special_bessel_j0(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_bessel_j0(@Const @ByRef Tensor self);

// aten::special_bessel_j0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_bessel_j0_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_bessel_j0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_bessel_j0_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_bessel_j1.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_bessel_j1_ops.h>


// aten::special_bessel_j1(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_bessel_j1(@Const @ByRef Tensor self);

// aten::special_bessel_j1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_bessel_j1_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_bessel_j1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_bessel_j1_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_bessel_y0.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_bessel_y0_ops.h>


// aten::special_bessel_y0(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_bessel_y0(@Const @ByRef Tensor self);

// aten::special_bessel_y0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_bessel_y0_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_bessel_y0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_bessel_y0_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_bessel_y1.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_bessel_y1_ops.h>


// aten::special_bessel_y1(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_bessel_y1(@Const @ByRef Tensor self);

// aten::special_bessel_y1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_bessel_y1_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_bessel_y1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_bessel_y1_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_chebyshev_polynomial_t.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_chebyshev_polynomial_t_ops.h>


// aten::special_chebyshev_polynomial_t(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_t(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_chebyshev_polynomial_t.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_t(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_chebyshev_polynomial_t.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_t(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_chebyshev_polynomial_t.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_t_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_chebyshev_polynomial_t.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_t_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_chebyshev_polynomial_t.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_t_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_chebyshev_polynomial_t.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_t_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_chebyshev_polynomial_t.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_t_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_chebyshev_polynomial_t.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_t_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_chebyshev_polynomial_u.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_chebyshev_polynomial_u_ops.h>


// aten::special_chebyshev_polynomial_u(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_u(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_chebyshev_polynomial_u.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_u(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_chebyshev_polynomial_u.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_u(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_chebyshev_polynomial_u.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_u_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_chebyshev_polynomial_u.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_u_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_chebyshev_polynomial_u.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_u_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_chebyshev_polynomial_u.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_u_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_chebyshev_polynomial_u.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_u_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_chebyshev_polynomial_u.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_u_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_chebyshev_polynomial_v.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_chebyshev_polynomial_v_ops.h>


// aten::special_chebyshev_polynomial_v(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_v(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_chebyshev_polynomial_v.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_v(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_chebyshev_polynomial_v.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_v(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_chebyshev_polynomial_v.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_v_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_chebyshev_polynomial_v.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_v_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_chebyshev_polynomial_v.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_v_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_chebyshev_polynomial_v.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_v_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_chebyshev_polynomial_v.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_v_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_chebyshev_polynomial_v.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_v_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_chebyshev_polynomial_w.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_chebyshev_polynomial_w_ops.h>


// aten::special_chebyshev_polynomial_w(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_w(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_chebyshev_polynomial_w.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_w(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_chebyshev_polynomial_w.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_chebyshev_polynomial_w(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_chebyshev_polynomial_w.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_w_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_chebyshev_polynomial_w.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_w_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_chebyshev_polynomial_w.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_w_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_chebyshev_polynomial_w.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_w_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_chebyshev_polynomial_w.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_w_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_chebyshev_polynomial_w.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_chebyshev_polynomial_w_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_digamma.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_digamma_ops.h>


// aten::special_digamma(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_digamma(@Const @ByRef Tensor self);

// aten::special_digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_digamma_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_digamma_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_entr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_entr_ops.h>


// aten::special_entr(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_entr(@Const @ByRef Tensor self);

// aten::special_entr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_entr_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_entr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_entr_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_erf.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_erf_ops.h>


// aten::special_erf(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_erf(@Const @ByRef Tensor self);

// aten::special_erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erf_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erf_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_erfc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_erfc_ops.h>


// aten::special_erfc(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_erfc(@Const @ByRef Tensor self);

// aten::special_erfc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erfc_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_erfc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erfc_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_erfcx.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_erfcx_ops.h>


// aten::special_erfcx(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_erfcx(@Const @ByRef Tensor self);

// aten::special_erfcx.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erfcx_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_erfcx.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erfcx_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_erfinv.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_erfinv_ops.h>


// aten::special_erfinv(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_erfinv(@Const @ByRef Tensor self);

// aten::special_erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erfinv_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_erfinv_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_exp2.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_exp2_ops.h>


// aten::special_exp2(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_exp2(@Const @ByRef Tensor self);

// aten::special_exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_exp2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_exp2_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_expit.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_expit_ops.h>


// aten::special_expit(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_expit(@Const @ByRef Tensor self);

// aten::special_expit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_expit_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_expit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_expit_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_expm1.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_expm1_ops.h>


// aten::special_expm1(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_expm1(@Const @ByRef Tensor self);

// aten::special_expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_expm1_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_expm1_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_gammainc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_gammainc_ops.h>


// aten::special_gammainc.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_gammainc_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::special_gammainc.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_gammainc_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_gammainc(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_gammainc(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/special_gammaincc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_gammaincc_ops.h>


// aten::special_gammaincc.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_gammaincc_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::special_gammaincc.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_gammaincc_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_gammaincc(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_gammaincc(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/special_gammaln.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_gammaln_ops.h>


// aten::special_gammaln(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_gammaln(@Const @ByRef Tensor self);

// aten::special_gammaln.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_gammaln_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_gammaln.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_gammaln_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_hermite_polynomial_h.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_hermite_polynomial_h_ops.h>


// aten::special_hermite_polynomial_h(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_hermite_polynomial_h(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_hermite_polynomial_h.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_hermite_polynomial_h(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_hermite_polynomial_h.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_hermite_polynomial_h(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_hermite_polynomial_h.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_h_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_hermite_polynomial_h.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_h_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_hermite_polynomial_h.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_h_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_hermite_polynomial_h.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_h_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_hermite_polynomial_h.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_h_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_hermite_polynomial_h.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_h_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_hermite_polynomial_he.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_hermite_polynomial_he_ops.h>


// aten::special_hermite_polynomial_he(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_hermite_polynomial_he(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_hermite_polynomial_he.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_hermite_polynomial_he(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_hermite_polynomial_he.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_hermite_polynomial_he(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_hermite_polynomial_he.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_he_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_hermite_polynomial_he.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_he_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_hermite_polynomial_he.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_he_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_hermite_polynomial_he.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_he_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_hermite_polynomial_he.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_he_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_hermite_polynomial_he.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_hermite_polynomial_he_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_i0.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_i0_ops.h>


// aten::special_i0(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_i0(@Const @ByRef Tensor self);

// aten::special_i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i0_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i0_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_i0e.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_i0e_ops.h>


// aten::special_i0e(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_i0e(@Const @ByRef Tensor self);

// aten::special_i0e.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i0e_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_i0e.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i0e_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_i1.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_i1_ops.h>


// aten::special_i1(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_i1(@Const @ByRef Tensor self);

// aten::special_i1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i1_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_i1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i1_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_i1e.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_i1e_ops.h>


// aten::special_i1e(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_i1e(@Const @ByRef Tensor self);

// aten::special_i1e.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i1e_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_i1e.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_i1e_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_laguerre_polynomial_l.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_laguerre_polynomial_l_ops.h>


// aten::special_laguerre_polynomial_l(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_laguerre_polynomial_l(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_laguerre_polynomial_l.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_laguerre_polynomial_l(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_laguerre_polynomial_l.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_laguerre_polynomial_l(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_laguerre_polynomial_l.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_laguerre_polynomial_l_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_laguerre_polynomial_l.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_laguerre_polynomial_l_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_laguerre_polynomial_l.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_laguerre_polynomial_l_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_laguerre_polynomial_l.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_laguerre_polynomial_l_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_laguerre_polynomial_l.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_laguerre_polynomial_l_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_laguerre_polynomial_l.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_laguerre_polynomial_l_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_legendre_polynomial_p.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_legendre_polynomial_p_ops.h>


// aten::special_legendre_polynomial_p(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_legendre_polynomial_p(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_legendre_polynomial_p.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_legendre_polynomial_p(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_legendre_polynomial_p.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_legendre_polynomial_p(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_legendre_polynomial_p.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_legendre_polynomial_p_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_legendre_polynomial_p.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_legendre_polynomial_p_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_legendre_polynomial_p.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_legendre_polynomial_p_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_legendre_polynomial_p.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_legendre_polynomial_p_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_legendre_polynomial_p.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_legendre_polynomial_p_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_legendre_polynomial_p.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_legendre_polynomial_p_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_log1p.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_log1p_ops.h>


// aten::special_log1p(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_log1p(@Const @ByRef Tensor self);

// aten::special_log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_log1p_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_log1p_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_log_ndtr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_log_ndtr_ops.h>


// aten::special_log_ndtr(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_log_ndtr(@Const @ByRef Tensor self);

// aten::special_log_ndtr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_log_ndtr_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_log_ndtr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_log_ndtr_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_log_softmax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_log_softmax_ops.h>


// aten::special_log_softmax(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor special_log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);




// Parsed from ATen/ops/special_logit.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_logit_ops.h>


// aten::special_logit(Tensor self, float? eps=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_logit(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByVal Tensor special_logit(@Const @ByRef Tensor self);

// aten::special_logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_logit_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByRef Tensor special_logit_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_logit_outf(@Const @ByRef Tensor self, @ByVal DoubleOptional eps, @ByRef Tensor out);




// Parsed from ATen/ops/special_logsumexp.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_logsumexp_ops.h>


// aten::special_logsumexp(Tensor self, int[1] dim, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_logsumexp(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor special_logsumexp(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor special_logsumexp(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor special_logsumexp(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::special_logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor special_logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor special_logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor special_logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);
// aten::special_logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_logsumexp_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor special_logsumexp_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);




// Parsed from ATen/ops/special_modified_bessel_i0.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_modified_bessel_i0_ops.h>


// aten::special_modified_bessel_i0(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_modified_bessel_i0(@Const @ByRef Tensor self);

// aten::special_modified_bessel_i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_modified_bessel_i0_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_modified_bessel_i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_modified_bessel_i0_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_modified_bessel_i1.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_modified_bessel_i1_ops.h>


// aten::special_modified_bessel_i1(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_modified_bessel_i1(@Const @ByRef Tensor self);

// aten::special_modified_bessel_i1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_modified_bessel_i1_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_modified_bessel_i1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_modified_bessel_i1_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_modified_bessel_k0.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_modified_bessel_k0_ops.h>


// aten::special_modified_bessel_k0(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_modified_bessel_k0(@Const @ByRef Tensor self);

// aten::special_modified_bessel_k0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_modified_bessel_k0_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_modified_bessel_k0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_modified_bessel_k0_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_modified_bessel_k1.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_modified_bessel_k1_ops.h>


// aten::special_modified_bessel_k1(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_modified_bessel_k1(@Const @ByRef Tensor self);

// aten::special_modified_bessel_k1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_modified_bessel_k1_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_modified_bessel_k1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_modified_bessel_k1_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_multigammaln.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_multigammaln_ops.h>


// aten::special_multigammaln(Tensor self, int p) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_multigammaln(@Const @ByRef Tensor self, @Cast("int64_t") long p);

// aten::special_multigammaln.out(Tensor self, int p, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_multigammaln_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long p);
// aten::special_multigammaln.out(Tensor self, int p, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_multigammaln_outf(@Const @ByRef Tensor self, @Cast("int64_t") long p, @ByRef Tensor out);




// Parsed from ATen/ops/special_ndtr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_ndtr_ops.h>


// aten::special_ndtr(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_ndtr(@Const @ByRef Tensor self);

// aten::special_ndtr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_ndtr_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_ndtr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_ndtr_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_ndtri.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_ndtri_ops.h>


// aten::special_ndtri(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_ndtri(@Const @ByRef Tensor self);

// aten::special_ndtri.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_ndtri_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_ndtri.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_ndtri_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_polygamma.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_polygamma_ops.h>


// aten::special_polygamma(int n, Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_polygamma(@Cast("int64_t") long n, @Const @ByRef Tensor self);

// aten::special_polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_polygamma_out(@ByRef Tensor out, @Cast("int64_t") long n, @Const @ByRef Tensor self);
// aten::special_polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_polygamma_outf(@Cast("int64_t") long n, @Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_psi.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_psi_ops.h>


// aten::special_psi(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_psi(@Const @ByRef Tensor self);

// aten::special_psi.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_psi_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_psi.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_psi_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_round.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_round_ops.h>


// aten::special_round(Tensor self, *, int decimals=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_round(@Const @ByRef Tensor self, @Cast("int64_t") long decimals/*=0*/);
@Namespace("at") public static native @ByVal Tensor special_round(@Const @ByRef Tensor self);

// aten::special_round.out(Tensor self, *, int decimals=0, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_round_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long decimals/*=0*/);
@Namespace("at") public static native @ByRef Tensor special_round_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_round.out(Tensor self, *, int decimals=0, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_round_outf(@Const @ByRef Tensor self, @Cast("int64_t") long decimals, @ByRef Tensor out);




// Parsed from ATen/ops/special_scaled_modified_bessel_k0.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_scaled_modified_bessel_k0_ops.h>


// aten::special_scaled_modified_bessel_k0(Tensor x) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_scaled_modified_bessel_k0(@Const @ByRef Tensor x);

// aten::special_scaled_modified_bessel_k0.out(Tensor x, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_scaled_modified_bessel_k0_out(@ByRef Tensor out, @Const @ByRef Tensor x);
// aten::special_scaled_modified_bessel_k0.out(Tensor x, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_scaled_modified_bessel_k0_outf(@Const @ByRef Tensor x, @ByRef Tensor out);




// Parsed from ATen/ops/special_scaled_modified_bessel_k1.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_scaled_modified_bessel_k1_ops.h>


// aten::special_scaled_modified_bessel_k1(Tensor x) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_scaled_modified_bessel_k1(@Const @ByRef Tensor x);

// aten::special_scaled_modified_bessel_k1.out(Tensor x, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_scaled_modified_bessel_k1_out(@ByRef Tensor out, @Const @ByRef Tensor x);
// aten::special_scaled_modified_bessel_k1.out(Tensor x, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_scaled_modified_bessel_k1_outf(@Const @ByRef Tensor x, @ByRef Tensor out);




// Parsed from ATen/ops/special_shifted_chebyshev_polynomial_t.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_shifted_chebyshev_polynomial_t_ops.h>


// aten::special_shifted_chebyshev_polynomial_t(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_t(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_shifted_chebyshev_polynomial_t.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_t(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_shifted_chebyshev_polynomial_t.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_t(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_shifted_chebyshev_polynomial_t.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_t_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_shifted_chebyshev_polynomial_t.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_t_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_shifted_chebyshev_polynomial_t.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_t_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_shifted_chebyshev_polynomial_t.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_t_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_shifted_chebyshev_polynomial_t.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_t_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_shifted_chebyshev_polynomial_t.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_t_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_shifted_chebyshev_polynomial_u.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_shifted_chebyshev_polynomial_u_ops.h>


// aten::special_shifted_chebyshev_polynomial_u(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_u(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_shifted_chebyshev_polynomial_u.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_u(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_shifted_chebyshev_polynomial_u.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_u(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_shifted_chebyshev_polynomial_u.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_u_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_shifted_chebyshev_polynomial_u.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_u_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_shifted_chebyshev_polynomial_u.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_u_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_shifted_chebyshev_polynomial_u.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_u_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_shifted_chebyshev_polynomial_u.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_u_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_shifted_chebyshev_polynomial_u.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_u_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_shifted_chebyshev_polynomial_v.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_shifted_chebyshev_polynomial_v_ops.h>


// aten::special_shifted_chebyshev_polynomial_v(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_v(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_shifted_chebyshev_polynomial_v.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_v(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_shifted_chebyshev_polynomial_v.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_v(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_shifted_chebyshev_polynomial_v.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_v_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_shifted_chebyshev_polynomial_v.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_v_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_shifted_chebyshev_polynomial_v.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_v_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_shifted_chebyshev_polynomial_v.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_v_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_shifted_chebyshev_polynomial_v.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_v_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_shifted_chebyshev_polynomial_v.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_v_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_shifted_chebyshev_polynomial_w.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_shifted_chebyshev_polynomial_w_ops.h>


// aten::special_shifted_chebyshev_polynomial_w(Tensor x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_w(@Const @ByRef Tensor x, @Const @ByRef Tensor n);

// aten::special_shifted_chebyshev_polynomial_w.x_scalar(Scalar x, Tensor n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_w(@Const @ByRef Scalar x, @Const @ByRef Tensor n);

// aten::special_shifted_chebyshev_polynomial_w.n_scalar(Tensor x, Scalar n) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_shifted_chebyshev_polynomial_w(@Const @ByRef Tensor x, @Const @ByRef Scalar n);

// aten::special_shifted_chebyshev_polynomial_w.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_w_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Tensor n);
// aten::special_shifted_chebyshev_polynomial_w.out(Tensor x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_w_outf(@Const @ByRef Tensor x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_shifted_chebyshev_polynomial_w.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_w_out(@ByRef Tensor out, @Const @ByRef Scalar x, @Const @ByRef Tensor n);
// aten::special_shifted_chebyshev_polynomial_w.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_w_outf(@Const @ByRef Scalar x, @Const @ByRef Tensor n, @ByRef Tensor out);

// aten::special_shifted_chebyshev_polynomial_w.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_w_out(@ByRef Tensor out, @Const @ByRef Tensor x, @Const @ByRef Scalar n);
// aten::special_shifted_chebyshev_polynomial_w.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_shifted_chebyshev_polynomial_w_outf(@Const @ByRef Tensor x, @Const @ByRef Scalar n, @ByRef Tensor out);




// Parsed from ATen/ops/special_sinc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_sinc_ops.h>


// aten::special_sinc(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_sinc(@Const @ByRef Tensor self);

// aten::special_sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_sinc_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::special_sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_sinc_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/special_softmax.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_softmax_ops.h>


// aten::special_softmax(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor special_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);




// Parsed from ATen/ops/special_spherical_bessel_j0.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_spherical_bessel_j0_ops.h>


// aten::special_spherical_bessel_j0(Tensor x) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_spherical_bessel_j0(@Const @ByRef Tensor x);

// aten::special_spherical_bessel_j0.out(Tensor x, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_spherical_bessel_j0_out(@ByRef Tensor out, @Const @ByRef Tensor x);
// aten::special_spherical_bessel_j0.out(Tensor x, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_spherical_bessel_j0_outf(@Const @ByRef Tensor x, @ByRef Tensor out);




// Parsed from ATen/ops/special_xlog1py.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_xlog1py_ops.h>


// aten::special_xlog1py(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_xlog1py(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::special_xlog1py.self_scalar(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_xlog1py(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::special_xlog1py.other_scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_xlog1py(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::special_xlog1py.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlog1py_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::special_xlog1py.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlog1py_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_xlog1py.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlog1py_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);
// aten::special_xlog1py.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlog1py_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_xlog1py.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlog1py_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::special_xlog1py.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlog1py_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);




// Parsed from ATen/ops/special_xlogy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_xlogy_ops.h>


// aten::special_xlogy(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_xlogy(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::special_xlogy.self_scalar(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_xlogy(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::special_xlogy.other_scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_xlogy(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::special_xlogy.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlogy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::special_xlogy.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlogy_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_xlogy.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlogy_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);
// aten::special_xlogy.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlogy_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_xlogy.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlogy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::special_xlogy.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_xlogy_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);




// Parsed from ATen/ops/special_zeta.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/special_zeta_ops.h>


// aten::special_zeta(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_zeta(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::special_zeta.self_scalar(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_zeta(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::special_zeta.other_scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor special_zeta(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::special_zeta.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_zeta_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::special_zeta.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_zeta_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_zeta.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_zeta_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);
// aten::special_zeta.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_zeta_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::special_zeta.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_zeta_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::special_zeta.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor special_zeta_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);




// Parsed from ATen/ops/split.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/split_ops.h>


// aten::split.Tensor(Tensor(a -> *) self, SymInt split_size, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split(@Const @ByRef Tensor self, @Cast("int64_t") long split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split(@Const @ByRef Tensor self, @Cast("int64_t") long split_size);


// aten::split.Tensor(Tensor(a -> *) self, SymInt split_size, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_symint(@Const @ByRef Tensor self, @ByVal SymInt split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_symint(@Const @ByRef Tensor self, @ByVal SymInt split_size);


// aten::split.sizes(Tensor(a -> *) self, SymInt[] split_size, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef split_size);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... split_size);


// aten::split.sizes(Tensor(a -> *) self, SymInt[] split_size, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_symint(@Const @ByRef Tensor self, @ByVal SymIntRef split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_symint(@Const @ByRef Tensor self, @ByVal SymIntRef split_size);





// Parsed from ATen/ops/split_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/split_copy_ops.h>


// aten::split_copy.Tensor(Tensor self, SymInt split_size, int dim=0) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_copy(@Const @ByRef Tensor self, @Cast("int64_t") long split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_copy(@Const @ByRef Tensor self, @Cast("int64_t") long split_size);


// aten::split_copy.Tensor(Tensor self, SymInt split_size, int dim=0) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_copy_symint(@Const @ByRef Tensor self, @ByVal SymInt split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_copy_symint(@Const @ByRef Tensor self, @ByVal SymInt split_size);


// aten::split_copy.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void split_copy_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @Cast("int64_t") long split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void split_copy_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @Cast("int64_t") long split_size);


// aten::split_copy.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void split_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long split_size, @Cast("int64_t") long dim, @ByVal TensorArrayRef out);


// aten::split_copy.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void split_copy_symint_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal SymInt split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void split_copy_symint_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal SymInt split_size);


// aten::split_copy.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void split_copy_symint_outf(@Const @ByRef Tensor self, @ByVal SymInt split_size, @Cast("int64_t") long dim, @ByVal TensorArrayRef out);





// Parsed from ATen/ops/split_with_sizes.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/split_with_sizes_ops.h>


// aten::split_with_sizes(Tensor(a -> *) self, SymInt[] split_sizes, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef split_sizes);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... split_sizes);


// aten::split_with_sizes(Tensor(a -> *) self, SymInt[] split_sizes, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_with_sizes_symint(@Const @ByRef Tensor self, @ByVal SymIntRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_with_sizes_symint(@Const @ByRef Tensor self, @ByVal SymIntRef split_sizes);





// Parsed from ATen/ops/split_with_sizes_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/split_with_sizes_copy_ops.h>


// aten::split_with_sizes_copy(Tensor self, SymInt[] split_sizes, int dim=0) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_with_sizes_copy(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_with_sizes_copy(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef split_sizes);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_with_sizes_copy(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_with_sizes_copy(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... split_sizes);


// aten::split_with_sizes_copy(Tensor self, SymInt[] split_sizes, int dim=0) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_with_sizes_copy_symint(@Const @ByRef Tensor self, @ByVal SymIntRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector split_with_sizes_copy_symint(@Const @ByRef Tensor self, @ByVal SymIntRef split_sizes);


// aten::split_with_sizes_copy.out(Tensor self, SymInt[] split_sizes, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void split_with_sizes_copy_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void split_with_sizes_copy_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef split_sizes);
@Namespace("at") public static native void split_with_sizes_copy_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void split_with_sizes_copy_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... split_sizes);


// aten::split_with_sizes_copy.out(Tensor self, SymInt[] split_sizes, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void split_with_sizes_copy_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef split_sizes, @Cast("int64_t") long dim, @ByVal TensorArrayRef out);
@Namespace("at") public static native void split_with_sizes_copy_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] split_sizes, @Cast("int64_t") long dim, @ByVal TensorArrayRef out);


// aten::split_with_sizes_copy.out(Tensor self, SymInt[] split_sizes, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void split_with_sizes_copy_symint_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal SymIntRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void split_with_sizes_copy_symint_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal SymIntRef split_sizes);


// aten::split_with_sizes_copy.out(Tensor self, SymInt[] split_sizes, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void split_with_sizes_copy_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef split_sizes, @Cast("int64_t") long dim, @ByVal TensorArrayRef out);





// Parsed from ATen/ops/sqrt.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sqrt_ops.h>


// aten::sqrt(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor sqrt(@Const @ByRef Tensor self);

// aten::sqrt_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sqrt_(@ByRef Tensor self);

// aten::sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sqrt_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sqrt_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/square.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/square_ops.h>


// aten::square(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor square(@Const @ByRef Tensor self);

// aten::square_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor square_(@ByRef Tensor self);

// aten::square.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor square_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::square.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor square_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/squeeze.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/squeeze_ops.h>


// aten::squeeze(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor squeeze(@Const @ByRef Tensor self);

// aten::squeeze.dim(Tensor(a) self, int dim) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor squeeze(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::squeeze.dimname(Tensor(a) self, Dimname dim) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor squeeze(@Const @ByRef Tensor self, @ByVal Dimname dim);

// aten::squeeze.dims(Tensor(a) self, int[] dim) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor squeeze(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor squeeze(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);




// Parsed from ATen/ops/squeeze_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/squeeze_copy_ops.h>


// aten::squeeze_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor squeeze_copy(@Const @ByRef Tensor self);

// aten::squeeze_copy.dim(Tensor self, int dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor squeeze_copy(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::squeeze_copy.dims(Tensor self, int[] dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor squeeze_copy(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor squeeze_copy(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::squeeze_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor squeeze_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::squeeze_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor squeeze_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::squeeze_copy.dim_out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor squeeze_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::squeeze_copy.dim_out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor squeeze_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor out);

// aten::squeeze_copy.dims_out(Tensor self, int[] dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor squeeze_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor squeeze_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);
// aten::squeeze_copy.dims_out(Tensor self, int[] dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor squeeze_copy_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor squeeze_copy_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByRef Tensor out);




// Parsed from ATen/ops/sspaddmm.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sspaddmm_ops.h>


// aten::sspaddmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor sspaddmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor sspaddmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);

// aten::sspaddmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sspaddmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar beta, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor sspaddmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);
// aten::sspaddmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sspaddmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @Const @ByRef Scalar beta, @Const @ByRef Scalar alpha, @ByRef Tensor out);




// Parsed from ATen/ops/stack.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/stack_ops.h>


// aten::stack(Tensor[] tensors, int dim=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor stack(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor stack(@ByVal TensorArrayRef tensors);

// aten::stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor stack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByRef Tensor stack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
// aten::stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor stack_outf(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim, @ByRef Tensor out);




// Parsed from ATen/ops/std.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/std_ops.h>


// aten::std(Tensor self, bool unbiased=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @Cast("bool") boolean unbiased);

// aten::std.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased);

// aten::std.correction(Tensor self, int[1]? dim=None, *, int? correction=None, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);

// aten::std.out(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased);
// aten::std.out(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor std_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::std.correction_out(Tensor self, int[1]? dim=None, *, int? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
// aten::std.correction_out(Tensor self, int[1]? dim=None, *, int? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor std_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::std.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased);

// aten::std.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased);
// aten::std.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::std.correction_names(Tensor self, Dimname[1] dim, *, int? correction=None, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);

// aten::std.correction_names_out(Tensor self, Dimname[1] dim, *, int? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
// aten::std.correction_names_out(Tensor self, Dimname[1] dim, *, int? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor std_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out);




// Parsed from ATen/ops/std_mean.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/std_mean_ops.h>


// aten::std_mean(Tensor self, bool unbiased=True) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, @Cast("bool") boolean unbiased);

// aten::std_mean.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased);
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased);

// aten::std_mean.correction(Tensor self, int[1]? dim=None, *, int? correction=None, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);

// aten::std_mean.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased);

// aten::std_mean.correction_names(Tensor self, Dimname[1] dim, *, int? correction=None, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple std_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);

// aten::std_mean.correction_out(Tensor self, int[1]? dim=None, *, int? correction=None, bool keepdim=False, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> std_mean_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> std_mean_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> std_mean_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
// aten::std_mean.correction_out(Tensor self, int[1]? dim=None, *, int? correction=None, bool keepdim=False, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> std_mean_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out0, @ByRef Tensor out1);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> std_mean_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/stft.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/stft_ops.h>


// aten::stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor stft(@Const @ByRef Tensor self, @Cast("int64_t") long n_fft, @ByVal LongOptional hop_length, @ByVal LongOptional win_length, @Const @ByRef TensorOptional window, @Cast("bool") boolean normalized, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional onesided, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional return_complex);

// aten::stft.center(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, str pad_mode="reflect", bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor stft(@Const @ByRef Tensor self, @Cast("int64_t") long n_fft, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional hop_length, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional win_length, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional window, @Cast("bool") boolean center/*=true*/, @ByVal(nullValue = "c10::string_view(\"reflect\")") @Cast("c10::string_view*") Pointer pad_mode, @Cast("bool") boolean normalized/*=false*/, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional onesided, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional return_complex);




// Parsed from ATen/ops/stride.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/stride_ops.h>


// aten::stride.int(Tensor self, int dim) -> int
@Namespace("at") public static native @Cast("int64_t") long __dispatch_stride(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::stride.Dimname(Tensor self, Dimname dim) -> int
@Namespace("at") public static native @Cast("int64_t") long stride(@Const @ByRef Tensor self, @ByVal Dimname dim);




// Parsed from ATen/ops/sub.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sub_ops.h>


// aten::sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sub_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor sub_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sub_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor sub(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor sub(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor sub(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor sub(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::sub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sub_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor sub_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::sub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sub_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef Scalar alpha, @ByRef Tensor out);




// Parsed from ATen/ops/subtract.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/subtract_ops.h>


// aten::subtract.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor subtract_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor subtract_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::subtract.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor subtract_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef Scalar alpha, @ByRef Tensor out);

// aten::subtract.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor subtract(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);

// aten::subtract.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
@Namespace("at") public static native @ByVal Tensor subtract(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar alpha);




// Parsed from ATen/ops/sum.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sum_ops.h>


// aten::sum(Tensor self, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self);

// aten::sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);

// aten::sum.dim_DimnameList(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);

// aten::sum.IntList_out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim);
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dim);
// aten::sum.IntList_out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sum_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor sum_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::sum.DimnameList_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
// aten::sum.DimnameList_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sum_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);

// aten::sum.out(Tensor self, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::sum.out(Tensor self, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor sum_outf(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/sum_to_size.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/sum_to_size_ops.h>






// Parsed from ATen/ops/svd.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/svd_ops.h>


// aten::svd.U(Tensor self, bool some=True, bool compute_uv=True, *, Tensor(a!) U, Tensor(b!) S, Tensor(c!) V) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) V)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> svd_out(@ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor V, @Const @ByRef Tensor self, @Cast("bool") boolean some/*=true*/, @Cast("bool") boolean compute_uv/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> svd_out(@ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor V, @Const @ByRef Tensor self);
// aten::svd.U(Tensor self, bool some=True, bool compute_uv=True, *, Tensor(a!) U, Tensor(b!) S, Tensor(c!) V) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) V)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> svd_outf(@Const @ByRef Tensor self, @Cast("bool") boolean some, @Cast("bool") boolean compute_uv, @ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor V);

// aten::svd(Tensor self, bool some=True, bool compute_uv=True) -> (Tensor U, Tensor S, Tensor V)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple svd(@Const @ByRef Tensor self, @Cast("bool") boolean some/*=true*/, @Cast("bool") boolean compute_uv/*=true*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple svd(@Const @ByRef Tensor self);




// Parsed from ATen/ops/swapaxes.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/swapaxes_ops.h>


// aten::swapaxes(Tensor(a) self, int axis0, int axis1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor swapaxes(@Const @ByRef Tensor self, @Cast("int64_t") long axis0, @Cast("int64_t") long axis1);




// Parsed from ATen/ops/swapdims.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/swapdims_ops.h>


// aten::swapdims(Tensor(a) self, int dim0, int dim1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor swapdims(@Const @ByRef Tensor self, @Cast("int64_t") long dim0, @Cast("int64_t") long dim1);




// Parsed from ATen/ops/t.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/t_ops.h>


// aten::t(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor t(@Const @ByRef Tensor self);




// Parsed from ATen/ops/t_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/t_copy_ops.h>


// aten::t_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor t_copy(@Const @ByRef Tensor self);

// aten::t_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor t_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::t_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor t_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/take.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/take_ops.h>


// aten::take.out(Tensor self, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor take_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor index);
// aten::take.out(Tensor self, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor take_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor index, @ByRef Tensor out);

// aten::take(Tensor self, Tensor index) -> Tensor
@Namespace("at") public static native @ByVal Tensor take(@Const @ByRef Tensor self, @Const @ByRef Tensor index);




// Parsed from ATen/ops/take_along_dim.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/take_along_dim_ops.h>


// aten::take_along_dim.out(Tensor self, Tensor indices, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor take_along_dim_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByRef Tensor take_along_dim_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices);
// aten::take_along_dim.out(Tensor self, Tensor indices, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor take_along_dim_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal LongOptional dim, @ByRef Tensor out);

// aten::take_along_dim(Tensor self, Tensor indices, int? dim=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor take_along_dim(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByVal Tensor take_along_dim(@Const @ByRef Tensor self, @Const @ByRef Tensor indices);




// Parsed from ATen/ops/tan.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/tan_ops.h>


// aten::tan(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor tan(@Const @ByRef Tensor self);

// aten::tan_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tan_(@ByRef Tensor self);

// aten::tan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tan_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::tan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tan_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/tanh.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/tanh_ops.h>


// aten::tanh(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor tanh(@Const @ByRef Tensor self);

// aten::tanh_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tanh_(@ByRef Tensor self);

// aten::tanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tanh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::tanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tanh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/tanh_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/tanh_backward_ops.h>


// aten::tanh_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tanh_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output);
// aten::tanh_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tanh_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @ByRef Tensor grad_input);

// aten::tanh_backward(Tensor grad_output, Tensor output) -> Tensor
@Namespace("at") public static native @ByVal Tensor tanh_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output);




// Parsed from ATen/ops/tensor_split.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/tensor_split_ops.h>


// aten::tensor_split.sections(Tensor(a -> *) self, SymInt sections, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @Cast("int64_t") long sections, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @Cast("int64_t") long sections);


// aten::tensor_split.sections(Tensor(a -> *) self, SymInt sections, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensor_split_symint(@Const @ByRef Tensor self, @ByVal SymInt sections, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensor_split_symint(@Const @ByRef Tensor self, @ByVal SymInt sections);


// aten::tensor_split.indices(Tensor(a -> *) self, SymInt[] indices, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef indices, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef indices);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] indices, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... indices);


// aten::tensor_split.indices(Tensor(a -> *) self, SymInt[] indices, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensor_split_symint(@Const @ByRef Tensor self, @ByVal SymIntRef indices, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensor_split_symint(@Const @ByRef Tensor self, @ByVal SymIntRef indices);


// aten::tensor_split.tensor_indices_or_sections(Tensor(a -> *) self, Tensor tensor_indices_or_sections, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor_indices_or_sections, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor_indices_or_sections);




// Parsed from ATen/ops/tensordot.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/tensordot_ops.h>


// aten::tensordot(Tensor self, Tensor other, int[] dims_self, int[] dims_other) -> Tensor
@Namespace("at") public static native @ByVal Tensor tensordot(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims_self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims_other);
@Namespace("at") public static native @ByVal Tensor tensordot(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dims_self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims_other);

// aten::tensordot.out(Tensor self, Tensor other, int[] dims_self, int[] dims_other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tensordot_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims_self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims_other);
@Namespace("at") public static native @ByRef Tensor tensordot_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dims_self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims_other);
// aten::tensordot.out(Tensor self, Tensor other, int[] dims_self, int[] dims_other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tensordot_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims_self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims_other, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor tensordot_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dims_self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dims_other, @ByRef Tensor out);




// Parsed from ATen/ops/thnn_conv2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/thnn_conv2d_ops.h>


// aten::thnn_conv2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
// aten::thnn_conv2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);

// aten::thnn_conv2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor thnn_conv2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor thnn_conv2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor thnn_conv2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "at::IntArrayRef(1)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::IntArrayRef(0)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByVal Tensor thnn_conv2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... kernel_size);




// Parsed from ATen/ops/threshold.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/threshold_ops.h>


// aten::threshold(Tensor self, Scalar threshold, Scalar value) -> Tensor
@Namespace("at") public static native @ByVal Tensor threshold(@Const @ByRef Tensor self, @Const @ByRef Scalar threshold, @Const @ByRef Scalar value);

// aten::threshold_(Tensor(a!) self, Scalar threshold, Scalar value) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor threshold_(@ByRef Tensor self, @Const @ByRef Scalar threshold, @Const @ByRef Scalar value);

// aten::threshold.out(Tensor self, Scalar threshold, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor threshold_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar threshold, @Const @ByRef Scalar value);
// aten::threshold.out(Tensor self, Scalar threshold, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor threshold_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar threshold, @Const @ByRef Scalar value, @ByRef Tensor out);




// Parsed from ATen/ops/threshold_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/threshold_backward_ops.h>


// aten::threshold_backward.grad_input(Tensor grad_output, Tensor self, Scalar threshold, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor threshold_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar threshold);
// aten::threshold_backward.grad_input(Tensor grad_output, Tensor self, Scalar threshold, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor threshold_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar threshold, @ByRef Tensor grad_input);

// aten::threshold_backward(Tensor grad_output, Tensor self, Scalar threshold) -> Tensor
@Namespace("at") public static native @ByVal Tensor threshold_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Scalar threshold);




// Parsed from ATen/ops/tile.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/tile_ops.h>


// aten::tile(Tensor self, int[] dims) -> Tensor
@Namespace("at") public static native @ByVal Tensor tile(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("at") public static native @ByVal Tensor tile(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dims);




// Parsed from ATen/ops/to.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/to_ops.h>






// Parsed from ATen/ops/to_dense.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/to_dense_ops.h>






// Parsed from ATen/ops/to_dense_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/to_dense_backward_ops.h>


// aten::to_dense_backward(Tensor grad, Tensor input) -> Tensor
@Namespace("at") public static native @ByVal Tensor to_dense_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor input);




// Parsed from ATen/ops/to_mkldnn.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/to_mkldnn_ops.h>


// aten::to_mkldnn.out(Tensor self, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_mkldnn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor to_mkldnn_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::to_mkldnn.out(Tensor self, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_mkldnn_outf(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);




// Parsed from ATen/ops/to_mkldnn_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/to_mkldnn_backward_ops.h>


// aten::to_mkldnn_backward(Tensor grad, Tensor input) -> Tensor
@Namespace("at") public static native @ByVal Tensor to_mkldnn_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor input);




// Parsed from ATen/ops/to_padded_tensor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/to_padded_tensor_ops.h>



// aten::to_padded_tensor.out(Tensor self, float padding, SymInt[]? output_size=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_padded_tensor_out(@ByRef Tensor out, @Const @ByRef Tensor self, double padding, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional output_size);
@Namespace("at") public static native @ByRef Tensor to_padded_tensor_out(@ByRef Tensor out, @Const @ByRef Tensor self, double padding);
@Namespace("at") public static native @ByRef Tensor to_padded_tensor_out(@ByRef Tensor out, @Const @ByRef Tensor self, double padding, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);


// aten::to_padded_tensor.out(Tensor self, float padding, SymInt[]? output_size=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_padded_tensor_outf(@Const @ByRef Tensor self, double padding, @ByVal LongArrayRefOptional output_size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor to_padded_tensor_outf(@Const @ByRef Tensor self, double padding, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByRef Tensor out);


// aten::to_padded_tensor.out(Tensor self, float padding, SymInt[]? output_size=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_padded_tensor_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, double padding, @ByVal(nullValue = "at::OptionalSymIntArrayRef(c10::nullopt)") SymIntArrayRefOptional output_size);
@Namespace("at") public static native @ByRef Tensor to_padded_tensor_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, double padding);


// aten::to_padded_tensor.out(Tensor self, float padding, SymInt[]? output_size=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_padded_tensor_symint_outf(@Const @ByRef Tensor self, double padding, @ByVal SymIntArrayRefOptional output_size, @ByRef Tensor out);





// Parsed from ATen/ops/to_sparse.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/to_sparse_ops.h>


// aten::to_sparse.sparse_dim_out(Tensor self, int sparse_dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_sparse_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long sparse_dim);
// aten::to_sparse.sparse_dim_out(Tensor self, int sparse_dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_sparse_outf(@Const @ByRef Tensor self, @Cast("int64_t") long sparse_dim, @ByRef Tensor out);

// aten::to_sparse.out(Tensor self, *, Layout? layout=None, int[2]? blocksize=None, int? dense_dim=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_sparse_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Layout>(c10::nullopt)") LayoutOptional layout, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional blocksize, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dense_dim);
@Namespace("at") public static native @ByRef Tensor to_sparse_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor to_sparse_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Layout>(c10::nullopt)") LayoutOptional layout, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] blocksize, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dense_dim);
// aten::to_sparse.out(Tensor self, *, Layout? layout=None, int[2]? blocksize=None, int? dense_dim=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_sparse_outf(@Const @ByRef Tensor self, @ByVal LayoutOptional layout, @ByVal LongArrayRefOptional blocksize, @ByVal LongOptional dense_dim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor to_sparse_outf(@Const @ByRef Tensor self, @ByVal LayoutOptional layout, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] blocksize, @ByVal LongOptional dense_dim, @ByRef Tensor out);




// Parsed from ATen/ops/to_sparse_bsc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/to_sparse_bsc_ops.h>


// aten::to_sparse_bsc.out(Tensor self, int[2] blocksize, int? dense_dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_sparse_bsc_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef blocksize, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dense_dim);
@Namespace("at") public static native @ByRef Tensor to_sparse_bsc_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef blocksize);
@Namespace("at") public static native @ByRef Tensor to_sparse_bsc_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] blocksize, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dense_dim);
@Namespace("at") public static native @ByRef Tensor to_sparse_bsc_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... blocksize);
// aten::to_sparse_bsc.out(Tensor self, int[2] blocksize, int? dense_dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_sparse_bsc_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef blocksize, @ByVal LongOptional dense_dim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor to_sparse_bsc_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] blocksize, @ByVal LongOptional dense_dim, @ByRef Tensor out);




// Parsed from ATen/ops/to_sparse_bsr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/to_sparse_bsr_ops.h>


// aten::to_sparse_bsr.out(Tensor self, int[2] blocksize, int? dense_dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_sparse_bsr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef blocksize, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dense_dim);
@Namespace("at") public static native @ByRef Tensor to_sparse_bsr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef blocksize);
@Namespace("at") public static native @ByRef Tensor to_sparse_bsr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] blocksize, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dense_dim);
@Namespace("at") public static native @ByRef Tensor to_sparse_bsr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... blocksize);
// aten::to_sparse_bsr.out(Tensor self, int[2] blocksize, int? dense_dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_sparse_bsr_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef blocksize, @ByVal LongOptional dense_dim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor to_sparse_bsr_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] blocksize, @ByVal LongOptional dense_dim, @ByRef Tensor out);




// Parsed from ATen/ops/to_sparse_csc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/to_sparse_csc_ops.h>


// aten::to_sparse_csc.out(Tensor self, int? dense_dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_sparse_csc_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dense_dim);
@Namespace("at") public static native @ByRef Tensor to_sparse_csc_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::to_sparse_csc.out(Tensor self, int? dense_dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_sparse_csc_outf(@Const @ByRef Tensor self, @ByVal LongOptional dense_dim, @ByRef Tensor out);




// Parsed from ATen/ops/to_sparse_csr.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/to_sparse_csr_ops.h>


// aten::to_sparse_csr.out(Tensor self, int? dense_dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_sparse_csr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dense_dim);
@Namespace("at") public static native @ByRef Tensor to_sparse_csr_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::to_sparse_csr.out(Tensor self, int? dense_dim=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor to_sparse_csr_outf(@Const @ByRef Tensor self, @ByVal LongOptional dense_dim, @ByRef Tensor out);




// Parsed from ATen/ops/topk.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/topk_ops.h>


// aten::topk.values(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> topk_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean largest/*=true*/, @Cast("bool") boolean sorted/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> topk_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k);
// aten::topk.values(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> topk_outf(@Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim, @Cast("bool") boolean largest, @Cast("bool") boolean sorted, @ByRef Tensor values, @ByRef Tensor indices);

// aten::topk(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)
@Namespace("at") public static native @ByVal TensorTensorTuple topk(@Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean largest/*=true*/, @Cast("bool") boolean sorted/*=true*/);
@Namespace("at") public static native @ByVal TensorTensorTuple topk(@Const @ByRef Tensor self, @Cast("int64_t") long k);




// Parsed from ATen/ops/trace.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/trace_ops.h>


// aten::trace(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor trace(@Const @ByRef Tensor self);

// aten::trace.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor trace_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::trace.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor trace_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/trace_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/trace_backward_ops.h>


// aten::trace_backward(Tensor grad, SymInt[] sizes) -> Tensor
@Namespace("at") public static native @ByVal Tensor trace_backward(@Const @ByRef Tensor grad, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("at") public static native @ByVal Tensor trace_backward(@Const @ByRef Tensor grad, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sizes);


// aten::trace_backward(Tensor grad, SymInt[] sizes) -> Tensor
@Namespace("at") public static native @ByVal Tensor trace_backward_symint(@Const @ByRef Tensor grad, @ByVal SymIntRef sizes);





// Parsed from ATen/ops/transpose.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/transpose_ops.h>


// aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor transpose(@Const @ByRef Tensor self, @Cast("int64_t") long dim0, @Cast("int64_t") long dim1);

// aten::transpose.Dimname(Tensor(a) self, Dimname dim0, Dimname dim1) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor transpose(@Const @ByRef Tensor self, @ByVal Dimname dim0, @ByVal Dimname dim1);




// Parsed from ATen/ops/transpose_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/transpose_copy_ops.h>


// aten::transpose_copy.int(Tensor self, int dim0, int dim1) -> Tensor
@Namespace("at") public static native @ByVal Tensor transpose_copy(@Const @ByRef Tensor self, @Cast("int64_t") long dim0, @Cast("int64_t") long dim1);

// aten::transpose_copy.int_out(Tensor self, int dim0, int dim1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor transpose_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim0, @Cast("int64_t") long dim1);
// aten::transpose_copy.int_out(Tensor self, int dim0, int dim1, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor transpose_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim0, @Cast("int64_t") long dim1, @ByRef Tensor out);




// Parsed from ATen/ops/trapezoid.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/trapezoid_ops.h>


// aten::trapezoid.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor trapezoid(@Const @ByRef Tensor y, @Const @ByRef Tensor x, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor trapezoid(@Const @ByRef Tensor y, @Const @ByRef Tensor x);

// aten::trapezoid.dx(Tensor y, *, Scalar dx=1, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor trapezoid(@Const @ByRef Tensor y, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar dx, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor trapezoid(@Const @ByRef Tensor y);




// Parsed from ATen/ops/trapz.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/trapz_ops.h>


// aten::trapz.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor trapz(@Const @ByRef Tensor y, @Const @ByRef Tensor x, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor trapz(@Const @ByRef Tensor y, @Const @ByRef Tensor x);

// aten::trapz.dx(Tensor y, *, float dx=1, int dim=-1) -> Tensor
@Namespace("at") public static native @ByVal Tensor trapz(@Const @ByRef Tensor y, double dx/*=1*/, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor trapz(@Const @ByRef Tensor y);




// Parsed from ATen/ops/triangular_solve.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/triangular_solve_ops.h>


// aten::triangular_solve.X(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False, *, Tensor(a!) X, Tensor(b!) M) -> (Tensor(a!) solution, Tensor(b!) cloned_coefficient)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> triangular_solve_out(@ByRef Tensor X, @ByRef Tensor M, @Const @ByRef Tensor self, @Const @ByRef Tensor A, @Cast("bool") boolean upper/*=true*/, @Cast("bool") boolean transpose/*=false*/, @Cast("bool") boolean unitriangular/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> triangular_solve_out(@ByRef Tensor X, @ByRef Tensor M, @Const @ByRef Tensor self, @Const @ByRef Tensor A);
// aten::triangular_solve.X(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False, *, Tensor(a!) X, Tensor(b!) M) -> (Tensor(a!) solution, Tensor(b!) cloned_coefficient)
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> triangular_solve_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor A, @Cast("bool") boolean upper, @Cast("bool") boolean transpose, @Cast("bool") boolean unitriangular, @ByRef Tensor X, @ByRef Tensor M);

// aten::triangular_solve(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False) -> (Tensor solution, Tensor cloned_coefficient)
@Namespace("at") public static native @ByVal TensorTensorTuple triangular_solve(@Const @ByRef Tensor self, @Const @ByRef Tensor A, @Cast("bool") boolean upper/*=true*/, @Cast("bool") boolean transpose/*=false*/, @Cast("bool") boolean unitriangular/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple triangular_solve(@Const @ByRef Tensor self, @Const @ByRef Tensor A);




// Parsed from ATen/ops/tril.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/tril_ops.h>


// aten::tril.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tril_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByRef Tensor tril_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::tril.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tril_outf(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal, @ByRef Tensor out);

// aten::tril(Tensor self, int diagonal=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor tril(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByVal Tensor tril(@Const @ByRef Tensor self);




// Parsed from ATen/ops/tril_indices.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/tril_indices_ops.h>


// aten::tril_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor tril_indices(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset/*=0*/, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor tril_indices(@Cast("int64_t") long row, @Cast("int64_t") long col);
// aten::tril_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor tril_indices(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::tril_indices.out(int row, int col, int offset=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tril_indices_out(@ByRef Tensor out, @Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset/*=0*/);
@Namespace("at") public static native @ByRef Tensor tril_indices_out(@ByRef Tensor out, @Cast("int64_t") long row, @Cast("int64_t") long col);
// aten::tril_indices.out(int row, int col, int offset=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor tril_indices_outf(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset, @ByRef Tensor out);




// Parsed from ATen/ops/triplet_margin_loss.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/triplet_margin_loss_ops.h>


// aten::triplet_margin_loss(Tensor anchor, Tensor positive, Tensor negative, float margin=1.0, float p=2, float eps=1e-06, bool swap=False, int reduction=Mean) -> Tensor
@Namespace("at") public static native @ByVal Tensor triplet_margin_loss(@Const @ByRef Tensor anchor, @Const @ByRef Tensor positive, @Const @ByRef Tensor negative, double margin/*=1.0*/, double p/*=2*/, double eps/*=1e-06*/, @Cast("bool") boolean swap/*=false*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor triplet_margin_loss(@Const @ByRef Tensor anchor, @Const @ByRef Tensor positive, @Const @ByRef Tensor negative);




// Parsed from ATen/ops/triu.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/triu_ops.h>


// aten::triu.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor triu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByRef Tensor triu_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::triu.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor triu_outf(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal, @ByRef Tensor out);

// aten::triu(Tensor self, int diagonal=0) -> Tensor
@Namespace("at") public static native @ByVal Tensor triu(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByVal Tensor triu(@Const @ByRef Tensor self);




// Parsed from ATen/ops/triu_indices.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/triu_indices_ops.h>


// aten::triu_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor triu_indices(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset/*=0*/, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor triu_indices(@Cast("int64_t") long row, @Cast("int64_t") long col);
// aten::triu_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor triu_indices(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::triu_indices.out(int row, int col, int offset=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor triu_indices_out(@ByRef Tensor out, @Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset/*=0*/);
@Namespace("at") public static native @ByRef Tensor triu_indices_out(@ByRef Tensor out, @Cast("int64_t") long row, @Cast("int64_t") long col);
// aten::triu_indices.out(int row, int col, int offset=0, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor triu_indices_outf(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset, @ByRef Tensor out);




// Parsed from ATen/ops/true_divide.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/true_divide_ops.h>


// aten::true_divide.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor true_divide(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::true_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor true_divide_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::true_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor true_divide_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::true_divide.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor true_divide(@Const @ByRef Tensor self, @Const @ByRef Scalar other);




// Parsed from ATen/ops/trunc.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/trunc_ops.h>


// aten::trunc(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor trunc(@Const @ByRef Tensor self);

// aten::trunc_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor trunc_(@ByRef Tensor self);

// aten::trunc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor trunc_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::trunc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor trunc_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/type_as.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/type_as_ops.h>






// Parsed from ATen/ops/unbind.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unbind_ops.h>


// aten::unbind.int(Tensor(a -> *) self, int dim=0) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unbind(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unbind(@Const @ByRef Tensor self);

// aten::unbind.Dimname(Tensor(a -> *) self, Dimname dim) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unbind(@Const @ByRef Tensor self, @ByVal Dimname dim);




// Parsed from ATen/ops/unbind_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unbind_copy_ops.h>


// aten::unbind_copy.int(Tensor self, int dim=0) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unbind_copy(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unbind_copy(@Const @ByRef Tensor self);

// aten::unbind_copy.int_out(Tensor self, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void unbind_copy_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void unbind_copy_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self);
// aten::unbind_copy.int_out(Tensor self, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void unbind_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal TensorArrayRef out);




// Parsed from ATen/ops/unflatten.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unflatten_ops.h>


// aten::unflatten.int(Tensor(a) self, int dim, int[] sizes) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor unflatten(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("at") public static native @ByVal Tensor unflatten(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... sizes);

// aten::unflatten.Dimname(Tensor(a) self, Dimname dim, int[] sizes, Dimname[] names) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor unflatten(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes, @ByVal DimnameArrayRef names);
@Namespace("at") public static native @ByVal Tensor unflatten(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes, @ByVal DimnameArrayRef names);




// Parsed from ATen/ops/unflatten_dense_tensors.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unflatten_dense_tensors_ops.h>


// aten::unflatten_dense_tensors(Tensor flat, Tensor[] tensors) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unflatten_dense_tensors(@Const @ByRef Tensor flat, @ByVal TensorArrayRef tensors);




// Parsed from ATen/ops/unfold.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unfold_ops.h>






// Parsed from ATen/ops/unfold_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unfold_backward_ops.h>


// aten::unfold_backward(Tensor grad_in, SymInt[] input_sizes, int dim, int size, int step) -> Tensor
@Namespace("at") public static native @ByVal Tensor unfold_backward(@Const @ByRef Tensor grad_in, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long size, @Cast("int64_t") long step);
@Namespace("at") public static native @ByVal Tensor unfold_backward(@Const @ByRef Tensor grad_in, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long size, @Cast("int64_t") long step);


// aten::unfold_backward(Tensor grad_in, SymInt[] input_sizes, int dim, int size, int step) -> Tensor
@Namespace("at") public static native @ByVal Tensor unfold_backward_symint(@Const @ByRef Tensor grad_in, @ByVal SymIntRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long size, @Cast("int64_t") long step);


// aten::unfold_backward.out(Tensor grad_in, SymInt[] input_sizes, int dim, int size, int step, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor unfold_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_in, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long size, @Cast("int64_t") long step);
@Namespace("at") public static native @ByRef Tensor unfold_backward_out(@ByRef Tensor out, @Const @ByRef Tensor grad_in, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long size, @Cast("int64_t") long step);


// aten::unfold_backward.out(Tensor grad_in, SymInt[] input_sizes, int dim, int size, int step, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor unfold_backward_outf(@Const @ByRef Tensor grad_in, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long size, @Cast("int64_t") long step, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor unfold_backward_outf(@Const @ByRef Tensor grad_in, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long size, @Cast("int64_t") long step, @ByRef Tensor out);


// aten::unfold_backward.out(Tensor grad_in, SymInt[] input_sizes, int dim, int size, int step, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor unfold_backward_symint_out(@ByRef Tensor out, @Const @ByRef Tensor grad_in, @ByVal SymIntRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long size, @Cast("int64_t") long step);


// aten::unfold_backward.out(Tensor grad_in, SymInt[] input_sizes, int dim, int size, int step, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor unfold_backward_symint_outf(@Const @ByRef Tensor grad_in, @ByVal SymIntRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long size, @Cast("int64_t") long step, @ByRef Tensor out);





// Parsed from ATen/ops/unfold_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unfold_copy_ops.h>


// aten::unfold_copy(Tensor self, int dimension, int size, int step) -> Tensor
@Namespace("at") public static native @ByVal Tensor unfold_copy(@Const @ByRef Tensor self, @Cast("int64_t") long dimension, @Cast("int64_t") long size, @Cast("int64_t") long step);

// aten::unfold_copy.out(Tensor self, int dimension, int size, int step, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor unfold_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dimension, @Cast("int64_t") long size, @Cast("int64_t") long step);
// aten::unfold_copy.out(Tensor self, int dimension, int size, int step, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor unfold_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dimension, @Cast("int64_t") long size, @Cast("int64_t") long step, @ByRef Tensor out);




// Parsed from ATen/ops/uniform.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/uniform_ops.h>


// aten::uniform.out(Tensor self, float from=0, float to=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor uniform_out(@ByRef Tensor out, @Const @ByRef Tensor self, double from/*=0*/, double to/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor uniform_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::uniform.out(Tensor self, float from=0, float to=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor uniform_outf(@Const @ByRef Tensor self, double from, double to, @ByVal GeneratorOptional generator, @ByRef Tensor out);

// aten::uniform(Tensor self, float from=0, float to=1, *, Generator? generator=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor uniform(@Const @ByRef Tensor self, double from/*=0*/, double to/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor uniform(@Const @ByRef Tensor self);




// Parsed from ATen/ops/unique_consecutive.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unique_consecutive_ops.h>


// aten::unique_consecutive(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple unique_consecutive(@Const @ByRef Tensor self, @Cast("bool") boolean return_inverse/*=false*/, @Cast("bool") boolean return_counts/*=false*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple unique_consecutive(@Const @ByRef Tensor self);

// aten::unique_consecutive.out(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> unique_consecutive_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor self, @Cast("bool") boolean return_inverse/*=false*/, @Cast("bool") boolean return_counts/*=false*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> unique_consecutive_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor self);
// aten::unique_consecutive.out(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> unique_consecutive_outf(@Const @ByRef Tensor self, @Cast("bool") boolean return_inverse, @Cast("bool") boolean return_counts, @ByVal LongOptional dim, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/unique_dim.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unique_dim_ops.h>


// aten::unique_dim(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple unique_dim(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean sorted/*=true*/, @Cast("bool") boolean return_inverse/*=false*/, @Cast("bool") boolean return_counts/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple unique_dim(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::unique_dim.out(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> unique_dim_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean sorted/*=true*/, @Cast("bool") boolean return_inverse/*=false*/, @Cast("bool") boolean return_counts/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> unique_dim_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::unique_dim.out(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> unique_dim_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean sorted, @Cast("bool") boolean return_inverse, @Cast("bool") boolean return_counts, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/unique_dim_consecutive.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unique_dim_consecutive_ops.h>


// aten::unique_dim_consecutive(Tensor self, int dim, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTensorTuple unique_dim_consecutive(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean return_inverse/*=false*/, @Cast("bool") boolean return_counts/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTensorTuple unique_dim_consecutive(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::unique_dim_consecutive.out(Tensor self, int dim, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> unique_dim_consecutive_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean return_inverse/*=false*/, @Cast("bool") boolean return_counts/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> unique_dim_consecutive_out(@ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::unique_dim_consecutive.out(Tensor self, int dim, bool return_inverse=False, bool return_counts=False, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> unique_dim_consecutive_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean return_inverse, @Cast("bool") boolean return_counts, @ByRef Tensor out0, @ByRef Tensor out1, @ByRef Tensor out2);




// Parsed from ATen/ops/unsafe_chunk.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unsafe_chunk_ops.h>


// aten::unsafe_chunk(Tensor self, int chunks, int dim=0) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unsafe_chunk(@Const @ByRef Tensor self, @Cast("int64_t") long chunks, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unsafe_chunk(@Const @ByRef Tensor self, @Cast("int64_t") long chunks);




// Parsed from ATen/ops/unsafe_split.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unsafe_split_ops.h>


// aten::unsafe_split.Tensor(Tensor self, SymInt split_size, int dim=0) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unsafe_split(@Const @ByRef Tensor self, @Cast("int64_t") long split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unsafe_split(@Const @ByRef Tensor self, @Cast("int64_t") long split_size);


// aten::unsafe_split.Tensor(Tensor self, SymInt split_size, int dim=0) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unsafe_split_symint(@Const @ByRef Tensor self, @ByVal SymInt split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unsafe_split_symint(@Const @ByRef Tensor self, @ByVal SymInt split_size);


// aten::unsafe_split.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void unsafe_split_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @Cast("int64_t") long split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void unsafe_split_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @Cast("int64_t") long split_size);


// aten::unsafe_split.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void unsafe_split_outf(@Const @ByRef Tensor self, @Cast("int64_t") long split_size, @Cast("int64_t") long dim, @ByVal TensorArrayRef out);


// aten::unsafe_split.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void unsafe_split_symint_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal SymInt split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void unsafe_split_symint_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal SymInt split_size);


// aten::unsafe_split.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void unsafe_split_symint_outf(@Const @ByRef Tensor self, @ByVal SymInt split_size, @Cast("int64_t") long dim, @ByVal TensorArrayRef out);





// Parsed from ATen/ops/unsafe_split_with_sizes.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unsafe_split_with_sizes_ops.h>


// aten::unsafe_split_with_sizes(Tensor self, SymInt[] split_sizes, int dim=0) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unsafe_split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unsafe_split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef split_sizes);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unsafe_split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unsafe_split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... split_sizes);


// aten::unsafe_split_with_sizes(Tensor self, SymInt[] split_sizes, int dim=0) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unsafe_split_with_sizes_symint(@Const @ByRef Tensor self, @ByVal SymIntRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector unsafe_split_with_sizes_symint(@Const @ByRef Tensor self, @ByVal SymIntRef split_sizes);


// aten::unsafe_split_with_sizes.out(Tensor self, SymInt[] split_sizes, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void unsafe_split_with_sizes_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void unsafe_split_with_sizes_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef split_sizes);
@Namespace("at") public static native void unsafe_split_with_sizes_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void unsafe_split_with_sizes_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... split_sizes);


// aten::unsafe_split_with_sizes.out(Tensor self, SymInt[] split_sizes, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void unsafe_split_with_sizes_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef split_sizes, @Cast("int64_t") long dim, @ByVal TensorArrayRef out);
@Namespace("at") public static native void unsafe_split_with_sizes_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] split_sizes, @Cast("int64_t") long dim, @ByVal TensorArrayRef out);


// aten::unsafe_split_with_sizes.out(Tensor self, SymInt[] split_sizes, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void unsafe_split_with_sizes_symint_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal SymIntRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native void unsafe_split_with_sizes_symint_out(@ByVal TensorArrayRef out, @Const @ByRef Tensor self, @ByVal SymIntRef split_sizes);


// aten::unsafe_split_with_sizes.out(Tensor self, SymInt[] split_sizes, int dim=0, *, Tensor(a!)[] out) -> ()
@Namespace("at") public static native void unsafe_split_with_sizes_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef split_sizes, @Cast("int64_t") long dim, @ByVal TensorArrayRef out);





// Parsed from ATen/ops/unsqueeze.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unsqueeze_ops.h>


// aten::unsqueeze(Tensor(a) self, int dim) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor unsqueeze(@Const @ByRef Tensor self, @Cast("int64_t") long dim);




// Parsed from ATen/ops/unsqueeze_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/unsqueeze_copy_ops.h>


// aten::unsqueeze_copy(Tensor self, int dim) -> Tensor
@Namespace("at") public static native @ByVal Tensor unsqueeze_copy(@Const @ByRef Tensor self, @Cast("int64_t") long dim);

// aten::unsqueeze_copy.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor unsqueeze_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
// aten::unsqueeze_copy.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor unsqueeze_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor out);




// Parsed from ATen/ops/upsample_bicubic2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_bicubic2d_ops.h>


// aten::upsample_bicubic2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);


// aten::upsample_bicubic2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);


// aten::upsample_bicubic2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);


// aten::upsample_bicubic2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::upsample_bicubic2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners);


// aten::upsample_bicubic2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::upsample_bicubic2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);


// aten::upsample_bicubic2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners);





// Parsed from ATen/ops/upsample_bicubic2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_bicubic2d_backward_ops.h>


// aten::upsample_bicubic2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);


// aten::upsample_bicubic2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::upsample_bicubic2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners);


// aten::upsample_bicubic2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::upsample_bicubic2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);


// aten::upsample_bicubic2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners);





// Parsed from ATen/ops/upsample_bilinear2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_bilinear2d_ops.h>


// aten::upsample_bilinear2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);


// aten::upsample_bilinear2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);


// aten::upsample_bilinear2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);


// aten::upsample_bilinear2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::upsample_bilinear2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners);


// aten::upsample_bilinear2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::upsample_bilinear2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);


// aten::upsample_bilinear2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners);





// Parsed from ATen/ops/upsample_bilinear2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_bilinear2d_backward_ops.h>


// aten::upsample_bilinear2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);


// aten::upsample_bilinear2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::upsample_bilinear2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners);


// aten::upsample_bilinear2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::upsample_bilinear2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);


// aten::upsample_bilinear2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners);





// Parsed from ATen/ops/upsample_linear1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_linear1d_ops.h>


// aten::upsample_linear1d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_linear1d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);


// aten::upsample_linear1d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);


// aten::upsample_linear1d.out(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);


// aten::upsample_linear1d.out(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales, @ByRef Tensor out);


// aten::upsample_linear1d.out(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners);


// aten::upsample_linear1d.out(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales, @ByRef Tensor out);


// aten::upsample_linear1d(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_linear1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);


// aten::upsample_linear1d(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners);





// Parsed from ATen/ops/upsample_linear1d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_linear1d_backward_ops.h>


// aten::upsample_linear1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, bool align_corners, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);


// aten::upsample_linear1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, bool align_corners, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales, @ByRef Tensor grad_input);


// aten::upsample_linear1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, bool align_corners, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners);


// aten::upsample_linear1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, bool align_corners, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales, @ByRef Tensor grad_input);


// aten::upsample_linear1d_backward(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, bool align_corners, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);


// aten::upsample_linear1d_backward(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, bool align_corners, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners);





// Parsed from ATen/ops/upsample_nearest1d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_nearest1d_ops.h>


// aten::upsample_nearest1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal DoubleArrayRefOptional scale_factors);


// aten::upsample_nearest1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);


// aten::upsample_nearest1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);


// aten::upsample_nearest1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal DoubleOptional scales, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal DoubleOptional scales, @ByRef Tensor out);


// aten::upsample_nearest1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size);


// aten::upsample_nearest1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal DoubleOptional scales, @ByRef Tensor out);


// aten::upsample_nearest1d(Tensor self, SymInt[1] output_size, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);


// aten::upsample_nearest1d(Tensor self, SymInt[1] output_size, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size);





// Parsed from ATen/ops/upsample_nearest1d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_nearest1d_backward_ops.h>


// aten::upsample_nearest1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... input_size);


// aten::upsample_nearest1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal DoubleOptional scales, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal DoubleOptional scales, @ByRef Tensor grad_input);


// aten::upsample_nearest1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size);


// aten::upsample_nearest1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @ByVal DoubleOptional scales, @ByRef Tensor grad_input);


// aten::upsample_nearest1d_backward(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... input_size);


// aten::upsample_nearest1d_backward(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size);





// Parsed from ATen/ops/upsample_nearest2d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_nearest2d_ops.h>


// aten::upsample_nearest2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal DoubleArrayRefOptional scale_factors);


// aten::upsample_nearest2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);


// aten::upsample_nearest2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);


// aten::upsample_nearest2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::upsample_nearest2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size);


// aten::upsample_nearest2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::upsample_nearest2d(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);


// aten::upsample_nearest2d(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size);





// Parsed from ATen/ops/upsample_nearest2d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_nearest2d_backward_ops.h>


// aten::upsample_nearest2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... input_size);


// aten::upsample_nearest2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::upsample_nearest2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size);


// aten::upsample_nearest2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::upsample_nearest2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... input_size);


// aten::upsample_nearest2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size);





// Parsed from ATen/ops/upsample_nearest3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_nearest3d_ops.h>


// aten::upsample_nearest3d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal DoubleArrayRefOptional scale_factors);


// aten::upsample_nearest3d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);


// aten::upsample_nearest3d.out(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);


// aten::upsample_nearest3d.out(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::upsample_nearest3d.out(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size);


// aten::upsample_nearest3d.out(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::upsample_nearest3d(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... output_size);


// aten::upsample_nearest3d(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size);





// Parsed from ATen/ops/upsample_nearest3d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_nearest3d_backward_ops.h>


// aten::upsample_nearest3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... input_size);


// aten::upsample_nearest3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::upsample_nearest3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size);


// aten::upsample_nearest3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::upsample_nearest3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... input_size);


// aten::upsample_nearest3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size);





// Parsed from ATen/ops/upsample_trilinear3d.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_trilinear3d_ops.h>


// aten::upsample_trilinear3d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);


// aten::upsample_trilinear3d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_symint(@Const @ByRef Tensor input, @ByVal SymIntArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);


// aten::upsample_trilinear3d.out(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);


// aten::upsample_trilinear3d.out(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::upsample_trilinear3d.out(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners);


// aten::upsample_trilinear3d.out(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);


// aten::upsample_trilinear3d(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);


// aten::upsample_trilinear3d(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_symint(@Const @ByRef Tensor self, @ByVal SymIntRef output_size, @Cast("bool") boolean align_corners);





// Parsed from ATen/ops/upsample_trilinear3d_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/upsample_trilinear3d_backward_ops.h>


// aten::upsample_trilinear3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);


// aten::upsample_trilinear3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::upsample_trilinear3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_symint_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners);


// aten::upsample_trilinear3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_symint_outf(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);


// aten::upsample_trilinear3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);


// aten::upsample_trilinear3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward_symint(@Const @ByRef Tensor grad_output, @ByVal SymIntRef output_size, @ByVal SymIntRef input_size, @Cast("bool") boolean align_corners);





// Parsed from ATen/ops/value_selecting_reduction_backward.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/value_selecting_reduction_backward_ops.h>


// aten::value_selecting_reduction_backward(Tensor grad, int dim, Tensor indices, SymInt[] sizes, bool keepdim) -> Tensor
@Namespace("at") public static native @ByVal Tensor value_selecting_reduction_backward(@Const @ByRef Tensor grad, @Cast("int64_t") long dim, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes, @Cast("bool") boolean keepdim);
@Namespace("at") public static native @ByVal Tensor value_selecting_reduction_backward(@Const @ByRef Tensor grad, @Cast("int64_t") long dim, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes, @Cast("bool") boolean keepdim);


// aten::value_selecting_reduction_backward(Tensor grad, int dim, Tensor indices, SymInt[] sizes, bool keepdim) -> Tensor
@Namespace("at") public static native @ByVal Tensor value_selecting_reduction_backward_symint(@Const @ByRef Tensor grad, @Cast("int64_t") long dim, @Const @ByRef Tensor indices, @ByVal SymIntRef sizes, @Cast("bool") boolean keepdim);





// Parsed from ATen/ops/values.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/values_ops.h>






// Parsed from ATen/ops/values_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/values_copy_ops.h>


// aten::values_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor values_copy(@Const @ByRef Tensor self);

// aten::values_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor values_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::values_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor values_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/vander.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/vander_ops.h>


// aten::vander(Tensor x, int? N=None, bool increasing=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor vander(@Const @ByRef Tensor x, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional N, @Cast("bool") boolean increasing/*=false*/);
@Namespace("at") public static native @ByVal Tensor vander(@Const @ByRef Tensor x);




// Parsed from ATen/ops/var.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/var_ops.h>


// aten::var(Tensor self, bool unbiased=True) -> Tensor
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @Cast("bool") boolean unbiased);

// aten::var.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased);

// aten::var.correction(Tensor self, int[1]? dim=None, *, int? correction=None, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);

// aten::var.out(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased);
// aten::var.out(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor var_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::var.correction_out(Tensor self, int[1]? dim=None, *, int? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
// aten::var.correction_out(Tensor self, int[1]? dim=None, *, int? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor var_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::var.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased);

// aten::var.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased);
// aten::var.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);

// aten::var.correction_names(Tensor self, Dimname[1] dim, *, int? correction=None, bool keepdim=False) -> Tensor
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);

// aten::var.correction_names_out(Tensor self, Dimname[1] dim, *, int? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
// aten::var.correction_names_out(Tensor self, Dimname[1] dim, *, int? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor var_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out);




// Parsed from ATen/ops/var_mean.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/var_mean_ops.h>


// aten::var_mean(Tensor self, bool unbiased=True) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, @Cast("bool") boolean unbiased);

// aten::var_mean.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean unbiased);
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased);

// aten::var_mean.correction(Tensor self, int[1]? dim=None, *, int? correction=None, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);

// aten::var_mean.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased);

// aten::var_mean.correction_names(Tensor self, Dimname[1] dim, *, int? correction=None, bool keepdim=False) -> (Tensor, Tensor)
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal TensorTensorTuple var_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);

// aten::var_mean.correction_out(Tensor self, int[1]? dim=None, *, int? correction=None, bool keepdim=False, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> var_mean_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> var_mean_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> var_mean_out(@ByRef Tensor out0, @ByRef Tensor out1, @Const @ByRef Tensor self, @ByVal(nullValue = "at::OptionalIntArrayRef(c10::nullopt)") @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional correction, @Cast("bool") boolean keepdim/*=false*/);
// aten::var_mean.correction_out(Tensor self, int[1]? dim=None, *, int? correction=None, bool keepdim=False, Tensor(a!) out0, Tensor(b!) out1) -> (Tensor(a!), Tensor(b!))
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> var_mean_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out0, @ByRef Tensor out1);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> var_mean_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal LongOptional correction, @Cast("bool") boolean keepdim, @ByRef Tensor out0, @ByRef Tensor out1);




// Parsed from ATen/ops/vdot.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/vdot_ops.h>


// aten::vdot(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor vdot(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::vdot.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor vdot_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::vdot.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor vdot_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);




// Parsed from ATen/ops/view.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/view_ops.h>






// Parsed from ATen/ops/view_as.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/view_as_ops.h>






// Parsed from ATen/ops/view_as_complex.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/view_as_complex_ops.h>


// aten::view_as_complex(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor view_as_complex(@Const @ByRef Tensor self);




// Parsed from ATen/ops/view_as_complex_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/view_as_complex_copy_ops.h>


// aten::view_as_complex_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor view_as_complex_copy(@Const @ByRef Tensor self);

// aten::view_as_complex_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor view_as_complex_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::view_as_complex_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor view_as_complex_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/view_as_real.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/view_as_real_ops.h>


// aten::view_as_real(Tensor(a) self) -> Tensor(a)
@Namespace("at") public static native @ByVal Tensor view_as_real(@Const @ByRef Tensor self);




// Parsed from ATen/ops/view_as_real_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/view_as_real_copy_ops.h>


// aten::view_as_real_copy(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal Tensor view_as_real_copy(@Const @ByRef Tensor self);

// aten::view_as_real_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor view_as_real_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::view_as_real_copy.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor view_as_real_copy_outf(@Const @ByRef Tensor self, @ByRef Tensor out);




// Parsed from ATen/ops/view_copy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/view_copy_ops.h>


// aten::view_copy(Tensor self, SymInt[] size) -> Tensor
@Namespace("at") public static native @ByVal Tensor view_copy(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor view_copy(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::view_copy(Tensor self, SymInt[] size) -> Tensor
@Namespace("at") public static native @ByVal Tensor view_copy_symint(@Const @ByRef Tensor self, @ByVal SymIntRef size);


// aten::view_copy.dtype(Tensor self, ScalarType dtype) -> Tensor
@Namespace("at") public static native @ByVal Tensor view_copy(@Const @ByRef Tensor self, ScalarType dtype);

// aten::view_copy.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor view_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor view_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::view_copy.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor view_copy_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor view_copy_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);


// aten::view_copy.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor view_copy_symint_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal SymIntRef size);


// aten::view_copy.out(Tensor self, SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor view_copy_symint_outf(@Const @ByRef Tensor self, @ByVal SymIntRef size, @ByRef Tensor out);


// aten::view_copy.dtype_out(Tensor self, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor view_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, ScalarType dtype);
// aten::view_copy.dtype_out(Tensor self, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor view_copy_outf(@Const @ByRef Tensor self, ScalarType dtype, @ByRef Tensor out);




// Parsed from ATen/ops/vsplit.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/vsplit_ops.h>


// aten::vsplit.int(Tensor(a -> *) self, int sections) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector vsplit(@Const @ByRef Tensor self, @Cast("int64_t") long sections);

// aten::vsplit.array(Tensor(a -> *) self, int[] indices) -> Tensor(a)[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector vsplit(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef indices);
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector vsplit(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... indices);




// Parsed from ATen/ops/vstack.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/vstack_ops.h>


// aten::vstack(Tensor[] tensors) -> Tensor
@Namespace("at") public static native @ByVal Tensor vstack(@ByVal TensorArrayRef tensors);

// aten::vstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor vstack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
// aten::vstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor vstack_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);




// Parsed from ATen/ops/where.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/where_ops.h>


// aten::where.self(Tensor condition, Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor where(@Const @ByRef Tensor condition, @Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::where.self_out(Tensor condition, Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor where_out(@ByRef Tensor out, @Const @ByRef Tensor condition, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::where.self_out(Tensor condition, Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor where_outf(@Const @ByRef Tensor condition, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::where.ScalarSelf(Tensor condition, Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor where(@Const @ByRef Tensor condition, @Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::where.ScalarOther(Tensor condition, Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor where(@Const @ByRef Tensor condition, @Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::where.Scalar(Tensor condition, Scalar self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor where(@Const @ByRef Tensor condition, @Const @ByRef Scalar self, @Const @ByRef Scalar other);

// aten::where(Tensor condition) -> Tensor[]
@Namespace("at") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector where(@Const @ByRef Tensor condition);




// Parsed from ATen/ops/xlogy.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/xlogy_ops.h>


// aten::xlogy.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor xlogy(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

// aten::xlogy.Scalar_Self(Scalar self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor xlogy(@Const @ByRef Scalar self, @Const @ByRef Tensor other);

// aten::xlogy.Scalar_Other(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor xlogy(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::xlogy_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_(@ByRef Tensor self, @Const @ByRef Tensor other);

// aten::xlogy_.Scalar_Other(Tensor(a!) self, Scalar other) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_(@ByRef Tensor self, @Const @ByRef Scalar other);

// aten::xlogy.OutTensor(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
// aten::xlogy.OutTensor(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::xlogy.OutScalar_Self(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_out(@ByRef Tensor out, @Const @ByRef Scalar self, @Const @ByRef Tensor other);
// aten::xlogy.OutScalar_Self(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_outf(@Const @ByRef Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);

// aten::xlogy.OutScalar_Other(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Scalar other);
// aten::xlogy.OutScalar_Other(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor xlogy_outf(@Const @ByRef Tensor self, @Const @ByRef Scalar other, @ByRef Tensor out);




// Parsed from ATen/ops/xor.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/xor_ops.h>


// aten::__xor__.Scalar(Tensor self, Scalar other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __xor__(@Const @ByRef Tensor self, @Const @ByRef Scalar other);

// aten::__xor__.Tensor(Tensor self, Tensor other) -> Tensor
@Namespace("at") public static native @ByVal Tensor __xor__(@Const @ByRef Tensor self, @Const @ByRef Tensor other);




// Parsed from ATen/ops/zero.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/zero_ops.h>


// aten::zero_(Tensor(a!) self) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zero_(@ByRef Tensor self);

// aten::zero.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zero_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::zero.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zero_outf(@Const @ByRef Tensor self, @ByRef Tensor out);

// aten::zero(Tensor self) -> Tensor
@Namespace("at") public static native @ByVal @Name("zero") Tensor _zero(@Const @ByRef Tensor self);




// Parsed from ATen/ops/zeros.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/zeros_ops.h>


// aten::zeros.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);
// aten::zeros.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);

// aten::zeros(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::zeros(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::zeros(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor zeros_symint(@ByVal SymIntRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor zeros_symint(@ByVal SymIntRef size);


// aten::zeros(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor zeros_symint(@ByVal SymIntRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);


// aten::zeros.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zeros_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor zeros_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);


// aten::zeros.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zeros_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor zeros_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);


// aten::zeros.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zeros_symint_out(@ByRef Tensor out, @ByVal SymIntRef size);


// aten::zeros.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zeros_symint_outf(@ByVal SymIntRef size, @ByRef Tensor out);


// aten::zeros.names_out(int[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zeros_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByRef Tensor zeros_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);
// aten::zeros.names_out(int[] size, *, Dimname[]? names, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zeros_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor zeros_outf(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByRef Tensor out);




// Parsed from ATen/ops/zeros_like.h

// #pragma once

// @generated by torchgen/gen.py from Function.h

// #include <ATen/Context.h>
// #include <ATen/DeviceGuard.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/Generator.h>
// #include <ATen/core/Reduction.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>



// #include <ATen/ops/zeros_like_ops.h>


// aten::zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor zeros_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor zeros_like(@Const @ByRef Tensor self);
// aten::zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
@Namespace("at") public static native @ByVal Tensor zeros_like(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);

// aten::zeros_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zeros_like_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor zeros_like_out(@ByRef Tensor out, @Const @ByRef Tensor self);
// aten::zeros_like.out(Tensor self, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
@Namespace("at") public static native @ByRef Tensor zeros_like_outf(@Const @ByRef Tensor self, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);




// Parsed from torch/autograd.h

// #pragma once

// #include <torch/csrc/autograd/autograd.h>
// #include <torch/csrc/autograd/autograd_not_implemented_fallback.h>
// #include <torch/csrc/autograd/custom_function.h>


// Parsed from torch/script.h

// #pragma once

// #include <torch/csrc/api/include/torch/types.h>
// #include <torch/csrc/autograd/InferenceMode.h>
// #include <torch/csrc/autograd/custom_function.h>
// #include <torch/csrc/autograd/generated/variable_factories.h>
// #include <torch/csrc/autograd/grad_mode.h>
// #include <torch/csrc/jit/runtime/custom_operator.h>
// #include <torch/csrc/jit/serialization/import.h>
// #include <torch/csrc/jit/serialization/pickle.h>
// #include <torch/custom_class.h>

// #include <ATen/ATen.h>


// Parsed from torch/csrc/Export.h

// #pragma once

// #include <c10/macros/Export.h>

// #ifdef THP_BUILD_MAIN_LIB
// #define TORCH_PYTHON_API C10_EXPORT
// #else
// #define TORCH_PYTHON_API C10_IMPORT
// #endif


// Parsed from torch/csrc/onnx/onnx.h

// #pragma once

@Namespace("torch::onnx") public enum OperatorExportTypes {
  ONNX(0), // Strict ONNX export
  ONNX_ATEN(1), // ONNX With ATen op everywhere
  ONNX_ATEN_FALLBACK(2), // ONNX export with ATen fallback
  ONNX_FALLTHROUGH(3);// Export supported ONNX ops. Pass through unsupported ops.

    public final int value;
    private OperatorExportTypes(int v) { this.value = v; }
    private OperatorExportTypes(OperatorExportTypes e) { this.value = e.value; }
    public OperatorExportTypes intern() { for (OperatorExportTypes e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("torch::onnx") public enum TrainingMode {
  EVAL(0), // Inference mode
  PRESERVE(1), // Preserve model state (eval/training)
  TRAINING(2);// Training mode

    public final int value;
    private TrainingMode(int v) { this.value = v; }
    private TrainingMode(TrainingMode e) { this.value = e.value; }
    public TrainingMode intern() { for (TrainingMode e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("torch::onnx") @MemberGetter public static native @Cast("const char") byte kOnnxNodeNameAttribute(int i);
@Namespace("torch::onnx") @MemberGetter public static native @Cast("const char*") BytePointer kOnnxNodeNameAttribute();

 // namespace onnx
 // namespace torch


// Parsed from torch/csrc/api/include/torch/imethod.h

// #pragma once
// #include <ATen/core/ivalue.h>
// #include <vector>
// Targeting ../IMethod.java



 // namespace torch


// Parsed from torch/csrc/api/include/torch/types.h

// #pragma once

// #include <ATen/ATen.h>

// #include <c10/util/Optional.h>

// #include <torch/csrc/autograd/generated/variable_factories.h>
// #include <torch/csrc/autograd/variable.h>

// TODO: These don't really belong here but torchvision builds in CI need them
// Remove once the torchvision version being compiled in CI is updated
// #include <ATen/core/dispatch/Dispatcher.h>
// #include <torch/library.h>

// NOTE [ Exposing declarations in `at::` to `torch::` ]
//
// The following line `using namespace at;` is responsible for exposing all
// declarations in `at::` namespace to `torch::` namespace.
//
// According to the rules laid out in
// https://en.cppreference.com/w/cpp/language/qualified_lookup, section
// "Namespace members":
// ```
// Qualified lookup within the scope of a namespace N first considers all
// declarations that are located in N and all declarations that are located in
// the inline namespace members of N (and, transitively, in their inline
// namespace members). If there are no declarations in that set then it
// considers declarations in all namespaces named by using-directives found in N
// and in all transitive inline namespace members of N.
// ```
//
// This means that if both `at::` and `torch::` namespaces have a function with
// the same signature (e.g. both `at::func()` and `torch::func()` exist), after
// `namespace torch { using namespace at; }`, when we call `torch::func()`, the
// `func()` function defined in `torch::` namespace will always be called, and
// the `func()` function defined in `at::` namespace is always hidden. // NOLINT

/** Fixed width dtypes. */

/** Rust-style short dtypes. */
 // namespace torch


// Parsed from torch/csrc/api/include/torch/cuda.h

// #pragma once

// #include <torch/csrc/Export.h>

// #include <cstddef>
// #include <cstdint>

/** Returns the number of CUDA devices available. */
@Namespace("torch::cuda") public static native @Cast("size_t") @Name("device_count") long cuda_device_count();

/** Returns true if at least one CUDA device is available. */
@Namespace("torch::cuda") public static native @Cast("bool") @Name("is_available") boolean cuda_is_available();

/** Returns true if CUDA is available, and CuDNN is available. */
@Namespace("torch::cuda") public static native @Cast("bool") boolean cudnn_is_available();

/** Sets the seed for the current GPU. */
@Namespace("torch::cuda") public static native @Name("manual_seed") void cuda_manual_seed(@Cast("uint64_t") long seed);

/** Sets the seed for all available GPUs. */
@Namespace("torch::cuda") public static native @Name("manual_seed_all") void cuda_manual_seed_all(@Cast("uint64_t") long seed);

/** Waits for all kernels in all streams on a CUDA device to complete. */
@Namespace("torch::cuda") public static native @Name("synchronize") void cuda_synchronize(@Cast("int64_t") long device_index/*=-1*/);
@Namespace("torch::cuda") public static native @Name("synchronize") void cuda_synchronize();

 // namespace cuda
 // namespace torch


// Parsed from torch/csrc/api/include/torch/ordered_dict.h

// #pragma once

// #include <cstdint>
// #include <initializer_list>
// #include <string>
// #include <unordered_map>
// #include <utility>
// #include <vector>
// Targeting ../StringTensorDict.java


// Targeting ../StringModuleDict.java


// Targeting ../StringAnyModuleDict.java


// Targeting ../StringSharedModuleDict.java


// Targeting ../StringTensorDictItem.java


// Targeting ../StringModuleDictItem.java


// Targeting ../StringAnyModuleDictItem.java


// Targeting ../StringSharedModuleDictItem.java



// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ OrderedDict ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



































































 // namespace torch


// Parsed from torch/csrc/utils/memory.h

// #pragma once

// #include <memory>

// Reference:
// https://github.com/llvm-mirror/libcxx/blob/master/include/memory#L3091


 // namespace torch


// Parsed from torch/csrc/utils/python_stub.h

// #pragma once
// Targeting ../_object.java




// Parsed from torch/csrc/utils/schema_info.h

// #pragma once

// #include <torch/csrc/jit/frontend/function_schema_parser.h>
// #include <unordered_set>
// Targeting ../SchemaInfo.java


 // namespace utils
 // namespace torch


// Parsed from torch/csrc/utils/variadic.h

// #pragma once

// #include <ATen/core/Tensor.h>
// #include <ATen/core/Variadic.h>
// #include <torch/csrc/autograd/variable.h>

// #include <cstdint>
// #include <tuple>
// #include <type_traits>
// #include <utility>
// Targeting ../Indices.java



// Decrements the index N, adds N-1 to the list of indices and forwards
// whatever we already have.
// Targeting ../MakeIndices.java



//===----------------------------------------------------------------------===//
//                                 Utilities
//===----------------------------------------------------------------------===//
// Targeting ../pack.java



// Targeting ../all_of.java


// Targeting ../any_of.java



 // namespace torch


// Parsed from torch/csrc/autograd/utils/warnings.h

// #pragma once
// #include <c10/util/Exception.h>

// #include <mutex>
// #include <vector>

// Warning handler for multi-threaded contexts. Gather warnings from
// all threads into a single queue, then process together at the end
// in the main thread.

 // namespace utils
 // namespace autograd
 // namespace torch


// Parsed from torch/csrc/autograd/anomaly_mode.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <memory>
// #include <string>
// Targeting ../AnomalyMode.java


// Targeting ../DetectAnomalyGuard.java


// Targeting ../AnomalyMetadata.java



 // namespace autograd
 // namespace torch


// Parsed from torch/csrc/autograd/edge.h

// #pragma once

// #include <cstdint>
// #include <functional>
// #include <memory>

// #include <c10/util/hash.h>
// Targeting ../Edge.java


 // namespace autograd
 // namespace torch

// The idiomatic way of enabling use of a custom type as the key of hash
// containers in C++11. This method removes the requirement of having to pass
// a custom hasher to std::unordered_{map, set}.
// See http://en.cppreference.com/w/cpp/utility/hash for more information.
 // namespace std


// Parsed from torch/csrc/autograd/grad_mode.h

// #pragma once

// #include <ATen/core/grad_mode.h>
// #include <torch/csrc/Export.h>

 // namespace autograd
 // namespace torch


// Parsed from torch/csrc/autograd/InferenceMode.h

// #pragma once

// #include <c10/core/InferenceMode.h>
// #include <torch/csrc/Export.h>


 // namespace torch


// Parsed from torch/csrc/autograd/input_metadata.h

// #pragma once

// #include <ATen/ExpandUtils.h>
// #include <ATen/NestedTensorImpl.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/Device.h>
// #include <c10/core/DeviceType.h>
// #include <c10/core/Stream.h>
// #include <c10/core/SymIntArrayRef.h>
// #include <c10/core/TensorImpl.h>
// #include <c10/core/impl/DeviceGuardImplInterface.h>
// #include <c10/util/DimVector.h>
// #include <c10/util/Exception.h>
// #include <c10/util/SmallVector.h>
// #include <c10/util/variant.h>

// #ifndef AT_PER_OPERATOR_HEADERS
// #include <ATen/Functions.h>
// #else
// #include <ATen/ops/zeros.h>
// #endif

// #include <cstdint>
// #include <utility>
// Targeting ../InputMetadata.java


 // namespace autograd
 // namespace torch


// Parsed from torch/csrc/autograd/function_hook.h

// #pragma once

// #include <ATen/Tensor.h>
// #include <torch/csrc/Export.h>
// #include <vector>

// A hook that's called on gradients
// Targeting ../FunctionPreHook.java


// Targeting ../FunctionPostHook.java



 // namespace autograd
 // namespace torch


// Parsed from torch/csrc/autograd/profiler.h

// #pragma once

// #include <torch/csrc/autograd/profiler_kineto.h>
// #include <torch/csrc/autograd/profiler_legacy.h>


// Parsed from torch/csrc/autograd/saved_variable_hooks.h

// #pragma once

// #include <ATen/core/Tensor.h>
// Targeting ../SavedVariableHooks.java



 // namespace autograd
 // namespace torch


// Parsed from torch/csrc/autograd/saved_variable.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <torch/csrc/autograd/forward_grad.h>
// #include <torch/csrc/autograd/saved_variable_hooks.h>

// #include <ATen/core/Tensor.h>

// #include <cstdint>
// #include <memory>

@Namespace("torch::autograd") public static native @Cast("const char*") BytePointer ERR_BACKWARD_TWICE(); public static native void ERR_BACKWARD_TWICE(BytePointer setter);
// Targeting ../SavedVariable.java


 // namespace autograd
 // namespace torch


// Parsed from torch/csrc/autograd/forward_grad.h

// #pragma once

// #include <ATen/core/Tensor.h>

// [ Using ForwardGrad ]
// ForwardGrad needs to be a shared_ptr to satisfy constraints of its inner
// design. But this shared_ptr must be uniquely associated with the object that
// stores it (as of writing, either AutogradMeta or SavedVariable). This object
// is called the "owning object" in the discussions below. This owning object
// must call `ForwardGrad::clear()` when it is destroyed to ensure that the
// ForwardGrad is properly de-allocated.

// This file contains two classes that are used to store forward AD gradients
// and ensure that they are scoped properly. Because forward AD runs
// concurrently with the evaluation of the function, we need a mechanism to
// separate different forward AD invocations and be able to compute the right
// gradients. We model such invocations as levels here. The particular scoping
// issue mentioned above has two main drivers:
//   - Ensure that we can conveniently use forward AD within a high level API
//   without
//     leaking the forward AD states outside.
//   - Ensure that we can keep the level that we expose to the user API simple
//   (an integer
//     that represents the nesting depth) while avoiding confusions when the
//     level index is re-used.

// The important external APIs from this file are:
//   - ForwardADLevel::get_next_idx() that can be used to enter a new level and
//   get its index
//   - ForwardADLevel::release_idx() that can be used to exit a given level.
//   - ForwardGrad() can be used to store a given forward gradient that will
//   handle the level
//     tracking automatically.

// The basic implementation strategy is as follows:
// Every tensor has a ForwardGrad, maintaining a map from levels to tangents.
// ForwardGrad is responsible for registering itself to the appropriate
// ForwardADLevel when a new tangent is added to it via ForwardGrad::set_value
// and to un-register itself from this same level if that tangent is removed via
// ForwardGrad::reset. The ForwardADLevel is created when a new level is entered
// via ForwardADLevel::get_next_idx. A reference to the new ForwardADLevel is
// stored into a global (for the whole process) vector that ensure it can be
// accessed via ForwardADLevel::get_by_idx. This reference is deleted when the
// index is released by the user when calling ForwardADLevel::release_idx. When
// it is destructed, the ForwardADLevel is responsible for clearing all the
// tangents for its level stored in all the ForwardGrad that registered with it.
//
// This process-wide level design, compared to a thread local one, allows us to
// use very simple user facing handle for the level (an int) while enabling
// cross-thread forward AD. The only required synchronization for the user is
// when entering and exiting the levels. Some discussion on alternative design
// is in https://github.com/pytorch/pytorch/pull/49097#discussion_r543716453 and
// can be refined in the future.

// Correctness of concurrency:
// Each class uses its own lock when reading or modifying internal storages.
// This allows in particular to safely remove tangents from ForwardGrad when the
// ForwardADLevel is being exited. We ensure no deadlock by ensuring that a
// methods never calls into another class's method while the local class's lock
// is held except in one single case: calling from ForwardADLevel's destructor
// into ForwardGrad::reset with update_level=false.

// The lifetime of these objects is as follows:
// The ForwardADLevel can be in three states:
//      - Initialized: where one of its reference is held by the global vector
//      and there may be more
//        references held by temporary variables in ForwardGrad's methods.
//      - About to be destructed: where "release_idx" has been called and the
//      only reason for the
//        ForwardADLevel not to be destructed right away is that some methods in
//        ForwardGrad have owning reference to it. This is done so that a
//        ForwardADLevel can never be destructed when a ForwardGrad is
//        registered with it and in the process of adding something to its
//        internal state.
//      - Being destructed: Here the ForwardADLevel is not referenced anymore
//      and can be safely reset
//        all of the ForwardGrad. Note that we can have more than one reset
//        being called here (which is ok) but we are guaranteed that there is at
//        least one.
// The ForwardGrad is simpler as there is no intermediary state and no special
// destructor for. The logic to unregister it from the different ForwardADLevel
// is done when the owning object (AutogradMeta or SavedVariable) is being
// destroyed.

// Other considered design:
// To avoid having the ForwardGrad::clear, we considered storing weak_ptr inside
// the ForwardADLevel. While this would work, it would mean that the set inside
// the ForwardADLevel would only grow unless we do an expensive linear scan to
// remove all the dangling weak pointers. Hence this approach was not used.

// Data structures in this file are optimized for this maximum number of levels.
// The number of levels corresponds to the degree of the gradient being
// computed using forward AD and we don't expect more than second order
// gradients to be common.
public static final int EXPECTED_MAX_LEVEL = 2;
// Targeting ../ForwardADLevel.java


// Targeting ../ForwardGrad.java



 // namespace autograd
 // namespace torch


// Parsed from torch/csrc/autograd/variable.h

// #pragma once

// #include <torch/csrc/utils/python_stub.h>

// #include <torch/csrc/Export.h>
// #include <torch/csrc/autograd/cpp_hook.h>
// #include <torch/csrc/autograd/edge.h>
// #include <torch/csrc/autograd/forward_grad.h>
// #include <torch/csrc/autograd/function_hook.h>

// #include <ATen/NamedTensorUtils.h>
// #include <ATen/core/Tensor.h>
// #include <c10/util/Exception.h>

// #include <cstdint>
// #include <memory>
// #include <mutex>
// #include <stdexcept>
// #include <string>
// #include <utility>
// #include <vector>

/** {@code Variable} is exactly the same as {@code Tensor} (i.e. we have {@code using Variable =
 *  at::Tensor}). This means you can perform all the usual mathematical and
 *  other operations you can perform on {@code Tensor}s also on {@code Variable}s.
 * 
 *  The only reason we are keeping the {@code Variable} class is backward
 *  compatibility with external user's legacy C++ frontend code. Our intention
 *  is to eliminate the {@code Variable} class in the near future. */

 // namespace autograd
 // namespace torch

// The following are all internal APIs and should not be shown in libtorch docs.
// Therefore, we wrap the following code with `#ifndef DOXYGEN_SHOULD_SKIP_THIS
// ... #endif`

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

/** Check if this type is supported by the autograd engine.
 *  If you change this, update the doc at the top of the
 *  torch/autograd/__init__.py file and
 *  "test_set_requires_grad_only_for_continuous_types" in test/test_autograd.py */
@Namespace("torch::autograd") public static native @Cast("bool") boolean isDifferentiableType(ScalarType t);

/**~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *                                 Variable
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  A {@code Variable} augments a {@code Tensor} with the ability to interact in our
 *  autograd machinery. Conceptually, {@code Variable}s travel along {@code Edge}s between
 *  {@code Node}s in the autograd graph. A {@code Variable} can either be a leaf, like a
 *  weight in a neural network, or an interior variable, when it is the result
 *  of an operation between variables. Every {@code Variable} also stores another
 *  {@code Variable} called its {@code grad} (gradient). If the variable is a leaf, its
 *  gradient will be accumulated into this variable.
 * 
 *  Every Tensor is a Variable, but sometimes we colloquially refer to Variables
 *  that don't require gradients as Tensors (since none of the autograd
 *  machinery for Variables applies).  Historically, Variables and Tensors
 *  were separate concepts, but now they are exactly the same (i.e. we have
 *  {@code using Variable = at::Tensor}).
 * 
 *                               Gradient Edges
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  Furthermore, {@code Variable}s have the notion of a {@code gradient_edge}, which is the
 *  edge in the autograd graph that connects the variable to a particular input
 *  of the gradient function that will be invoked with the variable during the
 *  backward pass. More precisely, this gradient function can be one of two
 *  things:
 *  1. A {@code grad_fn}, if the variable is in the interior of the graph. This is the
 *     gradient of the function that produced the variable.
 *  2. A {@code grad_accumulator}, if the variable is a leaf, which accumulates a
 *     scalar gradient value into its {@code grad} variable.
 * 
 *                                Versioning
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  Another major feature of {@code Variable}s are *versions*. Versions are
 *  incremented when an in-place mutation of a variable occurs. Versions are
 *  useful when constructing {@code SavedVariable}s, which take a snapshot of a
 *  {@code Variable} at a certain version. You can retrieve a {@code Variable}'s version
 *  through its {@code current_version()} method.
 * 
 *                                  Views
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  It is possible for a  {@code Variable} to be a *view* of another {@code Variable}, in
 *  which case it tracks that {@code Variable}'s data and autograd history. Beyond
 *  construction, the interface of a view is identical to that of a regular
 *  {@code Variable}. You can determine whether {@code Variable} is in fact a view by
 *  probing its {@code is_view()} method. Note that the *view* semantics are only
 *  meaningful for {@code Variable} relations that are relevant to autograd.
 *  See NOTE [ Autograd View Variables ] for more details.
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ */

// Private-ish functions for manipulating variables; we don't want to put them
// on Tensor proper

// WARNING: This may return a nullptr.  If you require AutogradMeta to return
// a materialized structure, use materialize_autograd_meta instead.
@Namespace("torch::autograd::impl") public static native AutogradMeta get_autograd_meta(@Const @ByRef TensorBase arg0);

// WARNING: This will return a nullptr if the Tensor is not a view.
@Namespace("torch::autograd::impl") public static native DifferentiableViewMeta get_view_autograd_meta(@Const @ByRef TensorBase arg0);

// Returns the current autograd meta, materializing it if it was previously
// none.  This counts as a *mutating* operation, so do not call it on
// "read-only" operators; in particular, this is NOT thread safe
@Namespace("torch::autograd::impl") public static native AutogradMeta materialize_autograd_meta(@Const @ByRef TensorBase arg0);

/** Set the gradient accumulator of the {@code Variable}. This is only applicable to
 *  leaf variables. Interior variables should call {@code set_gradient_edge()}. */

/** Attempts to get a pointer to the gradient accumulator of the {@code Variable},
 *  if it still exists. If the gradient accumulator function has been
 *  destroyed, returns a {@code nullptr}. */
@Namespace("torch::autograd::impl") public static native @SharedPtr Node try_get_grad_accumulator(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

/** Gets the gradient accumulator of the {@code Variable} if it has one, or else
 *  create one on the fly and return it. */
@Namespace("torch::autograd::impl") public static native @SharedPtr Node grad_accumulator(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

/** Returns the "canonical" gradient edge of this {@code Variable}, i.e. either the
 *  gradient function if this is an interior {@code Variable}, or the gradient
 *  accumulator otherwise. If the {@code Variable} is interior, the returned {@code Edge}
 *  will store the input index of the {@code Node} to which this variable is
 *  connected in its {@code input_nr} field. For leaves, the {@code input_nr} is always
 *  zero. Note that {@code set_gradient_edge} and {@code gradient_edge} are not
 *  symmetric. You must use {@code set_gradient_edge} to set the {@code grad_fn} and
 *  {@code set_grad_accumulator} to set the accumulator. */
@Namespace("torch::autograd::impl") public static native @ByVal Edge gradient_edge(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

/** Set the gradient edge -- i.e. {@code grad_fn} and {@code input_nr} -- of the
 *  {@code Variable}.
 *  NOTE: This will always set the {@code grad_fn}, even if this is a leaf variable,
 *  and never the {@code grad_accumulator}. For the latter, use
 *  {@code set_grad_accumulator}. This allows late construction of an interior
 *  {@code Variable}. */

///
@Namespace("torch::autograd::impl") public static native void set_gradient_edge(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0, @ByVal Edge edge);

// Autograd Graph Interaction
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

/** Update the {@code grad_fn} of an existing Variable. Called after in-place
 *  modifications.
 * 
 *  For View Variables:
 *  Called after in-place modifications. Modifies the grad_fn of the base
 *  Variable. */
@Namespace("torch::autograd::impl") public static native void rebase_history(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0, @ByVal Edge gradient_edge);

/** Gets the raw gradient function pointer, whatever it currently is. */
@Namespace("torch::autograd::impl") public static native Node grad_fn_unsafe(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

/** Increments the version count of this {@code Variable}. */
@Namespace("torch::autograd::impl") public static native void bump_version(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);
@Namespace("torch::autograd::impl") public static native void set_version_counter(
    @Cast("const torch::autograd::Variable*") @ByRef Tensor arg0,
    @Const @ByRef VariableVersion version_counter);

/** Retrieves this {@code Variable}s version counter. */
@Namespace("torch::autograd::impl") public static native @Const @ByRef VariableVersion version_counter(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

@Namespace("torch::autograd::impl") public static native void set_name(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0, @StdString BytePointer name);
@Namespace("torch::autograd::impl") public static native void set_name(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0, @StdString String name);

@Namespace("torch::autograd::impl") public static native void add_hook(
    @Const @ByRef TensorBase arg0,
    @UniquePtr @Cast({"", "std::unique_ptr<torch::autograd::FunctionPreHook>&&"}) FunctionPreHook hook);
@Namespace("torch::autograd::impl") public static native @ByRef FunctionPreHookVector hooks(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);
@Namespace("torch::autograd::impl") public static native void clear_hooks(@Const @ByRef TensorBase arg0);

@Namespace("torch::autograd::impl") public static native void create_cpp_hook(
    @Const @ByRef TensorBase arg0,
    @Cast("bool") boolean is_retains_grad_hooks/*=false*/);
@Namespace("torch::autograd::impl") public static native void create_cpp_hook(
    @Const @ByRef TensorBase arg0);

// Targeting ../AutogradMeta.java



//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
//                     DifferentiableViewMeta
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

/** NOTE [ Autograd View Variables ]
 * 
 *  Many operations return Variable that shares storage with an input Variable.
 *  The returned Variable is called a **view** Variable on the input **base**
 *  Variable.
 * 
 *  In PyTorch, we have two types of views: differentiable views, and
 *  non-differentiable views. In either type, to support proper version
 *  checking, the base and view Variables must always share the same
 *  version_counter.
 * 
 * 
 *  Differentiable Views
 *  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  This class allows to track both forward and backward AD differentiable
 *  views. These views can have different base as non-differentiable view for
 *  forward and backward mode AD are not the same.
 * 
 *  Most function are either both forward and backward differentiable views (for
 *  example: view, select, narrow, transpose, etc) or both not forward and not
 *  backward differentiable views (for example: indices, values, eq, lt, etc).
 *  But there are also functions that are forward but not backward
 *  differentiable views (only detach for now) or functions that are backward
 *  but not forward differentiable view (only make_dual and unpack dual for
 *  now).
 * 
 *  A concrete example of two views with different bases is as follow:
 * 
 *      # Have:
 *      #   dual is a dual Tensor that is neither a forward or backward view
 *      detached_dual = dual.detach()
 *      view = detached_dual.view_as(dual)
 *      # The forward base of view is dual
 *      # The backward base of view is detached_dual
 * 
 *  - Backward Mode View
 *  Differentiable views are the view variables where you want gradients to flow
 *  back to the base variables. Out-of-place operations on views are quite
 *  straightforward, but in-place ones are very tricky. Even if the base
 *  variable may not require grad when we create the view, we still need to
 *  track the view relation because future in-place ops may require back-proping
 *  through it. For example, we need to support
 * 
 *    (1) in-place operation on view, e.g.,
 * 
 *      # Have:
 *      #   base.requires_grad = False
 *      #   var.requires_grad = True
 *      base[1] = var  # i.e., base[1].copy_(var)
 *      torch.autograd.grad(base.sum(), var)  <- should return an all ones
 *      tensor
 * 
 *    (2) in-place operation on base after view is created, e.g.,
 * 
 *      # Have:
 *      #   base.requires_grad = False
 *      #   var.requires_grad = True
 *      view = base[1]
 *      base.copy_(var)
 *      torch.autograd.grad(view.sum(), var)  <- should return a tensor with
 *                                               var[1] filled with all ones and
 *                                               zeros everywhere else
 * 
 *  - Forward Mode View
 *  Forward differentiable views follow the same semantic as backward ones but
 *  show up differently as they are computed along with the forward evaluation.
 *  The hard examples above are thus very similar
 * 
 *    (1) in-place operation on view, e.g.,
 * 
 *      # Have:
 *      #   base is a regular Tensor
 *      #   var is a dual Tensor whose tangent is all ones
 *      base[1] = var  # i.e., base[1].copy_(var)
 *      # Now, base is a dual Tensor
 *      _, fw_grad = fwAD.unpack_dual(base) <- fw_grad should be a tensor with
 *                                               fw_grad[1] filled with all ones
 *                                               and zeros everywhere else
 * 
 *    (2) in-place operation on base after view is created, e.g.,
 * 
 *      # Have:
 *      #   base is a regular Tensor
 *      #   var is a dual Tensor whose tangent is all ones
 *      view = base[1]
 *      base.copy_(var)
 *      _, fw_grad = fwAD.unpack_dual(view) <- fw_grad should be an all ones
 *      tensor
 * 
 *  See Note [Forward Grad View/inplace] for more details on how we handle these
 *  hard cases.
 * 
 * 
 *  DifferentiableViewMeta is created to support gradient tracking of
 *  such **in-place** operations. In particular,
 *    + if an in-place op is done on base, the grad_fn field of the view may
 *      become stale. So accesses should always go through grad_fn(), which
 *      reconstructs an updated grad_fn if the version_counter has incremented.
 *      All other fields are always valid.
 *    + if an in-place op is done on view, in rebase_history() of view, which is
 *      called after every in-place op in VariableType.cpp, the grad_fn of base
 *      is updated.
 *    + if a single autograd Node returns multiple differentiable views, if any
 *      output is modified by an inplace operation, the autograd engine will
 *      make an equivalent graph (corresponding to the view operations) without
 *      using equivalent graph, where each output is treated as if it were
 *      produced by a distinct view operation. This discards the original (e.g.,
 *      user provided) grad_fn. If the provided grad_fn does more than the
 *      backward of the view, then the DifferentiableViewMeta must be created
 *      with creation_meta= CreationMeta::MULTI_OUTPUT_NODE to prevent the
 *      engine from ignoring the provided grad_fn.
 * 
 *  Interaction with GradMode:
 *  The particular case that we consider here is:
 * 
 *      # Have:
 *      #   base.requires_grad = True or False
 *      with torch.no_grad():
 *          view = base[1]
 *      base.requires_grad_()
 *      view.copy_(var)
 *      torch.autograd.grad(base.sum(), var)  <- what should it return?
 * 
 *  Given that this particular code example is ambiguous and can easily be
 *  replace by either moving both inside the no_grad block or both outside, we
 *  explicitly forbid it. For now, it is deprecated by a warning. This is
 *  achieved by setting creation_meta=CreationMeta::NO_GRAD_MODE for all
 *  differentiable views created in no_grad mode.
 * 
 *  See Note [View + Inplace update for base tensor]
 *  and Note [View + Inplace update for view tensor] for the details how
 *  autograd handles inplace update with view ops.
 * 
 *  Non-Differentiable Views
 *  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  In certain cases, although function outputs share storage with inputs, they
 *  will **never** require gradient history tracking. Instead of registering the
 *  view relation via DifferentiableViewMeta in autograd, the views will be
 *  using usual AutogradMeta and just share the version counters with the base
 *  Variables.
 *  Such views include:
 *    1. Views created from .detach()
 *    2. Views that are non-differentiable by its nature.
 *       E.g., {@code sparse_tensor.indices()} is a integral view on a (possibly)
 *       floating point tensor.
 *       See top of {@code derivatives.yaml} on how to specify that outputs of a
 *       function are non-differentiable.
 *  These are called non-differentiable views as the gradients do not flow
 *  through the view relation.
 * 
 *  Relevant logic for both differentiable and non-differentiable views is
 *  implemented in make_variable_(non_)differentiable_view below, and
 *  wrap_output of gen_variable_type.py.
 <p>
 *  NOTE [ View + Inplace detection ]
 * 
 *  We want to detect views followed by inplace as they are often forbidden to
 *  ensure correctness of the computed gradients. But since we want to only
 *  notify the user when both happen, we tag the DifferentiableViewMeta when the
 *  view is created via the {@code make_variable_*_view()} functions. This tag is then
 *  checked by the {@code check_inplace()} function from {@code VariableTypeUtils.h} that
 *  should be called before every inplace operation and to detect cases where
 *  other views are modified and this one is rebased by side effect, we also
 *  check in the {@code VariableHooks::grad_fn()}.
 <p>
 *  Flag that gives more information about when this view was created:
 *  - IN_CUSTOM_FUNCTION should be set when the view is created inside a custom
 *    autograd Function is returned.
 *  - NO_GRAD_MODE should be set when a view in created when GradMode is
 *  disabled
 *  - MULTI_OUTPUT_NODE should be set when a Node created by codegen code
 *  returns
 *    multiple differentiable views
 *  - Inference_MODE should be set when a view of normal tensor is created in
 *  InferenceMode.
 *  - DEFAULT is for all other cases */
@Namespace("torch::autograd") public enum CreationMeta {
  DEFAULT((byte)(0)),
  IN_CUSTOM_FUNCTION((byte)(1)),
  MULTI_OUTPUT_NODE((byte)(2)),
  NO_GRAD_MODE((byte)(3)),
  INFERENCE_MODE((byte)(4));

    public final byte value;
    private CreationMeta(byte v) { this.value = v; }
    private CreationMeta(CreationMeta e) { this.value = e.value; }
    public CreationMeta intern() { for (CreationMeta e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

/** Handles correctly propagating CreationMeta when a new view is created from a
 *  previous view. In general, we don't want the new view to be _less_
 *  restrictive than the previous view (it's okay to be _more_ restrictive). A
 *  CreationMeta value of DEFAULT is currently the least restrictive, as the
 *  behavior for all other CreationMeta values is to error out for in-place ops.
 *  A CreationMeta value of INFERENCE_MODE is currently the most restrictive, so
 *  it takes precedence in propagation. If this changes, the logic here will
 *  need to be updated to properly handle the new semantics. */
@Namespace("torch::autograd") public static native CreationMeta propagate_creation_meta(
    CreationMeta prev_view_creation_meta,
    CreationMeta new_view_creation_meta);
@Namespace("torch::autograd") public static native @Cast("torch::autograd::CreationMeta") byte propagate_creation_meta(
    @Cast("torch::autograd::CreationMeta") byte prev_view_creation_meta,
    @Cast("torch::autograd::CreationMeta") byte new_view_creation_meta);

/** Unified function to handle error checking when rebase happens
 *  indirect=true means that the caller is not doing the inplace, but the
 *  inplace happened somewhere else. */
@Namespace("torch::autograd") public static native void handle_view_on_rebase(
    DifferentiableViewMeta diff_view_meta,
    @Cast("bool") boolean indirect/*=false*/);
@Namespace("torch::autograd") public static native void handle_view_on_rebase(
    DifferentiableViewMeta diff_view_meta);
// Targeting ../DifferentiableViewMeta.java



//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
//                        Variable Implementation
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

// Factory Functions
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

/** Creates a {@code Variable} that is a *view* of another (*base*) variable.
 *  The {@code gradient_edge} is an optional (gradient_function, input_number) pair.
 *  {@code is_differentiable} is a bool that specifies whether this view is
 *  differentiable, i.e., whether the relation should be tracked by autograd.
 *  See NOTE [ Autograd View Variables ] for details.
 <p>
 *  NOTE: {@code allow_tensor_metadata_change} is set to true by default, because
 *  there are a lot of call sites to these factory functions that need to change
 *  the variable's size or storage afterwards, and they don't expect the
 *  original tensor (where the variable is created from) to be updated. Setting
 *  {@code allow_tensor_metadata_change_} to false by default would unnecessarily
 *  prevent those changes from happening and is undesirable. */

// See NOTE [ Autograd View Variables ] for details.
// Differentiable view. Track history with DifferentiableViewMeta.
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_differentiable_view(
    @Const @ByRef Tensor data,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer backward_info,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer forward_info,
    @Cast("bool") boolean shared_view_info,
    CreationMeta creation_meta,
    @Cast("bool") boolean allow_tensor_metadata_change/*=true*/);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_differentiable_view(
    @Const @ByRef Tensor data,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer backward_info,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer forward_info,
    @Cast("bool") boolean shared_view_info,
    CreationMeta creation_meta);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_differentiable_view(
    @Const @ByRef Tensor data,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer backward_info,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer forward_info,
    @Cast("bool") boolean shared_view_info,
    @Cast("torch::autograd::CreationMeta") byte creation_meta,
    @Cast("bool") boolean allow_tensor_metadata_change/*=true*/);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_differentiable_view(
    @Const @ByRef Tensor data,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer backward_info,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer forward_info,
    @Cast("bool") boolean shared_view_info,
    @Cast("torch::autograd::CreationMeta") byte creation_meta);

// See NOTE [ Autograd View Variables ] for details.
// Non-differentiable view. Just share version counter.

///
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_non_differentiable_view(
    @ByVal @Cast("torch::autograd::Variable*") Tensor base,
    @Const @ByRef Tensor data,
    @Cast("bool") boolean allow_tensor_metadata_change/*=true*/);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_non_differentiable_view(
    @ByVal @Cast("torch::autograd::Variable*") Tensor base,
    @Const @ByRef Tensor data);

/** Creates a {@code Variable} from the given {@code Tensor}, copying its underlying
 *  {@code TensorImpl}. {@code requires_grad} should be set only for leaves, and determines
 *  whether the {@code Variable} will accumulate gradients. NOTE: {@code data} must *not* be
 *  a {@code Variable} already. Its dynamic type *must* be {@code Tensor}.
 * 
 *  TODO: Eliminate this function as much as possible, as it can be expressed
 *  more clearly as detach() or a no-op in most call sites (especially when
 *  there is only one use of the variable). */
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable(
    @ByVal Tensor data,
    @Cast("bool") boolean requires_grad/*=false*/,
    @Cast("bool") boolean allow_tensor_metadata_change/*=true*/);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable(
    @ByVal Tensor data);

/** Creates a {@code Variable} from the given {@code Tensor}, copying its underlying
 *  {@code TensorImpl}. {@code gradient_edge} should be a (function, input_nr) pair
 *  specifying the function in the autograd graph, and what particular input of
 *  that function, this variable is connected to. */
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable(
    @ByVal Tensor data,
    @ByVal Edge gradient_edge,
    @Cast("bool") boolean allow_tensor_metadata_change/*=true*/);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable(
    @ByVal Tensor data,
    @ByVal Edge gradient_edge);

@Namespace("torch::autograd::utils") public static native @Cast("bool") boolean has_same_meta(@Cast("const torch::autograd::Variable*") @ByRef Tensor base, @Cast("const torch::autograd::Variable*") @ByRef Tensor other);

 // namespace utils
 // namespace autograd
 // namespace torch

// #endif /* DOXYGEN_SHOULD_SKIP_THIS */


// Parsed from torch/csrc/autograd/function.h

// #pragma once

// #include <torch/csrc/autograd/anomaly_mode.h>
// #include <torch/csrc/autograd/edge.h>
// #include <torch/csrc/autograd/grad_mode.h>
// #include <torch/csrc/autograd/graph_task.h>
// #include <torch/csrc/autograd/input_metadata.h>
// #include <torch/csrc/autograd/saved_variable.h>
// #include <torch/csrc/autograd/variable.h>
// #include <torch/csrc/utils/python_stub.h>
// #include <torch/csrc/utils/variadic.h>

// #include <ATen/SequenceNumber.h>
// #include <ATen/core/Tensor.h>
// #include <ATen/record_function.h>
// #include <c10/util/Exception.h>
// #include <c10/util/irange.h>

// #include <algorithm>
// #include <cstdint>
// #include <initializer_list>
// #include <memory>
// #include <string>
// #include <utility>
// #include <vector>

// #if C10_CLANG_HAS_WARNING("-Wshorten-64-to-32")
// #endif

// Custom deleter to prevent stack overflows.
@Namespace("torch::autograd") public static native void deleteNode(Node function);
// Targeting ../NodeGuard.java



// Return the Node currently being evaluated (if any)
// This is only set during the backward pass while a Node is being
// executed.
@Namespace("torch::autograd") public static native @SharedPtr Node get_current_node();
// Targeting ../Node.java


// Targeting ../TraceableFunction.java



//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
//                       Associated Free Nodes
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Implementation of `collect_next_edges` (see below).
 // namespace detail

/** Create an {@code Edge} between the given {@code variable} and the {@code function}, which is
 *  assumed to be the gradient function of this variable (i.e. the function
 *  through which this variable is backpropagated during the backward pass).
 *  This sets the {@code grad_fn} property of the {@code variable}. This function assumes
 *  that the {@code Variable} is a new input to the gradient function and its
 *  {@code input_nr} thus equal to {@code function->num_inputs()}. Additionally, it
 *  increments the {@code Node}'s number of inputs by one. Approximately
 *  equivalent to {@code variable.set_gradient_edge(function,
 *  function->add_input_metadata(variable.dispatch_type(), variable.sizes()))}.
 *  If you don't want the {@code Node}'s {@code num_inputs} to be incremented, use
 *  {@code set_gradient_edge} directly. */
@Namespace("torch::autograd") public static native void create_gradient_edge(
    @Cast("torch::autograd::Variable*") @ByRef Tensor variable,
    @SharedPtr Node function);

/** Return true if any of the variables in the list require a gradient. */
@Namespace("torch::autograd") public static native @Cast("bool") boolean any_variable_requires_grad(@Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector variables);

/** Return the next edges of all the given variables, or tuples of variables. */
 // namespace autograd
 // namespace torch



// Parsed from torch/csrc/autograd/custom_function.h

// #pragma once

// #include <ATen/core/ivalue.h>
// #include <c10/util/flat_hash_map.h>
// #include <c10/util/irange.h>
// #include <torch/csrc/autograd/function.h>
// #include <torch/csrc/autograd/variable.h>
// #include <vector>

@Namespace("torch::autograd") public static native @ByVal TensorOptionalVector _wrap_outputs(
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector input_vars,
    @Const @ByRef TensorImplSet non_differentiable,
    @Const @ByRef TensorImplSet dirty_inputs,
    @Const @ByVal TensorOptionalArrayRef raw_outputs,
    @SharedPtr Node cdata,
    @ByVal @Cast("torch::autograd::_jvp_fn_t*") Pointer jvp_user_function);

@Namespace("torch::autograd") public static native void check_variable_result(
    @Const @ByRef TensorBase original,
    @Const @ByRef TensorBase result,
    @StdString BytePointer hook_name);
@Namespace("torch::autograd") public static native void check_variable_result(
    @Const @ByRef TensorBase original,
    @Const @ByRef TensorBase result,
    @StdString String hook_name);

// Get the return type of the forward function of the custom Function class X

///
///
///
///
///

/** To use custom autograd operations, implement a Function subclass with
 *  static forward and backward functions:
 * 
 *  {@code forward} can take as many arguments as you want and should return either a
 *  variable list or a Variable. Use of any direct Variable arguments will be
 *  registered in the graph but no vectors/sets or any other data structures
 *  will be traversed. You can use c10::optional<Tensor> as one of the arguments
 *  and it will be registered as a variable in the graph if the argument has a
 *  value. It should take a pointer to {@code torch::autograd::AutogradContext} as the
 *  first argument. Variables can be saved in the {@code ctx} using
 *  {@code ctx->save_for_backward}
 *  (see {@code torch::autograd::AutogradContext::save_for_backward}) and other data
 *  can be saved in the {@code ctx->saved_data} map
 *  (see {@code torch::autograd::AutogradContext::saved_data})
 *  in the form of {@code <std::string, at::IValue>} pairs.
 * 
 *  {@code backward} should take a pointer to {@code torch::autograd::AutogradContext}
 *  and a variable list containing as many Variables as there were outputs from
 *  {@code forward} as arguments. It should return as many Variables as there were
 *  inputs with each of them containing the gradient w.r.t. its corresponding
 *  input. Variables saved in {@code forward} can be accessed with
 *  {@code ctx->get_saved_variables} (see
 *  {@code torch::autograd::AutogradContext::get_saved_variables}) and other saved
 *  data can be accessed from {@code ctx->saved_data}.
 * 
 *  For example:
 *  <pre>{@code
 *  class MyFunction : public Function<MyFunction> {
 *    public:
 *    static variable_list forward(AutogradContext *ctx, int n, Variable var) {
 *       // Save data for backward in context
 *       ctx->saved_data["n"] = n;
 *       var.mul_(2);
 *       // Mark var as modified by inplace operation
 *       ctx->mark_dirty({var});
 *       return {var};
 *    }
 * 
 *    static variable_list backward(AutogradContext *ctx, variable_list
 *    grad_output) {
 *       // Use data saved in forward
 *       auto n = ctx->saved_data["n"].toInt();
 *       return {grad_output[0]*n};
 *    }
 *  };
 *  }</pre>
 * 
 *  To use {@code MyFunction}:
 *  <pre>{@code
 *  Variable x;
 *  auto y = MyFunction::apply(6, x);
 *  // Example backward call
 *  y[0].sum().backward();
 *  }</pre> */
// Targeting ../AutogradContext.java


// Targeting ../VariableInfo.java



// CppNode<T> is the Node in the autograd graph that represents the user defined
// backward function for Function<T>. Calls to CppNode::apply are forward to
// T::backward().

@Namespace("torch::autograd") public static native @ByVal TensorOptionalVector to_optional(@Cast("torch::autograd::Variable*") @ByRef Tensor output);

@Namespace("torch::autograd") public static native @ByVal TensorOptionalVector to_optional(@ByRef TensorVector output);



// The logic here is the same as PyNode::apply, so changes to it should be done
// in both the places








 // namespace autograd
 // namespace torch


// Parsed from torch/csrc/autograd/autograd.h

// #pragma once

// #include <torch/csrc/autograd/variable.h>

/** Computes the sum of gradients of given tensors with respect to graph leaves.
 * 
 *  The graph is differentiated using the chain rule. If any of {@code }tensors{@code }
 *  are non-scalar (i.e. their data has more than one element) and require
 *  gradient, then the Jacobian-vector product would be computed, in this case
 *  the function additionally requires specifying {@code grad_tensors}. It should be a
 *  sequence of matching length, that contains the "vector" in the
 *  Jacobian-vector product, usually the gradient of the differentiated function
 *  w.r.t. corresponding tensors
 *  ({@code torch::Tensor()} is an acceptable value for all tensors that don't need
 *  gradient tensors).
 * 
 *  This function accumulates gradients in the leaves - you might need to zero
 *  them before calling it.
 * 
 *  @param tensors Tensors of which the derivative will be computed.
 *  @param grad_tensors The "vector" in the Jacobian-vector product, usually
 *  gradients
 *      w.r.t. each element of corresponding tensors. {@code torch::Tensor()} values
 *      can be specified for scalar Tensors or ones that don't require grad. If
 *      a {@code torch::Tensor()} value would be acceptable for all grad_tensors, then
 *      this argument is optional.
 *  @param retain_graph If {@code false}, the graph used to compute the grad will be
 *  freed.
 *      Note that in nearly all cases setting this option to {@code true} is not
 *      needed and often can be worked around in a much more efficient way.
 *      Defaults to the value of {@code create_graph}.
 *  @param create_graph If {@code true}, graph of the derivative will be constructed,
 *  allowing
 *      to compute higher order derivative products. Defaults to {@code false}.
 *  @param inputs Inputs w.r.t. which the gradient will be accumulated into
 *      {@code at::Tensor::grad}. All other Tensors will be ignored. If not provided,
 *      the gradient is accumulated into all the leaf Tensors that were used to
 *      compute param {@code tensors}. */
//      When inputs are provided and a given input is not a leaf,
//      the current implementation will call its grad_fn (even though it is not
//      strictly needed to get this gradients). It is an implementation detail
//      on which the user should not rely. See
//      https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for
//      more details.

///
///
@Namespace("torch::autograd") public static native void backward(
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensors,
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector grad_tensors/*={}*/,
    @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional retain_graph,
    @Cast("bool") boolean create_graph/*=false*/,
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector inputs/*={}*/);
@Namespace("torch::autograd") public static native void backward(
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensors);

/** Computes and returns the sum of gradients of outputs with respect to the
 *  inputs.
 * 
 *  {@code }grad_outputs{@code } should be a sequence of length matching {@code }output{@code }
 *  containing the "vector" in Jacobian-vector product, usually the pre-computed
 *  gradients w.r.t. each of the outputs. If an output doesn't require_grad,
 *  then the gradient can be {@code }torch::Tensor(){@code }).
 * 
 *  @param outputs outputs of the differentiated function.
 *  @param inputs Inputs w.r.t. which the gradient will be
 *      returned (and not accumulated into {@code }at::Tensor::grad{@code }).
 *  @param grad_outputs The "vector" in the Jacobian-vector product.
 *      Usually gradients w.r.t. each output. {@code torch::Tensor()} values can be
 *      specified for scalar Tensors or ones that don't require grad. If a
 *      {@code torch::Tensor()} value would be acceptable for all grad_tensors, then
 *      this argument is optional. Default: {@code {}}.
 *  @param retain_graph If {@code }false{@code }, the graph used to compute the grad
 *      will be freed. Note that in nearly all cases setting this option to
 *      {@code }true{@code } is not needed and often can be worked around in a much more
 *      efficient way. Defaults to the value of {@code }create_graph{@code }.
 *  @param create_graph If {@code }true{@code }, graph of the derivative will
 *      be constructed, allowing to compute higher order derivative products.
 *      Default: {@code }false{@code }.
 *  @param allow_unused If {@code }false{@code }, specifying inputs that were not
 *      used when computing outputs (and therefore their grad is always zero)
 *      is an error. Defaults to {@code }false{@code }. */
@Namespace("torch::autograd") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector grad(
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector outputs,
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector inputs,
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector grad_outputs/*={}*/,
    @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional retain_graph,
    @Cast("bool") boolean create_graph/*=false*/,
    @Cast("bool") boolean allow_unused/*=false*/);
@Namespace("torch::autograd") public static native @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector grad(
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector outputs,
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector inputs);

/** Creates a new dual level and returns its index. This level index should then
 *  be used to call into the other functions below. This API supports entering a
 *  new level before the previous one is exited. We call them nested forward AD
 *  levels. These can be used to compute higher order derivatives. */
@Namespace("torch::autograd::forward_ad") public static native @Cast("uint64_t") long enter_dual_level();

/** Exits the given level. This will clear up all the gradients from this level
 *  and all dual Tensors that had gradients for this level will become regular
 *  Tensors again. This function can only be used to exit the innermost nesting
 *  level and so exiting must happen in reverse order compared to the entering
 *  that was done with the function above. */
@Namespace("torch::autograd::forward_ad") public static native void exit_dual_level(@Cast("uint64_t") long level);

 // namespace forward_ad
 // namespace autograd
 // namespace torch


// Parsed from torch/csrc/autograd/generated/VariableType.h

// #pragma once

// @generated from ../tools/autograd/templates/VariableType.h

// #include <ATen/core/Tensor.h>
// #include <ATen/Context.h>

// #include <c10/util/intrusive_ptr.h>

// #include <torch/csrc/Export.h>
// #include <torch/csrc/autograd/autograd_not_implemented_fallback.h>

// #include <cstdint> // for size_t
// #include <functional> // for function
// #include <memory> // for unique_ptr
// #include <string>
// #include <vector>
// Targeting ../Quantizer.java



// This is temporary typedef to enable Quantizer in aten native function API
// we'll remove them when we are actually exposing Quantizer class
// to frontend
  @Namespace("torch::autograd::VariableType") public static native @Cast("at::DeprecatedTypeProperties**") @StdVector PointerPointer allCUDATypes();
  @Namespace("torch::autograd::VariableType") public static native @Cast("at::DeprecatedTypeProperties**") @StdVector PointerPointer allCPUTypes();

  
  
  
  


 // namespace torch::autograd


// Parsed from torch/csrc/autograd/generated/variable_factories.h

// #pragma once

// @generated from ../tools/autograd/templates/variable_factories.h

// #include <ATen/core/Tensor.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/grad_mode.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/core/MemoryFormat.h>
// #include <torch/csrc/api/include/torch/detail/TensorDataContainer.h>
// #include <torch/csrc/autograd/variable.h>

// #ifndef AT_PER_OPERATOR_HEADERS
// #include <ATen/Functions.h>
// #else
// #include <ATen/ops/from_blob.h>
// #include <ATen/ops/_cudnn_init_dropout_state.h>
// #include <ATen/ops/arange.h>
// #include <ATen/ops/arange.h>
// #include <ATen/ops/arange.h>
// #include <ATen/ops/bartlett_window.h>
// #include <ATen/ops/bartlett_window.h>
// #include <ATen/ops/blackman_window.h>
// #include <ATen/ops/blackman_window.h>
// #include <ATen/ops/empty.h>
// #include <ATen/ops/empty.h>
// #include <ATen/ops/_empty_affine_quantized.h>
// #include <ATen/ops/_empty_per_channel_affine_quantized.h>
// #include <ATen/ops/empty_quantized.h>
// #include <ATen/ops/empty_like.h>
// #include <ATen/ops/empty_strided.h>
// #include <ATen/ops/eye.h>
// #include <ATen/ops/eye.h>
// #include <ATen/ops/full.h>
// #include <ATen/ops/full.h>
// #include <ATen/ops/full_like.h>
// #include <ATen/ops/from_file.h>
// #include <ATen/ops/hann_window.h>
// #include <ATen/ops/hann_window.h>
// #include <ATen/ops/hamming_window.h>
// #include <ATen/ops/hamming_window.h>
// #include <ATen/ops/hamming_window.h>
// #include <ATen/ops/hamming_window.h>
// #include <ATen/ops/kaiser_window.h>
// #include <ATen/ops/kaiser_window.h>
// #include <ATen/ops/kaiser_window.h>
// #include <ATen/ops/linspace.h>
// #include <ATen/ops/logspace.h>
// #include <ATen/ops/ones.h>
// #include <ATen/ops/ones.h>
// #include <ATen/ops/ones_like.h>
// #include <ATen/ops/scalar_tensor.h>
// #include <ATen/ops/rand.h>
// #include <ATen/ops/rand.h>
// #include <ATen/ops/rand.h>
// #include <ATen/ops/rand.h>
// #include <ATen/ops/rand_like.h>
// #include <ATen/ops/randint.h>
// #include <ATen/ops/randint.h>
// #include <ATen/ops/randint.h>
// #include <ATen/ops/randint.h>
// #include <ATen/ops/randint_like.h>
// #include <ATen/ops/randint_like.h>
// #include <ATen/ops/randn.h>
// #include <ATen/ops/randn.h>
// #include <ATen/ops/randn.h>
// #include <ATen/ops/randn.h>
// #include <ATen/ops/randn_like.h>
// #include <ATen/ops/randperm.h>
// #include <ATen/ops/randperm.h>
// #include <ATen/ops/range.h>
// #include <ATen/ops/range.h>
// #include <ATen/ops/zeros.h>
// #include <ATen/ops/_efficientzerotensor.h>
// #include <ATen/ops/zeros.h>
// #include <ATen/ops/zeros_like.h>
// #include <ATen/ops/sparse_compressed_tensor.h>
// #include <ATen/ops/sparse_csr_tensor.h>
// #include <ATen/ops/sparse_csc_tensor.h>
// #include <ATen/ops/sparse_bsr_tensor.h>
// #include <ATen/ops/sparse_bsc_tensor.h>
// #include <ATen/ops/sparse_compressed_tensor.h>
// #include <ATen/ops/sparse_csr_tensor.h>
// #include <ATen/ops/sparse_csc_tensor.h>
// #include <ATen/ops/sparse_bsr_tensor.h>
// #include <ATen/ops/sparse_bsc_tensor.h>
// #include <ATen/ops/_sparse_compressed_tensor_unsafe.h>
// #include <ATen/ops/_sparse_csr_tensor_unsafe.h>
// #include <ATen/ops/_sparse_csc_tensor_unsafe.h>
// #include <ATen/ops/_sparse_bsr_tensor_unsafe.h>
// #include <ATen/ops/_sparse_bsc_tensor_unsafe.h>
// #include <ATen/ops/sparse_coo_tensor.h>
// #include <ATen/ops/sparse_coo_tensor.h>
// #include <ATen/ops/sparse_coo_tensor.h>
// #include <ATen/ops/_sparse_coo_tensor_unsafe.h>
// #include <ATen/ops/_sparse_coo_tensor_with_dims.h>
// #include <ATen/ops/_sparse_coo_tensor_with_dims_and_tensors.h>
// #include <ATen/ops/_to_copy.h>
// #include <ATen/ops/tril_indices.h>
// #include <ATen/ops/triu_indices.h>
// #include <ATen/ops/normal.h>
// #include <ATen/ops/fft_fftfreq.h>
// #include <ATen/ops/fft_rfftfreq.h>
// #endif

// #include <functional>
// #include <initializer_list>
// #include <utility>

/** NOTE: Currently {@code torch::tensor(...)} doesn't support mixed data types
 *  (i.e. {@code torch::tensor({{bool, 2.0}})} doesn't work). We might be able to
 *  support it in the future by iterating over all sub-lists to find
 *  the largest data type that can represent all of the elements, or by using
 *  variadic templates.
 * 
 *  NOTE: C++ {@code torch::tensor} with a floating-point type or an {@code at::ArrayRef} / {@code std::vector} /
 *  (nested) braced-init-list of floating-point types always produces a tensor of dtype
 *  {@code torch::get_default_dtype()}, matching Python {@code torch.tensor} behavior.
 * 
 *  NOTE: C++ {@code torch::tensor} with an integer type or an {@code at::ArrayRef} / {@code std::vector} /
 *  (nested) braced-init-list of integer types always produces a tensor of dtype {@code at::kLong}
 *  (aka. int64_t), matching Python {@code torch.tensor} behavior.
 * 
 *  NOTE: The following dtypes are not supported by {@code torch::tensor} currently:
 *  - {@code unsigned int}
 *  - {@code unsigned long int}
 *  - {@code unsigned long long int}
 *  - {@code long long int} */
@Namespace("torch") public static native @ByVal Tensor tensor(@ByVal @Cast("torch::detail::TensorDataContainer*") Pointer tensor_data_container, @Const @ByRef(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal Tensor tensor(@ByVal @Cast("torch::detail::TensorDataContainer*") Pointer tensor_data_container);

/** A generic deleter function. */

/** Exposes the given {@code data} as a {@code Tensor} without taking ownership of the
 *  original data. {@code sizes} should specify the shape of the tensor, {@code strides} the
 *  stride in each dimension. The {@code deleter} function (a
 *  {@code std::function<void(void*)>}) will be called on the {@code data} when the Tensor
 *  data would normally be deallocated. The {@code TensorOptions} specify additional
 *  configuration options for the returned tensor, such as what type to
 *  interpret the {@code data} as. */
@Namespace("torch") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @Const @ByRef Deleter deleter,
    @Const @ByRef(nullValue = "at::TensorOptions()") TensorOptions options);
@Namespace("torch") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @ByRef @Cast("void(*)(void*)") Pointer deleter,
    @Const @ByRef(nullValue = "at::TensorOptions()") TensorOptions options);
@Namespace("torch") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @ByRef @Cast("void(*)(void*)") long deleter,
    @Const @ByRef(nullValue = "at::TensorOptions()") TensorOptions options);
@Namespace("torch") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @Const @ByRef Deleter deleter,
    @Const @ByRef(nullValue = "at::TensorOptions()") TensorOptions options);
@Namespace("torch") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @ByRef @Cast("void(*)(void*)") Pointer deleter,
    @Const @ByRef(nullValue = "at::TensorOptions()") TensorOptions options);
@Namespace("torch") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @ByRef @Cast("void(*)(void*)") long deleter,
    @Const @ByRef(nullValue = "at::TensorOptions()") TensorOptions options);

/** Exposes the given {@code data} as a {@code Tensor} without taking ownership of the
 *  original data. {@code sizes} should specify the shape of the tensor, {@code strides} the
 *  stride in each dimension. The {@code TensorOptions}
 *  specify additional configuration options for the returned tensor, such as
 *  what type to interpret the {@code data} as. */

/** Exposes the given {@code data} as a {@code Tensor} without taking ownership of the
 *  original data. {@code sizes} should specify the shape of the tensor. The {@code deleter}
 *  (a {@code std::function<void(void*)>}) function will be called on the {@code data} when
 *  the Tensor data would normally be deallocated. The {@code TensorOptions} specify
 *  additional configuration options for the returned tensor, such as what type
 *  to interpret the {@code data} as. */

/** Exposes the given {@code data} as a {@code Tensor} without taking ownership of the
 *  original data. {@code sizes} should specify the shape of the tensor. The
 *  {@code TensorOptions} specify additional configuration options for the returned
 *  tensor, such as what type to interpret the {@code data} as. */

@Namespace("torch") public static native @ByVal @Name("_cudnn_init_dropout_state") Tensor torch__cudnn_init_dropout_state(double dropout, @Cast("bool") boolean train, @Cast("int64_t") long dropout_seed, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("arange") Tensor torch_arange(@Const @ByRef Scalar end, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("arange") Tensor torch_arange(@Const @ByRef Scalar end);
@Namespace("torch") public static native @ByVal @Name("arange") Tensor torch_arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("arange") Tensor torch_arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end);
@Namespace("torch") public static native @ByVal @Name("arange") Tensor torch_arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("arange") Tensor torch_arange(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef Scalar step);
@Namespace("torch") public static native @ByVal @Name("bartlett_window") Tensor torch_bartlett_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("bartlett_window") Tensor torch_bartlett_window(@Cast("int64_t") long window_length);
@Namespace("torch") public static native @ByVal @Name("bartlett_window") Tensor torch_bartlett_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("bartlett_window") Tensor torch_bartlett_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
@Namespace("torch") public static native @ByVal @Name("blackman_window") Tensor torch_blackman_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("blackman_window") Tensor torch_blackman_window(@Cast("int64_t") long window_length);
@Namespace("torch") public static native @ByVal @Name("blackman_window") Tensor torch_blackman_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("blackman_window") Tensor torch_blackman_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
@Namespace("torch") public static native @ByVal @Name("empty") Tensor torch_empty(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("empty") Tensor torch_empty(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("empty") Tensor torch_empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("empty") Tensor torch_empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("empty") Tensor torch_empty(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("empty") Tensor torch_empty(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("empty") Tensor torch_empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("empty") Tensor torch_empty(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("torch") public static native @ByVal @Name("_empty_affine_quantized") Tensor torch__empty_affine_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, double scale/*=1*/, @Cast("int64_t") long zero_point/*=0*/, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("_empty_affine_quantized") Tensor torch__empty_affine_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("_empty_affine_quantized") Tensor torch__empty_affine_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, double scale/*=1*/, @Cast("int64_t") long zero_point/*=0*/, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("_empty_affine_quantized") Tensor torch__empty_affine_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("torch") public static native @ByVal @Name("_empty_per_channel_affine_quantized") Tensor torch__empty_per_channel_affine_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("_empty_per_channel_affine_quantized") Tensor torch__empty_per_channel_affine_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis);
@Namespace("torch") public static native @ByVal @Name("_empty_per_channel_affine_quantized") Tensor torch__empty_per_channel_affine_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("_empty_per_channel_affine_quantized") Tensor torch__empty_per_channel_affine_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis);
@Namespace("torch") public static native @ByVal @Name("empty_quantized") Tensor torch_empty_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor qtensor, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("empty_quantized") Tensor torch_empty_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor qtensor);
@Namespace("torch") public static native @ByVal @Name("empty_quantized") Tensor torch_empty_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor qtensor, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("empty_quantized") Tensor torch_empty_quantized(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor qtensor);
@Namespace("torch") public static native @ByVal @Name("empty_like") Tensor torch_empty_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("empty_like") Tensor torch_empty_like(@Const @ByRef Tensor self);
@Namespace("torch") public static native @ByVal @Name("empty_strided") Tensor torch_empty_strided(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("empty_strided") Tensor torch_empty_strided(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("torch") public static native @ByVal @Name("empty_strided") Tensor torch_empty_strided(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("empty_strided") Tensor torch_empty_strided(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... stride);
@Namespace("torch") public static native @ByVal @Name("eye") Tensor torch_eye(@Cast("int64_t") long n, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("eye") Tensor torch_eye(@Cast("int64_t") long n);
@Namespace("torch") public static native @ByVal @Name("eye") Tensor torch_eye(@Cast("int64_t") long n, @Cast("int64_t") long m, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("eye") Tensor torch_eye(@Cast("int64_t") long n, @Cast("int64_t") long m);
@Namespace("torch") public static native @ByVal @Name("full") Tensor torch_full(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("full") Tensor torch_full(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("full") Tensor torch_full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("full") Tensor torch_full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("full") Tensor torch_full(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("full") Tensor torch_full(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Scalar fill_value);
@Namespace("torch") public static native @ByVal @Name("full") Tensor torch_full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("full") Tensor torch_full(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Scalar fill_value);
@Namespace("torch") public static native @ByVal @Name("full_like") Tensor torch_full_like(@Const @ByRef Tensor self, @Const @ByRef Scalar fill_value, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("full_like") Tensor torch_full_like(@Const @ByRef Tensor self, @Const @ByRef Scalar fill_value);
@Namespace("torch") public static native @ByVal @Name("from_file") Tensor torch_from_file(@ByVal @Cast("c10::string_view*") Pointer filename, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional shared, @ByVal(nullValue = "c10::optional<int64_t>(0)") LongOptional size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("from_file") Tensor torch_from_file(@ByVal @Cast("c10::string_view*") Pointer filename);
@Namespace("torch") public static native @ByVal @Name("hann_window") Tensor torch_hann_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("hann_window") Tensor torch_hann_window(@Cast("int64_t") long window_length);
@Namespace("torch") public static native @ByVal @Name("hann_window") Tensor torch_hann_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("hann_window") Tensor torch_hann_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
@Namespace("torch") public static native @ByVal @Name("hamming_window") Tensor torch_hamming_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("hamming_window") Tensor torch_hamming_window(@Cast("int64_t") long window_length);
@Namespace("torch") public static native @ByVal @Name("hamming_window") Tensor torch_hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("hamming_window") Tensor torch_hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
@Namespace("torch") public static native @ByVal @Name("hamming_window") Tensor torch_hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("hamming_window") Tensor torch_hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha);
@Namespace("torch") public static native @ByVal @Name("hamming_window") Tensor torch_hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, double beta, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("hamming_window") Tensor torch_hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, double beta);
@Namespace("torch") public static native @ByVal @Name("kaiser_window") Tensor torch_kaiser_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("kaiser_window") Tensor torch_kaiser_window(@Cast("int64_t") long window_length);
@Namespace("torch") public static native @ByVal @Name("kaiser_window") Tensor torch_kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("kaiser_window") Tensor torch_kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
@Namespace("torch") public static native @ByVal @Name("kaiser_window") Tensor torch_kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double beta, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("kaiser_window") Tensor torch_kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double beta);
@Namespace("torch") public static native @ByVal @Name("linspace") Tensor torch_linspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("linspace") Tensor torch_linspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps);
@Namespace("torch") public static native @ByVal @Name("logspace") Tensor torch_logspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps, double base/*=10.0*/, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("logspace") Tensor torch_logspace(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Cast("int64_t") long steps);
@Namespace("torch") public static native @ByVal @Name("ones") Tensor torch_ones(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("ones") Tensor torch_ones(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("ones") Tensor torch_ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("ones") Tensor torch_ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("ones") Tensor torch_ones(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("ones") Tensor torch_ones(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("ones") Tensor torch_ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("ones") Tensor torch_ones(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("torch") public static native @ByVal @Name("ones_like") Tensor torch_ones_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("ones_like") Tensor torch_ones_like(@Const @ByRef Tensor self);
@Namespace("torch") public static native @ByVal @Name("scalar_tensor") Tensor torch_scalar_tensor(@Const @ByRef Scalar s, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("scalar_tensor") Tensor torch_scalar_tensor(@Const @ByRef Scalar s);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("rand") Tensor torch_rand(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);
@Namespace("torch") public static native @ByVal @Name("rand_like") Tensor torch_rand_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("rand_like") Tensor torch_rand_like(@Const @ByRef Tensor self);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randint") Tensor torch_randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);
@Namespace("torch") public static native @ByVal @Name("randint_like") Tensor torch_randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long high, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("randint_like") Tensor torch_randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long high);
@Namespace("torch") public static native @ByVal @Name("randint_like") Tensor torch_randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("randint_like") Tensor torch_randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long low, @Cast("int64_t") long high);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randn") Tensor torch_randn(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("randn_like") Tensor torch_randn_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("randn_like") Tensor torch_randn_like(@Const @ByRef Tensor self);
@Namespace("torch") public static native @ByVal @Name("randperm") Tensor torch_randperm(@Cast("int64_t") long n, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randperm") Tensor torch_randperm(@Cast("int64_t") long n);
@Namespace("torch") public static native @ByVal @Name("randperm") Tensor torch_randperm(@Cast("int64_t") long n, @ByVal GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("randperm") Tensor torch_randperm(@Cast("int64_t") long n, @ByVal GeneratorOptional generator);
@Namespace("torch") public static native @ByVal @Name("range") Tensor torch_range(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @Const @ByRef(nullValue = "at::Scalar(1)") Scalar step, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("range") Tensor torch_range(@Const @ByRef Scalar start, @Const @ByRef Scalar end, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("zeros") Tensor torch_zeros(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("zeros") Tensor torch_zeros(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("zeros") Tensor torch_zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("zeros") Tensor torch_zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);
@Namespace("torch") public static native @ByVal @Name("_efficientzerotensor") Tensor torch__efficientzerotensor(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_efficientzerotensor") Tensor torch__efficientzerotensor(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("_efficientzerotensor") Tensor torch__efficientzerotensor(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_efficientzerotensor") Tensor torch__efficientzerotensor(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("torch") public static native @ByVal @Name("zeros") Tensor torch_zeros(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("zeros") Tensor torch_zeros(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("zeros") Tensor torch_zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("zeros") Tensor torch_zeros(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("torch") public static native @ByVal @Name("zeros_like") Tensor torch_zeros_like(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("zeros_like") Tensor torch_zeros_like(@Const @ByRef Tensor self);
@Namespace("torch") public static native @ByVal @Name("sparse_compressed_tensor") Tensor torch_sparse_compressed_tensor(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_compressed_tensor") Tensor torch_sparse_compressed_tensor(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_csr_tensor") Tensor torch_sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_csr_tensor") Tensor torch_sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_csc_tensor") Tensor torch_sparse_csc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_csc_tensor") Tensor torch_sparse_csc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_bsr_tensor") Tensor torch_sparse_bsr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_bsr_tensor") Tensor torch_sparse_bsr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_bsc_tensor") Tensor torch_sparse_bsc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_bsc_tensor") Tensor torch_sparse_bsc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_compressed_tensor") Tensor torch_sparse_compressed_tensor(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_csr_tensor") Tensor torch_sparse_csr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_csc_tensor") Tensor torch_sparse_csc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_bsr_tensor") Tensor torch_sparse_bsr_tensor(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_bsc_tensor") Tensor torch_sparse_bsc_tensor(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_compressed_tensor_unsafe") Tensor torch__sparse_compressed_tensor_unsafe(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_compressed_tensor_unsafe") Tensor torch__sparse_compressed_tensor_unsafe(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("_sparse_compressed_tensor_unsafe") Tensor torch__sparse_compressed_tensor_unsafe(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_compressed_tensor_unsafe") Tensor torch__sparse_compressed_tensor_unsafe(@Const @ByRef Tensor compressed_indices, @Const @ByRef Tensor plain_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("torch") public static native @ByVal @Name("_sparse_csr_tensor_unsafe") Tensor torch__sparse_csr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_csr_tensor_unsafe") Tensor torch__sparse_csr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("_sparse_csr_tensor_unsafe") Tensor torch__sparse_csr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_csr_tensor_unsafe") Tensor torch__sparse_csr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("torch") public static native @ByVal @Name("_sparse_csc_tensor_unsafe") Tensor torch__sparse_csc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_csc_tensor_unsafe") Tensor torch__sparse_csc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("_sparse_csc_tensor_unsafe") Tensor torch__sparse_csc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_csc_tensor_unsafe") Tensor torch__sparse_csc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("torch") public static native @ByVal @Name("_sparse_bsr_tensor_unsafe") Tensor torch__sparse_bsr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_bsr_tensor_unsafe") Tensor torch__sparse_bsr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("_sparse_bsr_tensor_unsafe") Tensor torch__sparse_bsr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_bsr_tensor_unsafe") Tensor torch__sparse_bsr_tensor_unsafe(@Const @ByRef Tensor crow_indices, @Const @ByRef Tensor col_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("torch") public static native @ByVal @Name("_sparse_bsc_tensor_unsafe") Tensor torch__sparse_bsc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_bsc_tensor_unsafe") Tensor torch__sparse_bsc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("_sparse_bsc_tensor_unsafe") Tensor torch__sparse_bsc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_bsc_tensor_unsafe") Tensor torch__sparse_bsc_tensor_unsafe(@Const @ByRef Tensor ccol_indices, @Const @ByRef Tensor row_indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("torch") public static native @ByVal @Name("sparse_coo_tensor") Tensor torch_sparse_coo_tensor(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_coo_tensor") Tensor torch_sparse_coo_tensor(@ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_coo_tensor") Tensor torch_sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_coo_tensor") Tensor torch_sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values);
@Namespace("torch") public static native @ByVal @Name("sparse_coo_tensor") Tensor torch_sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_coo_tensor") Tensor torch_sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("sparse_coo_tensor") Tensor torch_sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("sparse_coo_tensor") Tensor torch_sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("torch") public static native @ByVal @Name("_sparse_coo_tensor_unsafe") Tensor torch__sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_coo_tensor_unsafe") Tensor torch__sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("_sparse_coo_tensor_unsafe") Tensor torch__sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_coo_tensor_unsafe") Tensor torch__sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("torch") public static native @ByVal @Name("_sparse_coo_tensor_with_dims") Tensor torch__sparse_coo_tensor_with_dims(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_coo_tensor_with_dims") Tensor torch__sparse_coo_tensor_with_dims(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_coo_tensor_with_dims_and_tensors") Tensor torch__sparse_coo_tensor_with_dims_and_tensors(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_sparse_coo_tensor_with_dims_and_tensors") Tensor torch__sparse_coo_tensor_with_dims_and_tensors(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("_to_copy") Tensor torch__to_copy(@Const @ByRef Tensor self, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options, @Cast("bool") boolean non_blocking/*=false*/, @ByVal(nullValue = "c10::optional<at::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("torch") public static native @ByVal @Name("_to_copy") Tensor torch__to_copy(@Const @ByRef Tensor self);
@Namespace("torch") public static native @ByVal @Name("tril_indices") Tensor torch_tril_indices(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset/*=0*/, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("tril_indices") Tensor torch_tril_indices(@Cast("int64_t") long row, @Cast("int64_t") long col);
@Namespace("torch") public static native @ByVal @Name("triu_indices") Tensor torch_triu_indices(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset/*=0*/, @ByVal(nullValue = "at::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("triu_indices") Tensor torch_triu_indices(@Cast("int64_t") long row, @Cast("int64_t") long col);
@Namespace("torch") public static native @ByVal @Name("normal") Tensor torch_normal(double mean, double std, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("normal") Tensor torch_normal(double mean, double std, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("torch") public static native @ByVal @Name("normal") Tensor torch_normal(double mean, double std, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("normal") Tensor torch_normal(double mean, double std, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("torch") public static native @ByVal @Name("fft_fftfreq") Tensor torch_fft_fftfreq(@Cast("int64_t") long n, double d/*=1.0*/, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("fft_fftfreq") Tensor torch_fft_fftfreq(@Cast("int64_t") long n);
@Namespace("torch") public static native @ByVal @Name("fft_rfftfreq") Tensor torch_fft_rfftfreq(@Cast("int64_t") long n, double d/*=1.0*/, @ByVal(nullValue = "at::TensorOptions{}") TensorOptions options);
@Namespace("torch") public static native @ByVal @Name("fft_rfftfreq") Tensor torch_fft_rfftfreq(@Cast("int64_t") long n);

 // namespace torch


// Parsed from torch/csrc/jit/frontend/function_schema_parser.h

// #pragma once

// #include <ATen/core/function_schema.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/either.h>
// #include <string>


@Namespace("torch::jit") public static native @ByVal FunctionSchema parseSchema(@StdString BytePointer schema);
@Namespace("torch::jit") public static native @ByVal FunctionSchema parseSchema(@StdString String schema);
@Namespace("torch::jit") public static native @ByVal OperatorName parseName(@StdString BytePointer name);
@Namespace("torch::jit") public static native @ByVal OperatorName parseName(@StdString String name);

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/name_mangler.h

// #pragma once

// #include <ATen/core/qualified_name.h>
// #include <torch/csrc/Export.h>
// Targeting ../NameMangler.java



 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/parser_constants.h

// #pragma once
@Namespace("torch::jit") public static native @Cast("const char*") BytePointer valid_single_char_tokens(); public static native void valid_single_char_tokens(BytePointer setter);
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/source_range.h

// #pragma once
// #include <c10/util/Exception.h>
// #include <c10/util/Optional.h>

// #include <algorithm>
// #include <iostream>
// #include <iterator>
// #include <memory>
// #include <numeric>
// #include <unordered_map>
// Targeting ../StringCordView.java


// Targeting ../Source.java


// Targeting ../SourceRange.java


// Targeting ../OwnedSourceRange.java


// Targeting ../SourceRangeHasher.java


// Targeting ../StackEntry.java



@Namespace("torch::jit") public static native void format_stack_trace(
    @Cast("std::ostream*") @ByRef Pointer out,
    @Const @ByRef StackEntryVector entries);

@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef SourceRange range);
// Targeting ../TaggedRange.java



 // namespace jit
 // namespace torch
 // namespace std


// Parsed from torch/csrc/jit/frontend/sugared_value.h

// #pragma once
// #include <c10/util/Optional.h>
// #include <functional>
// #include <memory>
// #include <string>
// #include <utility>

// #include <ATen/core/symbol.h>
// #include <caffe2/serialize/versions.h>
// #include <torch/csrc/jit/api/module.h>
// #include <torch/csrc/jit/frontend/error_report.h>
// #include <torch/csrc/jit/frontend/schema_matching.h>
// #include <torch/csrc/jit/frontend/versioned_symbols.h>
// #include <torch/csrc/jit/ir/ir.h>
// Targeting ../SugaredValue.java


// Targeting ../SimpleValue.java


// Targeting ../BuiltinFunction.java


// Targeting ../SugaredTupleValue.java


// Targeting ../BuiltinModule.java


// Targeting ../ClassValue.java


// Targeting ../NamedTupleConstructor.java


// Targeting ../FunctionValue.java


// Targeting ../ClosureValue.java


// Targeting ../MethodValue.java


// Targeting ../PrintValue.java


// Targeting ../CastValue.java


// Targeting ../TensorCastValue.java


// Targeting ../MagicMethod.java


// Targeting ../SpecialFormValue.java


// Targeting ../LegacyTensorConstructor.java


// Targeting ../RangeValue.java


// Targeting ../IterableTree.java



@Namespace("torch::jit") public static native @ByVal ValueVector toValues(
    @ByRef Graph g,
    @ByVal NamedValueArrayRef nvs);
// Targeting ../SimpleSelf.java


// Targeting ../ExceptionMessageValue.java


// Targeting ../ExceptionValue.java


// Targeting ../SugaredEnumClass.java


// Targeting ../SliceValue.java



 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/resolver.h

// #pragma once

// #include <ATen/core/jit_type.h>
// #include <ATen/core/qualified_name.h>
// #include <torch/csrc/jit/frontend/sugared_value.h>
// Targeting ../Resolver.java


// Targeting ../NativeResolver.java



@Namespace("torch::jit") public static native @SharedPtr NativeResolver nativeResolver();
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/tracer.h

// #pragma once

// #include <ATen/core/Dimname.h>
// #include <ATen/core/class_type.h>
// #include <ATen/core/jit_type.h>
// #include <ATen/core/stack.h>
// #include <ATen/core/symbol.h>
// #include <c10/util/Exception.h>
// #include <torch/csrc/Export.h>

// #include <torch/csrc/jit/frontend/source_range.h>
// #include <torch/csrc/utils/variadic.h>

// #include <cstdint>
// #include <iostream>
// #include <memory>
// #include <mutex>
// #include <unordered_map>
// #include <vector>
// Targeting ../TracingState.java



// This is meant to be used as a thread local place, where we can store extra
// info that gets lost when we call into ATen from Python bindings. One example
// for when this happens is when we get an IntArrayRef argument with e.g. sizes
// for view. When tracing, those might be tensors, which let us encode extra
// data dependencies, but once they get to the ATen call where we actually have
// the tracing logic, they get converted into a raw IntArrayRef, and we loose
// all information. To prevent this, we temporarily stash it in here.
// NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)

// Retrieve or set the current tracing state. Returns a nullptr if tracing is
// disabled.
@Namespace("torch::jit::tracer") public static native @SharedPtr TracingState getTracingState();
@Namespace("torch::jit::tracer") public static native void setTracingState(@SharedPtr TracingState state);

@Namespace("torch::jit::tracer") public static native @Cast("bool") boolean isTracing();
// Targeting ../warn_fn_type.java


@Namespace("torch::jit::tracer") public static native @Cast("const char*") BytePointer WARN_PYTHON_DATAFLOW(); public static native void WARN_PYTHON_DATAFLOW(BytePointer setter);
@Namespace("torch::jit::tracer") public static native @Cast("const char*") BytePointer WARN_CONSTRUCTOR(); public static native void WARN_CONSTRUCTOR(BytePointer setter);
@Namespace("torch::jit::tracer") public static native @Cast("const char*") BytePointer WARN_RESIZE(); public static native void WARN_RESIZE(BytePointer setter);
@Namespace("torch::jit::tracer") public static native @Cast("const char*") BytePointer STRICT_TRACER_MSG(); public static native void STRICT_TRACER_MSG(BytePointer setter);
@Namespace("torch::jit::tracer") public static native void _do_warn(@Cast("const char*") BytePointer _reason, @Cast("const char*") BytePointer _kind);
@Namespace("torch::jit::tracer") public static native void _do_warn(String _reason, String _kind);
@Namespace("torch::jit::tracer") public static native void warn(@Cast("const char*") BytePointer _reason, @Cast("const char*") BytePointer _kind/*=nullptr*/);
@Namespace("torch::jit::tracer") public static native void warn(@Cast("const char*") BytePointer _reason);
@Namespace("torch::jit::tracer") public static native void warn(String _reason, String _kind/*=nullptr*/);
@Namespace("torch::jit::tracer") public static native void warn(String _reason);
@Namespace("torch::jit::tracer") public static native void setWarn(@ByVal @Cast("torch::jit::tracer::warn_fn_type*") warn_fn_type fn);
// Targeting ../NoWarn.java


// Targeting ../WithNestedTracingFrame.java


@Namespace("torch::jit::tracer") public static native void recordSourceLocation(JitNode n);
// Targeting ../V_JitNode.java


@Namespace("torch::jit::tracer") public static native void setRecordSourceLocation(V_JitNode v);

@Namespace("torch::jit::tracer") public static native @ByVal StackEntryVector pythonCallstack();
// Targeting ../StackEntryVector_V.java


@Namespace("torch::jit::tracer") public static native void setPythonCallstack(StackEntryVector_V v);

// Having finished adding a new 'node' to the graph IR 'setValueTrace'
// associates this node with an output variable, so that further operations
// involving this variable know which node in the IR to reference.
@Namespace("torch::jit::tracer") public static native void setValueTrace(@Const @ByRef IValue v, Value value);

@Namespace("torch::jit::tracer") public static native void delValueTrace(@Const @ByRef IValue var);

@Namespace("torch::jit::tracer") public static native @ByVal @Cast("std::function<void()>*") Pointer pauseTracing();

@Namespace("torch::jit::tracer") public static native Value getValueTrace(@Const @ByRef IValue var);



@Namespace("torch::jit::tracer") public static native void abandon();

// NB: those serve both as an intermediate steps in addInputs below,
// as well as the overloads that terminate template recursion
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, @Cast("int64_t") long value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, @Cast("int64_t") long value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, @ByVal SymInt value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, @ByVal SymInt value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @ByVal LongOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @ByVal LongOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, @Cast("bool") boolean value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, @Cast("bool") boolean value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef BoolOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef BoolOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, double value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, double value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef DoubleOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef DoubleOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, @Const @ByRef Scalar value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, @Const @ByRef Scalar value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef ScalarOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef ScalarOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, @Const @ByRef Tensor value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, @Const @ByRef Tensor value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef TensorOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef TensorOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, @ByVal SymIntRef value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, @ByVal SymIntRef value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @ByVal SymIntOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @ByVal SymIntOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef LongArrayRefOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef SymIntArrayRefOptional opt_value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef SymIntArrayRefOptional opt_value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @ByVal TensorArrayRef value,
    @Cast("bool") boolean allow_undefined/*=false*/);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @ByVal TensorArrayRef value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @ByVal TensorArrayRef value,
    @Cast("bool") boolean allow_undefined/*=false*/);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @ByVal TensorArrayRef value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector value,
    @Cast("bool") boolean allow_undefined/*=false*/);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector value,
    @Cast("bool") boolean allow_undefined/*=false*/);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @ByVal @Cast("c10::ArrayRef<c10::intrusive_ptr<c10::ivalue::Object> >*") Pointer value,
    @Const @SharedPtr @ByRef ClassType class_type);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @ByVal @Cast("c10::ArrayRef<c10::intrusive_ptr<c10::ivalue::Object> >*") Pointer value,
    @Const @SharedPtr @ByRef ClassType class_type);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, @ByVal DoubleArrayRef value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, @ByVal DoubleArrayRef value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef DoubleArrayRefOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef DoubleArrayRefOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @ByVal @Cast("const c10::string_view*") Pointer value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @ByVal @Cast("const c10::string_view*") Pointer value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, @ByVal Device value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, @ByVal Device value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, @ByVal Stream stream);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, @ByVal Stream stream);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, @ByVal Layout value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, @ByVal Layout value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, ScalarType value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, ScalarType value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef ScalarTypeOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef ScalarTypeOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef DeviceOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef DeviceOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef LayoutOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef LayoutOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, @Cast("const char*") BytePointer name, @ByVal MemoryFormat value);
@Namespace("torch::jit::tracer") public static native void addInputs(JitNode n, String name, @ByVal MemoryFormat value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @ByVal DimnameListOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @ByVal DimnameListOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef MemoryFormatOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef MemoryFormatOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef GeneratorOptional value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef GeneratorOptional value);

@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    @Cast("const char*") BytePointer name,
    @Const @ByRef BoolVector value);
@Namespace("torch::jit::tracer") public static native void addInputs(
    JitNode n,
    String name,
    @Const @ByRef BoolVector value);

@Namespace("torch::jit::tracer") public static native void ensureUniqueIfOutOfPlaced(
    @Cast("const char*") BytePointer name,
    @Const @ByRef Tensor tensor);
@Namespace("torch::jit::tracer") public static native void ensureUniqueIfOutOfPlaced(
    String name,
    @Const @ByRef Tensor tensor);
@Namespace("torch::jit::tracer") public static native void ensureUniqueIfOutOfPlaced(
    @Cast("const char*") BytePointer name,
    @Const @ByRef TensorOptional tensor);
@Namespace("torch::jit::tracer") public static native void ensureUniqueIfOutOfPlaced(
    String name,
    @Const @ByRef TensorOptional tensor);
@Namespace("torch::jit::tracer") public static native void addOutput(JitNode node, @Const @ByRef Tensor tensor);
@Namespace("torch::jit::tracer") public static native void setOutput(Value value, @Const @ByRef Tensor output);
@Namespace("torch::jit::tracer") public static native void addOutput(JitNode node, @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector list);
@Namespace("torch::jit::tracer") public static native void addOutput(
    JitNode node,
    @Cast("const c10::intrusive_ptr<c10::ivalue::Object>*") @ByRef Pointer output);

@Namespace("torch::jit::tracer") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor getSizeOf(
    @Cast("const torch::autograd::Variable*") @ByRef Tensor var,
    @Cast("int64_t") long dim);

@Namespace("torch::jit::tracer") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor getNumelOf(@Cast("const torch::autograd::Variable*") @ByRef Tensor var);

 // namespace tracer
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/lexer.h

// #pragma once
// #include <c10/macros/Macros.h>
// #include <c10/util/C++17.h>
// #include <c10/util/Exception.h>
// #include <torch/csrc/Export.h>
// #include <torch/csrc/jit/frontend/parser_constants.h>
// #include <torch/csrc/jit/frontend/source_range.h>
// #include <torch/csrc/jit/frontend/strtod.h>
// #include <algorithm>
// #include <clocale>
// #include <cstdlib>
// #include <memory>
// #include <sstream>
// #include <string>
// #include <vector>

// #if C10_CLANG_HAS_WARNING("-Wshorten-64-to-32")
// #endif

// single character tokens are just the character itself '+'
// multi-character tokens need an entry here
// if the third entry is not the empty string, it is used
// in the lexer to match this token.

// These kinds are also used in Tree.h as the kind of the AST node.
// Some kinds TK_APPLY, TK_LIST are only used in the AST and are not seen in the
// lexer.

// #define TC_FORALL_TOKEN_KINDS(_)
//   _(TK_EOF, "eof", "")
//   _(TK_WHITESPACE, "whitespace", "")
//   _(TK_WHITESPACE_EOF, "whitespace_eof", "")
//   _(TK_NUMBER, "number", "")
//   _(TK_NEWLINE, "newline", "")
//   _(TK_INDENT, "indent", "")
//   _(TK_DEDENT, "dedent", "")
//   _(TK_DEF, "def", "def")
//   _(TK_EQUIVALENT, "equivalent", "<=>")
//   _(TK_IDENT, "ident", "")
//   _(TK_STRING, "string", "")
//   _(TK_STRINGLITERAL, "string_literal", "")
//   _(TK_CONST, "const", "")
//   _(TK_LIST, "list", "")
//   _(TK_DICT, "dict", "")
//   _(TK_OPTION, "option", "")
//   _(TK_APPLY, "apply", "")
//   _(TK_COMPREHENSION, "comprehension", "")
//   _(TK_RANGE_CONSTRAINT, "range_constraint", "")
//   _(TK_PARAM, "param", "")
//   _(TK_INFERRED, "inferred", "")
//   _(TK_ACCESS, "access", "")
//   _(TK_ASSIGN, "assign", "")
//   _(TK_AUG_ASSIGN, "aug_assign", "")
//   _(TK_ATTRIBUTE, "attribute", "")
//   _(TK_IF, "if", "if")
//   _(TK_ELSE, "else", "else")
//   _(TK_ELIF, "elif", "elif")
//   _(TK_WHILE, "while", "while")
//   _(TK_EXPR_STMT, "expression statement", "")
//   _(TK_RETURN, "return", "return")
//   _(TK_IS, "is", "is")
//   _(TK_ISNOT, "is not", "is not")
//   _(TK_NE, "ne", "!=")
//   _(TK_EQ, "eq", "==")
//   _(TK_LE, "le", "<=")
//   _(TK_GE, "ge", ">=")
//   _(TK_FLOOR_DIV, "floordiv", "//")
//   _(TK_IF_EXPR, "if", "")
//   _(TK_TRUE, "True", "True")
//   _(TK_FALSE, "False", "False")
//   _(TK_NONE, "None", "None")
//   _(TK_AND, "and", "and")
//   _(TK_OR, "or", "or")
//   _(TK_NOT, "not", "not")
//   _(TK_LSHIFT, "<<", "<<")
//   _(TK_RSHIFT, ">>", ">>")
//   _(TK_CAST, "cast", "")
//   _(TK_PLUS_EQ, "+=", "+=")
//   _(TK_MINUS_EQ, "-=", "-=")
//   _(TK_TIMES_EQ, "*=", "*=")
//   _(TK_DIV_EQ, "/=", "/=")
//   _(TK_MOD_EQ, "%=", "%=")
//   _(TK_BIT_OR_EQ, "|=", "|=")
//   _(TK_BIT_AND_EQ, "&=", "&=")
//   _(TK_BIT_XOR_EQ, "^=", "^=")
//   _(TK_LSHIFT_EQ, "<<=", "<<=")
//   _(TK_RSHIFT_EQ, ">>=", ">>=")
//   _(TK_POW_EQ, "**=", "**=")
//   _(TK_GLOBAL, "global", "global")
//   _(TK_BUILT_IN, "built-in", "")
//   _(TK_SUBSCRIPT, "subscript", "")
//   _(TK_VAR, "variable", "")
//   _(TK_NOTHING, "nothing", "")
//   _(TK_DICT_LITERAL, "dict-literal", "")
//   _(TK_LIST_LITERAL, "list-literal", "")
//   _(TK_TUPLE_LITERAL, "tuple-literal", "")
//   _(TK_FOR, "for", "for")
//   _(TK_IN, "in", "in")
//   _(TK_NOTIN, "not in", "not in")
//   _(TK_STARRED, "starred", "")
//   _(TK_UNARY_MINUS, "unary minus", "")
//   _(TK_POW, "pow operator", "**")
//   _(TK_ARROW, "arrow", "->")
//   _(TK_DECL, "decl", "")
//   _(TK_SLICE_EXPR, "slice expr", "")
//   _(TK_TYPE_COMMENT, "type comment", "# type:")
//   _(TK_RAISE, "raise", "raise")
//   _(TK_ASSERT, "assert", "assert")
//   _(TK_DOTS, "dots", "...")
//   _(TK_LIST_COMP, "list comprehension", "")
//   _(TK_DICT_COMP, "dict comprehension", "")
//   _(TK_BREAK, "break", "break")
//   _(TK_CONTINUE, "continue", "continue")
//   _(TK_DELETE, "del", "del")
//   _(TK_PASS, "pass", "pass")
//   _(TK_CLASS_DEF, "class", "class")
//   _(TK_IMPORT, "import", "import")
//   _(TK_WITH, "with", "with")
//   _(TK_WITH_ITEM, "withitem", "")
//   _(TK_AS, "as", "as")
//   _(TK_PROP, "property", "")
//   _(TK_ELLIPSIS, "Ellipsis", "Ellipsis")
//   _(TK_NONE_TYPE, "NoneType", "NoneType")

@Namespace("torch::jit") public enum TokenKind {
  // we use characters to represent themselves so skip all valid characters
  // before
  // assigning enum values to multi-char tokens.
  TK_DUMMY_START(256),
  TK_EOF(257),
  TK_WHITESPACE(258),
  TK_WHITESPACE_EOF(259),
  TK_NUMBER(260),
  TK_NEWLINE(261),
  TK_INDENT(262),
  TK_DEDENT(263),
  TK_DEF(264),
  TK_EQUIVALENT(265),
  TK_IDENT(266),
  TK_STRING(267),
  TK_STRINGLITERAL(268),
  TK_CONST(269),
  TK_LIST(270),
  TK_DICT(271),
  TK_OPTION(272),
  TK_APPLY(273),
  TK_COMPREHENSION(274),
  TK_RANGE_CONSTRAINT(275),
  TK_PARAM(276),
  TK_INFERRED(277),
  TK_ACCESS(278),
  TK_ASSIGN(279),
  TK_AUG_ASSIGN(280),
  TK_ATTRIBUTE(281),
  TK_IF(282),
  TK_ELSE(283),
  TK_ELIF(284),
  TK_WHILE(285),
  TK_EXPR_STMT(286),
  TK_RETURN(287),
  TK_IS(288),
  TK_ISNOT(289),
  TK_NE(290),
  TK_EQ(291),
  TK_LE(292),
  TK_GE(293),
  TK_FLOOR_DIV(294),
  TK_IF_EXPR(295),
  TK_TRUE(296),
  TK_FALSE(297),
  TK_NONE(298),
  TK_AND(299),
  TK_OR(300),
  TK_NOT(301),
  TK_LSHIFT(302),
  TK_RSHIFT(303),
  TK_CAST(304),
  TK_PLUS_EQ(305),
  TK_MINUS_EQ(306),
  TK_TIMES_EQ(307),
  TK_DIV_EQ(308),
  TK_MOD_EQ(309),
  TK_BIT_OR_EQ(310),
  TK_BIT_AND_EQ(311),
  TK_BIT_XOR_EQ(312),
  TK_LSHIFT_EQ(313),
  TK_RSHIFT_EQ(314),
  TK_POW_EQ(315),
  TK_GLOBAL(316),
  TK_BUILT_IN(317),
  TK_SUBSCRIPT(318),
  TK_VAR(319),
  TK_NOTHING(320),
  TK_DICT_LITERAL(321),
  TK_LIST_LITERAL(322),
  TK_TUPLE_LITERAL(323),
  TK_FOR(324),
  TK_IN(325),
  TK_NOTIN(326),
  TK_STARRED(327),
  TK_UNARY_MINUS(328),
  TK_POW(329),
  TK_ARROW(330),
  TK_DECL(331),
  TK_SLICE_EXPR(332),
  TK_TYPE_COMMENT(333),
  TK_RAISE(334),
  TK_ASSERT(335),
  TK_DOTS(336),
  TK_LIST_COMP(337),
  TK_DICT_COMP(338),
  TK_BREAK(339),
  TK_CONTINUE(340),
  TK_DELETE(341),
  TK_PASS(342),
  TK_CLASS_DEF(343),
  TK_IMPORT(344),
  TK_WITH(345),
  TK_WITH_ITEM(346),
  TK_AS(347),
  TK_PROP(348),
  TK_ELLIPSIS(349),
  TK_NONE_TYPE(350);

    public final int value;
    private TokenKind(int v) { this.value = v; }
    private TokenKind(TokenKind e) { this.value = e.value; }
    public TokenKind intern() { for (TokenKind e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("torch::jit") public static native @StdString BytePointer kindToString(int kind);
@Namespace("torch::jit") public static native int stringToKind(@StdString BytePointer str);
@Namespace("torch::jit") public static native int stringToKind(@StdString String str);

// nested hash tables that indicate char-by-char what is a valid token.
// Targeting ../TokenTrie.java


// Targeting ../SharedParserData.java



@Namespace("torch::jit") public static native @ByRef SharedParserData sharedParserData();
// Targeting ../Token.java


// Targeting ../Lexer.java


 // namespace jit
 // namespace torch



// Parsed from torch/csrc/jit/frontend/strtod.h

// #pragma once

// #include <c10/macros/Macros.h>

@Namespace("torch::jit") public static native double strtod_c(@Cast("const char*") BytePointer nptr, @Cast("char**") PointerPointer endptr);
@Namespace("torch::jit") public static native double strtod_c(@Cast("const char*") BytePointer nptr, @Cast("char**") @ByPtrPtr BytePointer endptr);
@Namespace("torch::jit") public static native double strtod_c(String nptr, @Cast("char**") @ByPtrPtr ByteBuffer endptr);
@Namespace("torch::jit") public static native double strtod_c(@Cast("const char*") BytePointer nptr, @Cast("char**") @ByPtrPtr byte[] endptr);
@Namespace("torch::jit") public static native double strtod_c(String nptr, @Cast("char**") @ByPtrPtr BytePointer endptr);
@Namespace("torch::jit") public static native double strtod_c(@Cast("const char*") BytePointer nptr, @Cast("char**") @ByPtrPtr ByteBuffer endptr);
@Namespace("torch::jit") public static native double strtod_c(String nptr, @Cast("char**") @ByPtrPtr byte[] endptr);
@Namespace("torch::jit") public static native float strtof_c(@Cast("const char*") BytePointer nptr, @Cast("char**") PointerPointer endptr);
@Namespace("torch::jit") public static native float strtof_c(@Cast("const char*") BytePointer nptr, @Cast("char**") @ByPtrPtr BytePointer endptr);
@Namespace("torch::jit") public static native float strtof_c(String nptr, @Cast("char**") @ByPtrPtr ByteBuffer endptr);
@Namespace("torch::jit") public static native float strtof_c(@Cast("const char*") BytePointer nptr, @Cast("char**") @ByPtrPtr byte[] endptr);
@Namespace("torch::jit") public static native float strtof_c(String nptr, @Cast("char**") @ByPtrPtr BytePointer endptr);
@Namespace("torch::jit") public static native float strtof_c(@Cast("const char*") BytePointer nptr, @Cast("char**") @ByPtrPtr ByteBuffer endptr);
@Namespace("torch::jit") public static native float strtof_c(String nptr, @Cast("char**") @ByPtrPtr byte[] endptr);

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/tree.h

// #pragma once

// #include <functional>
// #include <memory>
// #include <unordered_map>
// #include <vector>

// #include <c10/util/SmallVector.h>
// #include <c10/util/intrusive_ptr.h>
// #include <torch/csrc/jit/frontend/lexer.h>

// Trees are used to represent all forms of TC IR, pre- and post-typechecking.
// Rather than have a full class hierarchy for all TC statements, trees are a
// slight variation of Lisp s-expressions. For instance, the expression a*b+1
// is represented as:
// (+ (* (ident a) (ident b)) (const 1))
// Atoms like 'a', 'b', and '1' are represented by subclasses of Tree which
// define stringValue(). Everything else is a Compound object, which has a
// 'kind' that is a token from lexer.h's TokenKind enum. Single-character
// operators like '+' are represented using the character itself (so, add.kind()
// would be '+'). Each Compound object also contains a list of subtrees and is
// associated with a SourceRange for error reporting.
// Memory management of trees is done using intrusive_ptr.
// Targeting ../Tree.java


// Targeting ../JitString.java



@Namespace("torch::jit") public static native @ByVal SourceRange mergeRanges(@ByVal SourceRange c, @Cast("const torch::jit::TreeList*") @ByRef Pointer others);
// Targeting ../Compound.java


// Targeting ../pretty_tree.java



@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @ByVal pretty_tree t_);

@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Cast("const torch::jit::TreeRef*") @ByRef Pointer t);

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/error_report.h

// #pragma once

// #include <c10/util/Optional.h>
// #include <torch/csrc/jit/frontend/tree.h>
// Targeting ../Call.java


// Targeting ../ErrorReport.java



 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/frontend/tree_views.h

// #pragma once
// #include <c10/util/string_utils.h>
// #include <torch/csrc/jit/frontend/error_report.h>
// #include <torch/csrc/jit/frontend/strtod.h>
// #include <torch/csrc/jit/frontend/tree.h>

// #include <c10/util/complex.h>
// #include <functional>
// #include <iostream>
// #include <string>
// #include <utility>
// Targeting ../TreeView.java


// Targeting ../DefMaybe.java


// Targeting ../ExprMaybe.java


// Targeting ../VarMaybe.java


// Targeting ../Ident.java


// Targeting ../Stmt.java


// Targeting ../Expr.java


// Targeting ../Attribute.java


// Targeting ../Param.java


// Targeting ../Decl.java


// Targeting ../Def.java


// Targeting ../Property.java


// Targeting ../ClassDef.java




// Targeting ../If.java


// Targeting ../While.java


// Targeting ../For.java


// Targeting ../ListComp.java


// Targeting ../DictComp.java


// Targeting ../Global.java


// Targeting ../AugAssignKind.java


// Targeting ../AugAssign.java


// Targeting ../Assign.java


// Targeting ../Return.java


// Targeting ../Raise.java


// Targeting ../Assert.java


// Targeting ../Pass.java


// Targeting ../Dots.java


// Targeting ../Break.java


// Targeting ../Continue.java


// Targeting ../ExprStmt.java


// Targeting ../BinOp.java


// Targeting ../UnaryOp.java


// Targeting ../ConstExpr.java


// Targeting ../StringLiteral.java


// Targeting ../Apply.java


// Targeting ../Select.java


// Targeting ../SliceExpr.java


// Targeting ../Subscript.java


// Targeting ../Var.java


// Targeting ../WithItem.java


// Targeting ../With.java


// Targeting ../TernaryIf.java


// Targeting ../ListLiteral.java


// Targeting ../TupleLiteral.java


// Targeting ../DictLiteral.java


// Targeting ../Starred.java


// Targeting ../Delete.java



 // namespace jit
 // namespace torch

 // namespace std


// Parsed from torch/csrc/jit/ir/attributes.h

// #pragma once
// #include <ATen/core/Tensor.h>
// #include <string>
// #include <vector>

// #include <ATen/core/jit_type_base.h>
// #include <ATen/core/symbol.h>

// #include <torch/csrc/Export.h>

@Namespace("torch::jit") @MemberGetter public static native int max_tensor_display_size();

@Name("torch::jit::AttributeKind") public enum JitAttributeKind {
  f(0),
  fs(1),
  c(2),
  cs(3),
  i(4),
  is(5),
  s(6),
  ss(7),
  t(8),
  ts(9),
  g(10),
  gs(11),
  ty(12),
  tys(13),
  ival(14);

    public final int value;
    private JitAttributeKind(int v) { this.value = v; }
    private JitAttributeKind(JitAttributeKind e) { this.value = e.value; }
    public JitAttributeKind intern() { for (JitAttributeKind e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
@Namespace("torch::jit") public static native @Cast("const char*") BytePointer toString(JitAttributeKind kind);
// Targeting ../AttributeValue.java


// Targeting ../GraphAttr.java


// Targeting ../GraphsAttr.java


// Targeting ../IRAttributeError.java


 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/ir/constants.h

// #pragma once
// #include <ATen/core/ivalue.h>
// #include <ATen/core/jit_type.h>
// #include <torch/csrc/Export.h>
// #include <torch/csrc/jit/frontend/source_range.h>
// #include <torch/csrc/jit/ir/scope.h>

// helpers for handling constants in the IR
// - create constant nodes from ints, floats, complex, intlist, Tensors, and
// other types
// - implement primitive constant ops.

// thrown when insertConstant cannot encode the IValue into a graph

@Namespace("torch::jit") public static native Value insertConstant(
    @ByRef Graph g,
    @Const @ByRef IValue val,
    @ByVal(nullValue = "c10::optional<torch::jit::SourceRange>(c10::nullopt)") SourceRangeOptional loc,
    @ByVal(nullValue = "c10::optional<torch::jit::ScopePtr>(c10::nullopt)") @Cast("c10::optional<torch::jit::ScopePtr>*") ScopeOptional scope);
@Namespace("torch::jit") public static native Value insertConstant(
    @ByRef Graph g,
    @Const @ByRef IValue val);

// note: prefer g.insertConsant(val, loc) which does exactly the same thing
// this function is only declared/defined here because its implementation is
// closely related to the implementation of prim::Constant that is also in
// constants.cpp.
//
// returns a c10::nullopt if the IValue kind cannot be inserted as a constant
@Namespace("torch::jit") public static native @ByVal ValueOptional tryInsertConstant(
    @ByRef Graph g,
    @Const @ByRef IValue val,
    @ByVal(nullValue = "c10::optional<torch::jit::SourceRange>(c10::nullopt)") SourceRangeOptional loc,
    @ByVal(nullValue = "c10::optional<torch::jit::ScopePtr>(c10::nullopt)") @Cast("c10::optional<torch::jit::ScopePtr>*") ScopeOptional scope);
@Namespace("torch::jit") public static native @ByVal ValueOptional tryInsertConstant(
    @ByRef Graph g,
    @Const @ByRef IValue val);

////////////////////////////////////////////////////////////////////////////////
// Helper for retrieving constants
////////////////////////////////////////////////////////////////////////////////

// attempt to convert a (possibly constant) Value* into an interpreter value
// (IValue). returns c10::nullopt if the Value* was not constant
@Namespace("torch::jit") public static native @ByVal IValueOptional toIValue(@Const Value v);

// if a value is a constant then try to turn into type T using the
// same rules as the interpreter
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/ir/graph_node_list.h

// #pragma once

// #include <c10/util/Exception.h>

// Intrusive doubly linked lists with sane reverse iterators.
// The header file is named generic_graph_node_list.h because it is ONLY
// used for Graph's Node lists, and if you want to use it for other
// things, you will have to do some refactoring.
//
// At the moment, the templated type T must support a few operations:
//
//  - It must have a field: T* next_in_graph[2] = { nullptr, nullptr };
//    which are used for the intrusive linked list pointers.
//
//  - It must have a method 'destroy()', which removes T from the
//    list and frees a T.
//
// In practice, we are only using it with Node and const Node.  'destroy()'
// needs to be renegotiated if you want to use this somewhere else.
//
// Regardless of the iteration direction, iterators always physically point
// to the element they logically point to, rather than
// the off-by-one behavior for all standard library reverse iterators like
// std::list.

// The list is includes two sentinel nodes, one at the beginning and one at the
// end with a circular link between them. It is an error to insert nodes after
// the end sentinel node but before the beginning node:

// Visualization showing only the next() links:
//  HEAD -> first -> second  -> ... -> last -> TAIL
//   ^------------------------------------------

// Visualization showing only the prev() links:
//  HEAD <- first <- second  <- ... <- last <- TAIL
//   ------------------------------------------^

@Namespace("torch::jit") @MemberGetter public static native int kNextDirection();
public static final int kNextDirection = kNextDirection();
@Namespace("torch::jit") @MemberGetter public static native int kPrevDirection();
public static final int kPrevDirection = kPrevDirection();
// Targeting ../graph_node_list_iterator.java


// Targeting ../graph_node_list.java



 // namespace jit
 // namespace torch

 // namespace std


// Parsed from torch/csrc/jit/ir/named_value.h

// #pragma once
// #include <ATen/core/ivalue.h>
// #include <torch/csrc/jit/frontend/source_range.h>
// #include <torch/csrc/jit/ir/constants.h>
// #include <torch/csrc/utils/variadic.h>
// Targeting ../NamedValue.java



 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/ir/scope.h

// #pragma once
// #include <ATen/core/jit_type.h>
// #include <ATen/core/symbol.h>
// #include <c10/util/Optional.h>
// #include <c10/util/intrusive_ptr.h>
// #include <torch/csrc/Export.h>
// #include <torch/csrc/jit/frontend/source_range.h>
// #include <unordered_map>
@Namespace("torch::jit") @MemberGetter public static native @Cast("const size_t") long kModuleInstanceInfo();

 // namespace utils

// Scope is a node of a trie that represents the tree of nested scopes.
// Individual scopes are pushed and popped from Graph, which holds a
// pointer to the current scope. Each Node in Graph holds a pointer
// to the scope that was current when the node was created.
// The trie never needs to shrink, it only grows until it is disposed
// of when Graph is deallocated. Hence, pointers to scopes held by nodes
// will always be valid as long as Graph is alive.
// Targeting ../Scope.java


// Targeting ../ModuleInstanceInfo.java



/**
 * InlinedCallStack is an element in a list representing callstack of functions
 * that have been inlined.
 *
 * Each such element holds info about the current callsite (Function and
 * SourceRange) and a pointer to the next element in the list. The last element
 * in the list represents the innermost function that was inlined.
 *
 * For instance, if a node has a callstack
 *    [foo, source_range1] -> [bar, source_range2]
 * it means that this node was originally from function 'bar' that was called
 * at 'source_range2' in function 'foo' that was called in the current function
 * at 'source_range1'.
 *
 * If a node did not come from any inlined function, its callstack will be
 * empty.
 *
 * The callstack lists only grow, we never remove elements from them, which
 * allows us to reuse same elements in different lists. For instance, if we
 * inline function 'bar' to 'foo' and then inline 'foo' to two functions 'ham'
 * and 'baz', the callstacks would look like:
 *
 *  [baz, source_range3]  --
 *                           \
 *                             --> [foo, source_range1] -> [bar, source_range2]
 *                           /
 *  [ham, source_range4]  --
 */
// Targeting ../InlinedCallStack.java



// {source range, node name, InlinedCallStack}
// We store node name because same debug infor will be used for
// profiling as well, so we need to know op names as well.
@Namespace("torch::jit") @MemberGetter public static native @Cast("const size_t") long kDebugInfoTupleSourceRangeIndex();
@Namespace("torch::jit") @MemberGetter public static native @Cast("const size_t") long kDebugInfoTupleNodeNameIndex();
@Namespace("torch::jit") @MemberGetter public static native @Cast("const size_t") long kDebugInfoTupleInlinedCSIndex();
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/ir/ir.h

// #pragma once

// #include <torch/csrc/jit/ir/attributes.h>
// #include <torch/csrc/jit/ir/graph_node_list.h>
// #include <torch/csrc/jit/ir/named_value.h>
// #include <torch/csrc/jit/ir/scope.h>
// #include <torch/csrc/jit/runtime/operator.h>

// #include <torch/csrc/Export.h>
// #include <torch/csrc/utils/python_stub.h>
// #include <torch/csrc/utils/schema_info.h>

// #include <ATen/Utils.h>
// #include <ATen/core/Tensor.h>
// #include <ATen/core/dynamic_type.h>
// #include <ATen/core/enum_type.h>
// #include <ATen/core/functional.h>
// #include <ATen/core/interned_strings.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/jit_type.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Optional.h>

// #include <functional>
// #include <iostream>
// #include <unordered_set>
// #include <vector>

// Forward declare, the real meat is in python_ir.cpp
@Namespace("torch::jit::utils") public static native @StdString BytePointer getNodesModuleHierarchy(@Const @ByRef JitNode n);

// Targeting ../AliasDb.java



// #define C10_USING(T) using ::c10::T;
// #undef C10_USING

// #define C10_USING(T) using ::c10::T##Ptr;
// #undef C10_USING



// #if !defined(USE_ROCM)
// #endif

// Targeting ../MatchedSchema.java



// A Graph represents one "function" of computation.
// It uses a simple ownership model where the graph owns all the nodes inside
// it. All references inside the graph are raw pointers. Destroying the Graph
// will invalidate any pointers to nodes in the graph.

// Node is the base class of the IR graph. It represents one computation
// and dependencies on a list of Values. The "prim-ops", so to speak.

// A Value represents an input or output to node that is either a
// Tensor or an opaque Handle object, as determined by type().

@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Graph g);
@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef JitNode n);

// A list of nodes, with inputs and outputs
// Targeting ../Use.java



// Note [User node does not uniquely identify use]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// A while back, we wrote some code manipulating uses that looked like this:
//
//    for (auto& use : used_val->uses_) {
//      if (use.user == this_node) {
//        use.offset += 1;
//        break;
//      }
//    }
//
// This code is trying to find a particular use (our node's use) to update it.
// However, it's wrong: there may be *multiple* uses of a value %x in a node,
// as might be the case in this IR:
//
//    %y = Add %x %x
//
// In this case, there are two uses of %x whose user is the node 'Add %x %x'.
// So, "use induced by this node" is not a well-formed concept.
//
// If you are looking for "use induced by an input", it's best to use
// findUseForInput() to get it.

// the list types are intentionally simple, but we type-def
// them here so if we need to change them, refactoring will be easier
// Targeting ../BlockWrap.java


// Targeting ../JitNodeWrap.java


// Targeting ../ValueWrap.java


// Targeting ../Value.java


// Targeting ../JitNode.java


// Targeting ../Block.java


// Targeting ../Graph.java


// Targeting ../WithInsertPoint.java


// Targeting ../WithCurrentScope.java



// NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)








/************* All nodes not required to be defined before Graph **************/
// Targeting ../ProfileIValueOp.java


// Targeting ../PythonOp.java



@Namespace("torch::jit") public static native void LintGraph(@Const @SharedPtr @ByRef Graph graph);

@Namespace("torch::jit") public static native @ByVal ValueArrayRef createTupleUnpack(Value v);

/** Insert graph \p CALLEE into graph \p G using \p INPUTS as input values.
 * The insertion happens at the current insertion point.
 * Optionally, one can also pass \p VALUE_MAP to get a map between \p CALLEE
 * values and their cloned copies in \p G.
 */
@Namespace("torch::jit") public static native @ByVal ValueVector insertGraph(
    @ByRef Graph g,
    @ByRef Graph callee,
    @ByVal ValueArrayRef inputs);
@Namespace("torch::jit") public static native @ByVal ValueVector insertGraph(
    @ByRef Graph g,
    @ByRef Graph callee,
    @ByVal ValueArrayRef inputs,
    @ByRef ValueValueMap value_map);

/** Insert function \p CALLEE after node \p TO_REPLACE, remove the node and
 * replace all its uses with corresponding outputs of the inserted function.
 * This asserts that the number of outputs of the original node and the
 * graph are the same.
 */
@Namespace("torch::jit") public static native @ByVal ValueVector inlineCallTo(
    JitNode to_replace,
    GraphFunction callee,
    @Cast("bool") boolean use_graph/*=true*/);
@Namespace("torch::jit") public static native @ByVal ValueVector inlineCallTo(
    JitNode to_replace,
    GraphFunction callee);

@Namespace("torch::jit") public static native @ByVal ValueVector inlineCallTo(
    JitNode to_replace,
    GraphFunction callee,
    Graph callee_graph);

/** If there is only one value in \p OUTPUTS and its kind is Tuple, insert a
 * tuple unpack node and return the resulting values.
 */
@Namespace("torch::jit") public static native @ByVal ValueVector unpackOutputs(@Const @ByRef ValueVector outputs);

@Namespace("torch::jit") public static native @Cast("torch::jit::Node**") @StdVector PointerPointer findAllNodes(@ByRef Graph g, @ByVal Symbol kind, @Cast("bool") boolean recurse);
@Namespace("torch::jit") public static native @Cast("torch::jit::Node**") @StdVector PointerPointer findAllNodes(@ByRef Block b, @ByVal Symbol kind, @Cast("bool") boolean recurse);
@Namespace("torch::jit") public static native @Cast("torch::jit::Node**") @StdVector PointerPointer findAllNodes(
    @ByVal BlockArrayRef a,
    @ByVal Symbol kind,
    @Cast("bool") boolean recurse);
// Targeting ../OperatorSet.java



 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/ir/type_hashing.h

// #pragma once

// #include <ATen/core/jit_type.h>
// #include <torch/csrc/jit/ir/ir.h>
// Targeting ../HashType.java


// Targeting ../EqualType.java



 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/passes/shape_analysis.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <torch/csrc/jit/ir/ir.h>
// #include <memory>
// Targeting ../propagation_error.java


// Targeting ../PropertyPropBase.java



@Namespace("torch::jit") public static native void EraseShapeInformation(@Const @SharedPtr @ByRef Graph graph);
@Namespace("torch::jit") public static native void PropagateInputShapes(@Const @SharedPtr @ByRef Graph graph);

@Namespace("torch::jit") public static native @Cast("bool") boolean mergeTypes(
    @ByVal ValueArrayRef lhs,
    @ByVal ValueArrayRef rhs,
    @ByVal ValueArrayRef outputs);

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/python/update_graph_executor_opt.h

// #pragma once
// #include <torch/csrc/Export.h>
@Namespace("torch::jit") public static native void setGraphExecutorOptimize(@Cast("bool") boolean o);
@Namespace("torch::jit") public static native @Cast("bool") boolean getGraphExecutorOptimize();
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/runtime/argument_spec.h

// #pragma once

// #include <ATen/core/jit_type.h>
// #include <ATen/core/stack.h>
// #include <c10/util/hash.h>
// #include <c10/util/irange.h>
// #include <torch/csrc/Export.h>
// #include <torch/csrc/autograd/variable.h>
// #include <torch/csrc/jit/ir/ir.h>
// #include <iostream>
// #include <vector>

// #if C10_CLANG_HAS_WARNING("-Wshorten-64-to-32")
// #endif
// Targeting ../ArgumentInfo.java


// Targeting ../ArgumentSpec.java


@Namespace("torch::jit") @MemberGetter public static native @Cast("const size_t") long ARG_SPEC_DEPTH_LIMIT();
public static final long ARG_SPEC_DEPTH_LIMIT = ARG_SPEC_DEPTH_LIMIT();

// Targeting ../ArgumentSpecCreator.java


// Targeting ../CompleteArgumentInfoPOD.java


// Targeting ../CompleteArgumentSpec.java


// Targeting ../CompleteArgumentInfo.java



@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef ArgumentInfo info);

@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef ArgumentSpec spec);

@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(
    @Cast("std::ostream*") @ByRef Pointer out,
    @Const @ByRef CompleteArgumentInfo info);

@Namespace("torch::jit") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(
    @Cast("std::ostream*") @ByRef Pointer out,
    @Const @ByRef CompleteArgumentSpec spec);



@Namespace("torch::jit") public static native @ByVal ByteOptional convertOptional(
    @Const @ByRef ScalarTypeOptional from);

 // namespace jit
 // namespace torch
 // namespace std



// Parsed from torch/csrc/jit/runtime/instruction.h

// #pragma once

// #include <cstdint>
// #include <typeinfo>
// #include <unordered_set>
// instruction look like:
// op_code X, N
// meaning of X, N depend on the op:
// O - index into operator table
// R - index into register table
// I - literal integer
// C - index into constant table
// P - jump offset relative to beginning of current instruction
// F - index into function table
// T - index into the type table, used for guard instructions
// S - index into object slots
// C - index into code table

// #define FORALL_OPCODES(_)
//   _(OP, "O") /* invoke operator X */
//   _(OPN, "OI") /* invoke vararg operator X with N arguments */
//   _(LOAD, "R") /* push a value from a register X */
//   _(MOVE, "R") /* push a value from register X, clearing the register */
//   _(STOREN, "RI") /* store N values to registers [X, X+N) */
//   _(STORE, "R") /* store 1 value to registers X */
//   _(DROP, "") /* drop 1 value from the top of the stack */
//   _(DROPR, "R") /* clear register X */
//   _(LOADC, "C") /* push the constant X */
//   _(JF, "P") /* pop the top of the stack, if false, branch to P */
//   _(JMP, "P") /* unconditional branch to X */
//   _(LOOP, "PI") /* perform a loop, X is where to branch if cond is false */
//   _(RET, "") /* exit execution */
//   _(WAIT, "") /* wait for a future to be complete */
//   _(CALL, "F") /* call function X */
//   _(GUARD, "T") /* check a guard against type_table, true if passes */
//   _(TYPECHECK, "TN") /* check each type of input[i] against type_table[X+N] */
//   _(FAIL_GUARD, "T") /* fail a guard, patch back to GUARD */
//   _(PROFILE_OP, "F") /* get a callback from profile_function_table at X */
//   _(TAIL_CALL, "F") /* replace current frame with function F */
//   _(INTERFACE_CALL, "CI") /* call method X on the first argument (of N) */
//   _(GET_ATTR, "S") /* get attribute from slot X in an Object */
//   _(SET_ATTR, "S") /* set attribute to slot X in an Object */
//   _(LIST_UNPACK, "I") /* unpack list expecting length I */
//   _(TUPLE_CONSTRUCT, "I") /* construct a tuple using X inputs */
//   _(NAMED_TUPLE_CONSTRUCT,
//     "TI") /* construct a tuple of type X, using N inputs */
//   _(LIST_CONSTRUCT, "TI") /* construct a list of type X, using N inputs */
//   _(DICT_CONSTRUCT, "TI") /* construct a dict of type X, using N inputs */
//   _(CREATE_OBJECT, "T") /* create an object of type X */
//   _(ISINSTANCE, "TI") /* check object is one of  types[X:X+N]  */
//   _(TUPLE_SLICE, "II") /* slice tup[X:(X+N)] */
//   _(TUPLE_INDEX, "") /* get the value from a tuple at that index */
//   _(RAISE_EXCEPTION, "") /* throws the exception from Python */
//   _(DICT_INDEX, "") /* gets the value from the dict for given key */
//   _(UNCHECKED_CAST, "") /* perform an unchecked cast operation */
//   _(__IS__, "") /* performs `is` operator from Python */
//   _(UN_INITIALIZED,
//     "") /* sets default values to varaibles that are  un initialized */
//   _(__ISNOT__, "") /* performs `is not` operator from Python  */
//   _(FORMAT, "I") /* performs string format function `f strings` or `{}.format` \
//                     the number of inputs in stored in X */
//   _(DEVICE, "") /* invokes aten::device for a Tensor */
//   _(DTYPE, "") /* invokes aten::dtype for a Tensor */
//   _(DIM, "") /* invokes aten::dim for a Tensor */
//   _(__NOT__, "") /* performs `not` operator from Python  */
//   _(TO_LIST, "") /* convert the input to a list */
//   _(NUM_TO_TENSOR,
//     "") /* performs the conversion of a number/scalar to Tensor */
//   _(IS_CUDA, "") /* invokes aten::is_cuda for a Tensor */
//   _(FORK, "CN") /* launch a thread to run code entry x with N inputs  */
//   _(WARN, "I") /* emit a warning with line information */
//   _(ENTER, "EN") /* enter scope of a contextmanager */
//   _(EXIT, "EX") /* exit the last entered contextmanager */
//   _(AWAITABLE, "CN") /* initialize await for code entry x with N inputs  */

@Namespace("torch::jit") public enum OpCode {
  OP((byte)(0)), /* invoke operator X */
  OPN((byte)(1)), /* invoke vararg operator X with N arguments */
  LOAD((byte)(2)), /* push a value from a register X */
  MOVE((byte)(3)), /* push a value from register X, clearing the register */
  STOREN((byte)(4)), /* store N values to registers [X, X+N) */
  STORE((byte)(5)), /* store 1 value to registers X */
  DROP((byte)(6)), /* drop 1 value from the top of the stack */
  DROPR((byte)(7)), /* clear register X */
  LOADC((byte)(8)), /* push the constant X */
  JF((byte)(9)), /* pop the top of the stack, if false, branch to P */
  JMP((byte)(10)), /* unconditional branch to X */
  LOOP((byte)(11)), /* perform a loop, X is where to branch if cond is false */
  RET((byte)(12)), /* exit execution */
  WAIT((byte)(13)), /* wait for a future to be complete */
  CALL((byte)(14)), /* call function X */
  GUARD((byte)(15)), /* check a guard against type_table, true if passes */
  TYPECHECK((byte)(16)), /* check each type of input[i] against type_table[X+N] */
  FAIL_GUARD((byte)(17)), /* fail a guard, patch back to GUARD */
  PROFILE_OP((byte)(18)), /* get a callback from profile_function_table at X */
  TAIL_CALL((byte)(19)), /* replace current frame with function F */
  INTERFACE_CALL((byte)(20)), /* call method X on the first argument (of N) */
  GET_ATTR((byte)(21)), /* get attribute from slot X in an Object */
  SET_ATTR((byte)(22)), /* set attribute to slot X in an Object */
  LIST_UNPACK((byte)(23)), /* unpack list expecting length I */
  TUPLE_CONSTRUCT((byte)(24)), /* construct a tuple using X inputs */
  NAMED_TUPLE_CONSTRUCT((byte)(25)), /* construct a tuple of type X, using N inputs */
  LIST_CONSTRUCT((byte)(26)), /* construct a list of type X, using N inputs */
  DICT_CONSTRUCT((byte)(27)), /* construct a dict of type X, using N inputs */
  CREATE_OBJECT((byte)(28)), /* create an object of type X */
  ISINSTANCE((byte)(29)), /* check object is one of  types[X:X+N]  */
  TUPLE_SLICE((byte)(30)), /* slice tup[X:(X+N)] */
  TUPLE_INDEX((byte)(31)), /* get the value from a tuple at that index */
  RAISE_EXCEPTION((byte)(32)), /* throws the exception from Python */
  DICT_INDEX((byte)(33)), /* gets the value from the dict for given key */
  UNCHECKED_CAST((byte)(34)), /* perform an unchecked cast operation */
  __IS__((byte)(35)), /* performs `is` operator from Python */
  UN_INITIALIZED((byte)(36)), /* sets default values to varaibles that are  un initialized */
  __ISNOT__((byte)(37)), /* performs `is not` operator from Python  */
  FORMAT((byte)(38)), /* performs string format function `f strings` or `{}.format` \
                     the number of inputs in stored in X */
  DEVICE((byte)(39)), /* invokes aten::device for a Tensor */
  DTYPE((byte)(40)), /* invokes aten::dtype for a Tensor */
  DIM((byte)(41)), /* invokes aten::dim for a Tensor */
  __NOT__((byte)(42)), /* performs `not` operator from Python  */
  TO_LIST((byte)(43)), /* convert the input to a list */
  NUM_TO_TENSOR((byte)(44)), /* performs the conversion of a number/scalar to Tensor */
  IS_CUDA((byte)(45)), /* invokes aten::is_cuda for a Tensor */
  FORK((byte)(46)), /* launch a thread to run code entry x with N inputs  */
  WARN((byte)(47)), /* emit a warning with line information */
  ENTER((byte)(48)), /* enter scope of a contextmanager */
  EXIT((byte)(49)), /* exit the last entered contextmanager */
  AWAITABLE((byte)(50));

    public final byte value;
    private OpCode(byte v) { this.value = v; }
    private OpCode(OpCode e) { this.value = e.value; }
    public OpCode intern() { for (OpCode e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../Instruction.java








 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/runtime/interpreter.h

// #pragma once
// #include <c10/util/Optional.h>
// #include <memory>
// #include <vector>

// #include <ATen/ThreadLocalState.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/jit_type.h>
// #include <torch/csrc/Export.h>
// #include <torch/csrc/jit/frontend/source_range.h>

// #if C10_CLANG_HAS_WARNING("-Wdeprecated-copy-dtor")
// #endif



 // namespace at
 // namespace c10
// Targeting ../CodeImpl.java



// Targeting ../InterpreterStateImpl.java


// Targeting ../Code.java


// Targeting ../MobileCode.java


// Targeting ../InterpreterState.java


// Targeting ../Suspend.java


// Targeting ../InterpreterContinuation.java



// what is the tensors type, including state from the current execution context
// that modifies how the tensor behaves. For instance if no_grad is enabled
// this will cause the TensorType to have requires_grad=False.
@Namespace("torch::jit") public static native @SharedPtr @ByVal TensorType tensorTypeInCurrentExecutionContext(
    @Const @ByRef Tensor t);

// current (TLS) TorchScript interpreter callstack
@Namespace("torch::jit") public static native @ByVal StackEntryVector currentCallstack();
@Namespace("torch::jit") public static native @ByVal StringVector currentModuleHierarchy();

 // namespace jit
 // namespace torch



// Parsed from torch/csrc/jit/runtime/graph_executor.h

// #pragma once

// #include <atomic>
// #include <memory>

// #include <torch/csrc/jit/ir/ir.h>
// #include <torch/csrc/jit/python/update_graph_executor_opt.h>
// #include <torch/csrc/jit/runtime/argument_spec.h>
// #include <torch/csrc/jit/runtime/interpreter.h>
// #include <torch/csrc/jit/runtime/variable_tensor_list.h>



@Namespace("torch::jit") public enum ExecutorExecutionMode {
  SIMPLE(0),
  PROFILING(1);

    public final int value;
    private ExecutorExecutionMode(int v) { this.value = v; }
    private ExecutorExecutionMode(ExecutorExecutionMode e) { this.value = e.value; }
    public ExecutorExecutionMode intern() { for (ExecutorExecutionMode e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../ExecutionPlan.java


// Targeting ../GraphExecutorState.java


// Targeting ../EnableProfilingGuard.java


// Targeting ../GraphExecutorImplBase.java


// Targeting ../GraphExecutor.java



@Namespace("torch::jit") public static native JitNode replaceBlockWithFallbackGraph(
    Block b,
    @ByVal ValueArrayRef inputs);

// These passes need to run before it is valid to pass to the interpreter
// regardless of whether sizes have been specialized or not.
@Namespace("torch::jit") public static native void runRequiredPasses(@Const @SharedPtr @ByRef Graph g);

@Namespace("torch::jit") public static native void debugSetFusionGroupInlining(@Cast("bool") boolean state);
@Namespace("torch::jit") public static native @Cast("bool") boolean getFusionGroupInlining();

@Namespace("torch::jit") public static native void debugSetAutodiffSubgraphInlining(@Cast("bool") boolean state);
@Namespace("torch::jit") public static native @SharedPtr @ByVal Graph lastExecutedOptimizedGraph();
@Namespace("torch::jit") public static native @Cast("size_t") long getBailoutDepth();
@Namespace("torch::jit") public static native @Cast("bool") boolean IsNewExecutorEnabled();
// Targeting ../GraphOptimizerEnabledGuard.java







// for debugging information we expose a way to get the last actually
// run graph. Previous approaches allowed querying the GraphExecutor
// for what graph it would run in certain circumstances (graphFor), but
// this is fragile because we sometimes change how these decisions are made.
// This interface still allows our tests to look at optimized graphs, but
// with less plumbing.
 // namespace detail

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/runtime/operator_options.h

// #pragma once

// #include <ATen/core/dispatch/OperatorOptions.h>

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/runtime/operator.h

// in memory description of all ATen Ops similar to Caffe2 schema
// once C10 exists this can be removed, or stubbed out, but we need
// it now to implement correct semantic checking for script
// #pragma once

// #include <ATen/core/dispatch/Dispatcher.h>
// #include <ATen/core/dispatch/OperatorOptions.h>
// #include <ATen/core/op_registration/op_allowlist.h>
// #include <ATen/core/stack.h>
// #include <c10/util/Exception.h>
// #include <torch/csrc/jit/frontend/function_schema_parser.h>
// #include <torch/csrc/jit/runtime/operator_options.h>
// #include <torch/library.h>

// #include <ATen/core/function_schema.h>
// #include <ATen/core/symbol.h>

// #include <functional>
// #include <initializer_list>
// #include <memory>
// #include <string>
// #include <unordered_map>
// #include <utility>
// #include <vector>
// Targeting ../OperationCreator.java


// Targeting ../Operator.java



@Namespace("torch::jit") public static native @StdString BytePointer canonicalSchemaString(@Const @ByRef FunctionSchema schema);

@Namespace("torch::jit") public static native @Const @ByVal OperatorVector getAllOperators();
@Namespace("torch::jit") public static native @Const @ByRef OperatorVector getAllOperatorsFor(
    @ByVal Symbol name);

// given a operator with an overload name, find the specific operator related to
// it, may return nullptr if no operator exists.
@Namespace("torch::jit") public static native @SharedPtr @ByVal Operator findOperatorFor(
    @Const @ByRef OperatorName full_name);

@Namespace("torch::jit") public static native @ByVal SymbolVector findSimilarOperators(@ByVal Symbol input_op);

@Namespace("torch::jit") public static native void registerOperator(@ByRef(true) Operator op);
@Namespace("torch::jit") public static native void deregisterOperator(@Const @ByRef FunctionSchema schema);

// XXX: this function is meant to be used with string literals only!
@Namespace("torch::jit") public static native @SharedPtr @ByVal Operator getOperatorForLiteral(
    @Cast("const char*") BytePointer signature);
@Namespace("torch::jit") public static native @SharedPtr @ByVal Operator getOperatorForLiteral(
    String signature);

// Ensure the thing that registers c10 ops is defined.
// Otherwise, our registry will not have c10 ops. You can run into this
// scenario if you're querying registered ops during static init.
//
// This fn is defined in register_c10_ops.cpp
@Namespace("torch::jit") public static native void ensure_c10_registerer_defined();

// Used to assert that unschematized operators have an analysis method written
@Namespace("torch::jit") public static native @Cast("bool") boolean aliasAnalysisHasSpecialCaseFor(@ByVal Symbol sym);

// A factory function to generate an optional operator. It has two
// instantiations depending on the template bool arg value. The arg can be a
// compile-time function for the selective op registration based on schema
// string.

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/runtime/custom_operator.h

// #pragma once

// #include <ATen/core/op_registration/op_registration.h>
// #include <ATen/core/stack.h>
// #include <torch/csrc/jit/runtime/operator.h>
// Targeting ../RegisterOperators.java



 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/api/compilation_unit.h

// #pragma once
// #include <ATen/core/function.h>
// #include <c10/util/Exception.h>
// #include <torch/csrc/jit/api/function_impl.h>
// #include <torch/csrc/jit/frontend/name_mangler.h>
// #include <torch/csrc/jit/frontend/source_range.h>
// #include <torch/csrc/jit/ir/ir.h>
// #include <torch/csrc/jit/runtime/graph_executor.h>

// #include <torch/csrc/Export.h>
// #include <torch/csrc/utils/memory.h>

// #include <ATen/core/function_schema.h>
// #include <ATen/core/qualified_name.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Optional.h>

// #include <functional>
// #include <memory>
// #include <mutex>
// #include <ostream>
// #include <string>
// #include <unordered_map>
// #include <vector>
// Targeting ../Self.java


// Targeting ../CompilationUnit.java


// Targeting ../StrongFunctionPtr.java


// We once had a `script::` namespace that was deleted. This is for backcompat
// of the public API; new code should not use this type alias.
 // namespace script
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/api/function_impl.h

// #pragma once

// #include <ATen/core/function.h>
// #include <torch/csrc/jit/ir/ir.h>
// #include <torch/csrc/jit/runtime/graph_executor.h>
// #include <torch/csrc/utils/memory.h>
// Targeting ../GraphFunction.java



// Short hands for dynamic_cast<GraphFunction*>.
@Namespace("torch::jit") public static native @NoException(true) GraphFunction tryToGraphFunction(@ByRef Function arg0);
@Namespace("torch::jit") public static native @ByRef GraphFunction toGraphFunction(@ByRef Function arg0);

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/api/method.h

// #pragma once

// #include <ATen/core/function.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/stack.h>
// #include <torch/csrc/api/include/torch/imethod.h>
// #include <torch/csrc/jit/api/function_impl.h>
// Targeting ../Method.java


// We once had a `script::` namespace that was deleted. This is for backcompat
// of the public API; new code should not use this type alias.
 // namespace script

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/api/object.h

// #pragma once

// #include <ATen/core/functional.h>
// #include <ATen/core/ivalue.h>
// #include <c10/util/Optional.h>
// #include <torch/csrc/jit/api/method.h>

// #include <utility>

// Throw this in C++ land if `attr` fails. This will be converted to a Python
// AttributeError by the Python binding code
// Targeting ../JitObject.java


// We once had a `script::` namespace that was deleted. This is for backcompat
// of the public API; new code should not use this type alias.
 // namespace script
 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/api/module.h

// #pragma once
// #include <c10/util/Exception.h>
// #include <torch/csrc/autograd/variable.h>
// #include <torch/csrc/jit/api/object.h>
// #include <torch/csrc/jit/frontend/source_range.h>
// #include <torch/csrc/jit/ir/ir.h>
// #include <torch/csrc/jit/ir/named_value.h>
// #include <torch/csrc/jit/runtime/argument_spec.h>
// #include <torch/csrc/jit/runtime/graph_executor.h>

// #include <torch/csrc/Export.h>
// #include <torch/csrc/api/include/torch/ordered_dict.h>
// #include <torch/csrc/jit/api/compilation_unit.h>
// #include <torch/csrc/utils/memory.h>

// #include <ATen/core/function_schema.h>
// #include <ATen/core/qualified_name.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Optional.h>
// #include <c10/util/irange.h>

// #include <functional>
// #include <memory>
// #include <mutex>
// #include <ostream>
// #include <string>
// #include <unordered_map>
// #include <unordered_set>
// #include <utility>
// #include <vector>

// This file contains classes which assist in desugaring Python style
// modules and their methods into flattened graphs which don't have any
// function calls.
// Map which stores filename to content.
// Targeting ../NamedJitModule.java


// Targeting ../NamedTensor.java


// Targeting ../NamedIValue.java


 // namespace detail
// Targeting ../JitModule.java



// C++ equivalent api of `torch.jit.freeze`. See documentation there for
// details.
@Namespace("torch::jit") public static native @ByVal JitModule freeze(
    @Const @ByRef JitModule module,
    @ByVal(nullValue = "c10::optional<std::vector<std::string> >(c10::nullopt)") StringVectorOptional preserved_attrs,
    @Cast("bool") boolean optimize_numerics/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule freeze(
    @Const @ByRef JitModule module);

// C++ equivalent api of `torch.jit.optimize_for_inference`. See documentation
// there for details.
@Namespace("torch::jit") public static native @ByVal JitModule optimize_for_inference(
    @ByRef JitModule module,
    @Const @ByRef(nullValue = "std::vector<std::string>{}") StringVector other_methods);
@Namespace("torch::jit") public static native @ByVal JitModule optimize_for_inference(
    @ByRef JitModule module);

@Namespace("torch::jit") public enum FusionBehavior { STATIC(0), DYNAMIC(1);

    public final int value;
    private FusionBehavior(int v) { this.value = v; }
    private FusionBehavior(FusionBehavior e) { this.value = e.value; }
    public FusionBehavior intern() { for (FusionBehavior e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// clang-format off
/*
Sets the type and number of specializations that can occur during fusion.

Usage: provide a list of pairs (type, depth) where type is one of STATIC or DYNAMIC
and depth is an integer.

Behavior - static vs dynamic:
    In STATIC fusion, fused ops are compiled to have fixed input shapes. The shape is determined
    based on some initial profiling runs.
    In DYNAMIC fusion, fused ops are compiled to have variable input shapes, so that multiple
    shapes are possible.

In both cases, we also recompile on new striding behavior, device, or dtype.

Behavior - fallback functions & depth:
    When an input doesn't match the format required by the specialized compiled op, it will run
    a fallback function. Fallback functions are recursively be compiled and specialized based
    on the observed tensor shapes. Since compilation can be slow, the "depth" parameter is provided to
    limit the number of specializations that can be compiled, before giving up on recompiling and
    falling back to a completely un-fused, un-specialized implementation.

The list of (type, depth) pairs controls the type of specializations and the number of
specializations. For example: [(STATIC, 2), (DYNAMIC, 2)] indicates that the first
two specializations will use static fusions, the following two specializations will use
dynamic fusion, and any inputs that satisfy none of the 4 options will run an
unfused implementation.

NB: in the future, if more as more fusion backends are added there may be more granular
apis for specific fusers.
*/
// clang-format on
@Namespace("torch::jit") public static native @ByVal FusionStrategy getFusionStrategy();
// returns previous strategy
@Namespace("torch::jit") public static native @ByVal FusionStrategy setFusionStrategy(@ByRef FusionStrategy fusion_strategy);
// Targeting ../SlotCursor.java




// Targeting ../module_iterator.java


// Targeting ../named_module_iterator.java


// Targeting ../parameter_iterator.java


// Targeting ../named_parameter_iterator.java


// Targeting ../attribute_iterator.java


// Targeting ../named_attribute_iterator.java


// Targeting ../buffer_iterator.java


// Targeting ../named_buffer_iterator.java


// Targeting ../module_list.java


// Targeting ../named_module_list.java


// Targeting ../parameter_list.java


// Targeting ../named_parameter_list.java


// Targeting ../attribute_list.java


// Targeting ../named_attribute_list.java


// Targeting ../buffer_list.java


// Targeting ../named_buffer_list.java


// Targeting ../ModulePolicy.java


// Targeting ../ParameterPolicy.java


// Targeting ../BufferPolicy.java


// Targeting ../AttributePolicy.java


// Targeting ../NamedModulePolicy.java


// Targeting ../NamedParameterPolicy.java


// Targeting ../NamedAttributePolicy.java


// Targeting ../NamedBufferPolicy.java



 // namespace detail

@Namespace("torch::jit") public static native @Cast("bool*") @ByRef BoolPointer getInlineEverythingMode();
// We once had a `script::` namespace that was deleted. This is for backcompat
// of the public API; new code should not use this type alias.
 // namespace script

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/serialization/source_range_serialization.h

// #pragma once

// #include <c10/core/Allocator.h>
// #include <torch/csrc/jit/frontend/source_range.h>

// #include <ATen/core/ivalue.h>

// #include <unordered_map>
// #include <vector>

// Targeting ../SourceRangeSerializer.java


@Namespace("torch::jit") @MemberGetter public static native @Cast("const size_t") long kByteOffsetIndex();
public static final long kByteOffsetIndex = kByteOffsetIndex();
@Namespace("torch::jit") @MemberGetter public static native @Cast("const size_t") long kSourceRangeIndex();
public static final long kSourceRangeIndex = kSourceRangeIndex();
@Namespace("torch::jit") @MemberGetter public static native @Cast("const size_t") long kSourceRangeTagIndex();
public static final long kSourceRangeTagIndex = kSourceRangeTagIndex();
@Namespace("torch::jit") @MemberGetter public static native @ByRef @Cast("const c10::string_view*") Pointer kFormatWithStringTable();
// Targeting ../SourceRangePickler.java


// Targeting ../SourceRangeDeserializer.java


// Targeting ../SourceRangeUnpickler.java



@Namespace("torch::jit") public static native void setShouldUseFormatWithStringTable(
    @Cast("bool") boolean should_use_format_with_string_table);

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/serialization/pickler.h

// #pragma once

// #include <ATen/core/qualified_name.h>
// #include <string>
// #include <utility>
// #include <vector>

// #include <ATen/Utils.h>
// #include <ATen/core/ivalue.h>
// #include <ATen/core/jit_type.h>
// #include <c10/util/ArrayRef.h>
// #include <torch/csrc/Export.h>

// See Python's pickletools.py for a detailed description of each of these codes
@Namespace("torch::jit") public enum PickleOpCode {
  MARK((byte)('(')),
  STOP((byte)('.')),
  POP((byte)('0')),
  POP_MARK((byte)('1')),
  DUP((byte)('2')),
  FLOAT((byte)('F')),
  INT((byte)('I')),
  BININT((byte)('J')),
  BININT1((byte)('K')),
  LONG((byte)('L')),
  BININT2((byte)('M')),
  NONE((byte)('N')),
  PERSID((byte)('P')),
  BINPERSID((byte)('Q')),
  REDUCE((byte)('R')),
  STRING((byte)('S')),
  BINSTRING((byte)('T')),
  SHORT_BINSTRING((byte)('U')),
  // NB: Avoid using UNICODE as it is a macro in the Windows API
  UNICODE_((byte)('V')),
  BINUNICODE((byte)('X')),
  APPEND((byte)('a')),
  BUILD((byte)('b')),
  GLOBAL((byte)('c')),
  DICT((byte)('d')),
  EMPTY_DICT((byte)('}')),
  APPENDS((byte)('e')),
  GET((byte)('g')),
  BINGET((byte)('h')),
  INST((byte)('i')),
  LONG_BINGET((byte)('j')),
  LIST((byte)('l')),
  EMPTY_LIST((byte)(']')),
  OBJ((byte)('o')),
  PUT((byte)('p')),
  BINPUT((byte)('q')),
  LONG_BINPUT((byte)('r')),
  SETITEM((byte)('s')),
  TUPLE((byte)('t')),
  EMPTY_TUPLE((byte)(')')),
  SETITEMS((byte)('u')),
  BINFLOAT((byte)('G')),

  // Protocol 2
  PROTO((byte)(0x80)),
  NEWOBJ((byte)(0x81)),
  EXT1((byte)(0x82)),
  EXT2((byte)(0x83)),
  EXT4((byte)(0x84)),
  TUPLE1((byte)(0x85)),
  TUPLE2((byte)(0x86)),
  TUPLE3((byte)(0x87)),
  NEWTRUE((byte)(0x88)),
  NEWFALSE((byte)(0x89)),
  LONG1((byte)(0x8a)),
  LONG4((byte)(0x8b)),

  // Protocol 3 (Python 3.x)
  BINBYTES((byte)('B')),
  SHORT_BINBYTES((byte)('C')),

  // Protocol 4
  SHORT_BINUNICODE((byte)(0x8c)),
  BINUNICODE8((byte)(0x8d)),
  BINBYTES8((byte)(0x8e)),
  EMPTY_SET((byte)(0x8f)),
  ADDITEMS((byte)(0x90)),
  FROZENSET((byte)(0x91)),
  NEWOBJ_EX((byte)(0x92)),
  STACK_GLOBAL((byte)(0x93)),
  MEMOIZE((byte)(0x94)),
  FRAME((byte)(0x95));

    public final byte value;
    private PickleOpCode(byte v) { this.value = v; }
    private PickleOpCode(PickleOpCode e) { this.value = e.value; }
    public PickleOpCode intern() { for (PickleOpCode e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../WriteableTensorData.java





// Targeting ../Pickler.java



// returns a (tensor, record_size) for a tensor, converting it to a CPU tensor
// if it was CUDA and to_cpu is True.
@Namespace("torch::jit") public static native @ByVal WriteableTensorData getWriteableTensorData(@Const @ByRef Tensor tensor, @Cast("bool") boolean to_cpu/*=true*/);
@Namespace("torch::jit") public static native @ByVal WriteableTensorData getWriteableTensorData(@Const @ByRef Tensor tensor);

// return the value of the tensor's storage pointer


// if the cls has __getstate__/__setstate__
// assert they have the right schema and return true,
// otherwise return false


// Return a map of Tensor Metadata for serialization.
// For now, it only takes care of `conj` and `neg` bit.
@Namespace("torch::jit") public static native @ByVal StringBoolMap getTensorMetadata(
    @Const @ByRef Tensor t);

// set Tensor Metadata based on the map.
// Refer: getTensorMathdata
@Namespace("torch::jit") public static native void setTensorMetadata(
    @Const @ByRef Tensor t,
    @ByVal StringBoolMap metadata);

// set Tensor metadata based on the map.
// NOTE: This overload is required by unpickler.cpp
@Namespace("torch::jit") public static native void setTensorMetadata(
    @Const @ByRef Tensor t,
    @ByVal GenericDict metadata_idict);

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/serialization/unpickler.h

// #pragma once

// #include <ATen/core/ivalue.h>
// #include <c10/util/ArrayRef.h>
// #include <caffe2/serialize/inline_container.h>
// #include <torch/csrc/Export.h>
// #include <torch/csrc/jit/frontend/script_type_parser.h>
// #include <torch/csrc/jit/serialization/pickler.h>
// Targeting ../Unpickler.java





 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/serialization/import.h

// #pragma once

// #include <ATen/core/ivalue.h>
// #include <caffe2/serialize/inline_container.h>
// #include <torch/csrc/jit/api/module.h>
// #include <torch/csrc/jit/ir/ir.h>
// #include <torch/csrc/jit/serialization/unpickler.h>

// #include <istream>
// Targeting ../ReadAdapterInterface.java


 // namespace serialize
 // namespace caffe2

@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @StdString BytePointer filename,
    @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @StdString BytePointer filename);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @StdString String filename,
    @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @StdString String filename);

@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @Cast("std::istream*") @ByRef Pointer in,
    @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @Cast("std::istream*") @ByRef Pointer in);

@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @UniquePtr ReadAdapterInterface rai,
    @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @UniquePtr ReadAdapterInterface rai);

@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @StdString BytePointer filename,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files,
    @Cast("bool") boolean load_debug_files/*=true*/,
    @Cast("bool") boolean restore_shapes/*=false*/);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @StdString BytePointer filename,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @StdString String filename,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files,
    @Cast("bool") boolean load_debug_files/*=true*/,
    @Cast("bool") boolean restore_shapes/*=false*/);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @StdString String filename,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files);

// For reading unified serialization format from torch.Package
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @ByVal @Cast("std::shared_ptr<caffe2::serialize::PyTorchStreamReader>*") Pointer reader,
    @SharedPtr DeserializationStorageContext storage_context,
    @ByVal DeviceOptional device,
    @StdString BytePointer ts_id);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @ByVal @Cast("std::shared_ptr<caffe2::serialize::PyTorchStreamReader>*") Pointer reader,
    @SharedPtr DeserializationStorageContext storage_context,
    @ByVal DeviceOptional device,
    @StdString String ts_id);

@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @Cast("std::istream*") @ByRef Pointer in,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files,
    @Cast("bool") boolean load_debug_files/*=true*/,
    @Cast("bool") boolean restore_shapes/*=false*/);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @Cast("std::istream*") @ByRef Pointer in,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files);

@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @UniquePtr ReadAdapterInterface rai,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule import_ir_module(
    @SharedPtr CompilationUnit cu,
    @UniquePtr ReadAdapterInterface rai,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files);

/** Loads a serialized {@code Module} from the given {@code istream}.
 * 
 *  The istream must contain a serialized {@code Module}, exported via
 *  {@code torch::jit::ExportModule} in C++. */
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @Cast("std::istream*") @ByRef Pointer in,
    @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @Cast("std::istream*") @ByRef Pointer in);


///
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @Cast("std::istream*") @ByRef Pointer in,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @Cast("std::istream*") @ByRef Pointer in,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files);

/** Loads a serialized {@code Module} from the given {@code filename}.
 * 
 *  The file stored at the location given in {@code filename} must contain a
 *  serialized {@code Module}, exported either via {@code ScriptModule.save()} in
 *  Python or {@code torch::jit::ExportModule} in C++. */
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @StdString BytePointer filename,
    @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @StdString BytePointer filename);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @StdString String filename,
    @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @StdString String filename);


///
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @StdString BytePointer filename,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @StdString BytePointer filename,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @StdString String filename,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @StdString String filename,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files);

/** Loads a serialized {@code Module} from the given shared_ptr {@code rai}.
 * 
 *  The reader adapter, which is for customized input stream, must contain a
 *  serialized {@code Module}, exported either via {@code ScriptModule.save()} in
 *  Python or {@code torch::jit::ExportModule} in C++. */
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @SharedPtr ReadAdapterInterface rai,
    @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional device,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @SharedPtr ReadAdapterInterface rai);

@Namespace("torch::jit") public static native @ByVal JitModule load(
    @SharedPtr ReadAdapterInterface rai,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files,
    @Cast("bool") boolean load_debug_files/*=true*/);
@Namespace("torch::jit") public static native @ByVal JitModule load(
    @SharedPtr ReadAdapterInterface rai,
    @ByVal DeviceOptional device,
    @ByRef ExtraFilesMap extra_files);

@Namespace("torch::jit") public static native @ByVal JitModule jitModuleFromSourceAndConstants(
    @Const @ByRef IValue ivalue,
    @Const @ByRef ExtraFilesMap source,
    @Const @ByRef IValueVector constants,
    int version);

@Namespace("torch::jit") public static native @ByVal JitModule parse_and_initialize_jit_module(
    @Cast("char*") @SharedPtr BytePointer data,
    @Cast("size_t") long size,
    @ByRef ExtraFilesMap extra_files,
    @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("torch::jit") public static native @ByVal JitModule parse_and_initialize_jit_module(
    @Cast("char*") @SharedPtr BytePointer data,
    @Cast("size_t") long size,
    @ByRef ExtraFilesMap extra_files);
@Namespace("torch::jit") public static native @ByVal JitModule parse_and_initialize_jit_module(
    @Cast("char*") @SharedPtr ByteBuffer data,
    @Cast("size_t") long size,
    @ByRef ExtraFilesMap extra_files,
    @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("torch::jit") public static native @ByVal JitModule parse_and_initialize_jit_module(
    @Cast("char*") @SharedPtr ByteBuffer data,
    @Cast("size_t") long size,
    @ByRef ExtraFilesMap extra_files);
@Namespace("torch::jit") public static native @ByVal JitModule parse_and_initialize_jit_module(
    @Cast("char*") @SharedPtr byte[] data,
    @Cast("size_t") long size,
    @ByRef ExtraFilesMap extra_files,
    @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("torch::jit") public static native @ByVal JitModule parse_and_initialize_jit_module(
    @Cast("char*") @SharedPtr byte[] data,
    @Cast("size_t") long size,
    @ByRef ExtraFilesMap extra_files);

@Namespace("torch::jit") public static native @ByVal JitModule load_jit_module_from_file(
    @StdString BytePointer filename,
    @ByRef ExtraFilesMap extra_files,
    @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("torch::jit") public static native @ByVal JitModule load_jit_module_from_file(
    @StdString BytePointer filename,
    @ByRef ExtraFilesMap extra_files);
@Namespace("torch::jit") public static native @ByVal JitModule load_jit_module_from_file(
    @StdString String filename,
    @ByRef ExtraFilesMap extra_files,
    @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("torch::jit") public static native @ByVal JitModule load_jit_module_from_file(
    @StdString String filename,
    @ByRef ExtraFilesMap extra_files);

@Namespace("torch::jit") public static native @ByVal JitModule load_jit_module_from_stream(
    @Cast("std::istream*") @ByRef Pointer in,
    @ByRef ExtraFilesMap extra_files,
    @ByVal(nullValue = "c10::optional<at::Device>(c10::nullopt)") DeviceOptional device);
@Namespace("torch::jit") public static native @ByVal JitModule load_jit_module_from_stream(
    @Cast("std::istream*") @ByRef Pointer in,
    @ByRef ExtraFilesMap extra_files);

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/serialization/pickle.h

// #pragma once

// #include <ATen/core/ivalue.h>
// #include <c10/util/ArrayRef.h>
// #include <caffe2/serialize/inline_container.h>
// #include <torch/csrc/Export.h>
// #include <torch/csrc/jit/serialization/pickler.h>
// #include <torch/csrc/jit/serialization/unpickler.h>

/** Pickle an IValue by calling a function to handle writing the data.
 * 
 *  {@code writer} is a function that takes in a pointer to a chunk of memory and its
 *  size and consumes it.
 * 
 *  See {@code jit::pickle} for more details. */

///
///
///
///
///
///
///
///
@Namespace("torch::jit") public static native void pickle(
    @ByVal Writer writer,
    @Const @ByRef IValue ivalue,
    TensorVector tensor_table/*=nullptr*/);
@Namespace("torch::jit") public static native void pickle(
    @ByVal Writer writer,
    @Const @ByRef IValue ivalue);

/** Save a {@code torch::IValue} in a format compatible with Python's {@code pickle} module
 * 
 *  If present, {@code tensor_table} is a pointer to a table in which tensors that
 *  are contained within {@code ivalue} are stored, and the bytes returned by the
 *  pickler will only include references to these tensors in the table. This can
 *  be used to keep the binary blob size small.
 *  If not provided, tensors are stored in the same byte stream as the pickle
 *  data, similar to {@code torch.save()} in eager Python.
 * 
 *  Pickled values can be loaded in Python and C++:
 *  \rst
 *  .. code-block:: cpp
 * 
 *   torch::IValue float_value(2.3);
 * 
 *   // TODO: when tensors are stored in the pickle, delete this
 *   std::vector<at::Tensor> tensor_table;
 *   auto data = torch::jit::pickle(float_value, &tensor_table);
 * 
 *   std::vector<torch::IValue> ivalues =
 *       torch::jit::unpickle(data.data(), data.size());
 * 
 *  .. code-block:: python
 * 
 *    values = torch.load('data.pkl')
 *    print(values)
 * 
 *  \endrst */
@Namespace("torch::jit") public static native @Cast("char*") @StdVector BytePointer pickle(
    @Const @ByRef IValue ivalue,
    TensorVector tensor_table/*=nullptr*/);
@Namespace("torch::jit") public static native @Cast("char*") @StdVector BytePointer pickle(
    @Const @ByRef IValue ivalue);

/** Save a {@code torch::IValue} in a format that can be loaded by both
 *  {@code torch::pickle_load} in C++ and {@code torch.load} in Python. */
@Namespace("torch::jit") public static native @Cast("char*") @StdVector BytePointer pickle_save(@Const @ByRef IValue ivalue);

/** Deserialize a {@code torch::IValue} from bytes produced by either
 *  {@code torch::pickle_save} in C++ or {@code torch.save} in Python */
@Namespace("torch::jit") public static native @ByVal IValue pickle_load(@Cast("char*") @StdVector BytePointer data);
@Namespace("torch::jit") public static native @ByVal IValue pickle_load(@Cast("char*") @StdVector ByteBuffer data);
@Namespace("torch::jit") public static native @ByVal IValue pickle_load(@Cast("char*") @StdVector byte[] data);
// Targeting ../TypeParser.java



///
///
@Namespace("torch::jit") public static native @ByVal IValue unpickle(
    @ByVal Reader reader,
    @ByVal @Cast("torch::jit::TypeResolver*") Pointer type_resolver,
    @ByVal TensorArrayRef tensor_table,
    TypeParser type_parser/*=torch::jit::Unpickler::defaultTypeParser*/);
@Namespace("torch::jit") public static native @ByVal IValue unpickle(
    @ByVal Reader reader,
    @ByVal @Cast("torch::jit::TypeResolver*") Pointer type_resolver,
    @ByVal TensorArrayRef tensor_table);

/** Decode a chunk of memory containing pickled data into its {@code torch::IValue}s.
 * 
 *  If any {@code torch::IValue}s in the pickled data are {@code Object}s, then a
 *  {@code class_resolver} function must be provided.
 * 
 *  See {@code torch::pickle} for details. */
@Namespace("torch::jit") public static native @ByVal IValue unpickle(
    @Cast("const char*") BytePointer data,
    @Cast("size_t") long size,
    @ByVal(nullValue = "torch::jit::TypeResolver(nullptr)") @Cast("torch::jit::TypeResolver*") Pointer type_resolver,
    @ByVal(nullValue = "c10::ArrayRef<at::Tensor>{}") TensorArrayRef tensor_table,
    TypeParser type_parser/*=torch::jit::Unpickler::defaultTypeParser*/);
@Namespace("torch::jit") public static native @ByVal IValue unpickle(
    @Cast("const char*") BytePointer data,
    @Cast("size_t") long size);
@Namespace("torch::jit") public static native @ByVal IValue unpickle(
    String data,
    @Cast("size_t") long size,
    @ByVal(nullValue = "torch::jit::TypeResolver(nullptr)") @Cast("torch::jit::TypeResolver*") Pointer type_resolver,
    @ByVal(nullValue = "c10::ArrayRef<at::Tensor>{}") TensorArrayRef tensor_table,
    TypeParser type_parser/*=torch::jit::Unpickler::defaultTypeParser*/);
@Namespace("torch::jit") public static native @ByVal IValue unpickle(
    String data,
    @Cast("size_t") long size);

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/serialization/python_print.h

// #pragma once
// #include <torch/csrc/Export.h>
// #include <torch/csrc/jit/api/module.h>
// #include <torch/csrc/jit/ir/ir.h>
// #include <iostream>
// #include <vector>
// Targeting ../PythonPrintImpl.java


// Targeting ../PrintDepsTable.java


// Targeting ../PythonPrint.java





@Namespace("torch::jit") public static native void jitModuleToPythonCodeAndConstants(
    @Const @ByRef JitModule module,
    ExtraFilesMap jit_sources,
    IValueVector constants
);

 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/serialization/type_name_uniquer.h

// #pragma once

// #include <torch/csrc/jit/frontend/name_mangler.h>
// #include <torch/csrc/jit/ir/type_hashing.h>
// Targeting ../TypeNameUniquer.java


 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/serialization/storage_context.h

// #pragma once

// #include <ATen/core/ivalue.h>
// Targeting ../SerializationStorageContext.java


// Targeting ../DeserializationStorageContext.java



 // namespace jit
 // namespace torch


// Parsed from torch/csrc/jit/serialization/export.h

// #pragma once

// #include <caffe2/serialize/inline_container.h>
// #include <torch/csrc/jit/api/module.h>
// #include <torch/csrc/jit/ir/ir.h>
// #include <torch/csrc/jit/serialization/export_bytecode.h>
// #include <torch/csrc/jit/serialization/flatbuffer_serializer.h>
// #include <torch/csrc/jit/serialization/pickler.h>
// #include <torch/csrc/jit/serialization/python_print.h>
// #include <torch/csrc/jit/serialization/storage_context.h>
// #include <torch/csrc/jit/serialization/type_name_uniquer.h>
// #include <torch/csrc/onnx/onnx.h>
// #include <ostream>


// This map is used to keep track of parameters that should be exported
// externally. When `defer_weight_export` is true, the returned map contains
// kv pairs that map {external reference name} -> {at::Tensor to be exported}.
// It is the responsibility of the caller to export these appropriately.
//
// For example, when exporting to a zip archive, the caller may write out files
// for each entry in the export map, with the filename being the key and the
// file contents being the raw tensor data.

// Used for modularized export settling function and node attributes.





@Namespace("torch::jit") public static native void check_onnx_proto(@StdString BytePointer proto_string);
@Namespace("torch::jit") public static native void check_onnx_proto(@StdString String proto_string);
// Targeting ../ScriptModuleSerializer.java



// For testing purposes
@Namespace("torch::jit") public static native @StdString BytePointer pretty_print_onnx(
    @Const @SharedPtr @ByRef Graph graph,
    @Const @ByRef StringTensorMap initializers,
    @Cast("int64_t") long onnx_opset_version,
    @Cast("bool") boolean defer_weight_export,
    OperatorExportTypes operator_export_type/*=torch::onnx::OperatorExportTypes::ONNX*/,
    @Cast("bool") boolean google_printer/*=false*/,
    @Cast("bool") boolean keep_initializers_as_inputs/*=true*/,
    @Const @ByRef(nullValue = "std::map<std::string,int>{}") StringIntMap custom_opsets,
    @Cast("bool") boolean add_node_names/*=true*/);
@Namespace("torch::jit") public static native @StdString BytePointer pretty_print_onnx(
    @Const @SharedPtr @ByRef Graph graph,
    @Const @ByRef StringTensorMap initializers,
    @Cast("int64_t") long onnx_opset_version,
    @Cast("bool") boolean defer_weight_export);
@Namespace("torch::jit") public static native @StdString String pretty_print_onnx(
    @Const @SharedPtr @ByRef Graph graph,
    @Const @ByRef StringTensorMap initializers,
    @Cast("int64_t") long onnx_opset_version,
    @Cast("bool") boolean defer_weight_export,
    @Cast("torch::onnx::OperatorExportTypes") int operator_export_type/*=torch::onnx::OperatorExportTypes::ONNX*/,
    @Cast("bool") boolean google_printer/*=false*/,
    @Cast("bool") boolean keep_initializers_as_inputs/*=true*/,
    @Const @ByRef(nullValue = "std::map<std::string,int>{}") StringIntMap custom_opsets,
    @Cast("bool") boolean add_node_names/*=true*/);

@Namespace("torch::jit") public static native void ExportModule(
    @Const @ByRef JitModule module,
    @Cast("std::ostream*") @ByRef Pointer out,
    @Const @ByRef(nullValue = "torch::jit::ExtraFilesMap()") ExtraFilesMap metadata,
    @Cast("bool") boolean bytecode_format/*=false*/,
    @Cast("bool") boolean save_mobile_debug_info/*=false*/,
    @Cast("bool") boolean use_flatbuffer/*=false*/);
@Namespace("torch::jit") public static native void ExportModule(
    @Const @ByRef JitModule module,
    @Cast("std::ostream*") @ByRef Pointer out);

@Namespace("torch::jit") public static native void ExportModule(
    @Const @ByRef JitModule module,
    @StdString BytePointer filename,
    @Const @ByRef(nullValue = "torch::jit::ExtraFilesMap()") ExtraFilesMap metadata,
    @Cast("bool") boolean bytecode_format/*=false*/,
    @Cast("bool") boolean save_mobile_debug_info/*=false*/,
    @Cast("bool") boolean use_flatbuffer/*=false*/);
@Namespace("torch::jit") public static native void ExportModule(
    @Const @ByRef JitModule module,
    @StdString BytePointer filename);
@Namespace("torch::jit") public static native void ExportModule(
    @Const @ByRef JitModule module,
    @StdString String filename,
    @Const @ByRef(nullValue = "torch::jit::ExtraFilesMap()") ExtraFilesMap metadata,
    @Cast("bool") boolean bytecode_format/*=false*/,
    @Cast("bool") boolean save_mobile_debug_info/*=false*/,
    @Cast("bool") boolean use_flatbuffer/*=false*/);
@Namespace("torch::jit") public static native void ExportModule(
    @Const @ByRef JitModule module,
    @StdString String filename);

@Namespace("torch::jit") public static native void ExportModule(
    @Const @ByRef JitModule module,
    @Const @ByRef WriteFunction writer_func,
    @Const @ByRef(nullValue = "torch::jit::ExtraFilesMap()") ExtraFilesMap metadata,
    @Cast("bool") boolean bytecode_format/*=false*/,
    @Cast("bool") boolean save_mobile_debug_info/*=false*/,
    @Cast("bool") boolean use_flatbuffer/*=false*/);
@Namespace("torch::jit") public static native void ExportModule(
    @Const @ByRef JitModule module,
    @Const @ByRef WriteFunction writer_func);

// Write the bytes of a pickle archive and the tensors referenced inside that
// archive
@Namespace("torch::jit") public static native void writeArchiveAndTensors(
    @StdString BytePointer archive_name,
    @Cast("const char*") BytePointer pickle_bytes,
    @Cast("size_t") long size,
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensors,
    @Cast("caffe2::serialize::PyTorchStreamWriter*") @ByRef Pointer out);
@Namespace("torch::jit") public static native void writeArchiveAndTensors(
    @StdString String archive_name,
    String pickle_bytes,
    @Cast("size_t") long size,
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector tensors,
    @Cast("caffe2::serialize::PyTorchStreamWriter*") @ByRef Pointer out);

// Surrounding system can install an additional hook to produce extra files
// with metadata based on environment every time a module is serialized.
@Namespace("torch::jit") public static native void SetExportModuleExtraFilesHook(@ByVal @Cast("torch::jit::ExportModuleExtraFilesHook*") Pointer hook);

/**
 * Generates new bytecode for a Script module and returns what the op list
 * would be for a LiteScriptModule based off the current code base. If you
 * have a LiteScriptModule and want to get the currently present
 * list of ops call _export_operator_list instead.
 */
@Namespace("torch::jit") public static native @ByVal StringVector export_opnames(@Const @ByRef JitModule m);
// Targeting ../BytecodeEmitMode.java


// Targeting ../BytecodeEmitModeGuard.java



@Namespace("torch::jit") public static native @ByVal IValue to_tuple(@ByVal IValueVector ivalues);
@Namespace("torch::jit") public static native @ByVal IValue Table(@StdVector EnumNameValue entries);

// TODO remove these switches once interface call is rolled out.
@Namespace("torch::jit") public static native void enableMobileInterfaceCallExport();




@Namespace("torch::jit") public static native void save_jit_module(
    @Const @ByRef JitModule module,
    @StdString BytePointer filename,
    @Const @ByRef(nullValue = "torch::jit::ExtraFilesMap()") ExtraFilesMap extra_files);
@Namespace("torch::jit") public static native void save_jit_module(
    @Const @ByRef JitModule module,
    @StdString BytePointer filename);
@Namespace("torch::jit") public static native void save_jit_module(
    @Const @ByRef JitModule module,
    @StdString String filename,
    @Const @ByRef(nullValue = "torch::jit::ExtraFilesMap()") ExtraFilesMap extra_files);
@Namespace("torch::jit") public static native void save_jit_module(
    @Const @ByRef JitModule module,
    @StdString String filename);

@Namespace("torch::jit") public static native @ByVal @Cast("torch::jit::DetachedBuffer::UniqueDetachedBuffer*") Pointer save_jit_module_to_bytes(
    @Const @ByRef JitModule module,
    @Const @ByRef(nullValue = "torch::jit::ExtraFilesMap()") ExtraFilesMap extra_files);
@Namespace("torch::jit") public static native @ByVal @Cast("torch::jit::DetachedBuffer::UniqueDetachedBuffer*") Pointer save_jit_module_to_bytes(
    @Const @ByRef JitModule module);

@Namespace("torch::jit") public static native void save_jit_module_to_write_func(
    @Const @ByRef JitModule module,
    @Const @ByRef ExtraFilesMap extra_files,
    @Cast("bool") boolean save_mobile_debug_info,
    @Const @ByRef WriteFunction writer_func);

 // namespace jit
 // namespace torch


// Parsed from torch/arg.h

// #pragma once

// #include <utility>

// #define TORCH_ARG(T, name)
//  public:
//   inline auto name(const T& new_##name)->decltype(*this) { /* NOLINT */
//     this->name##_ = new_##name;
//     return *this;
//   }
//   inline auto name(T&& new_##name)->decltype(*this) { /* NOLINT */
//     this->name##_ = std::move(new_##name);
//     return *this;
//   }
//   inline const T& name() const noexcept { /* NOLINT */
//     return this->name##_;
//   }
//   inline T& name() noexcept { /* NOLINT */
//     return this->name##_;
//   }
// 
//  private:
//   T name##_ /* NOLINT */


// Parsed from torch/enum.h

// #pragma once

// #include <string>

// #include <ATen/core/Reduction.h>
// #include <c10/util/Exception.h>
// #include <c10/util/variant.h>
// #include <torch/csrc/Export.h>

// #define TORCH_ENUM_DECLARE(name)
//   namespace torch {
//   namespace enumtype {
//   /*                                                                  \
//    NOTE: We need to provide the default constructor for each struct, \
//    otherwise Clang 3.8 would complain:                               \
//    ```                                                               \
//    error: default initialization of an object of const type 'const   \
//    enumtype::Enum1' without a user-provided default constructor      \
//    ```                                                               \
//  */
//   struct k##name {
//     k##name() {}
//   };
//   }
//   TORCH_API extern const enumtype::k##name k##name;
//   }

// #define TORCH_ENUM_DEFINE(name)
//   namespace torch {
//   const enumtype::k##name k##name;
//   }

// #define TORCH_ENUM_PRETTY_PRINT(name)
//   std::string operator()(const enumtype::k##name& v) const {
//     std::string k("k");
//     return k + #name;
//   }

// NOTE: Backstory on why we need the following two macros:
//
// Consider the following options class:
//
// ```
// struct TORCH_API SomeOptions {
//   typedef c10::variant<enumtype::kNone, enumtype::kMean, enumtype::kSum>
//   reduction_t; SomeOptions(reduction_t reduction = torch::kMean) :
//   reduction_(reduction) {}
//
//   TORCH_ARG(reduction_t, reduction);
// };
// ```
//
// and the functional that uses it:
//
// ```
// Tensor some_functional(
//     const Tensor& input,
//     SomeOptions options = {}) {
//   ...
// }
// ```
//
// Normally, we would expect this to work:
//
// `F::some_functional(input, torch::kNone)`
//
// However, it throws the following error instead:
//
// ```
// error: could not convert `torch::kNone` from `const torch::enumtype::kNone`
// to `torch::nn::SomeOptions`
// ```
//
// To get around this problem, we explicitly provide the following constructors
// for `SomeOptions`:
//
// ```
// SomeOptions(torch::enumtype::kNone reduction) : reduction_(torch::kNone) {}
// SomeOptions(torch::enumtype::kMean reduction) : reduction_(torch::kMean) {}
// SomeOptions(torch::enumtype::kSum reduction) : reduction_(torch::kSum) {}
// ```
//
// so that the conversion from `torch::kNone` to `SomeOptions` would work.
//
// Note that we also provide the default constructor `SomeOptions() {}`, so that
// `SomeOptions options = {}` can work.
// #define TORCH_OPTIONS_CTOR_VARIANT_ARG3(
//     OPTIONS_NAME, ARG_NAME, TYPE1, TYPE2, TYPE3)
//   OPTIONS_NAME() = default;
//   OPTIONS_NAME(torch::enumtype::TYPE1 ARG_NAME) : ARG_NAME##_(torch::TYPE1) {}
//   OPTIONS_NAME(torch::enumtype::TYPE2 ARG_NAME) : ARG_NAME##_(torch::TYPE2) {}
//   OPTIONS_NAME(torch::enumtype::TYPE3 ARG_NAME) : ARG_NAME##_(torch::TYPE3) {}

// #define TORCH_OPTIONS_CTOR_VARIANT_ARG4(
//     OPTIONS_NAME, ARG_NAME, TYPE1, TYPE2, TYPE3, TYPE4)
//   OPTIONS_NAME() = default;
//   OPTIONS_NAME(torch::enumtype::TYPE1 ARG_NAME) : ARG_NAME##_(torch::TYPE1) {}
//   OPTIONS_NAME(torch::enumtype::TYPE2 ARG_NAME) : ARG_NAME##_(torch::TYPE2) {}
//   OPTIONS_NAME(torch::enumtype::TYPE3 ARG_NAME) : ARG_NAME##_(torch::TYPE3) {}
//   OPTIONS_NAME(torch::enumtype::TYPE4 ARG_NAME) : ARG_NAME##_(torch::TYPE4) {}
// Targeting ../kLinear.java

  
// Targeting ../kConv1D.java

  
// Targeting ../kConv2D.java

  
// Targeting ../kConv3D.java

  
// Targeting ../kConvTranspose1D.java

  
// Targeting ../kConvTranspose2D.java

  
// Targeting ../kConvTranspose3D.java

  
// Targeting ../kSigmoid.java

  
// Targeting ../kTanh.java

  
// Targeting ../kReLU.java

  
// Targeting ../kGELU.java

  
// Targeting ../kSiLU.java

  
// Targeting ../kMish.java

  
// Targeting ../kLeakyReLU.java

  
// Targeting ../kFanIn.java

  
// Targeting ../kFanOut.java

  
// Targeting ../kConstant.java

  
// Targeting ../kReflect.java

  
// Targeting ../kReplicate.java

  
// Targeting ../kCircular.java

  
// Targeting ../kNearest.java

  
// Targeting ../kBilinear.java

  
// Targeting ../kBicubic.java

  
// Targeting ../kTrilinear.java

  
// Targeting ../kArea.java

  
// Targeting ../kNearestExact.java

  
// Targeting ../kSum.java

  
// Targeting ../kMean.java

  
// Targeting ../kMax.java

  
// Targeting ../kNone.java

  
// Targeting ../kBatchMean.java

  
// Targeting ../kZeros.java

  
// Targeting ../kBorder.java

  
// Targeting ../kReflection.java

  
// Targeting ../kRNN_TANH.java

  
// Targeting ../kRNN_RELU.java

  
// Targeting ../kLSTM.java

  
// Targeting ../kGRU.java

  
// Targeting ../kValid.java

  
// Targeting ../kSame.java

  
// Targeting ../_compute_enum_name.java



 // namespace enumtype
 // namespace torch


// Parsed from torch/types.h

// #pragma once

// #include <ATen/ATen.h>

// #include <c10/util/Optional.h>

// #include <torch/csrc/autograd/generated/variable_factories.h>
// #include <torch/csrc/autograd/variable.h>

// TODO: These don't really belong here but torchvision builds in CI need them
// Remove once the torchvision version being compiled in CI is updated
// #include <ATen/core/dispatch/Dispatcher.h>
// #include <torch/library.h>

// NOTE [ Exposing declarations in `at::` to `torch::` ]
//
// The following line `using namespace at;` is responsible for exposing all
// declarations in `at::` namespace to `torch::` namespace.
//
// According to the rules laid out in
// https://en.cppreference.com/w/cpp/language/qualified_lookup, section
// "Namespace members":
// ```
// Qualified lookup within the scope of a namespace N first considers all
// declarations that are located in N and all declarations that are located in
// the inline namespace members of N (and, transitively, in their inline
// namespace members). If there are no declarations in that set then it
// considers declarations in all namespaces named by using-directives found in N
// and in all transitive inline namespace members of N.
// ```
//
// This means that if both `at::` and `torch::` namespaces have a function with
// the same signature (e.g. both `at::func()` and `torch::func()` exist), after
// `namespace torch { using namespace at; }`, when we call `torch::func()`, the
// `func()` function defined in `torch::` namespace will always be called, and
// the `func()` function defined in `at::` namespace is always hidden. // NOLINT

/** Fixed width dtypes. */

/** Rust-style short dtypes. */
 // namespace torch


// Parsed from torch/utils.h

// #pragma once

// #include <ATen/Parallel.h>
// #include <ATen/record_function.h>
// #include <torch/csrc/api/include/torch/types.h>
// #include <torch/csrc/autograd/grad_mode.h>
// #include <torch/csrc/autograd/profiler.h>
// #include <cstdint>

/** A RAII, thread-local guard that disabled gradient calculation.
 * 
 *  Disabling gradient calculation is useful for inference, when you are sure
 *  that you will not call {@code at::Tensor::backward}. It will reduce memory
 *  consumption for computations that would otherwise have {@code requires_grad() ==
 *  true}.
 * 
 *  In this mode, the result of every computation will have
 *  {@code requires_grad() == false}, even when the inputs have {@code requires_grad() ==
 *  true}.
 * 
 *  This context manager is thread-local; it will not affect computation
 *  in other threads.
 * 
 *  Example:
 *  <pre>{@code
 *  auto x = torch::tensor({1.}, torch::requires_grad());
 *  {
 *    torch::NoGradGuard no_grad;
 *    auto y = x * 2;
 *    std::cout << y.requires_grad() << std::endl; // prints `false`
 *  }
 *  {
 *    auto doubler = [](torch::Tensor x) {
 *      torch::NoGradGuard no_grad;
 *      return x * 2;
 *    };
 *    auto z = doubler(x);
 *    std::cout << z.requires_grad() << std::endl; // prints `false`
 *  }
 *  }</pre> */

///
///
///
///

/** A RAII, thread-local guard that sets gradient calculation to on or off.
 * 
 *  {@code }AutoGradMode{@code } will enable or disable grads based on its argument
 *  {@code enabled}.
 * 
 *  This context manager is thread-local; it will not affect computation
 *  in other threads.
 * 
 *  @param enabled: Flag whether to enable grad ({@code }true{@code }), or disable
 *               ({@code }false{@code }). This can be used to conditionally enable
 *               gradients.
 * 
 *  Example:
 *  <pre>{@code
 *  auto x = torch::tensor({1.}, torch::requires_grad());
 *  {
 *    torch::AutoGradMode enable_grad(true);
 *    auto y = x * 2;
 *    std::cout << y.requires_grad() << std::endl; // prints `true`
 *  }
 *  {
 *    torch::AutoGradMode enable_grad(false);
 *    auto y = x * 2;
 *    std::cout << y.requires_grad() << std::endl; // prints `false`
 *  }
 *  }</pre> */

/** Sets the global random seed for all newly created CPU and CUDA tensors. */

// Called during new thread initialization

// Returns the number of threads used in parallel region.

// Sets the number of threads to be used in parallel region.

// Returns the number of threads used for inter-op parallelism.

// Sets the number of threads to be used for inter-op parallelism.

// Returns true if both t1, t2 are undefined or both are defined and equal
@Namespace("torch") public static native @Cast("bool") boolean equal_if_defined(@ByVal Tensor t1, @ByVal Tensor t2);

// RecordFunction API

 // namespace torch


// Parsed from torch/data.h

// #pragma once

// #include <torch/data/dataloader.h>
// #include <torch/data/datasets.h>
// #include <torch/data/samplers.h>
// #include <torch/data/transforms.h>

// Some "exports".
 // namespace data
 // namespace torch


// Parsed from torch/data/example.h

// #pragma once

// #include <torch/types.h>
// Targeting ../Example.java


// Targeting ../TensorExample.java


// Targeting ../NoTarget.java


 // namespace example

/** A specialization for {@code Example} that does not have a target.
 * 
 *  This class exists so that code can be written for a templated {@code Example}
 *  type, and work both for labeled and unlabeled datasets. */
 // namespace data
 // namespace torch


// Parsed from torch/data/iterator.h

// #pragma once

// #include <torch/csrc/utils/variadic.h>
// #include <torch/types.h>

// #include <c10/util/Exception.h>

// #include <functional>
// #include <iterator>
// #include <memory>
// #include <type_traits>
// #include <utility>
// For increased safety and more separated logic, this implementation of
// `Iterator` consists of a `ValidIterator` and a `SentinelIterator`. A
// `ValidIterator` yields new batches until the `DataLoader` is exhausted. While
// the `DataLoader` is not exhausted, `ValidIterator`s compare equal if they are
// the same object. When the `ValidIterator` becomes exhausted, it compares
// equal to the `SentinelIterator`, but not before. Half the code here is to
// implement double dispatch for the comparison. Got damnit, C++.

/** Base class for the {@code ValidIterator} and {@code SentinelIterator} */

// Targeting ../ExampleIterator.java


// Targeting ../ExampleVectorIterator.java


// Targeting ../ExampleVectorOptionalIterator.java


 // namespace data
 // namespace torch


// Parsed from torch/data/worker_exception.h

// #pragma once

// #include <exception>
// #include <string>
// #include <utility>
// Targeting ../WorkerException.java



 // namespace data
 // namespace torch


// Parsed from torch/data/dataloader.h

// #pragma once

// #include <torch/data/dataloader/stateful.h>
// #include <torch/data/dataloader/stateless.h>

// #include <torch/csrc/utils/memory.h>
// #include <torch/csrc/utils/variadic.h>

// #include <c10/util/Exception.h>

// #include <cstddef>
// #include <memory>
// #include <type_traits>
// #include <utility>

/** Creates a {@code DataLoader} instance for a stateless {@code dataset}, a {@code sampler} and
 *  some {@code options}. */

/** Creates a {@code DataLoader} instance for a stateless {@code dataset} and some
 *  {@code options}. A sampler (by default a {@code RandomSampler}) will be constructed from
 *  the size of the dataset. */

/** Creates a {@code DataLoader} for a stateful {@code dataset} and some {@code options}. */
 // namespace data
 // namespace torch


// Parsed from torch/data/dataloader/base.h

// #pragma once

// #include <torch/data/dataloader_options.h>
// #include <torch/data/detail/data_shuttle.h>
// #include <torch/data/detail/sequencers.h>
// #include <torch/data/iterator.h>
// #include <torch/data/samplers/random.h>
// #include <torch/data/worker_exception.h>
// #include <torch/types.h>

// #include <torch/csrc/utils/memory.h>
// #include <torch/csrc/utils/variadic.h>

// #include <c10/util/Exception.h>
// #include <c10/util/irange.h>

// #include <cstddef>
// #include <exception>
// #include <memory>
// #include <thread>
// #include <type_traits>
// #include <utility>
// #include <vector>
// Targeting ../ChunkRandomDataLoaderBase.java


// Targeting ../MNISTRandomDataLoaderBase.java


 // namespace data
 // namespace torch


// Parsed from torch/data/dataloader_options.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/types.h>

// #include <chrono>
// #include <cstddef>
// Targeting ../DataLoaderOptions.java


// Targeting ../FullDataLoaderOptions.java


 // namespace data
 // namespace torch


// Parsed from torch/data/dataloader/stateful.h

// #pragma once

// #include <c10/util/irange.h>
// #include <torch/data/dataloader/base.h>

// #include <cstddef>
// #include <thread>
// #include <utility>
// Targeting ../ChunkRandomDataLoader.java


 // namespace data
 // namespace torch


// Parsed from torch/data/dataloader/stateless.h

// #pragma once

// #include <torch/data/dataloader/base.h>
// #include <torch/data/worker_exception.h>

// #include <torch/csrc/utils/memory.h>

// #include <c10/util/Exception.h>
// #include <c10/util/irange.h>

// #include <cstddef>
// #include <thread>
// #include <utility>
// Targeting ../MNISTRandomDataLoader.java


 // namespace data
 // namespace torch


// Parsed from torch/data/datasets.h

// #pragma once

// #include <torch/data/datasets/base.h>
// #include <torch/data/datasets/chunk.h>
// #include <torch/data/datasets/map.h>
// #include <torch/data/datasets/mnist.h>
// #include <torch/data/datasets/shared.h>
// #include <torch/data/datasets/stateful.h>
// #include <torch/data/datasets/tensor.h>


// Parsed from torch/data/datasets/base.h

// #pragma once

// #include <torch/data/example.h>
// #include <torch/types.h>

// #include <c10/util/ArrayRef.h>

// #include <cstddef>
// #include <cstdint>
// #include <type_traits>
// #include <utility>
// #include <vector> // NOLINT
 // namespace datasets
 // namespace data
 // namespace torch

// Targeting ../ChunkBatchDataset.java


// Targeting ../ChunkBatchSharedBatchDataset.java


// Targeting ../ChunkMapBatchDataset.java


// Targeting ../MNISTBatchDataset.java


// Targeting ../MNISTMapBatchDataset.java


// Targeting ../TensorExampleBatchDataset.java


// Targeting ../MNISTDataset.java


// Targeting ../TensorExampleDataset.java



/** A {@code StreamDataset} represents a dataset that is a potentially infinite
 *  stream. It takes as batch index only a number, which is the batch size, and
 *  yields that many elements from the stream. */
 // namespace datasets
 // namespace data
 // namespace torch


// Parsed from torch/data/datasets/chunk.h

// #pragma once

// #include <c10/util/irange.h>
// #include <torch/arg.h>
// #include <torch/csrc/utils/memory.h>
// #include <torch/data/datasets/stateful.h>
// #include <torch/data/samplers.h>
// #include <queue>
// #include <thread>

// #include <torch/serialize.h>
// Targeting ../ChunkDataReader.java


/** BatchDataBuffer manages a queue of UnwrappedBatchData. After a new chunk is
 *  loaded, BatchDataBuffer splits it into small batches and push them into the
 *  queue. When get_batch is called from data loader, it pops cached batches and
 *  return. If the cache is empty, it either waits to load more chunks or return
 *  null if all chunks are loaded. */

// Targeting ../ChunkDatasetOptions.java


// Targeting ../ChunkDataset.java


 // namespace datasets
 // namespace data
 // namespace torch


// Parsed from torch/data/datasets/map.h

// #pragma once

// #include <torch/data/datasets/base.h>
// #include <torch/types.h>

// #include <c10/util/ArrayRef.h>

// #include <cstddef>
// #include <type_traits>
// #include <utility>

// Targeting ../ChunkMapDataset.java


// Targeting ../MNISTMapDataset.java



/** Creates a {@code MapDataset} with the given dataset and transform. */

 // namespace datasets
 // namespace data
 // namespace torch


// Parsed from torch/data/datasets/mnist.h

// #pragma once

// #include <torch/data/datasets/base.h>
// #include <torch/data/example.h>
// #include <torch/types.h>

// #include <torch/csrc/Export.h>

// #include <cstddef>
// #include <string>
// Targeting ../MNIST.java


 // namespace datasets
 // namespace data
 // namespace torch


// Parsed from torch/data/datasets/shared.h

// #pragma once

// #include <torch/data/datasets/base.h>

// #include <memory>
// #include <utility>
// Targeting ../ChunkSharedBatchDataset.java



/** Constructs a new {@code SharedBatchDataset} by creating a
 *  {@code shared_ptr<UnderlyingDatase>}. All arguments are forwarded to
 *  {@code make_shared<UnderlyingDataset>}. */
 // namespace datasets
 // namespace data
 // namespace torch


// Parsed from torch/data/datasets/stateful.h

// #pragma once

// #include <torch/data/datasets/base.h>
// #include <torch/data/example.h>

// #include <cstddef>
// #include <vector>
 // namespace serialize
 // namespace torch
// Targeting ../ChunkStatefulDataset.java



/** Serializes a statefulDataset to {@code OutputArchive}. */

/** Deserializes a statefulDataset from an {@code InputArchive}. */

 // namespace datasets
 // namespace data
 // namespace torch


// Parsed from torch/data/datasets/tensor.h

// #pragma once

// #include <torch/data/datasets/base.h>
// #include <torch/data/example.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <vector>
// Targeting ../TensorDataset.java



 // namespace datasets
 // namespace data
 // namespace torch


// Parsed from torch/data/samplers.h

// #pragma once

// #include <torch/data/samplers/base.h>
// #include <torch/data/samplers/custom_batch_request.h>
// #include <torch/data/samplers/distributed.h>
// #include <torch/data/samplers/random.h>
// #include <torch/data/samplers/sequential.h>
// #include <torch/data/samplers/serialize.h>
// #include <torch/data/samplers/stream.h>


// Parsed from torch/data/samplers/base.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <mutex>
// #include <vector>
 // namespace serialize
 // namespace torch
// Targeting ../Sampler.java


// Targeting ../BatchSizeSampler.java



 // namespace samplers
 // namespace data
 // namespace torch


// Parsed from torch/data/samplers/custom_batch_request.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <cstddef>
// Targeting ../CustomBatchRequest.java


 // namespace samplers
 // namespace data
 // namespace torch


// Parsed from torch/data/samplers/distributed.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <torch/data/samplers/base.h>

// #include <cstddef>
// #include <vector>
 // namespace serialize
 // namespace torch
// Targeting ../DistributedSampler.java


// Targeting ../DistributedRandomSampler.java


// Targeting ../DistributedSequentialSampler.java



 // namespace samplers
 // namespace data
 // namespace torch


// Parsed from torch/data/samplers/random.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <torch/data/samplers/base.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <vector>
 // namespace serialize
 // namespace torch
// Targeting ../RandomSampler.java


 // namespace samplers
 // namespace data
 // namespace torch


// Parsed from torch/data/samplers/sequential.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <torch/data/samplers/base.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <vector>
 // namespace serialize
 // namespace torch
// Targeting ../SequentialSampler.java



 // namespace samplers
 // namespace data
 // namespace torch


// Parsed from torch/data/samplers/serialize.h

// #pragma once

// #include <torch/data/samplers/base.h>
// #include <torch/serialize/archive.h>
/** Serializes a {@code Sampler} into an {@code OutputArchive}. */

/** Deserializes a {@code Sampler} from an {@code InputArchive}. */
 // namespace samplers
 // namespace data
 // namespace torch


// Parsed from torch/data/samplers/stream.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <torch/data/samplers/base.h>
// #include <torch/data/samplers/custom_batch_request.h>
// #include <torch/types.h>

// #include <cstddef>
 // namespace serialize
 // namespace torch
// Targeting ../BatchSize.java


// Targeting ../StreamSampler.java



 // namespace samplers
 // namespace data
 // namespace torch


// Parsed from torch/data/transforms.h

// #pragma once

// #include <torch/data/transforms/base.h>
// #include <torch/data/transforms/collate.h>
// #include <torch/data/transforms/lambda.h>
// #include <torch/data/transforms/stack.h>
// #include <torch/data/transforms/tensor.h>


// Parsed from torch/data/transforms/base.h

// #pragma once

// #include <torch/types.h>

// #include <utility>
// #include <vector>
// Targeting ../ExampleCollation.java



/** A transformation of individual input examples to individual output examples.
 * 
 *  Just like a {@code Dataset} is a {@code BatchDataset}, a {@code Transform} is a
 *  {@code BatchTransform} that can operate on the level of individual examples rather
 *  than entire batches. The batch-level transform is implemented (by default)
 *  in terms of the example-level transform, though this can be customized. */
 // namespace transforms
 // namespace data
 // namespace torch


// Parsed from torch/data/transforms/collate.h

// #pragma once

// #include <torch/data/example.h>
// #include <torch/data/transforms/lambda.h>

// #include <vector>

/** A {@code Collation} is a transform that reduces a batch into a single value.
 *  The result is a {@code BatchDataset} that has the type of the single value as its
 *  {@code BatchType}. */

///
///

/** A {@code Collate} allows passing a custom function to reduce/collate a batch
 *  into a single value. It's effectively the lambda version of {@code Collation},
 *  which you could subclass and override {@code operator()} to achieve the same.
 * 
 *  \rst
 *  .. code-block:: cpp
 *    using namespace torch::data;
 * 
 *    auto dataset = datasets::MNIST("path/to/mnist")
 *      .map(transforms::Collate<Example<>>([](std::vector<Example<>> e) {
 *        return std::move(e.front());
 *      }));
 *  \endrst */
 // namespace transforms
 // namespace data
 // namespace torch


// Parsed from torch/data/transforms/lambda.h

// #pragma once

// #include <torch/data/transforms/base.h>

// #include <functional>
// #include <utility>
// #include <vector>

/** A {@code BatchTransform} that applies a user-provided functor to a batch. */

// A `Transform` that applies a user-provided functor to individual examples.

 // namespace transforms
 // namespace data
 // namespace torch


// Parsed from torch/data/transforms/stack.h

// #pragma once

// #include <torch/data/example.h>
// #include <torch/data/transforms/collate.h>
// #include <torch/types.h>

// #include <utility>
// #include <vector>
// Targeting ../ExampleStack.java



/** A {@code Collation} for {@code Example<Tensor, NoTarget>} types that stacks all data
 *  tensors into one tensor. */
 // namespace transforms
 // namespace data
 // namespace torch


// Parsed from torch/data/transforms/tensor.h

// #pragma once

// #include <torch/data/example.h>
// #include <torch/data/transforms/base.h>
// #include <torch/types.h>

// #include <functional>
// #include <utility>

/** A {@code Transform} that is specialized for the typical {@code Example<Tensor, Tensor>}
 *  combination. It exposes a single {@code operator()} interface hook (for
 *  subclasses), and calls this function on input {@code Example} objects. */

/** A {@code Lambda} specialized for the typical {@code Example<Tensor, Tensor>} input type. */

/** Normalizes input tensors by subtracting the supplied mean and dividing by
 *  the given standard deviation. */
 // namespace transforms
 // namespace data
 // namespace torch


// Parsed from torch/serialize.h

// #pragma once

// #include <c10/util/irange.h>
// #include <torch/csrc/Export.h>
// #include <torch/serialize/archive.h>
// #include <torch/serialize/tensor.h>

// #include <utility>

/** Serializes the given {@code value}.
 *  There must be an overload of {@code operator<<} between {@code serialize::OutputArchive}
 *  and {@code Value} for this method to be well-formed. Currently, such an overload
 *  is provided for (subclasses of):
 * 
 *  - {@code torch::nn::Module},
 *  - {@code torch::optim::Optimizer}
 *  - {@code torch::Tensor}
 * 
 *  To perform the serialization, a {@code serialize::OutputArchive} is constructed,
 *  and all arguments after the {@code value} are forwarded to its {@code save_to} method.
 *  For example, you can pass a filename, or an {@code ostream}.
 * 
 *  \rst
 *  .. code-block:: cpp
 * 
 *    torch::nn::Linear model(3, 4);
 *    torch::save(model, "model.pt");
 * 
 *    torch::optim::SGD sgd(/*lr=* /0.9);
 *    std::ostringstream stream;
 *    // Note that the same stream cannot be used in multiple torch::save(...)
 *    // invocations, otherwise the header will be corrupted.
 *    torch::save(sgd, stream);
 * 
 *    auto tensor = torch::ones({3, 4});
 *    torch::save(tensor, "my_tensor.pt");
 *  \endrst */

/** Serializes the given {@code tensor_vec} of type {@code std::vector<torch::Tensor>}.
 * 
 *  To perform the serialization, a {@code serialize::OutputArchive} is constructed,
 *  and all arguments after the {@code tensor_vec} are forwarded to its {@code save_to}
 *  method. For example, you can pass a filename, or an {@code ostream}.
 * 
 *  \rst
 *  .. code-block:: cpp
 * 
 *    std::vector<torch::Tensor> tensor_vec = { torch::randn({1, 2}),
 *    torch::randn({3, 4}) }; torch::save(tensor_vec, "my_tensor_vec.pt");
 * 
 *    std::vector<torch::Tensor> tensor_vec = { torch::randn({5, 6}),
 *    torch::randn({7, 8}) }; std::ostringstream stream;
 *    // Note that the same stream cannot be used in multiple torch::save(...)
 *    // invocations, otherwise the header will be corrupted.
 *    torch::save(tensor_vec, stream);
 *  \endrst */

/** Deserializes the given {@code value}.
 *  There must be an overload of {@code operator>>} between {@code serialize::InputArchive}
 *  and {@code Value} for this method to be well-formed. Currently, such an overload
 *  is provided for (subclasses of):
 * 
 *  - {@code torch::nn::Module},
 *  - {@code torch::optim::Optimizer}
 *  - {@code torch::Tensor}
 * 
 *  To perform the serialization, a {@code serialize::InputArchive} is constructed,
 *  and all arguments after the {@code value} are forwarded to its {@code load_from} method.
 *  For example, you can pass a filename, or an {@code istream}.
 * 
 *  \rst
 *  .. code-block:: cpp
 * 
 *    torch::nn::Linear model(3, 4);
 *    torch::load(model, "model.pt");
 * 
 *    torch::optim::SGD sgd(/*lr=* /0.9);
 *    std::istringstream stream("...");
 *    torch::load(sgd, stream);
 * 
 *    auto tensor = torch::ones({3, 4});
 *    torch::load(tensor, "my_tensor.pt");
 *  \endrst */

/** Deserializes the given {@code tensor_vec} of type {@code std::vector<torch::Tensor>}.
 * 
 *  To perform the serialization, a {@code serialize::InputArchive} is constructed,
 *  and all arguments after the {@code value} are forwarded to its {@code load_from} method.
 *  For example, you can pass a filename, or an {@code istream}.
 * 
 *  \rst
 *  .. code-block:: cpp
 * 
 *    std::vector<torch::Tensor> tensor_vec;
 *    torch::load(tensor_vec, "my_tensor_vec.pt");
 * 
 *    std::vector<torch::Tensor> tensor_vec;
 *    std::istringstream stream("...");
 *    torch::load(tensor_vec, stream);
 *  \endrst */
 // namespace torch


// Parsed from torch/serialize/archive.h

// #pragma once

// #include <torch/serialize/input-archive.h>
// #include <torch/serialize/output-archive.h>


// Parsed from torch/serialize/input-archive.h

// #pragma once

// #include <c10/core/Device.h>
// #include <c10/util/Optional.h>
// #include <torch/csrc/Export.h>
// #include <torch/csrc/jit/api/module.h>
// #include <torch/types.h>

// #include <iosfwd>
// #include <memory>
// #include <string>
// #include <utility>
 // namespace at
 // namespace jit
 // namespace torch
// Targeting ../InputArchive.java


 // namespace serialize
 // namespace torch


// Parsed from torch/serialize/output-archive.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <torch/csrc/jit/api/module.h>

// #include <iosfwd>
// #include <memory>
// #include <string>
// #include <utility>
 // namespace at
 // namespace jit

// Targeting ../OutputArchive.java


 // namespace serialize
 // namespace torch


// Parsed from torch/serialize/tensor.h

// #pragma once

// #include <torch/serialize/archive.h>
// #include <torch/types.h>
@Namespace("torch") public static native @ByRef @Name("operator <<") OutputArchive shiftLeft(
    @ByRef OutputArchive archive,
    @Const @ByRef Tensor tensor);

@Namespace("torch") public static native @ByRef @Name("operator >>") InputArchive shiftRight(
    @ByRef InputArchive archive,
    @ByRef Tensor tensor);
 // namespace torch


// Parsed from torch/nn.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional.h>
// #include <torch/nn/init.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules.h>
// #include <torch/nn/options.h>
// #include <torch/nn/pimpl.h>
// #include <torch/nn/utils.h>


// Parsed from torch/nn/cloneable.h

// #pragma once

// #include <torch/nn/module.h>
// #include <torch/types.h>
// #include <torch/utils.h>

// #include <c10/core/TensorOptions.h>
// #include <c10/util/Exception.h>

// #include <memory>
// #include <utility>
// Targeting ../ModuleDictImplCloneable.java


// Targeting ../ModuleListImplCloneable.java


// Targeting ../SequentialImplCloneable.java


// Targeting ../ParameterDictImplCloneable.java


// Targeting ../ParameterListImplCloneable.java


// Targeting ../AdaptiveLogSoftmaxWithLossImplCloneable.java


// Targeting ../BatchNorm1dImplCloneable.java


// Targeting ../InstanceNorm1dImplCloneable.java


// Targeting ../Conv1dImplCloneable.java


// Targeting ../ConvTranspose1dImplCloneable.java


// Targeting ../DropoutImplCloneable.java


// Targeting ../BatchNorm2dImplCloneable.java


// Targeting ../InstanceNorm2dImplCloneable.java


// Targeting ../Conv2dImplCloneable.java


// Targeting ../ConvTranspose2dImplCloneable.java


// Targeting ../Dropout2dImplCloneable.java


// Targeting ../BatchNorm3dImplCloneable.java


// Targeting ../InstanceNorm3dImplCloneable.java


// Targeting ../Conv3dImplCloneable.java


// Targeting ../ConvTranspose3dImplCloneable.java


// Targeting ../Dropout3dImplCloneable.java


// Targeting ../AlphaDropoutImplCloneable.java


// Targeting ../FeatureAlphaDropoutImplCloneable.java


// Targeting ../CosineSimilarityImplCloneable.java


// Targeting ../PairwiseDistanceImplCloneable.java


// Targeting ../EmbeddingImplCloneable.java


// Targeting ../EmbeddingBagImplCloneable.java


// Targeting ../FoldImplCloneable.java


// Targeting ../UnfoldImplCloneable.java


// Targeting ../IdentityImplCloneable.java


// Targeting ../LinearImplCloneable.java


// Targeting ../BilinearImplCloneable.java


// Targeting ../FlattenImplCloneable.java


// Targeting ../UnflattenImplCloneable.java


// Targeting ../L1LossImplCloneable.java


// Targeting ../KLDivLossImplCloneable.java


// Targeting ../MSELossImplCloneable.java


// Targeting ../BCELossImplCloneable.java


// Targeting ../HingeEmbeddingLossImplCloneable.java


// Targeting ../MultiMarginLossImplCloneable.java


// Targeting ../CosineEmbeddingLossImplCloneable.java


// Targeting ../SmoothL1LossImplCloneable.java


// Targeting ../HuberLossImplCloneable.java


// Targeting ../MultiLabelMarginLossImplCloneable.java


// Targeting ../SoftMarginLossImplCloneable.java


// Targeting ../MultiLabelSoftMarginLossImplCloneable.java


// Targeting ../TripletMarginLossImplCloneable.java


// Targeting ../TripletMarginWithDistanceLossImplCloneable.java


// Targeting ../CTCLossImplCloneable.java


// Targeting ../PoissonNLLLossImplCloneable.java


// Targeting ../MarginRankingLossImplCloneable.java


// Targeting ../NLLLossImplCloneable.java


// Targeting ../CrossEntropyLossImplCloneable.java


// Targeting ../BCEWithLogitsLossImplCloneable.java


// Targeting ../ReflectionPad1dImplCloneable.java


// Targeting ../ReplicationPad1dImplCloneable.java


// Targeting ../ConstantPad1dImplCloneable.java


// Targeting ../AvgPool1dImplCloneable.java


// Targeting ../MaxPool1dImplCloneable.java


// Targeting ../AdaptiveAvgPool1dImplCloneable.java


// Targeting ../AdaptiveMaxPool1dImplCloneable.java


// Targeting ../MaxUnpool1dImplCloneable.java


// Targeting ../LPPool1dImplCloneable.java


// Targeting ../ReflectionPad2dImplCloneable.java


// Targeting ../ReplicationPad2dImplCloneable.java


// Targeting ../ConstantPad2dImplCloneable.java


// Targeting ../ZeroPad2dImplCloneable.java


// Targeting ../AvgPool2dImplCloneable.java


// Targeting ../MaxPool2dImplCloneable.java


// Targeting ../AdaptiveAvgPool2dImplCloneable.java


// Targeting ../AdaptiveMaxPool2dImplCloneable.java


// Targeting ../MaxUnpool2dImplCloneable.java


// Targeting ../FractionalMaxPool2dImplCloneable.java


// Targeting ../LPPool2dImplCloneable.java


// Targeting ../ReflectionPad3dImplCloneable.java


// Targeting ../ReplicationPad3dImplCloneable.java


// Targeting ../ConstantPad3dImplCloneable.java


// Targeting ../AvgPool3dImplCloneable.java


// Targeting ../MaxPool3dImplCloneable.java


// Targeting ../AdaptiveAvgPool3dImplCloneable.java


// Targeting ../AdaptiveMaxPool3dImplCloneable.java


// Targeting ../MaxUnpool3dImplCloneable.java


// Targeting ../FractionalMaxPool3dImplCloneable.java


// Targeting ../RNNImplCloneable.java


// Targeting ../LSTMImplCloneable.java


// Targeting ../GRUImplCloneable.java


// Targeting ../RNNCellImplCloneable.java


// Targeting ../LSTMCellImplCloneable.java


// Targeting ../GRUCellImplCloneable.java


// Targeting ../PixelShuffleImplCloneable.java


// Targeting ../PixelUnshuffleImplCloneable.java


// Targeting ../UpsampleImplCloneable.java


// Targeting ../ELUImplCloneable.java


// Targeting ../SELUImplCloneable.java


// Targeting ../HardshrinkImplCloneable.java


// Targeting ../HardtanhImplCloneable.java


// Targeting ../LeakyReLUImplCloneable.java


// Targeting ../LogSigmoidImplCloneable.java


// Targeting ../SoftmaxImplCloneable.java


// Targeting ../SoftminImplCloneable.java


// Targeting ../LogSoftmaxImplCloneable.java


// Targeting ../Softmax2dImplCloneable.java


// Targeting ../PReLUImplCloneable.java


// Targeting ../ReLUImplCloneable.java


// Targeting ../ReLU6ImplCloneable.java


// Targeting ../RReLUImplCloneable.java


// Targeting ../CELUImplCloneable.java


// Targeting ../GLUImplCloneable.java


// Targeting ../GELUImplCloneable.java


// Targeting ../SiLUImplCloneable.java


// Targeting ../MishImplCloneable.java


// Targeting ../SigmoidImplCloneable.java


// Targeting ../SoftplusImplCloneable.java


// Targeting ../SoftshrinkImplCloneable.java


// Targeting ../SoftsignImplCloneable.java


// Targeting ../TanhImplCloneable.java


// Targeting ../TanhshrinkImplCloneable.java


// Targeting ../ThresholdImplCloneable.java


// Targeting ../MultiheadAttentionImplCloneable.java


// Targeting ../LayerNormImplCloneable.java


// Targeting ../LocalResponseNormImplCloneable.java


// Targeting ../CrossMapLRN2dImplCloneable.java


// Targeting ../GroupNormImplCloneable.java


// Targeting ../TransformerEncoderLayerImplCloneable.java


// Targeting ../TransformerDecoderLayerImplCloneable.java


// Targeting ../TransformerEncoderImplCloneable.java


// Targeting ../TransformerDecoderImplCloneable.java


// Targeting ../TransformerImplCloneable.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/init.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/types.h>

 // namespace init
 // namespace nn

/** Return the recommended gain value for the given nonlinearity function. */
@Namespace("torch::nn::init") public static native double calculate_gain(
    @ByVal NonlinearityType nonlinearity,
    double param/*=0.01*/);
@Namespace("torch::nn::init") public static native double calculate_gain(
    @ByVal NonlinearityType nonlinearity);

/** Fills the given {@code tensor} with the provided {@code value} in-place, and returns it.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor constant_(@ByVal Tensor tensor, @ByVal Scalar value);

/** Fills the given {@code tensor} with the Dirac delta function in-place, and returns
 *  it. No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor dirac_(@ByVal Tensor tensor);

/** Fills the given 2-dimensional {@code matrix} with an identity matrix.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor eye_(@ByVal Tensor matrix);

/** Fills the given 2-dimensional {@code matrix} with values drawn from a normal
 *  distribution parameterized by {@code mean} and {@code std}.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor normal_(@ByVal Tensor tensor, double mean/*=0*/, double std/*=1*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor normal_(@ByVal Tensor tensor);

/** Fills the given {@code tensor} with ones.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor ones_(@ByVal Tensor tensor);

/** Fills the input {@code Tensor} with a (semi) orthogonal matrix, as described in
 *  "Exact solutions to the nonlinear dynamics of learning in deep linear neural
 *  networks" - Saxe, A. et al. (2013). The input tensor must have at least 2
 *  dimensions, and for tensors with more than 2 dimensions the trailing
 *  dimensions are flattened.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor orthogonal_(@ByVal Tensor tensor, double gain/*=1.0*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor orthogonal_(@ByVal Tensor tensor);

/** Fills the 2D input {@code Tensor} as a sparse matrix, where the
 *  non-zero elements will be drawn from a centered normal distribution
 *  with the given standard deviation {@code std}, as described in "Deep learning via
 *  Hessian-free optimization" - Martens, J. (2010). The {@code sparsity} is a real
 *  value between 0 and 1 that controls the fraction of elements in each column
 *  to be set to zero.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor sparse_(@ByVal Tensor tensor, double sparsity, double std/*=0.01*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor sparse_(@ByVal Tensor tensor, double sparsity);

/** Fills the given 2-dimensional {@code matrix} with values drawn from a uniform
 *  distribution parameterized by {@code low} and {@code high}.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor uniform_(@ByVal Tensor tensor, double low/*=0*/, double high/*=1*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor uniform_(@ByVal Tensor tensor);

/** Fills the input {@code Tensor} with values according to the method
 *  described in "Delving deep into rectifiers: Surpassing human-level
 *  performance on ImageNet classification" - He, K. et al. (2015), using a
 *  normal distribution. Also known as He initialization.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor kaiming_normal_(
    @ByVal Tensor tensor,
    double a/*=0*/,
    @ByVal(nullValue = "torch::nn::init::FanModeType(torch::kFanIn)") FanModeType mode,
    @ByVal(nullValue = "torch::nn::init::NonlinearityType(torch::kLeakyReLU)") NonlinearityType nonlinearity);
@Namespace("torch::nn::init") public static native @ByVal Tensor kaiming_normal_(
    @ByVal Tensor tensor);

/** Fills the input {@code Tensor} with values according to the method
 *  described in "Delving deep into rectifiers: Surpassing human-level
 *  performance on ImageNet classification" - He, K. et al. (2015), using a
 *  uniform distribution. Also known as He initialization.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor kaiming_uniform_(
    @ByVal Tensor tensor,
    double a/*=0*/,
    @ByVal(nullValue = "torch::nn::init::FanModeType(torch::kFanIn)") FanModeType mode,
    @ByVal(nullValue = "torch::nn::init::NonlinearityType(torch::kLeakyReLU)") NonlinearityType nonlinearity);
@Namespace("torch::nn::init") public static native @ByVal Tensor kaiming_uniform_(
    @ByVal Tensor tensor);

/** Fills the input {@code Tensor} with values according to the method
 *  described in "Understanding the difficulty of training deep feedforward
 *  neural networks" - Glorot, X. & Bengio, Y. (2010). Values are scaled by the
 *  {@code gain} parameter. No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor xavier_normal_(@ByVal Tensor tensor, double gain/*=1.0*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor xavier_normal_(@ByVal Tensor tensor);

/** Fills the input {@code Tensor} with values according to the method
 *  described in "Understanding the difficulty of training deep feedforward
 *  neural networks" - Glorot, X. & Bengio, Y. (2010), using a uniform
 *  distribution. Values are scaled by the {@code gain} parameter
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor xavier_uniform_(@ByVal Tensor tensor, double gain/*=1.0*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor xavier_uniform_(@ByVal Tensor tensor);

/** Fills the given {@code tensor} with zeros.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor zeros_(@ByVal Tensor tensor);

@Namespace("torch::nn::init") public static native @ByVal @Cast("std::tuple<int64_t,int64_t>*") LongPointer _calculate_fan_in_and_fan_out(
    @Const @ByRef Tensor tensor);

 // namespace init
 // namespace nn
 // namespace torch


// Parsed from torch/nn/pimpl.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/detail/static.h>
// #include <torch/serialize/archive.h>
// #include <torch/types.h>

// #include <torch/csrc/utils/variadic.h>

// #include <memory>
// #include <type_traits>
// #include <utility>
// Dump all the template metaprogramming in this file.
// #include <torch/csrc/api/include/torch/nn/pimpl-inl.h>
 // namespace detail
// Targeting ../ModuleHolder.java


// Targeting ../ModuleDictImplModuleHolder.java


// Targeting ../ModuleListImplModuleHolder.java


// Targeting ../SequentialImplModuleHolder.java


// Targeting ../ParameterDictImplModuleHolder.java


// Targeting ../ParameterListImplModuleHolder.java


// Targeting ../AdaptiveLogSoftmaxWithLossImplModuleHolder.java


// Targeting ../BatchNorm1dImplModuleHolder.java


// Targeting ../InstanceNorm1dImplModuleHolder.java


// Targeting ../Conv1dImplModuleHolder.java


// Targeting ../ConvTranspose1dImplModuleHolder.java


// Targeting ../DropoutImplModuleHolder.java


// Targeting ../BatchNorm2dImplModuleHolder.java


// Targeting ../InstanceNorm2dImplModuleHolder.java


// Targeting ../Conv2dImplModuleHolder.java


// Targeting ../ConvTranspose2dImplModuleHolder.java


// Targeting ../Dropout2dImplModuleHolder.java


// Targeting ../BatchNorm3dImplModuleHolder.java


// Targeting ../InstanceNorm3dImplModuleHolder.java


// Targeting ../Conv3dImplModuleHolder.java


// Targeting ../ConvTranspose3dImplModuleHolder.java


// Targeting ../Dropout3dImplModuleHolder.java


// Targeting ../AlphaDropoutImplModuleHolder.java


// Targeting ../FeatureAlphaDropoutImplModuleHolder.java


// Targeting ../CosineSimilarityImplModuleHolder.java


// Targeting ../PairwiseDistanceImplModuleHolder.java


// Targeting ../EmbeddingImplModuleHolder.java


// Targeting ../EmbeddingBagImplModuleHolder.java


// Targeting ../FoldImplModuleHolder.java


// Targeting ../UnfoldImplModuleHolder.java


// Targeting ../IdentityImplModuleHolder.java


// Targeting ../LinearImplModuleHolder.java


// Targeting ../BilinearImplModuleHolder.java


// Targeting ../FlattenImplModuleHolder.java


// Targeting ../UnflattenImplModuleHolder.java


// Targeting ../L1LossImplModuleHolder.java


// Targeting ../KLDivLossImplModuleHolder.java


// Targeting ../MSELossImplModuleHolder.java


// Targeting ../BCELossImplModuleHolder.java


// Targeting ../HingeEmbeddingLossImplModuleHolder.java


// Targeting ../MultiMarginLossImplModuleHolder.java


// Targeting ../CosineEmbeddingLossImplModuleHolder.java


// Targeting ../SmoothL1LossImplModuleHolder.java


// Targeting ../HuberLossImplModuleHolder.java


// Targeting ../MultiLabelMarginLossImplModuleHolder.java


// Targeting ../SoftMarginLossImplModuleHolder.java


// Targeting ../MultiLabelSoftMarginLossImplModuleHolder.java


// Targeting ../TripletMarginLossImplModuleHolder.java


// Targeting ../TripletMarginWithDistanceLossImplModuleHolder.java


// Targeting ../CTCLossImplModuleHolder.java


// Targeting ../PoissonNLLLossImplModuleHolder.java


// Targeting ../MarginRankingLossImplModuleHolder.java


// Targeting ../NLLLossImplModuleHolder.java


// Targeting ../CrossEntropyLossImplModuleHolder.java


// Targeting ../BCEWithLogitsLossImplModuleHolder.java


// Targeting ../ReflectionPad1dImplModuleHolder.java


// Targeting ../ReplicationPad1dImplModuleHolder.java


// Targeting ../ConstantPad1dImplModuleHolder.java


// Targeting ../AvgPool1dImplModuleHolder.java


// Targeting ../MaxPool1dImplModuleHolder.java


// Targeting ../AdaptiveAvgPool1dImplModuleHolder.java


// Targeting ../AdaptiveMaxPool1dImplModuleHolder.java


// Targeting ../MaxUnpool1dImplModuleHolder.java


// Targeting ../LPPool1dImplModuleHolder.java


// Targeting ../ReflectionPad2dImplModuleHolder.java


// Targeting ../ReplicationPad2dImplModuleHolder.java


// Targeting ../ConstantPad2dImplModuleHolder.java


// Targeting ../ZeroPad2dImplModuleHolder.java


// Targeting ../AvgPool2dImplModuleHolder.java


// Targeting ../MaxPool2dImplModuleHolder.java


// Targeting ../AdaptiveAvgPool2dImplModuleHolder.java


// Targeting ../AdaptiveMaxPool2dImplModuleHolder.java


// Targeting ../MaxUnpool2dImplModuleHolder.java


// Targeting ../FractionalMaxPool2dImplModuleHolder.java


// Targeting ../LPPool2dImplModuleHolder.java


// Targeting ../ReflectionPad3dImplModuleHolder.java


// Targeting ../ReplicationPad3dImplModuleHolder.java


// Targeting ../ConstantPad3dImplModuleHolder.java


// Targeting ../AvgPool3dImplModuleHolder.java


// Targeting ../MaxPool3dImplModuleHolder.java


// Targeting ../AdaptiveAvgPool3dImplModuleHolder.java


// Targeting ../AdaptiveMaxPool3dImplModuleHolder.java


// Targeting ../MaxUnpool3dImplModuleHolder.java


// Targeting ../FractionalMaxPool3dImplModuleHolder.java


// Targeting ../RNNImplModuleHolder.java


// Targeting ../LSTMImplModuleHolder.java


// Targeting ../GRUImplModuleHolder.java


// Targeting ../RNNCellImplModuleHolder.java


// Targeting ../LSTMCellImplModuleHolder.java


// Targeting ../GRUCellImplModuleHolder.java


// Targeting ../PixelShuffleImplModuleHolder.java


// Targeting ../PixelUnshuffleImplModuleHolder.java


// Targeting ../UpsampleImplModuleHolder.java


// Targeting ../ELUImplModuleHolder.java


// Targeting ../SELUImplModuleHolder.java


// Targeting ../HardshrinkImplModuleHolder.java


// Targeting ../HardtanhImplModuleHolder.java


// Targeting ../LeakyReLUImplModuleHolder.java


// Targeting ../LogSigmoidImplModuleHolder.java


// Targeting ../SoftmaxImplModuleHolder.java


// Targeting ../SoftminImplModuleHolder.java


// Targeting ../LogSoftmaxImplModuleHolder.java


// Targeting ../Softmax2dImplModuleHolder.java


// Targeting ../PReLUImplModuleHolder.java


// Targeting ../ReLUImplModuleHolder.java


// Targeting ../ReLU6ImplModuleHolder.java


// Targeting ../RReLUImplModuleHolder.java


// Targeting ../CELUImplModuleHolder.java


// Targeting ../GLUImplModuleHolder.java


// Targeting ../GELUImplModuleHolder.java


// Targeting ../SiLUImplModuleHolder.java


// Targeting ../MishImplModuleHolder.java


// Targeting ../SigmoidImplModuleHolder.java


// Targeting ../SoftplusImplModuleHolder.java


// Targeting ../SoftshrinkImplModuleHolder.java


// Targeting ../SoftsignImplModuleHolder.java


// Targeting ../TanhImplModuleHolder.java


// Targeting ../TanhshrinkImplModuleHolder.java


// Targeting ../ThresholdImplModuleHolder.java


// Targeting ../MultiheadAttentionImplModuleHolder.java


// Targeting ../LayerNormImplModuleHolder.java


// Targeting ../LocalResponseNormImplModuleHolder.java


// Targeting ../CrossMapLRN2dImplModuleHolder.java


// Targeting ../GroupNormImplModuleHolder.java


// Targeting ../TransformerEncoderLayerImplModuleHolder.java


// Targeting ../TransformerDecoderLayerImplModuleHolder.java


// Targeting ../TransformerEncoderImplModuleHolder.java


// Targeting ../TransformerDecoderImplModuleHolder.java


// Targeting ../TransformerImplModuleHolder.java



/** Pretty prints the given {@code Module} into the {@code ostream}. */

/** Serializes a {@code ModuleHolder} into an {@code OutputArchive}. */

/** Deserializes a {@code ModuleHolder} from an {@code InputArchive}. */

 // namespace nn
 // namespace torch

// Workaround for CUDA 10.2 and below not allowing attribute unused on
// using declarations.
// #ifdef __CUDACC__
// #define TORCH_UNUSED_EXCEPT_CUDA
// #else
// #define TORCH_UNUSED_EXCEPT_CUDA C10_UNUSED
// #endif

/** Defines a class {@code Name} which inherits from {@code nn::ModuleHolder} to provide a
 *  wrapper over a {@code std::shared_ptr<ImplType>}.
 *  {@code Impl} is a type alias for {@code ImplType} which provides a way to call static
 *  method of {@code ImplType}. */
// #define TORCH_MODULE_IMPL(Name, ImplType)
//   class Name : public torch::nn::ModuleHolder<ImplType> { /* NOLINT */
//    public:
//     using torch::nn::ModuleHolder<ImplType>::ModuleHolder;
//     using Impl TORCH_UNUSED_EXCEPT_CUDA = ImplType;
//   }

/** Like {@code TORCH_MODULE_IMPL}, but defaults the {@code ImplType} name to {@code <Name>Impl}. */
// #define TORCH_MODULE(Name) TORCH_MODULE_IMPL(Name, Name##Impl)


// Parsed from torch/nn/utils.h

// #pragma once

// #include <torch/nn/utils/clip_grad.h>
// #include <torch/nn/utils/convert_parameters.h>
// #include <torch/nn/utils/rnn.h>


// Parsed from torch/nn/utils/clip_grad.h

// #pragma once

// #include <torch/csrc/Export.h>

// #include <utility>

// Clips gradient norm of a vector of Tensors.
// See
// https://pytorch.org/docs/stable/nn.html?highlight=clip_grad_norm#torch.nn.utils.clip_grad_norm_
// for more details about this module.
//
// Difference with the python version: unlike the python version, even when
// skipping the finiteness checks (error_if_nonfinite = false), this function
// will introduce a device <=> CPU synchronization (for devices where that makes
// sense!) in order to return a CPU-side `double`. This C++ version therefore
// cannot be run fully asynchronously w.r.t. the device of the gradients.
@Namespace("torch::nn::utils") public static native double clip_grad_norm_(
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector parameters,
    double max_norm,
    double norm_type/*=2.0*/,
    @Cast("bool") boolean error_if_nonfinite/*=false*/);
@Namespace("torch::nn::utils") public static native double clip_grad_norm_(
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector parameters,
    double max_norm);

// A wrapper around clip_grad_norm_ that allows us to call the function with a
// braced-init-list of Tensors.

// A wrapper around clip_grad_norm_ that allows us to call the function with a
// single Tensor.
@Namespace("torch::nn::utils") public static native double clip_grad_norm_(
    @ByVal Tensor parameter,
    double max_norm,
    double norm_type/*=2.0*/,
    @Cast("bool") boolean error_if_nonfinite/*=false*/);
@Namespace("torch::nn::utils") public static native double clip_grad_norm_(
    @ByVal Tensor parameter,
    double max_norm);

// Clips gradient of an iterable of parameters at specified value.
// Gradients are modified in-place.
// See https://pytorch.org/docs/stable/nn.html#clip-grad-value
// for more details about this module.
@Namespace("torch::nn::utils") public static native void clip_grad_value_(
    @Cast({"", "std::vector<at::Tensor>"}) @StdMove TensorVector parameters,
    double clip_value);

// A wrapper around clip_grad_value_ that allows us to call the function with a
// braced-init-list of Tensors.

// A wrapper around clip_grad_value_ that allows us to call the function with a
// single Tensor.
@Namespace("torch::nn::utils") public static native void clip_grad_value_(@ByVal Tensor parameter, double clip_value);

 // namespace utils
 // namespace nn
 // namespace torch


// Parsed from torch/nn/utils/convert_parameters.h

// #pragma once

// #include <torch/csrc/Export.h>
// #include <torch/types.h>

// This helper function is to check if the parameters are located
// in the same device. Currently, the conversion between model parameters
// and single vector form is not supported for multiple allocations,
// e.g. parameters in different GPUs, or mixture of CPU/GPU.
@Namespace("torch::nn::utils") public static native @ByVal LongOptional _check_param_device(
    @Const @ByRef Tensor param,
    @ByVal LongOptional old_param_device);

// Convert parameters to one vector
@Namespace("torch::nn::utils") public static native @ByVal Tensor parameters_to_vector(
    @StdVector Tensor parameters);

// Convert one vector to the parameters
@Namespace("torch::nn::utils") public static native void vector_to_parameters(
    @Const @ByRef Tensor vec,
    @StdVector Tensor parameters);

 // namespace utils
 // namespace nn
 // namespace torch


// Parsed from torch/nn/utils/rnn.h

// #pragma once

// #include <c10/util/irange.h>
// #include <torch/types.h>

// #include <utility>


///
///
///
///
///
///
///
@Namespace("torch::nn::utils::rnn") public static native @ByVal Tensor invert_permutation(@Const @ByRef Tensor permutation);
// Targeting ../PackedSequence.java



/** Packs a Tensor containing padded sequences of variable length.
 * 
 *  {@code input} can be of size {@code }T x B x *{@code } where {@code T} is the length of the
 *  longest sequence (equal to {@code }lengths[0]{@code }), {@code }B{@code } is the batch size, and
 *  {@code }*{@code } is any number of dimensions (including 0). If {@code }batch_first{@code } is
 *  {@code }true{@code }, {@code }B x T x *{@code } {@code input} is expected.
 * 
 *  For unsorted sequences, use {@code enforce_sorted = false}. If {@code enforce_sorted} is
 *  {@code }true{@code }, the sequences should be sorted by length in a decreasing order,
 *  i.e.
 *  {@code }input[:,0]{@code } should be the longest sequence, and {@code }input[:,B-1]{@code } the
 *  shortest one.
 * 
 *  Note:
 *      This function accepts any input that has at least two dimensions. You
 *      can apply it to pack the labels, and use the output of the RNN with
 *      them to compute the loss directly. A Tensor can be retrieved from
 *      a {@code PackedSequence} object by calling its {@code }.data(){@code } function.
 * 
 *  Arguments:
 *      input (Tensor): padded batch of variable length sequences.
 *      lengths (Tensor): list of sequences lengths of each batch element.
 *      batch_first (bool, optional): if {@code }true{@code }, the input is expected in {@code }B
 *      x T x *{@code }
 *          format. Default: {@code }false{@code }.
 *      enforce_sorted (bool, optional): if {@code }true{@code }, the input is expected to
 *          contain sequences sorted by length in a decreasing order. If
 *          {@code }false{@code }, this condition is not checked. Default: {@code }true{@code }.
 * 
 *  Returns:
 *      a {@code PackedSequence} object */

///
///
///
///
///
@Namespace("torch::nn::utils::rnn") public static native @ByVal PackedSequence pack_padded_sequence(
    @ByVal Tensor input,
    @ByVal Tensor lengths,
    @Cast("bool") boolean batch_first/*=false*/,
    @Cast("bool") boolean enforce_sorted/*=true*/);
@Namespace("torch::nn::utils::rnn") public static native @ByVal PackedSequence pack_padded_sequence(
    @ByVal Tensor input,
    @ByVal Tensor lengths);

/** Pads a packed batch of variable length sequences.
 * 
 *  It is an inverse operation to {@code pack_padded_sequence}.
 * 
 *  The returned Tensor's data will be of size {@code }T x B x *{@code }, where {@code T} is the
 *  length of the longest sequence and {@code B} is the batch size. If {@code }batch_first{@code }
 *  is true, the data will be transposed into {@code }B x T x *{@code } format.
 * 
 *  Batch elements will be ordered decreasingly by their length.
 * 
 *  Arguments:
 *      sequence (PackedSequence): batch to pad
 *      batch_first (bool, optional): if {@code }true{@code }, the output will be in {@code }B x T
 *      x *{@code }
 *          format.
 *      padding_value (double, optional): values for padded elements.
 *      total_length (int64_t, optional): if specified, the output will be
 *      padded to
 *          have length {@code total_length}. This method will throw error
 *          if {@code total_length} is less than the max sequence length in
 *          {@code sequence}.
 * 
 *  Returns:
 *      Tuple of Tensor containing the padded sequence, and a Tensor
 *      containing the list of lengths of each sequence in the batch. */

///
///
///
///
///
@Namespace("torch::nn::utils::rnn") public static native @ByVal TensorTensorTuple pad_packed_sequence(
    @ByVal PackedSequence sequence,
    @Cast("bool") boolean batch_first/*=false*/,
    double padding_value/*=0.0*/,
    @ByVal(nullValue = "c10::optional<int64_t>(torch::nullopt)") LongOptional total_length);
@Namespace("torch::nn::utils::rnn") public static native @ByVal TensorTensorTuple pad_packed_sequence(
    @ByVal PackedSequence sequence);

/** Pad a list of variable length Tensors with {@code }padding_value{@code }
 * 
 *  {@code }pad_sequence{@code } stacks a list of Tensors along a new dimension,
 *  and pads them to equal length. For example, if the input is list of
 *  sequences with size {@code }L x *{@code } and if batch_first is false, and {@code }T x B x *{@code }
 *  otherwise.
 * 
 *  {@code B} is batch size. It is equal to the number of elements in {@code }sequences{@code }.
 *  {@code T} is length of the longest sequence.
 *  {@code L} is length of the sequence.
 *  {@code *} is any number of trailing dimensions, including none.
 * 
 *  Note:
 *      This function returns a Tensor of size {@code }T x B x *{@code } or {@code }B x T x *{@code }
 *      where {@code T} is the length of the longest sequence. This function assumes
 *      trailing dimensions and type of all the Tensors in sequences are same.
 * 
 *  Arguments:
 *      sequences (torch::ArrayRef<Tensor>): list of variable length sequences.
 *      batch_first (bool, optional): output will be in {@code }B x T x *{@code } if true,
 *      or in
 *          {@code }T x B x *{@code } otherwise
 *      padding_value (double, optional): value for padded elements. Default: 0.
 * 
 *  Returns:
 *      Tensor of size {@code }T x B x *{@code } if {@code batch_first} is {@code }false{@code }.
 *      Tensor of size {@code }B x T x *{@code } otherwise */

/** Packs a list of variable length Tensors
 * 
 *  {@code }sequences{@code } should be a list of Tensors of size {@code }L x *{@code }, where {@code L} is
 *  the length of a sequence and {@code *} is any number of trailing dimensions,
 *  including zero.
 * 
 *  For unsorted sequences, use {@code enforce_sorted = false}. If {@code }enforce_sorted{@code }
 *  is {@code }true{@code }, the sequences should be sorted in the order of decreasing
 *  length.
 * 
 * 
 *  Arguments:
 *      sequences (torch::ArrayRef<Tensor>): A list of sequences of decreasing
 *      length. enforce_sorted (bool, optional): if {@code }true{@code }, checks that the
 *      input
 *          contains sequences sorted by length in a decreasing order. If
 *          {@code }false{@code }, this condition is not checked. Default: {@code }true{@code }.
 * 
 *  Returns:
 *      a {@code PackedSequence} object */
@Namespace("torch::nn::utils::rnn") public static native @ByVal PackedSequence pack_sequence(
    @ByVal TensorArrayRef sequences,
    @Cast("bool") boolean enforce_sorted/*=true*/);
@Namespace("torch::nn::utils::rnn") public static native @ByVal PackedSequence pack_sequence(
    @ByVal TensorArrayRef sequences);

 // namespace rnn
 // namespace utils
 // namespace nn
 // namespace torch


// Parsed from torch/nn/options.h

// #pragma once

// #include <torch/nn/options/batchnorm.h>
// #include <torch/nn/options/conv.h>
// #include <torch/nn/options/dropout.h>
// #include <torch/nn/options/fold.h>
// #include <torch/nn/options/linear.h>
// #include <torch/nn/options/loss.h>
// #include <torch/nn/options/normalization.h>
// #include <torch/nn/options/padding.h>
// #include <torch/nn/options/pixelshuffle.h>
// #include <torch/nn/options/pooling.h>
// #include <torch/nn/options/rnn.h>
// #include <torch/nn/options/transformer.h>
// #include <torch/nn/options/transformercoder.h>
// #include <torch/nn/options/transformerlayer.h>
// #include <torch/nn/options/upsampling.h>
// #include <torch/nn/options/vision.h>


// Parsed from torch/nn/options/activation.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/types.h>
// Targeting ../ELUOptions.java


/** Options for {@code torch::nn::functional::elu}.
 * 
 *  See the documentation for {@code torch::nn::ELUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::elu(x, F::ELUFuncOptions().alpha(0.42).inplace(true));
 *  }</pre> */

// Targeting ../SELUOptions.java


/** Options for {@code torch::nn::functional::selu}.
 * 
 *  See the documentation for {@code torch::nn::SELUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::selu(input, F::SELUFuncOptions(false));
 *  }</pre> */

// Targeting ../GLUOptions.java


/** Options for {@code torch::nn::functional::glu}.
 * 
 *  See the documentation for {@code torch::nn::GLUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::glu(input, GLUFuncOptions(1));
 *  }</pre> */

// Targeting ../GELUOptions.java


/** Options for {@code torch::nn::functional::gelu}.
 * 
 *  See the documentation for {@code torch::nn::GELUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::gelu(input, F::GELUFuncOptions().approximate("none"));
 *  }</pre> */

// Targeting ../HardshrinkOptions.java


/** Options for {@code torch::nn::functional::hardshrink}.
 * 
 *  See the documentation for {@code torch::nn::HardshrinkOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::hardshrink(x, F::HardshrinkFuncOptions().lambda(0.42));
 *  }</pre> */

// Targeting ../HardtanhOptions.java


/** Options for {@code torch::nn::functional::hardtanh}.
 * 
 *  See the documentation for {@code torch::nn::HardtanhOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::hardtanh(x,
 *  F::HardtanhFuncOptions().min_val(-1.0).max_val(1.0).inplace(true));
 *  }</pre> */

// Targeting ../LeakyReLUOptions.java


/** Options for {@code torch::nn::functional::leaky_relu}.
 * 
 *  See the documentation for {@code torch::nn::LeakyReLUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::leaky_relu(x,
 *  F::LeakyReLUFuncOptions().negative_slope(0.42).inplace(true));
 *  }</pre> */

// Targeting ../SoftmaxOptions.java



// ============================================================================
// Targeting ../SoftmaxFuncOptions.java




// Targeting ../SoftminOptions.java



// ============================================================================
// Targeting ../SoftminFuncOptions.java




// Targeting ../LogSoftmaxOptions.java



// ============================================================================
// Targeting ../LogSoftmaxFuncOptions.java




// Targeting ../PReLUOptions.java


// Targeting ../ReLUOptions.java


/** Options for {@code torch::nn::functional::relu}.
 * 
 *  See the documentation for {@code torch::nn::ReLUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::relu(x, F::ReLUFuncOptions().inplace(true));
 *  }</pre> */

// Targeting ../ReLU6Options.java


/** Options for {@code torch::nn::functional::relu6}.
 * 
 *  See the documentation for {@code torch::nn::ReLU6Options} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::relu6(x, F::ReLU6FuncOptions().inplace(true));
 *  }</pre> */

// Targeting ../RReLUOptions.java



// ============================================================================
// Targeting ../RReLUFuncOptions.java




// Targeting ../CELUOptions.java


/** Options for {@code torch::nn::functional::celu}.
 * 
 *  See the documentation for {@code torch::nn::CELUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::celu(x, F::CELUFuncOptions().alpha(0.42).inplace(true));
 *  }</pre> */

// Targeting ../SoftplusOptions.java


/** Options for {@code torch::nn::functional::softplus}.
 * 
 *  See the documentation for {@code torch::nn::SoftplusOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::softplus(x, F::SoftplusFuncOptions().beta(0.5).threshold(3.0));
 *  }</pre> */

// Targeting ../SoftshrinkOptions.java


/** Options for {@code torch::nn::functional::softshrink}.
 * 
 *  See the documentation for {@code torch::nn::SoftshrinkOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::softshrink(x, F::SoftshrinkFuncOptions(0.42));
 *  }</pre> */

// Targeting ../ThresholdOptions.java


/** Options for {@code torch::nn::functional::threshold}.
 * 
 *  See the documentation for {@code torch::nn::ThresholdOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::threshold(x, F::ThresholdFuncOptions(0.5, 0.5).inplace(true));
 *  }</pre> */
 // namespace functional

// ============================================================================
// Targeting ../GumbelSoftmaxFuncOptions.java




// Targeting ../MultiheadAttentionOptions.java



// ============================================================================
// Targeting ../MultiheadAttentionForwardFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/adaptive.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/types.h>
// Targeting ../AdaptiveLogSoftmaxWithLossOptions.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/batchnorm.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/types.h>
// Targeting ../BatchNormOptions.java



/** Options for the {@code BatchNorm1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  BatchNorm1d
 *  model(BatchNorm1dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */

///

/** Options for the {@code BatchNorm2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  BatchNorm2d
 *  model(BatchNorm2dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */

///

/** Options for the {@code BatchNorm3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  BatchNorm3d
 *  model(BatchNorm3dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */

// ============================================================================
// Targeting ../BatchNormFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/conv.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/expanding_array.h>
// #include <torch/types.h>
// Targeting ../DetailConv1dOptions.java


// Targeting ../DetailConv2dOptions.java


// Targeting ../DetailConv3dOptions.java




// Targeting ../Conv1dOptions.java


// Targeting ../Conv2dOptions.java


// Targeting ../Conv3dOptions.java



/** {@code ConvOptions} specialized for the {@code Conv1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  Conv1d model(Conv1dOptions(3, 2, 3).stride(1).bias(false));
 *  }</pre> */

///

/** {@code ConvOptions} specialized for the {@code Conv2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  Conv2d model(Conv2dOptions(3, 2, 3).stride(1).bias(false));
 *  }</pre> */

///

/** {@code ConvOptions} specialized for the {@code Conv3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  Conv3d model(Conv3dOptions(3, 2, 3).stride(1).bias(false));
 *  }</pre> */

// ============================================================================
// Targeting ../Conv1dFuncOptions.java


// Targeting ../Conv2dFuncOptions.java


// Targeting ../Conv3dFuncOptions.java



/** {@code ConvFuncOptions} specialized for {@code torch::nn::functional::conv1d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv1d(x, weight, F::Conv1dFuncOptions().stride(1));
 *  }</pre> */

///

/** {@code ConvFuncOptions} specialized for {@code torch::nn::functional::conv2d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv2d(x, weight, F::Conv2dFuncOptions().stride(1));
 *  }</pre> */

///

/** {@code ConvFuncOptions} specialized for {@code torch::nn::functional::conv3d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv3d(x, weight, F::Conv3dFuncOptions().stride(1));
 *  }</pre> */


// Targeting ../ConvTranspose1dOptions.java


// Targeting ../ConvTranspose2dOptions.java


// Targeting ../ConvTranspose3dOptions.java



/** {@code ConvTransposeOptions} specialized for the {@code ConvTranspose1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConvTranspose1d model(ConvTranspose1dOptions(3, 2,
 *  3).stride(1).bias(false));
 *  }</pre> */

///

/** {@code ConvTransposeOptions} specialized for the {@code ConvTranspose2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConvTranspose2d model(ConvTranspose2dOptions(3, 2,
 *  3).stride(1).bias(false));
 *  }</pre> */

///

/** {@code ConvTransposeOptions} specialized for the {@code ConvTranspose3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConvTranspose3d model(ConvTranspose3dOptions(2, 2,
 *  2).stride(1).bias(false));
 *  }</pre> */

// ============================================================================
// Targeting ../ConvTranspose1dFuncOptions.java


// Targeting ../ConvTranspose2dFuncOptions.java


// Targeting ../ConvTranspose3dFuncOptions.java



/** {@code ConvTransposeFuncOptions} specialized for
 *  {@code torch::nn::functional::conv_transpose1d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv_transpose1d(x, weight, F::ConvTranspose1dFuncOptions().stride(1));
 *  }</pre> */

///

/** {@code ConvTransposeFuncOptions} specialized for
 *  {@code torch::nn::functional::conv_transpose2d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv_transpose2d(x, weight, F::ConvTranspose2dFuncOptions().stride(1));
 *  }</pre> */

///

/** {@code ConvTransposeFuncOptions} specialized for
 *  {@code torch::nn::functional::conv_transpose3d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv_transpose3d(x, weight, F::ConvTranspose3dFuncOptions().stride(1));
 *  }</pre> */

 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/distance.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/types.h>
// Targeting ../CosineSimilarityOptions.java


/** Options for {@code torch::nn::functional::cosine_similarity}.
 * 
 *  See the documentation for {@code torch::nn::CosineSimilarityOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::cosine_similarity(input1, input2,
 *  F::CosineSimilarityFuncOptions().dim(1));
 *  }</pre> */

// Targeting ../PairwiseDistanceOptions.java


/** Options for {@code torch::nn::functional::pairwise_distance}.
 * 
 *  See the documentation for {@code torch::nn::PairwiseDistanceOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::pairwise_distance(input1, input2, F::PairwiseDistanceFuncOptions().p(1));
 *  }</pre> */
 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/dropout.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/types.h>
// Targeting ../DropoutOptions.java



/** Options for the {@code Dropout2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  Dropout2d model(Dropout2dOptions().p(0.42).inplace(true));
 *  }</pre> */

///

/** Options for the {@code Dropout3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  Dropout3d model(Dropout3dOptions().p(0.42).inplace(true));
 *  }</pre> */

///

/** Options for the {@code AlphaDropout} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AlphaDropout model(AlphaDropoutOptions(0.2).inplace(true));
 *  }</pre> */

///

/** Options for the {@code FeatureAlphaDropout} module.
 * 
 *  Example:
 *  <pre>{@code
 *  FeatureAlphaDropout model(FeatureAlphaDropoutOptions(0.2).inplace(true));
 *  }</pre> */
// Targeting ../DropoutFuncOptions.java



/** Options for {@code torch::nn::functional::dropout2d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::dropout2d(input, F::Dropout2dFuncOptions().p(0.5));
 *  }</pre> */

///

/** Options for {@code torch::nn::functional::dropout3d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::dropout3d(input, F::Dropout3dFuncOptions().p(0.5));
 *  }</pre> */

///
// Targeting ../AlphaDropoutFuncOptions.java


// Targeting ../FeatureAlphaDropoutFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/embedding.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/types.h>
// Targeting ../EmbeddingOptions.java


// Targeting ../EmbeddingFromPretrainedOptions.java



// ============================================================================
// Targeting ../EmbeddingFuncOptions.java



 // namespace functional

// ============================================================================


///
// Targeting ../EmbeddingBagOptions.java


// Targeting ../EmbeddingBagFromPretrainedOptions.java



// ============================================================================
// Targeting ../EmbeddingBagFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/fold.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/expanding_array.h>
// #include <torch/types.h>
// Targeting ../FoldOptions.java


/** Options for {@code torch::nn::functional::fold}.
 * 
 *  See the documentation for {@code torch::nn::FoldOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::fold(input, F::FoldFuncOptions({3, 2}, {2, 2}));
 *  }</pre> */

// Targeting ../UnfoldOptions.java


/** Options for {@code torch::nn::functional::unfold}.
 * 
 *  See the documentation for {@code torch::nn::UnfoldOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::unfold(input, F::UnfoldFuncOptions({2, 2}).padding(1).stride(2));
 *  }</pre> */
 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/linear.h

// #pragma once

// #include <c10/util/variant.h>
// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/types.h>
// Targeting ../LinearOptions.java


// Targeting ../FlattenOptions.java


// Targeting ../UnflattenOptions.java


// Targeting ../BilinearOptions.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/loss.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/types.h>
// Targeting ../L1LossOptions.java


/** Options for {@code torch::nn::functional::l1_loss}.
 * 
 *  See the documentation for {@code torch::nn::L1LossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::l1_loss(input, target, F::L1LossFuncOptions(torch::kNone));
 *  }</pre> */

// Targeting ../KLDivLossOptions.java


/** Options for {@code torch::nn::functional::kl_div}.
 * 
 *  See the documentation for {@code torch::nn::KLDivLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::kl_div(input, target,
 *  F::KLDivFuncOptions().reduction(torch::kNone).log_target(false));
 *  }</pre> */

// Targeting ../MSELossOptions.java


/** Options for {@code torch::nn::functional::mse_loss}.
 * 
 *  See the documentation for {@code torch::nn::MSELossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::mse_loss(input, target, F::MSELossFuncOptions(torch::kNone));
 *  }</pre> */

// Targeting ../BCELossOptions.java


/** Options for {@code torch::nn::functional::binary_cross_entropy}.
 * 
 *  See the documentation for {@code torch::nn::BCELossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::binary_cross_entropy(input, target,
 *  F::BinaryCrossEntropyFuncOptions().weight(weight));
 *  }</pre> */

// Targeting ../HingeEmbeddingLossOptions.java


/** Options for {@code torch::nn::functional::hinge_embedding_loss}.
 * 
 *  See the documentation for {@code torch::nn::HingeEmbeddingLossOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::hinge_embedding_loss(input, target,
 *  F::HingeEmbeddingLossFuncOptions().margin(2));
 *  }</pre> */

// Targeting ../MultiMarginLossOptions.java


/** Options for {@code torch::nn::functional::multi_margin_loss}.
 * 
 *  See the documentation for {@code torch::nn::MultiMarginLossOptions} class to learn
 *  what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::multi_margin_loss(input, target,
 *  F::MultiMarginLossFuncOptions().margin(2).weight(weight));
 *  }</pre> */

// Targeting ../CosineEmbeddingLossOptions.java


/** Options for {@code torch::nn::functional::cosine_embedding_loss}.
 * 
 *  See the documentation for {@code torch::nn::CosineEmbeddingLossOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::cosine_embedding_loss(input1, input2, target,
 *  F::CosineEmbeddingLossFuncOptions().margin(0.5));
 *  }</pre> */

// Targeting ../MultiLabelMarginLossOptions.java


/** Options for {@code torch::nn::functional::multilabel_margin_loss}.
 * 
 *  See the documentation for {@code torch::nn::MultiLabelMarginLossOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::multilabel_margin_loss(input, target,
 *  F::MultilabelMarginLossFuncOptions(torch::kNone));
 *  }</pre> */

// Targeting ../SoftMarginLossOptions.java


/** Options for {@code torch::nn::functional::soft_margin_loss}.
 * 
 *  See the documentation for {@code torch::nn::SoftMarginLossOptions} class to learn
 *  what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::soft_margin_loss(input, target,
 *  F::SoftMarginLossFuncOptions(torch::kNone));
 *  }</pre> */

// Targeting ../MultiLabelSoftMarginLossOptions.java


/** Options for {@code torch::nn::functional::multilabel_soft_margin_loss}.
 * 
 *  See the documentation for {@code torch::nn::MultiLabelSoftMarginLossOptions} class
 *  to learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::multilabel_soft_margin_loss(input, target,
 *  F::MultilabelSoftMarginLossFuncOptions().reduction(torch::kNone).weight(weight));
 *  }</pre> */

// Targeting ../TripletMarginLossOptions.java


/** Options for {@code torch::nn::functional::triplet_margin_loss}.
 * 
 *  See the documentation for {@code torch::nn::TripletMarginLossOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::triplet_margin_loss(anchor, positive, negative,
 *  F::TripletMarginLossFuncOptions().margin(1.0));
 *  }</pre> */

// Targeting ../TripletMarginWithDistanceLossOptions.java


/** Options for {@code torch::nn::functional::triplet_margin_with_distance_loss}.
 * 
 *  See the documentation for {@code torch::nn::TripletMarginWithDistanceLossOptions}
 *  class to learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::triplet_margin_with_distance_loss(anchor, positive, negative,
 *  F::TripletMarginWithDistanceLossFuncOptions().margin(1.0));
 *  }</pre> */

// Targeting ../CTCLossOptions.java


/** Options for {@code torch::nn::functional::ctc_loss}.
 * 
 *  See the documentation for {@code torch::nn::CTCLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::ctc_loss(log_probs, targets, input_lengths, target_lengths,
 *  F::CTCLossFuncOptions().reduction(torch::kNone));
 *  }</pre> */

// Targeting ../SmoothL1LossOptions.java


/** Options for {@code torch::nn::functional::smooth_l1_loss}.
 * 
 *  See the documentation for {@code torch::nn::SmoothL1LossOptions} class to learn
 *  what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::smooth_l1_loss(input, target, F::SmoothL1LossFuncOptions(torch::kNone));
 *  }</pre> */

// Targeting ../HuberLossOptions.java


/** Options for {@code torch::nn::functional::huber_loss}.
 * 
 *  See the documentation for {@code torch::nn::HuberLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::huber_loss(input, target, F::HuberLossFuncOptions(torch::kNone));
 *  }</pre> */

// Targeting ../PoissonNLLLossOptions.java


/** Options for {@code torch::nn::functional::poisson_nll_loss}.
 * 
 *  See the documentation for {@code torch::nn::PoissonNLLLossOptions} class to learn
 *  what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::poisson_nll_loss(input, target,
 *  F::PoissonNLLLossFuncOptions().reduction(torch::kNone));
 *  }</pre> */

// Targeting ../MarginRankingLossOptions.java


/** Options for {@code torch::nn::functional::margin_ranking_loss}.
 * 
 *  See the documentation for {@code torch::nn::MarginRankingLossOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::margin_ranking_loss(input1, input2, target,
 *  F::MarginRankingLossFuncOptions().margin(0.5).reduction(torch::kSum));
 *  }</pre> */

// Targeting ../NLLLossOptions.java


/** Options for {@code torch::nn::functional::nll_loss}.
 * 
 *  See the documentation for {@code torch::nn::NLLLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::nll_loss(input, target,
 *  F::NLLLossFuncOptions().ignore_index(-100).reduction(torch::kMean));
 *  }</pre> */

// Targeting ../CrossEntropyLossOptions.java


/** Options for {@code torch::nn::functional::cross_entropy}.
 * 
 *  See the documentation for {@code torch::nn::CrossEntropyLossOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::cross_entropy(input, target,
 *  F::CrossEntropyFuncOptions().ignore_index(-100).reduction(torch::kMean));
 *  }</pre> */

// Targeting ../BCEWithLogitsLossOptions.java


/** Options for {@code torch::nn::functional::binary_cross_entropy_with_logits}.
 * 
 *  See the documentation for {@code torch::nn::BCEWithLogitsLossOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::binary_cross_entropy_with_logits(input, target,
 *  F::BinaryCrossEntropyWithLogitsFuncOptions().pos_weight(pos_weight).reduction(torch::kSum));
 *  }</pre> */
 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/normalization.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/types.h>
// #include <vector>
// Targeting ../LayerNormOptions.java



// ============================================================================
// Targeting ../LayerNormFuncOptions.java




// Targeting ../LocalResponseNormOptions.java


/** Options for {@code torch::nn::functional::local_response_norm}.
 * 
 *  See the documentation for {@code torch::nn::LocalResponseNormOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::local_response_norm(x, F::LocalResponseNormFuncOptions(2));
 *  }</pre> */

// Targeting ../CrossMapLRN2dOptions.java



// ============================================================================
// Targeting ../NormalizeFuncOptions.java




// Targeting ../GroupNormOptions.java



// ============================================================================
// Targeting ../GroupNormFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/padding.h

// #pragma once

// #include <c10/util/variant.h>
// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/expanding_array.h>
// #include <torch/types.h>
// Targeting ../ReflectionPad1dOptions.java


// Targeting ../ReflectionPad2dOptions.java


// Targeting ../ReflectionPad3dOptions.java



/** {@code ReflectionPadOptions} specialized for the {@code ReflectionPad1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ReflectionPad1d model(ReflectionPad1dOptions({3, 1}));
 *  }</pre> */

///

/** {@code ReflectionPadOptions} specialized for the {@code ReflectionPad2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ReflectionPad2d model(ReflectionPad2dOptions({1, 1, 2, 0}));
 *  }</pre> */

///

/** {@code ReflectionPadOptions} specialized for the {@code ReflectionPad3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ReflectionPad3d model(ReflectionPad3dOptions({1, 1, 2, 0, 1, 1}));
 *  }</pre> */
// Targeting ../ReplicationPad1dOptions.java


// Targeting ../ReplicationPad2dOptions.java


// Targeting ../ReplicationPad3dOptions.java



/** {@code ReplicationPadOptions} specialized for the {@code ReplicationPad1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ReplicationPad1d model(ReplicationPad1dOptions({3, 1}));
 *  }</pre> */

///

/** {@code ReplicationPadOptions} specialized for the {@code ReplicationPad2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ReplicationPad2d model(ReplicationPad2dOptions({1, 1, 2, 0}));
 *  }</pre> */

///

/** {@code ReplicationPadOptions} specialized for the {@code ReplicationPad3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ReplicationPad3d model(ReplicationPad3dOptions({1, 2, 1, 2, 1, 2}));
 *  }</pre> */

///
// Targeting ../ZeroPad2dOptions.java


// Targeting ../ConstantPad1dOptions.java


// Targeting ../ConstantPad2dOptions.java


// Targeting ../ConstantPad3dOptions.java



/** {@code ConstantPadOptions} specialized for the {@code ConstantPad1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConstantPad1d model(ConstantPad1dOptions({3, 1}, 3.5));
 *  }</pre> */

///

/** {@code ConstantPadOptions} specialized for the {@code ConstantPad2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConstantPad2d model(ConstantPad2dOptions({3, 0, 2, 1}, 3.5));
 *  }</pre> */

///

/** {@code ConstantPadOptions} specialized for the {@code ConstantPad3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConstantPad3d model(ConstantPad3dOptions({1, 2, 1, 2, 1, 2}, 3.5));
 *  }</pre> */

// ============================================================================
// Targeting ../PadFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/pixelshuffle.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/types.h>
// Targeting ../PixelShuffleOptions.java


// Targeting ../PixelUnshuffleOptions.java


/** Options for {@code torch::nn::functional::pixel_shuffle}.
 * 
 *  See the documentation for {@code torch::nn::PixelShuffleOptions} class to learn
 *  what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::pixel_shuffle(x, F::PixelShuffleFuncOptions(2));
 *  }</pre> */

///
///

/** Options for {@code torch::nn::functional::pixel_unshuffle}.
 * 
 *  See the documentation for {@code torch::nn::PixelUnshuffleOptions} class to learn
 *  what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::pixel_unshuffle(x, F::PixelUnshuffleFuncOptions(2));
 *  }</pre> */
 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/pooling.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/expanding_array.h>
// #include <torch/types.h>
// Targeting ../AvgPool1dOptions.java


// Targeting ../AvgPool2dOptions.java


// Targeting ../AvgPool3dOptions.java



/** {@code AvgPoolOptions} specialized for the {@code AvgPool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AvgPool1d model(AvgPool1dOptions(3).stride(2));
 *  }</pre> */

///

/** {@code AvgPoolOptions} specialized for the {@code AvgPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AvgPool2d model(AvgPool2dOptions({3, 2}).stride({2, 2}));
 *  }</pre> */

///

/** {@code AvgPoolOptions} specialized for the {@code AvgPool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AvgPool3d model(AvgPool3dOptions(5).stride(2));
 *  }</pre> */
/** Options for {@code torch::nn::functional::avg_pool1d}.
 * 
 *  See the documentation for {@code torch::nn::AvgPool1dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::avg_pool1d(x, F::AvgPool1dFuncOptions(3).stride(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::avg_pool2d}.
 * 
 *  See the documentation for {@code torch::nn::AvgPool2dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::avg_pool2d(x, F::AvgPool2dFuncOptions(3).stride(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::avg_pool3d}.
 * 
 *  See the documentation for {@code torch::nn::AvgPool3dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::avg_pool3d(x, F::AvgPool3dFuncOptions(3).stride(2));
 *  }</pre> */

// Targeting ../MaxPool1dOptions.java


// Targeting ../MaxPool2dOptions.java


// Targeting ../MaxPool3dOptions.java



/** {@code MaxPoolOptions} specialized for the {@code MaxPool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxPool1d model(MaxPool1dOptions(3).stride(2));
 *  }</pre> */

///

/** {@code MaxPoolOptions} specialized for the {@code MaxPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxPool2d model(MaxPool2dOptions({3, 2}).stride({2, 2}));
 *  }</pre> */

///

/** {@code MaxPoolOptions} specialized for the {@code MaxPool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxPool3d model(MaxPool3dOptions(3).stride(2));
 *  }</pre> */
/** Options for {@code torch::nn::functional::max_pool1d} and
 *  {@code torch::nn::functional::max_pool1d_with_indices}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_pool1d(x, F::MaxPool1dFuncOptions(3).stride(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::max_pool2d} and
 *  {@code torch::nn::functional::max_pool2d_with_indices}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_pool2d(x, F::MaxPool2dFuncOptions(3).stride(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::max_pool3d} and
 *  {@code torch::nn::functional::max_pool3d_with_indices}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_pool3d(x, F::MaxPool3dFuncOptions(3).stride(2));
 *  }</pre> */

// Targeting ../AdaptiveMaxPool1dOptions.java


// Targeting ../AdaptiveMaxPool2dOptions.java


// Targeting ../AdaptiveMaxPool3dOptions.java



/** {@code AdaptiveMaxPoolOptions} specialized for the {@code AdaptiveMaxPool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveMaxPool1d model(AdaptiveMaxPool1dOptions(3));
 *  }</pre> */

///

/** {@code AdaptiveMaxPoolOptions} specialized for the {@code AdaptiveMaxPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveMaxPool2d model(AdaptiveMaxPool2dOptions({3, 2}));
 *  }</pre> */

///

/** {@code AdaptiveMaxPoolOptions} specialized for the {@code AdaptiveMaxPool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveMaxPool3d model(AdaptiveMaxPool3dOptions(3));
 *  }</pre> */
/** Options for {@code torch::nn::functional::adaptive_max_pool1d} and
 *  {@code torch::nn::functional::adaptive_max_pool1d_with_indices}
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_max_pool1d(x, F::AdaptiveMaxPool1dFuncOptions(3));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::adaptive_max_pool2d} and
 *  {@code torch::nn::functional::adaptive_max_pool2d_with_indices}
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_max_pool2d(x, F::AdaptiveMaxPool2dFuncOptions(3));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::adaptive_max_pool3d} and
 *  {@code torch::nn::functional::adaptive_max_pool3d_with_indices}
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_max_pool3d(x, F::AdaptiveMaxPool3dFuncOptions(3));
 *  }</pre> */

// Targeting ../AdaptiveAvgPool1dOptions.java


// Targeting ../AdaptiveAvgPool2dOptions.java


// Targeting ../AdaptiveAvgPool3dOptions.java



/** {@code AdaptiveAvgPoolOptions} specialized for the {@code AdaptiveAvgPool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveAvgPool1d model(AdaptiveAvgPool1dOptions(5));
 *  }</pre> */

///

/** {@code AdaptiveAvgPoolOptions} specialized for the {@code AdaptiveAvgPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveAvgPool2d model(AdaptiveAvgPool2dOptions({3, 2}));
 *  }</pre> */

///

/** {@code AdaptiveAvgPoolOptions} specialized for the {@code AdaptiveAvgPool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveAvgPool3d model(AdaptiveAvgPool3dOptions(3));
 *  }</pre> */
/** Options for {@code torch::nn::functional::adaptive_avg_pool1d}.
 * 
 *  See the documentation for {@code torch::nn::AdaptiveAvgPool1dOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_avg_pool1d(x, F::AdaptiveAvgPool1dFuncOptions(3));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::adaptive_avg_pool2d}.
 * 
 *  See the documentation for {@code torch::nn::AdaptiveAvgPool2dOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_avg_pool2d(x, F::AdaptiveAvgPool2dFuncOptions(3));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::adaptive_avg_pool3d}.
 * 
 *  See the documentation for {@code torch::nn::AdaptiveAvgPool3dOptions} class to
 *  learn what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_avg_pool3d(x, F::AdaptiveAvgPool3dFuncOptions(3));
 *  }</pre> */

// Targeting ../MaxUnpool1dOptions.java


// Targeting ../MaxUnpool2dOptions.java


// Targeting ../MaxUnpool3dOptions.java



/** {@code MaxUnpoolOptions} specialized for the {@code MaxUnpool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxUnpool1d model(MaxUnpool1dOptions(3).stride(2).padding(1));
 *  }</pre> */

///

/** {@code MaxUnpoolOptions} specialized for the {@code MaxUnpool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxUnpool2d model(MaxUnpool2dOptions(3).stride(2).padding(1));
 *  }</pre> */

///

/** {@code MaxUnpoolOptions} specialized for the {@code MaxUnpool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxUnpool3d model(MaxUnpool3dOptions(3).stride(2).padding(1));
 *  }</pre> */

// ============================================================================
// Targeting ../MaxUnpool1dFuncOptions.java


// Targeting ../MaxUnpool2dFuncOptions.java


// Targeting ../MaxUnpool3dFuncOptions.java



/** {@code MaxUnpoolFuncOptions} specialized for
 *  {@code torch::nn::functional::max_unpool1d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_unpool1d(x, indices,
 *  F::MaxUnpool1dFuncOptions(3).stride(2).padding(1));
 *  }</pre> */

///

/** {@code MaxUnpoolFuncOptions} specialized for
 *  {@code torch::nn::functional::max_unpool2d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_unpool2d(x, indices,
 *  F::MaxUnpool2dFuncOptions(3).stride(2).padding(1));
 *  }</pre> */

///

/** {@code MaxUnpoolFuncOptions} specialized for
 *  {@code torch::nn::functional::max_unpool3d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_unpool3d(x, indices, F::MaxUnpool3dFuncOptions(3));
 *  }</pre> */


// Targeting ../FractionalMaxPool1dOptions.java


// Targeting ../FractionalMaxPool2dOptions.java


// Targeting ../FractionalMaxPool3dOptions.java



/** {@code FractionalMaxPoolOptions} specialized for the {@code FractionalMaxPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  FractionalMaxPool2d model(FractionalMaxPool2dOptions(5).output_size(1));
 *  }</pre> */

///

/** {@code FractionalMaxPoolOptions} specialized for the {@code FractionalMaxPool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  FractionalMaxPool3d model(FractionalMaxPool3dOptions(5).output_size(1));
 *  }</pre> */
/** Options for {@code torch::nn::functional::fractional_max_pool2d} and
 *  {@code torch::nn::functional::fractional_max_pool2d_with_indices}
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::fractional_max_pool2d(x,
 *  F::FractionalMaxPool2dFuncOptions(3).output_size(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::fractional_max_pool3d} and
 *  {@code torch::nn::functional::fractional_max_pool3d_with_indices}
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::fractional_max_pool3d(x,
 *  F::FractionalMaxPool3dFuncOptions(3).output_size(2));
 *  }</pre> */

// Targeting ../LPPool1dOptions.java


// Targeting ../LPPool2dOptions.java


// Targeting ../LPPool3dOptions.java



/** {@code LPPoolOptions} specialized for the {@code LPPool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  LPPool1d model(LPPool1dOptions(1, 2).stride(5).ceil_mode(true));
 *  }</pre> */

///

/** {@code LPPoolOptions} specialized for the {@code LPPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  LPPool2d model(LPPool2dOptions(1, std::vector<int64_t>({3, 4})).stride({5,
 *  6}).ceil_mode(true));
 *  }</pre> */
/** Options for {@code torch::nn::functional::lp_pool1d}.
 * 
 *  See the documentation for {@code torch::nn::LPPool1dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::lp_pool1d(x, F::LPPool1dFuncOptions(2, 3).stride(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::lp_pool2d}.
 * 
 *  See the documentation for {@code torch::nn::LPPool2dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::lp_pool2d(x, F::LPPool2dFuncOptions(2, {2, 3}).stride(2));
 *  }</pre> */
 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/rnn.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/types.h>
// Targeting ../RNNOptionsBase.java




// Targeting ../RNNOptions.java


// Targeting ../LSTMOptions.java


// Targeting ../GRUOptions.java


// Targeting ../RNNCellOptionsBase.java




// Targeting ../RNNCellOptions.java


// Targeting ../LSTMCellOptions.java


// Targeting ../GRUCellOptions.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/upsampling.h

// #pragma once

// #include <c10/util/variant.h>
// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/expanding_array.h>
// #include <torch/types.h>

// #include <vector>
// Targeting ../UpsampleOptions.java


// Targeting ../InterpolateFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/vision.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/types.h>
// Targeting ../GridSampleFuncOptions.java



 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/instancenorm.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/nn/options/batchnorm.h>
// #include <torch/types.h>
// Targeting ../InstanceNormOptions.java



/** Options for the {@code InstanceNorm1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  InstanceNorm1d
 *  model(InstanceNorm1dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */

///

/** Options for the {@code InstanceNorm2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  InstanceNorm2d
 *  model(InstanceNorm2dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */

///

/** Options for the {@code InstanceNorm3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  InstanceNorm3d
 *  model(InstanceNorm3dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */
// Targeting ../InstanceNormFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/transformerlayer.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/types.h>


///
// Targeting ../TransformerEncoderLayerOptions.java


// Targeting ../TransformerDecoderLayerOptions.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/transformercoder.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/types.h>

// #include <torch/nn/modules/container/any.h>
// #include <torch/nn/modules/transformerlayer.h>
// Targeting ../TransformerEncoderOptions.java


// Targeting ../TransformerDecoderOptions.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/transformer.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>
// #include <torch/enum.h>
// #include <torch/types.h>

// #include <torch/nn/modules/container/any.h>
// #include <torch/nn/options/transformerlayer.h>
// Targeting ../TransformerOptions.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional.h

// #pragma once

// #include <torch/nn/functional/batchnorm.h>
// #include <torch/nn/functional/conv.h>
// #include <torch/nn/functional/distance.h>
// #include <torch/nn/functional/dropout.h>
// #include <torch/nn/functional/embedding.h>
// #include <torch/nn/functional/fold.h>
// #include <torch/nn/functional/instancenorm.h>
// #include <torch/nn/functional/linear.h>
// #include <torch/nn/functional/loss.h>
// #include <torch/nn/functional/normalization.h>
// #include <torch/nn/functional/padding.h>
// #include <torch/nn/functional/pixelshuffle.h>
// #include <torch/nn/functional/pooling.h>
// #include <torch/nn/functional/upsampling.h>
// #include <torch/nn/functional/vision.h>


// Parsed from torch/nn/functional/activation.h

// #pragma once

// #include <ATen/Dispatch.h>
// #include <torch/nn/functional/dropout.h>
// #include <torch/nn/functional/linear.h>
// #include <torch/nn/options/activation.h>
// #include <torch/nn/options/dropout.h>
// #include <torch/nn/options/linear.h>
// #include <torch/types.h>
// #include <limits>
// #include <utility>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor elu(@ByVal Tensor input, double alpha, @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.elu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::ELUFuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::elu(x, F::ELUFuncOptions().alpha(0.42).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor elu(@ByVal Tensor input, @Cast("const torch::nn::functional::ELUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::ELUFuncOptions{}") ELUOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor selu(@ByVal Tensor input, @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.selu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SELUFuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::selu(input, F::SELUFuncOptions(false));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor selu(@ByVal Tensor input, @Cast("const torch::nn::functional::SELUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::SELUFuncOptions{}") SELUOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor hardshrink(@Const @ByRef Tensor input, double lambda);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.hardshrink
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::HardshrinkFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::hardshrink(x, F::HardshrinkFuncOptions().lambda(0.42));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor hardshrink(
    @Const @ByRef Tensor input,
    @Cast("const torch::nn::functional::HardshrinkFuncOptions*") @ByRef(nullValue = "torch::nn::functional::HardshrinkFuncOptions{}") HardshrinkOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor hardtanh(
    @ByVal Tensor input,
    double min_val,
    double max_val,
    @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.hardtanh
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::HardtanhFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::hardtanh(x,
/** F::HardtanhFuncOptions().min_val(-1.0).max_val(1.0).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor hardtanh(@ByVal Tensor input, @Cast("const torch::nn::functional::HardtanhFuncOptions*") @ByRef(nullValue = "torch::nn::functional::HardtanhFuncOptions{}") HardtanhOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor leaky_relu(@ByVal Tensor input, double negative_slope, @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.leaky_relu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::LeakyReLUFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::leaky_relu(x,
/** F::LeakyReLUFuncOptions().negative_slope(0.42).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor leaky_relu(
    @ByVal Tensor input,
    @Cast("const torch::nn::functional::LeakyReLUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::LeakyReLUFuncOptions{}") LeakyReLUOptions options);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal Tensor logsigmoid(@Const @ByRef Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor gumbel_softmax(
    @Const @ByRef Tensor logits,
    double tau,
    @Cast("bool") boolean hard,
    int dim);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.gumbel_softmax
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::GumbelSoftmaxFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::gumbel_softmax(logits, F::GumbelSoftmaxFuncOptions().hard(true).dim(-1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor gumbel_softmax(
    @Const @ByRef Tensor logits,
    @Const @ByRef(nullValue = "torch::nn::functional::GumbelSoftmaxFuncOptions{}") GumbelSoftmaxFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor gumbel_softmax(
    @Const @ByRef Tensor logits);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.softmax
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SoftmaxFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::softmax(input, F::SoftmaxFuncOptions(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor softmax(@Const @ByRef Tensor input, @Const @ByRef SoftmaxFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor softmin(
    @Const @ByRef Tensor input,
    @Cast("int64_t") long dim,
    @ByVal ScalarTypeOptional dtype);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.softmin
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SoftminFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::softmin(input, F::SoftminFuncOptions(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor softmin(@Const @ByRef Tensor input, @Const @ByRef SoftminFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.log_softmax
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::LogSoftmaxFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::log_softmax(input, LogSoftmaxFuncOptions(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor log_softmax(
    @Const @ByRef Tensor input,
    @Const @ByRef LogSoftmaxFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.glu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::GLUFuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::glu(input, GLUFuncOptions(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor glu(@Const @ByRef Tensor input, @Cast("const torch::nn::functional::GLUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::GLUFuncOptions{}") GLUOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor gelu(@Const @ByRef Tensor input, @StdString BytePointer approximate);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor gelu(@Const @ByRef Tensor input, @StdString String approximate);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

@Namespace("torch::nn::functional") public static native @ByVal Tensor gelu(@Const @ByRef Tensor input, @Cast("const torch::nn::functional::GELUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::GELUFuncOptions{}") GELUOptions options);

// ============================================================================

// ============================================================================

// ============================================================================

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor relu(@ByVal Tensor input, @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.relu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::ReLUFuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::relu(x, F::ReLUFuncOptions().inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor relu(@ByVal Tensor input, @Cast("const torch::nn::functional::ReLUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::ReLUFuncOptions{}") ReLUOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor relu6(@ByVal Tensor input, @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.relu6
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::ReLU6FuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::relu6(x, F::ReLU6FuncOptions().inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor relu6(@ByVal Tensor input, @Cast("const torch::nn::functional::ReLU6FuncOptions*") @ByRef(nullValue = "torch::nn::functional::ReLU6FuncOptions{}") ReLU6Options options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor rrelu(
    @ByVal Tensor input,
    double lower,
    double upper,
    @Cast("bool") boolean training,
    @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.rrelu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::RReLUFuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::rrelu(x, F::RReLUFuncOptions().lower(0.1).upper(0.4).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor rrelu(@ByVal Tensor input, @Const @ByRef(nullValue = "torch::nn::functional::RReLUFuncOptions{}") RReLUFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor celu(@ByVal Tensor input, double alpha, @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.celu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::CELUFuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::celu(x, F::CELUFuncOptions().alpha(0.42).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor celu(@ByVal Tensor input, @Cast("const torch::nn::functional::CELUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::CELUFuncOptions{}") CELUOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor softplus(@Const @ByRef Tensor input, double beta, double threshold);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.softplus
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SoftplusFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::softplus(x, F::SoftplusFuncOptions().beta(0.5).threshold(3.0));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor softplus(
    @Const @ByRef Tensor input,
    @Cast("const torch::nn::functional::SoftplusFuncOptions*") @ByRef(nullValue = "torch::nn::functional::SoftplusFuncOptions{}") SoftplusOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor softshrink(@Const @ByRef Tensor input, double lambda);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.softshrink
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SoftshrinkFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::softshrink(x, F::SoftshrinkFuncOptions(0.42));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor softshrink(
    @Const @ByRef Tensor input,
    @Cast("const torch::nn::functional::SoftshrinkFuncOptions*") @ByRef(nullValue = "torch::nn::functional::SoftshrinkFuncOptions{}") SoftshrinkOptions options);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal Tensor softsign(@Const @ByRef Tensor input);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal Tensor tanhshrink(@Const @ByRef Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor threshold(
    @ByVal Tensor input,
    double threshold,
    double value,
    @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.threshold
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::ThresholdFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::threshold(x, F::ThresholdFuncOptions(0.5, 0.5).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor threshold(@ByVal Tensor input, @Cast("const torch::nn::functional::ThresholdFuncOptions*") @ByRef ThresholdOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal TensorTensorTuple multi_head_attention_forward(
    @Const @ByRef Tensor query,
    @Const @ByRef Tensor key,
    @Const @ByRef Tensor value,
    @Cast("int64_t") long embed_dim_to_check,
    @Cast("int64_t") long num_heads,
    @Const @ByRef Tensor in_proj_weight,
    @Const @ByRef Tensor in_proj_bias,
    @Const @ByRef Tensor bias_k,
    @Const @ByRef Tensor bias_v,
    @Cast("bool") boolean add_zero_attn,
    double dropout_p,
    @Const @ByRef Tensor out_proj_weight,
    @Const @ByRef Tensor out_proj_bias,
    @Cast("bool") boolean training/*=true*/,
    @Const @ByRef(nullValue = "at::Tensor{}") Tensor key_padding_mask,
    @Cast("bool") boolean need_weights/*=true*/,
    @Const @ByRef(nullValue = "at::Tensor{}") Tensor attn_mask,
    @Cast("bool") boolean use_separate_proj_weight/*=false*/,
    @Const @ByRef(nullValue = "at::Tensor{}") Tensor q_proj_weight,
    @Const @ByRef(nullValue = "at::Tensor{}") Tensor k_proj_weight,
    @Const @ByRef(nullValue = "at::Tensor{}") Tensor v_proj_weight,
    @Const @ByRef(nullValue = "at::Tensor{}") Tensor static_k,
    @Const @ByRef(nullValue = "at::Tensor{}") Tensor static_v,
    @Cast("bool") boolean average_attn_weights/*=true*/);
@Namespace("torch::nn::functional::detail") public static native @ByVal TensorTensorTuple multi_head_attention_forward(
    @Const @ByRef Tensor query,
    @Const @ByRef Tensor key,
    @Const @ByRef Tensor value,
    @Cast("int64_t") long embed_dim_to_check,
    @Cast("int64_t") long num_heads,
    @Const @ByRef Tensor in_proj_weight,
    @Const @ByRef Tensor in_proj_bias,
    @Const @ByRef Tensor bias_k,
    @Const @ByRef Tensor bias_v,
    @Cast("bool") boolean add_zero_attn,
    double dropout_p,
    @Const @ByRef Tensor out_proj_weight,
    @Const @ByRef Tensor out_proj_bias);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

@Namespace("torch::nn::functional") public static native @ByVal TensorTensorTuple multi_head_attention_forward(
    @Const @ByRef Tensor query,
    @Const @ByRef Tensor key,
    @Const @ByRef Tensor value,
    @Const @ByRef MultiheadAttentionForwardFuncOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/batchnorm.h

// #pragma once

// #include <c10/util/irange.h>
// #include <torch/nn/options/batchnorm.h>
// #include <torch/types.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor batch_norm(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor running_mean,
    @Const @ByRef Tensor running_var,
    @ByVal Tensor weight,
    @ByVal Tensor bias,
    @Cast("bool") boolean training,
    @ByVal DoubleOptional momentum,
    double eps);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.batch_norm
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::BatchNormFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::batch_norm(input, mean, variance,
/** F::BatchNormFuncOptions().weight(weight).bias(bias).momentum(0.1).eps(1e-05).training(false));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor batch_norm(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor running_mean,
    @Const @ByRef Tensor running_var,
    @Const @ByRef(nullValue = "torch::nn::functional::BatchNormFuncOptions{}") BatchNormFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor batch_norm(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor running_mean,
    @Const @ByRef Tensor running_var);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/conv.h

// #pragma once

// #include <torch/nn/options/conv.h>
// #include <torch/types.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::nn::functional::detail") public static native @StdString BytePointer padding_unwrap(@ByVal kValid arg0);

@Namespace("torch::nn::functional::detail") public static native @StdString BytePointer padding_unwrap(@ByVal kSame arg0);

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv1d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
    @Const @ByRef conv_padding_t1 padding,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer dilation,
    @Cast("int64_t") long groups);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::Conv1dFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv1d(x, weight, F::Conv1dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv1d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::nn::functional::Conv1dFuncOptions{}") Conv1dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv2d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
    @Const @ByRef conv_padding_t2 padding,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer dilation,
    @Cast("int64_t") long groups);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::Conv2dFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv2d(x, weight, F::Conv2dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv2d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::nn::functional::Conv2dFuncOptions{}") Conv2dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv3d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer stride,
    @Const @ByRef conv_padding_t3 padding,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer dilation,
    @Cast("int64_t") long groups);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::Conv3dFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv3d(x, weight, F::Conv3dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv3d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::nn::functional::Conv3dFuncOptions{}") Conv3dFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose1d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding,
    @Cast("int64_t") long groups,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose1d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding,
    @Cast("int64_t") long groups,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv_transpose1d
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::ConvTranspose1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv_transpose1d(x, weight, F::ConvTranspose1dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv_transpose1d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::nn::functional::ConvTranspose1dFuncOptions{}") ConvTranspose1dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose2d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding,
    @Cast("int64_t") long groups,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose2d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding,
    @Cast("int64_t") long groups,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv_transpose2d
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::ConvTranspose2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv_transpose2d(x, weight, F::ConvTranspose2dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv_transpose2d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::nn::functional::ConvTranspose2dFuncOptions{}") ConvTranspose2dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose3d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding,
    @Cast("int64_t") long groups,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose3d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] output_padding,
    @Cast("int64_t") long groups,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... dilation);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv_transpose3d
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::ConvTranspose3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv_transpose3d(x, weight, F::ConvTranspose3dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv_transpose3d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::nn::functional::ConvTranspose3dFuncOptions{}") ConvTranspose3dFuncOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/distance.h

// #pragma once

// #include <torch/nn/options/distance.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.cosine_similarity
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::CosineSimilarityFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::cosine_similarity(input1, input2,
/** F::CosineSimilarityFuncOptions().dim(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor cosine_similarity(
    @Const @ByRef Tensor x1,
    @Const @ByRef Tensor x2,
    @Cast("const torch::nn::functional::CosineSimilarityFuncOptions*") @ByRef(nullValue = "torch::nn::functional::CosineSimilarityFuncOptions{}") CosineSimilarityOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.pairwise_distance
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::PairwiseDistanceFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::pairwise_distance(input1, input2, F::PairwiseDistanceFuncOptions().p(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor pairwise_distance(
    @Const @ByRef Tensor x1,
    @Const @ByRef Tensor x2,
    @Cast("const torch::nn::functional::PairwiseDistanceFuncOptions*") @ByRef(nullValue = "torch::nn::functional::PairwiseDistanceFuncOptions{}") PairwiseDistanceOptions options);

// ============================================================================

/** Computes the p-norm distance between every pair of row vectors in the input.
 *  This function will be faster if the rows are contiguous. */

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/dropout.h

// #pragma once

// #include <torch/nn/options/dropout.h>

// #include <utility>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor dropout(@ByVal Tensor input, double p, @Cast("bool") boolean training, @Cast("bool") boolean inplace);

 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.dropout
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::DropoutFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::dropout(input, F::DropoutFuncOptions().p(0.5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout(@ByVal Tensor input, @Const @ByRef(nullValue = "torch::nn::functional::DropoutFuncOptions{}") DropoutFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout(@ByVal Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor dropout2d(@ByVal Tensor input, double p, @Cast("bool") boolean training, @Cast("bool") boolean inplace);

 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.dropout2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::Dropout2dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::dropout2d(input, F::Dropout2dFuncOptions().p(0.5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout2d(
    @ByVal Tensor input,
    @Cast("const torch::nn::functional::Dropout2dFuncOptions*") @ByRef(nullValue = "torch::nn::functional::Dropout2dFuncOptions{}") DropoutFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout2d(
    @ByVal Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor dropout3d(@ByVal Tensor input, double p, @Cast("bool") boolean training, @Cast("bool") boolean inplace);

 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.dropout3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::Dropout3dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::dropout3d(input, F::Dropout3dFuncOptions().p(0.5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout3d(
    @ByVal Tensor input,
    @Cast("const torch::nn::functional::Dropout3dFuncOptions*") @ByRef(nullValue = "torch::nn::functional::Dropout3dFuncOptions{}") DropoutFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout3d(
    @ByVal Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor alpha_dropout(
    @ByVal Tensor input,
    double p,
    @Cast("bool") boolean training,
    @Cast("bool") boolean inplace);

 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.alpha_dropout
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AlphaDropoutFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::alpha_dropout(input,
/** F::AlphaDropoutFuncOptions().p(0.5).training(false));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor alpha_dropout(
    @ByVal Tensor input,
    @Const @ByRef(nullValue = "torch::nn::functional::AlphaDropoutFuncOptions{}") AlphaDropoutFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor alpha_dropout(
    @ByVal Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor feature_alpha_dropout(
    @ByVal Tensor input,
    double p,
    @Cast("bool") boolean training,
    @Cast("bool") boolean inplace);

 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.feature_alpha_dropout
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::FeatureAlphaDropoutFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::feature_alpha_dropout(input,
/** F::FeatureAlphaDropoutFuncOptions().p(0.5).training(false));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor feature_alpha_dropout(
    @ByVal Tensor input,
    @Const @ByRef(nullValue = "torch::nn::functional::FeatureAlphaDropoutFuncOptions{}") FeatureAlphaDropoutFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor feature_alpha_dropout(
    @ByVal Tensor input);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/embedding.h

// #pragma once

// #include <torch/nn/options/embedding.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native void _no_grad_embedding_renorm_(
    @ByVal Tensor weight,
    @Const @ByRef Tensor input,
    float max_norm,
    float norm_type);

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor embedding(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @ByVal LongOptional padding_idx,
    @ByVal DoubleOptional max_norm,
    double norm_type,
    @Cast("bool") boolean scale_grad_by_freq,
    @Cast("bool") boolean sparse);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.embedding
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::EmbeddingFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::embedding(input, weight,
/** F::EmbeddingFuncOptions().norm_type(2.5).scale_grad_by_freq(true).sparse(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor embedding(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::nn::functional::EmbeddingFuncOptions{}") EmbeddingFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor embedding_bag(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor offsets,
    @ByVal DoubleOptional max_norm,
    double norm_type,
    @Cast("bool") boolean scale_grad_by_freq,
    @ByVal EmbeddingBagMode mode,
    @Cast("bool") boolean sparse,
    @Const @ByRef Tensor per_sample_weights,
    @Cast("bool") boolean include_last_offset,
    @ByVal LongOptional padding_idx);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.embedding_bag
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::EmbeddingBagFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::embedding_bag(input, weight,
/** F::EmbeddingBagFuncOptions().mode(torch::kSum).offsets(offsets));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor embedding_bag(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::nn::functional::EmbeddingBagFuncOptions{}") EmbeddingBagFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor embedding_bag(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/fold.h

// #pragma once

// #include <torch/nn/options/fold.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor fold(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer output_size,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer dilation,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.fold
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::FoldFuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::fold(input, F::FoldFuncOptions({3, 2}, {2, 2}));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor fold(@Const @ByRef Tensor input, @Cast("const torch::nn::functional::FoldFuncOptions*") @ByRef FoldOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor unfold(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer dilation,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.unfold
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::UnfoldFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::unfold(input, F::UnfoldFuncOptions({2, 2}).padding(1).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor unfold(@Const @ByRef Tensor input, @Cast("const torch::nn::functional::UnfoldFuncOptions*") @ByRef UnfoldOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/linear.h

// #pragma once

// #include <torch/types.h>

@Namespace("torch::nn::functional") public static native @ByVal Tensor bilinear(
    @Const @ByRef Tensor input1,
    @Const @ByRef Tensor input2,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "at::Tensor()") Tensor bias);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal Tensor linear(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "at::Tensor{}") Tensor bias);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/loss.h

// #pragma once

// #include <ATen/ExpandUtils.h>
// #include <torch/nn/functional/activation.h>
// #include <torch/nn/options/loss.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.l1_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::L1LossFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::l1_loss(input, target, F::L1LossFuncOptions(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::L1LossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::L1LossFuncOptions{}") L1LossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor kl_div(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal kldiv_loss_reduction_t reduction,
    @Cast("bool") boolean log_target/*=false*/);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor kl_div(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal kldiv_loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.kl_div
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::KLDivFuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::kl_div(input, target,
/** F::KLDivFuncOptions.reduction(torch::kNone).log_target(false));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor kl_div(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::KLDivFuncOptions*") @ByRef(nullValue = "torch::nn::functional::KLDivFuncOptions{}") KLDivLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor mse_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.mse_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MSELossFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::mse_loss(input, target, F::MSELossFuncOptions(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor mse_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::MSELossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::MSELossFuncOptions{}") MSELossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor binary_cross_entropy(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Const @ByRef Tensor weight,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.binary_cross_entropy
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::BinaryCrossEntropyFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::binary_cross_entropy(input, target,
/** F::BinaryCrossEntropyFuncOptions().weight(weight));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor binary_cross_entropy(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::BinaryCrossEntropyFuncOptions*") @ByRef(nullValue = "torch::nn::functional::BinaryCrossEntropyFuncOptions{}") BCELossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor hinge_embedding_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    double margin,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.hinge_embedding_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::HingeEmbeddingLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::hinge_embedding_loss(input, target,
/** F::HingeEmbeddingLossFuncOptions().margin(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor hinge_embedding_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::HingeEmbeddingLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::HingeEmbeddingLossFuncOptions{}") HingeEmbeddingLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor multi_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("int64_t") long p,
    double margin,
    @Const @ByRef Tensor weight,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.multi_margin_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::MultiMarginLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::multi_margin_loss(input, target,
/** F::MultiMarginLossFuncOptions().margin(2).weight(weight));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor multi_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::MultiMarginLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::MultiMarginLossFuncOptions{}") MultiMarginLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor cosine_embedding_loss(
    @Const @ByRef Tensor input1,
    @Const @ByRef Tensor input2,
    @Const @ByRef Tensor target,
    double margin,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.cosine_embedding_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::CosineEmbeddingLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::cosine_embedding_loss(input1, input2, target,
/** F::CosineEmbeddingLossFuncOptions().margin(0.5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor cosine_embedding_loss(
    @Const @ByRef Tensor input1,
    @Const @ByRef Tensor input2,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::CosineEmbeddingLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::CosineEmbeddingLossFuncOptions{}") CosineEmbeddingLossOptions options);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal Tensor _smooth_l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    double beta/*=1.*/);
@Namespace("torch::nn::functional") public static native @ByVal Tensor _smooth_l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor smooth_l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal loss_reduction_t reduction,
    double beta/*=1.*/);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor smooth_l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.smooth_l1_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SmoothL1LossFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::smooth_l1_loss(input, target, F::SmoothL1LossFuncOptions(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor smooth_l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::SmoothL1LossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::SmoothL1LossFuncOptions{}") SmoothL1LossOptions options,
    double beta/*=1.*/);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor huber_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal loss_reduction_t reduction,
    double delta/*=1.*/);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor huber_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.huber_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::HuberLossFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::huber_loss(input, target,
/** F::HuberLossFuncOptions().reduction(torch::kNone).delta(0.5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor huber_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::HuberLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::HuberLossFuncOptions{}") HuberLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor multilabel_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.multilabel_margin_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::MultilabelMarginLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::multilabel_margin_loss(input, target,
/** F::MultilabelMarginLossFuncOptions(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor multilabel_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::MultilabelMarginLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::MultilabelMarginLossFuncOptions{}") MultiLabelMarginLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor soft_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.soft_margin_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SoftMarginLossFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::soft_margin_loss(input, target,
/** F::SoftMarginLossFuncOptions(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor soft_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::SoftMarginLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::SoftMarginLossFuncOptions{}") SoftMarginLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor multilabel_soft_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Const @ByRef Tensor weight,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.multilabel_soft_margin_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::MultilabelSoftMarginLossFuncOptions} class to learn
/** what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::multilabel_soft_margin_loss(input, target,
/** F::MultilabelSoftMarginLossFuncOptions().reduction(torch::kNone).weight(weight));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor multilabel_soft_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::MultilabelSoftMarginLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::MultilabelSoftMarginLossFuncOptions{}") MultiLabelSoftMarginLossOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor multilabel_soft_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor triplet_margin_loss(
    @Const @ByRef Tensor anchor,
    @Const @ByRef Tensor positive,
    @Const @ByRef Tensor negative,
    double margin,
    double p,
    double eps,
    @Cast("bool") boolean swap,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.triplet_margin_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::TripletMarginLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::triplet_margin_loss(anchor, positive, negative,
/** F::TripletMarginLossFuncOptions().margin(1.0));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor triplet_margin_loss(
    @Const @ByRef Tensor anchor,
    @Const @ByRef Tensor positive,
    @Const @ByRef Tensor negative,
    @Cast("const torch::nn::functional::TripletMarginLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::TripletMarginLossFuncOptions{}") TripletMarginLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor triplet_margin_with_distance_loss(
    @Const @ByRef Tensor anchor,
    @Const @ByRef Tensor positive,
    @Const @ByRef Tensor negative,
    @ByVal @Cast("c10::optional<torch::nn::functional::TripletMarginWithDistanceLossFuncOptions::distance_function_t>*") Pointer distance_function,
    double margin,
    @Cast("bool") boolean swap,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.triplet_margin_with_distance_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::TripletMarginWithDistanceLossFuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::triplet_margin_with_distance_loss(anchor, positive, negative,
/** F::TripletMarginWithDistanceLossFuncOptions().margin(1.0));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor triplet_margin_with_distance_loss(
    @Const @ByRef Tensor anchor,
    @Const @ByRef Tensor positive,
    @Const @ByRef Tensor negative,
    @Cast("const torch::nn::functional::TripletMarginWithDistanceLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::TripletMarginWithDistanceLossFuncOptions{}") TripletMarginWithDistanceLossOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor triplet_margin_with_distance_loss(
    @Const @ByRef Tensor anchor,
    @Const @ByRef Tensor positive,
    @Const @ByRef Tensor negative);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor ctc_loss(
    @Const @ByRef Tensor log_probs,
    @Const @ByRef Tensor targets,
    @Const @ByRef Tensor input_lengths,
    @Const @ByRef Tensor target_lengths,
    @Cast("int64_t") long blank,
    @ByVal loss_reduction_t reduction,
    @Cast("bool") boolean zero_infinity);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.ctc_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::CTCLossFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::ctc_loss(log_probs, targets, input_lengths, target_lengths,
/** F::CTCLossFuncOptions().reduction(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor ctc_loss(
    @Const @ByRef Tensor log_probs,
    @Const @ByRef Tensor targets,
    @Const @ByRef Tensor input_lengths,
    @Const @ByRef Tensor target_lengths,
    @Cast("const torch::nn::functional::CTCLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::CTCLossFuncOptions{}") CTCLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor poisson_nll_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("bool") boolean log_input,
    @Cast("bool") boolean full,
    double eps,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.poisson_nll_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::PoissonNLLLossFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::poisson_nll_loss(input, target,
/** F::PoissonNLLLossFuncOptions().reduction(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor poisson_nll_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::PoissonNLLLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::PoissonNLLLossFuncOptions{}") PoissonNLLLossOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor poisson_nll_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor margin_ranking_loss(
    @Const @ByRef Tensor input1,
    @Const @ByRef Tensor input2,
    @Const @ByRef Tensor target,
    double margin,
    @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.margin_ranking_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::MarginRankingLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::margin_ranking_loss(input1, input2, target,
/** F::MarginRankingLossFuncOptions().margin(0.5).reduction(torch::kSum));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor margin_ranking_loss(
    @Const @ByRef Tensor input1,
    @Const @ByRef Tensor input2,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::MarginRankingLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::MarginRankingLossFuncOptions{}") MarginRankingLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor nll_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Const @ByRef Tensor weight,
    @Cast("int64_t") long ignore_index,
    @Const @ByVal loss_reduction_t reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.nll_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::NLLLossFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::nll_loss(input, target,
/** F::NLLLossFuncOptions().ignore_index(-100).reduction(torch::kMean));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor nll_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::NLLLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::NLLLossFuncOptions{}") NLLLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor cross_entropy(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Const @ByRef Tensor weight,
    @Cast("int64_t") long ignore_index,
    @ByVal loss_reduction_t reduction,
    double label_smoothing);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.cross_entropy
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::CrossEntropyFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::cross_entropy(input, target,
/** F::CrossEntropyFuncOptions().ignore_index(-100).reduction(torch::kMean));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor cross_entropy(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::CrossEntropyFuncOptions*") @ByRef(nullValue = "torch::nn::functional::CrossEntropyFuncOptions{}") CrossEntropyLossOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor cross_entropy(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor binary_cross_entropy_with_logits(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Const @ByRef Tensor weight,
    @ByVal loss_reduction_t reduction,
    @Const @ByRef Tensor pos_weight);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.binary_cross_entropy_with_logits
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::BinaryCrossEntropyWithLogitsFuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::binary_cross_entropy_with_logits(input, target,
/** F::BinaryCrossEntropyWithLogitsFuncOptions().pos_weight(pos_weight).reduction(torch::kSum));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor binary_cross_entropy_with_logits(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::BinaryCrossEntropyWithLogitsFuncOptions*") @ByRef(nullValue = "torch::nn::functional::BinaryCrossEntropyWithLogitsFuncOptions{}") BCEWithLogitsLossOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/normalization.h

// #pragma once

// #include <torch/nn/functional/padding.h>
// #include <torch/nn/functional/pooling.h>
// #include <torch/nn/options/normalization.h>
// #include <torch/types.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor normalize(
    @Const @ByRef Tensor input,
    double p,
    @Cast("int64_t") long dim,
    double eps,
    @ByVal TensorOptional out);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.normalize
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::NormalizeFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::normalize(input, F::NormalizeFuncOptions().p(1).dim(-1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor normalize(
    @Const @ByRef Tensor input,
    @ByVal(nullValue = "torch::nn::functional::NormalizeFuncOptions{}") NormalizeFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor normalize(
    @Const @ByRef Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor layer_norm(
    @Const @ByRef Tensor input,
    @Cast("const std::vector<int64_t>*") @ByRef LongVector normalized_shape,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    double eps);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.layer_norm
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::LayerNormFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::layer_norm(input, F::LayerNormFuncOptions({2, 2}).eps(2e-5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor layer_norm(
    @Const @ByRef Tensor input,
    @Const @ByRef LayerNormFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor local_response_norm(
    @Const @ByRef Tensor input,
    @Cast("int64_t") long size,
    double alpha,
    double beta,
    double k);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.local_response_norm
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::LocalResponseNormFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::local_response_norm(x, F::LocalResponseNormFuncOptions(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor local_response_norm(
    @Const @ByRef Tensor input,
    @Cast("const torch::nn::functional::LocalResponseNormFuncOptions*") @ByRef LocalResponseNormOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor group_norm(
    @Const @ByRef Tensor input,
    @Cast("int64_t") long num_groups,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    double eps);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.group_norm
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::GroupNormFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::group_norm(input, F::GroupNormFuncOptions(2).eps(2e-5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor group_norm(
    @Const @ByRef Tensor input,
    @Const @ByRef GroupNormFuncOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/padding.h

// #pragma once

// #include <ATen/PadNd.h>
// #include <torch/nn/options/padding.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor pad(
    @Const @ByRef Tensor input,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef pad,
    @ByVal pad_mode_t mode,
    double value);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor pad(
    @Const @ByRef Tensor input,
    @ByVal @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] pad,
    @ByVal pad_mode_t mode,
    double value);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.pad
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::PadFuncOptions} class to
/** learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::pad(input, F::PadFuncOptions({1, 2, 2, 1, 1,
/** 2}).mode(torch::kReplicate));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor pad(@Const @ByRef Tensor input, @Const @ByRef PadFuncOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/pixelshuffle.h

// #pragma once

// #include <torch/nn/options/pixelshuffle.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.pixel_shuffle
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::PixelShuffleFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::pixel_shuffle(x, F::PixelShuffleFuncOptions(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor pixel_shuffle(
    @Const @ByRef Tensor input,
    @Cast("const torch::nn::functional::PixelShuffleFuncOptions*") @ByRef PixelShuffleOptions options);

@Namespace("torch::nn::functional") public static native @ByVal Tensor pixel_unshuffle(
    @Const @ByRef Tensor input,
    @Cast("const torch::nn::functional::PixelUnshuffleFuncOptions*") @ByRef PixelUnshuffleOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/pooling.h

// #pragma once

// #include <c10/util/irange.h>
// #include <torch/nn/functional/activation.h>
// #include <torch/nn/modules/utils.h>
// #include <torch/nn/options/pooling.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor avg_pool1d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer padding,
    @Cast("bool") boolean ceil_mode,
    @Cast("bool") boolean count_include_pad);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.avg_pool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AvgPool1dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::avg_pool1d(x, F::AvgPool1dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor avg_pool1d(
    @Const @ByRef Tensor input,
    @Const @ByRef AvgPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor avg_pool2d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
    @Cast("bool") boolean ceil_mode,
    @Cast("bool") boolean count_include_pad,
    @ByVal LongOptional divisor_override);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.avg_pool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AvgPool2dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::avg_pool2d(x, F::AvgPool2dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor avg_pool2d(
    @Const @ByRef Tensor input,
    @Const @ByRef AvgPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor avg_pool3d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer padding,
    @Cast("bool") boolean ceil_mode,
    @Cast("bool") boolean count_include_pad,
    @ByVal LongOptional divisor_override);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.avg_pool3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AvgPool3dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::avg_pool3d(x, F::AvgPool3dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor avg_pool3d(
    @Const @ByRef Tensor input,
    @Const @ByRef AvgPool3dOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_pool1d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer padding,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer dilation,
    @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_pool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxPool1dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool1d(x, F::MaxPool1dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_pool1d(
    @Const @ByRef Tensor input,
    @Const @ByRef MaxPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal TensorTensorTuple max_pool1d_with_indices(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer padding,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer dilation,
    @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::MaxPool1dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool1d_with_indices(x, F::MaxPool1dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal TensorTensorTuple max_pool1d_with_indices(
    @Const @ByRef Tensor input,
    @Const @ByRef MaxPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_pool2d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer dilation,
    @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_pool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxPool2dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool2d(x, F::MaxPool2dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_pool2d(
    @Const @ByRef Tensor input,
    @Const @ByRef MaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal TensorTensorTuple max_pool2d_with_indices(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer dilation,
    @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::MaxPool2dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool2d_with_indices(x, F::MaxPool2dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal TensorTensorTuple max_pool2d_with_indices(
    @Const @ByRef Tensor input,
    @Const @ByRef MaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_pool3d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer padding,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer dilation,
    @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_pool3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxPool3dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool3d(x, F::MaxPool3dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_pool3d(
    @Const @ByRef Tensor input,
    @Const @ByRef MaxPool3dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal TensorTensorTuple max_pool3d_with_indices(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer padding,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer dilation,
    @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::MaxPool3dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool3d_with_indices(x, F::MaxPool3dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal TensorTensorTuple max_pool3d_with_indices(
    @Const @ByRef Tensor input,
    @Const @ByRef MaxPool3dOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal TensorTensorTuple adaptive_max_pool1d_with_indices(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer output_size);
 // namespace detail

/** See the documentation for
 *  {@code torch::nn::functional::AdaptiveMaxPool1dFuncOptions} class to learn what
 *  optional arguments are supported for this functional.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_max_pool1d_with_indices(x, F::AdaptiveMaxPool1dFuncOptions(3));
 *  }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal TensorTensorTuple adaptive_max_pool1d_with_indices(
    @Const @ByRef Tensor input,
    @Const @ByRef AdaptiveMaxPool1dOptions options);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_max_pool1d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_max_pool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::AdaptiveMaxPool1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_max_pool1d(x, F::AdaptiveMaxPool1dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_max_pool1d(
    @Const @ByRef Tensor input,
    @Const @ByRef AdaptiveMaxPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal TensorTensorTuple adaptive_max_pool2d_with_indices(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<2>*") LongOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for
/** {@code torch::nn::functional::AdaptiveMaxPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_max_pool2d_with_indices(x, F::AdaptiveMaxPool2dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal TensorTensorTuple adaptive_max_pool2d_with_indices(
    @Const @ByRef Tensor input,
    @Const @ByRef AdaptiveMaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_max_pool2d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<2>*") LongOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_max_pool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::AdaptiveMaxPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_max_pool2d(x, F::AdaptiveMaxPool2dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_max_pool2d(
    @Const @ByRef Tensor input,
    @Const @ByRef AdaptiveMaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal TensorTensorTuple adaptive_max_pool3d_with_indices(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<3>*") LongOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for
/** {@code torch::nn::functional::AdaptiveMaxPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_max_pool3d_with_indices(x, F::AdaptiveMaxPool3dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal TensorTensorTuple adaptive_max_pool3d_with_indices(
    @Const @ByRef Tensor input,
    @Const @ByRef AdaptiveMaxPool3dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_max_pool3d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<3>*") LongOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_max_pool3d
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::AdaptiveMaxPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_max_pool3d(x, F::AdaptiveMaxPool3dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_max_pool3d(
    @Const @ByRef Tensor input,
    @Const @ByRef AdaptiveMaxPool3dOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_avg_pool1d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_avg_pool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::AdaptiveAvgPool1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_avg_pool1d(x, F::AdaptiveAvgPool1dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_avg_pool1d(
    @Const @ByRef Tensor input,
    @Const @ByRef AdaptiveAvgPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_avg_pool2d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<2>*") LongOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_avg_pool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::AdaptiveAvgPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_avg_pool2d(x, F::AdaptiveAvgPool2dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_avg_pool2d(
    @Const @ByRef Tensor input,
    @Const @ByRef AdaptiveAvgPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_avg_pool3d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<3>*") LongOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_avg_pool3d
/** about the exact behavior of this functional.
/**
/** See the documentation for
/** {@code torch::nn::functional::AdaptiveAvgPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_avg_pool3d(x, F::AdaptiveAvgPool3dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_avg_pool3d(
    @Const @ByRef Tensor input,
    @Const @ByRef AdaptiveAvgPool3dOptions options);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector _unpool_output_size(
    @Const @ByRef Tensor input,
    @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size,
    @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride,
    @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding,
    @Const @ByRef LongVectorOptional output_size);
@Namespace("torch::nn::functional") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector _unpool_output_size(
    @Const @ByRef Tensor input,
    @ByRef @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] kernel_size,
    @ByRef @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] stride,
    @ByRef @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] padding,
    @Const @ByRef LongVectorOptional output_size);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_unpool1d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor indices,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer padding,
    @Const @ByRef LongVectorOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_unpool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxUnpool1dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_unpool1d(x, indices,
/** F::MaxUnpool1dFuncOptions(3).stride(2).padding(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_unpool1d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor indices,
    @Const @ByRef MaxUnpool1dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_unpool2d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor indices,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
    @Const @ByRef LongVectorOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_unpool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxUnpool2dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_unpool2d(x, indices,
/** F::MaxUnpool2dFuncOptions(3).stride(2).padding(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_unpool2d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor indices,
    @Const @ByRef MaxUnpool2dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_unpool3d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor indices,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer padding,
    @Const @ByRef LongVectorOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_unpool3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxUnpool3dFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_unpool3d(x, indices, F::MaxUnpool3dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_unpool3d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor indices,
    @Const @ByRef MaxUnpool3dFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal TensorTensorTuple fractional_max_pool2d_with_indices(
    @Const @ByRef Tensor input,
    @Cast("const torch::ExpandingArray<2>*") @ByRef LongPointer kernel_size,
    @Cast("const c10::optional<torch::ExpandingArray<2> >*") @ByRef LongExpandingArrayOptional output_size,
    @Cast("const c10::optional<torch::ExpandingArray<2,double> >*") @ByRef DoubleExpandingArrayOptional output_ratio,
    @Const @ByRef Tensor _random_samples);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for
/** {@code torch::nn::functional::FractionalMaxPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::fractional_max_pool2d_with_indices(x,
/** F::FractionalMaxPool2dFuncOptions(3).output_size(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal TensorTensorTuple fractional_max_pool2d_with_indices(
    @Const @ByRef Tensor input,
    @Const @ByRef FractionalMaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor fractional_max_pool2d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
    @ByVal @Cast("c10::optional<torch::ExpandingArray<2> >*") LongExpandingArrayOptional output_size,
    @ByVal @Cast("c10::optional<torch::ExpandingArray<2,double> >*") DoubleExpandingArrayOptional output_ratio,
    @Const @ByRef Tensor _random_samples);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for
/** {@code torch::nn::functional::FractionalMaxPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::fractional_max_pool2d(x,
/** F::FractionalMaxPool2dFuncOptions(3).output_size(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor fractional_max_pool2d(
    @Const @ByRef Tensor input,
    @Const @ByRef FractionalMaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal TensorTensorTuple fractional_max_pool3d_with_indices(
    @Const @ByRef Tensor input,
    @Cast("const torch::ExpandingArray<3>*") @ByRef LongPointer kernel_size,
    @Cast("const c10::optional<torch::ExpandingArray<3> >*") @ByRef LongExpandingArrayOptional output_size,
    @Cast("const c10::optional<torch::ExpandingArray<3,double> >*") @ByRef DoubleExpandingArrayOptional output_ratio,
    @Const @ByRef Tensor _random_samples);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for
/** {@code torch::nn::functional::FractionalMaxPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::fractional_max_pool3d_with_indices(x,
/** F::FractionalMaxPool3dFuncOptions(3).output_size(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal TensorTensorTuple fractional_max_pool3d_with_indices(
    @Const @ByRef Tensor input,
    @Const @ByRef FractionalMaxPool3dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor fractional_max_pool3d(
    @Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer kernel_size,
    @ByVal @Cast("c10::optional<torch::ExpandingArray<3> >*") LongExpandingArrayOptional output_size,
    @ByVal @Cast("c10::optional<torch::ExpandingArray<3,double> >*") DoubleExpandingArrayOptional output_ratio,
    @Const @ByRef Tensor _random_samples);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for
/** {@code torch::nn::functional::FractionalMaxPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::fractional_max_pool3d(x,
/** F::FractionalMaxPool3dFuncOptions(3).output_size(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor fractional_max_pool3d(
    @Const @ByRef Tensor input,
    @Const @ByRef FractionalMaxPool3dOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor lp_pool1d(
    @Const @ByRef Tensor input,
    double norm_type,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
    @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.lp_pool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::LPPool1dFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::lp_pool1d(x, F::LPPool1dFuncOptions(2, 3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor lp_pool1d(
    @Const @ByRef Tensor input,
    @Const @ByRef LPPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor lp_pool2d(
    @Const @ByRef Tensor input,
    double norm_type,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
    @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.lp_pool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::LPPool2dFuncOptions} class
/** to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::lp_pool2d(x, F::LPPool2dFuncOptions(2, {2, 3}).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor lp_pool2d(
    @Const @ByRef Tensor input,
    @Const @ByRef LPPool2dOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/upsampling.h

// #pragma once

// #include <c10/util/irange.h>
// #include <torch/nn/functional/pooling.h>
// #include <torch/nn/options/upsampling.h>

// #include <cmath>
// #include <utility>

@Namespace("torch::nn::functional") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector _interp_output_size(
    @Cast("int64_t") long dim,
    @ByVal @Cast("std::tuple<at::Tensor,c10::optional<std::vector<int64_t> >,c10::optional<std::vector<double> >,c10::optional<bool> >*") Pointer closed_over_args);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor interpolate(
    @Const @ByRef Tensor input,
    @Const @ByRef LongVectorOptional size,
    @Const @ByRef DoubleVectorOptional scale_factor,
    @ByVal interpolate_mode_t mode,
    @ByVal BoolOptional align_corners,
    @ByVal BoolOptional recompute_scale_factor,
    @Cast("bool") boolean antialias);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.interpolate
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::InterpolateFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::interpolate(input,
/** F::InterpolateFuncOptions().size({4}).mode(torch::kNearest));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor interpolate(
    @Const @ByRef Tensor input,
    @Const @ByRef(nullValue = "torch::nn::functional::InterpolateFuncOptions{}") InterpolateFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor interpolate(
    @Const @ByRef Tensor input);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/vision.h

// #pragma once

// #include <torch/nn/options/vision.h>
// #include <torch/types.h>

@Namespace("torch::nn::functional") public static native @ByVal Tensor affine_grid(
    @Const @ByRef Tensor theta,
    @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size,
    @Cast("bool") boolean align_corners/*=false*/);
@Namespace("torch::nn::functional") public static native @ByVal Tensor affine_grid(
    @Const @ByRef Tensor theta,
    @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("torch::nn::functional") public static native @ByVal Tensor affine_grid(
    @Const @ByRef Tensor theta,
    @ByRef @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long[] size,
    @Cast("bool") boolean align_corners/*=false*/);
@Namespace("torch::nn::functional") public static native @ByVal Tensor affine_grid(
    @Const @ByRef Tensor theta,
    @ByRef @Cast({"int64_t*", "c10::ArrayRef<int64_t>", "std::vector<int64_t>&"}) @StdVector long... size);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor grid_sample(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor grid,
    @ByVal grid_sample_mode_t mode,
    @ByVal grid_sample_padding_mode_t padding_mode,
    @ByVal BoolOptional align_corners);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.grid_sample
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::GridSampleFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::grid_sample(input, grid,
/** F::GridSampleFuncOptions().mode(torch::kBilinear).padding_mode(torch::kZeros).align_corners(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor grid_sample(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor grid,
    @Const @ByRef(nullValue = "torch::nn::functional::GridSampleFuncOptions{}") GridSampleFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor grid_sample(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor grid);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/instancenorm.h

// #pragma once

// #include <torch/nn/options/instancenorm.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor instance_norm(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor running_mean,
    @Const @ByRef Tensor running_var,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @Cast("bool") boolean use_input_stats,
    double momentum,
    double eps);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See
/** https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.instance_norm
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::InstanceNormFuncOptions}
/** class to learn what optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::instance_norm(input,
/** F::InstanceNormFuncOptions().running_mean(mean).running_var(variance).weight(weight).bias(bias).momentum(0.1).eps(1e-5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor instance_norm(
    @Const @ByRef Tensor input,
    @Const @ByRef(nullValue = "torch::nn::functional::InstanceNormFuncOptions{}") InstanceNormFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor instance_norm(
    @Const @ByRef Tensor input);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/module.h

// #pragma once

// #include <torch/nn/modules/container/any_module_holder.h>
// #include <torch/nn/modules/container/any_value.h>
// #include <torch/nn/pimpl.h>
// #include <torch/ordered_dict.h>
// #include <torch/serialize/archive.h>
// #include <torch/types.h>

// #include <ATen/ATen.h>

// #include <functional>
// #include <iosfwd>
// #include <map>
// #include <memory>
// #include <string>
// #include <type_traits>
// Targeting ../Module.java


@Namespace("torch::nn") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer stream, @Const @ByRef Module module);


/** Serialize a {@code Module} pointer into an {@code OutputArchive}. */
@Namespace("torch::nn") public static native @ByRef @Name("operator <<") OutputArchive shiftLeft(
    @ByRef OutputArchive archive,
    @SharedPtr @Cast({"", "std::shared_ptr<torch::nn::Module>"}) Module module);

/** Deserializes a {@code Module} from an {@code InputArchive}. */
@Namespace("torch::nn") public static native @ByRef @Name("operator >>") InputArchive shiftRight(
    @ByRef InputArchive archive,
    @SharedPtr @Cast({"", "std::shared_ptr<torch::nn::Module>"}) Module module);

// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ nn::Module ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



















 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules.h

// #pragma once

// Common
// #include <torch/nn/modules/common.h>

// Containers
// #include <torch/nn/modules/container/any.h>
// #include <torch/nn/modules/container/functional.h>
// #include <torch/nn/modules/container/moduledict.h>
// #include <torch/nn/modules/container/modulelist.h>
// #include <torch/nn/modules/container/named_any.h>
// #include <torch/nn/modules/container/parameterdict.h>
// #include <torch/nn/modules/container/parameterlist.h>
// #include <torch/nn/modules/container/sequential.h>

// Layers
// #include <torch/nn/modules/activation.h>
// #include <torch/nn/modules/adaptive.h>
// #include <torch/nn/modules/batchnorm.h>
// #include <torch/nn/modules/conv.h>
// #include <torch/nn/modules/distance.h>
// #include <torch/nn/modules/dropout.h>
// #include <torch/nn/modules/embedding.h>
// #include <torch/nn/modules/fold.h>
// #include <torch/nn/modules/instancenorm.h>
// #include <torch/nn/modules/linear.h>
// #include <torch/nn/modules/loss.h>
// #include <torch/nn/modules/normalization.h>
// #include <torch/nn/modules/padding.h>
// #include <torch/nn/modules/pixelshuffle.h>
// #include <torch/nn/modules/pooling.h>
// #include <torch/nn/modules/rnn.h>
// #include <torch/nn/modules/transformer.h>
// #include <torch/nn/modules/transformercoder.h>
// #include <torch/nn/modules/transformerlayer.h>
// #include <torch/nn/modules/upsampling.h>


// Parsed from torch/nn/modules/common.h


///
///
///
///
///
// #pragma once

/** This macro enables a module with default arguments in its forward method
 *  to be used in a Sequential module.
 * 
 *  Example usage:
 * 
 *  Let's say we have a module declared like this:
 *  <pre>{@code
 *  struct MImpl : torch::nn::Module {
 *   public:
 *    explicit MImpl(int value_) : value(value_) {}
 *    torch::Tensor forward(int a, int b = 2, double c = 3.0) {
 *      return torch::tensor(a + b + c);
 *    }
 *   private:
 *    int value;
 *  };
 *  TORCH_MODULE(M);
 *  }</pre>
 * 
 *  If we try to use it in a Sequential module and run forward:
 *  <pre>{@code
 *  torch::nn::Sequential seq(M(1));
 *  seq->forward(1);
 *  }</pre>
 * 
 *  We will receive the following error message:
 *  <pre>{@code
 *  MImpl's forward() method expects 3 argument(s), but received 1.
 *  If MImpl's forward() method has default arguments, please make sure
 *  the forward() method is declared with a corresponding
 *  `FORWARD_HAS_DEFAULT_ARGS` macro.
 *  }</pre>
 * 
 *  The right way to fix this error is to use the {@code FORWARD_HAS_DEFAULT_ARGS}
 *  macro when declaring the module:
 *  <pre>{@code
 *  struct MImpl : torch::nn::Module {
 *   public:
 *    explicit MImpl(int value_) : value(value_) {}
 *    torch::Tensor forward(int a, int b = 2, double c = 3.0) {
 *      return torch::tensor(a + b + c);
 *    }
 *   protected:
 *    /*
 *    NOTE: looking at the argument list of `forward`:
 *    `forward(int a, int b = 2, double c = 3.0)`
 *    we saw the following default arguments:
 *    ----------------------------------------------------------------
 *    0-based index of default |         Default value of arg
 *    arg in forward arg list  |  (wrapped by `torch::nn::AnyValue()`)
 *    ----------------------------------------------------------------
 *                1            |       torch::nn::AnyValue(2)
 *                2            |       torch::nn::AnyValue(3.0)
 *    ----------------------------------------------------------------
 *    Thus we pass the following arguments to the `FORWARD_HAS_DEFAULT_ARGS`
 *    macro:
 *    * /
 *    FORWARD_HAS_DEFAULT_ARGS({1, torch::nn::AnyValue(2)}, {2,
 *    torch::nn::AnyValue(3.0)})
 *   private:
 *    int value;
 *  };
 *  TORCH_MODULE(M);
 *  }</pre>
 *  Now, running the following would work:
 *  <pre>{@code
 *  torch::nn::Sequential seq(M(1));
 *  seq->forward(1);  // This correctly populates the default arguments for
 *  `MImpl::forward`
 *  }</pre> */
// #define FORWARD_HAS_DEFAULT_ARGS(...)
//   template <typename ModuleType, typename... ArgumentTypes>
//   friend struct torch::nn::AnyModuleHolder;
//   bool _forward_has_default_args() override {
//     return true;
//   }
//   unsigned int _forward_num_required_args() override {
//     std::pair<unsigned int, torch::nn::AnyValue> args_info[] = {__VA_ARGS__};
//     return args_info[0].first;
//   }
//   std::vector<torch::nn::AnyValue> _forward_populate_default_args(
//       std::vector<torch::nn::AnyValue>&& arguments) override {
//     std::pair<unsigned int, torch::nn::AnyValue> args_info[] = {__VA_ARGS__};
//     unsigned int num_all_args = std::rbegin(args_info)->first + 1;
//     TORCH_INTERNAL_ASSERT(
//         arguments.size() >= _forward_num_required_args() &&
//         arguments.size() <= num_all_args);
//     std::vector<torch::nn::AnyValue> ret = std::move(arguments);
//     ret.reserve(num_all_args);
//     for (auto& arg_info : args_info) {
//       if (arg_info.first > ret.size() - 1)
//         ret.emplace_back(std::move(arg_info.second));
//     }
//     return ret;
//   }


// Parsed from torch/nn/modules/container/any.h

// #pragma once

// #include <torch/detail/static.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/container/any_module_holder.h>
// #include <torch/nn/modules/container/any_value.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/autograd/variable.h>
// #include <torch/csrc/utils/memory.h>
// #include <torch/csrc/utils/variadic.h>

// #include <ATen/Device.h>

// #include <memory>
// #include <type_traits>
// #include <typeinfo>
// #include <utility>
// #include <vector>
// Targeting ../AnyModule.java



// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ AnyModule ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

































// Private Methods







 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/container/moduledict.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/ordered_dict.h>
// #include <vector>
// Targeting ../ModuleDictImpl.java


// Targeting ../ModuleDict.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/container/modulelist.h

// #pragma once

// #include <c10/util/irange.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>

// #include <utility>
// #include <vector>
// Targeting ../ModuleListImpl.java


// Targeting ../ModuleList.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/container/named_any.h

// #pragma once

// #include <torch/detail/static.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/container/any.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/autograd/variable.h>
// #include <torch/csrc/utils/memory.h>
// #include <torch/csrc/utils/variadic.h>

// #include <ATen/Device.h>

// #include <initializer_list>
// #include <memory>
// #include <type_traits>
// #include <typeinfo>
// #include <utility>
// #include <vector>
// Targeting ../NamedAnyModule.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/container/sequential.h

// #pragma once

// #include <torch/detail/static.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/container/any.h>
// #include <torch/nn/modules/container/named_any.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <c10/util/Exception.h>

// #include <cstdint>
// #include <memory>
// #include <ostream>
// #include <string>
// #include <type_traits>
// #include <utility>
// #include <vector>
// Targeting ../SequentialImpl.java


// Targeting ../Sequential.java


 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/container/parameterdict.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/pimpl.h>
// #include <torch/ordered_dict.h>
// #include <utility>
// #include <vector>
// Targeting ../ParameterDictImpl.java


// Targeting ../ParameterDict.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/container/parameterlist.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>

// #include <vector>
// Targeting ../ParameterListImpl.java


// Targeting ../ParameterList.java


 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/adaptive.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/activation.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/container/modulelist.h>
// #include <torch/nn/modules/container/sequential.h>
// #include <torch/nn/modules/linear.h>
// #include <torch/nn/options/adaptive.h>
// Targeting ../ASMoutput.java


// Targeting ../AdaptiveLogSoftmaxWithLossImpl.java


// Targeting ../AdaptiveLogSoftmaxWithLoss.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/batchnorm.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/batchnorm.h>
// #include <torch/nn/init.h>
// #include <torch/nn/options/batchnorm.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <cstdint>
// Targeting ../BatchNorm1dImplBaseBase.java


// Targeting ../InstanceNorm1dImplBaseBase.java


// Targeting ../BatchNorm2dImplBaseBase.java


// Targeting ../InstanceNorm2dImplBaseBase.java


// Targeting ../BatchNorm3dImplBaseBase.java


// Targeting ../InstanceNorm3dImplBaseBase.java


// Targeting ../BatchNorm1dImplBase.java


// Targeting ../BatchNorm2dImplBase.java


// Targeting ../BatchNorm3dImplBase.java


// Targeting ../BatchNorm1dImpl.java


// Targeting ../BatchNorm1d.java


// Targeting ../BatchNorm2dImpl.java


// Targeting ../BatchNorm2d.java


// Targeting ../BatchNorm3dImpl.java


// Targeting ../BatchNorm3d.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/instancenorm.h

// #pragma once

// #include <torch/nn/modules/batchnorm.h>
// #include <torch/nn/options/instancenorm.h>
// Targeting ../InstanceNorm1dImplBase.java


// Targeting ../InstanceNorm2dImplBase.java


// Targeting ../InstanceNorm3dImplBase.java


// Targeting ../InstanceNorm1dImpl.java


// Targeting ../InstanceNorm1d.java


// Targeting ../InstanceNorm2dImpl.java


// Targeting ../InstanceNorm2d.java


// Targeting ../InstanceNorm3dImpl.java


// Targeting ../InstanceNorm3d.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/conv.h

// #pragma once

// #include <c10/util/irange.h>
// #include <c10/util/overloaded.h>

// #include <torch/expanding_array.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/init.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/modules/utils.h>
// #include <torch/nn/options/conv.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/Export.h>

// #include <cstddef>
// #include <vector>
// Targeting ../Conv1dImplBase.java


// Targeting ../ConvTranspose1dImplBaseBase.java


// Targeting ../Conv2dImplBase.java


// Targeting ../ConvTranspose2dImplBaseBase.java


// Targeting ../Conv3dImplBase.java


// Targeting ../ConvTranspose3dImplBaseBase.java


// Targeting ../Conv1dImpl.java


// Targeting ../Conv1d.java


// Targeting ../Conv2dImpl.java


// Targeting ../Conv2d.java


// Targeting ../Conv3dImpl.java


// Targeting ../Conv3d.java


// Targeting ../ConvTranspose1dImplBase.java


// Targeting ../ConvTranspose2dImplBase.java


// Targeting ../ConvTranspose3dImplBase.java


// Targeting ../ConvTranspose1dImpl.java


// Targeting ../ConvTranspose1d.java


// Targeting ../ConvTranspose2dImpl.java


// Targeting ../ConvTranspose2d.java


// Targeting ../ConvTranspose3dImpl.java


// Targeting ../ConvTranspose3d.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/dropout.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/options/dropout.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/Export.h>

// #include <cstddef>
// #include <vector>
// Targeting ../DropoutImplBase.java


// Targeting ../Dropout2dImplBase.java


// Targeting ../Dropout3dImplBase.java


// Targeting ../AlphaDropoutImplBase.java


// Targeting ../FeatureAlphaDropoutImplBase.java




// Targeting ../DropoutImpl.java


// Targeting ../Dropout.java


// Targeting ../Dropout2dImpl.java


// Targeting ../Dropout2d.java


// Targeting ../Dropout3dImpl.java


// Targeting ../Dropout3d.java


// Targeting ../AlphaDropoutImpl.java


// Targeting ../AlphaDropout.java


// Targeting ../FeatureAlphaDropoutImpl.java


// Targeting ../FeatureAlphaDropout.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/distance.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/distance.h>
// #include <torch/nn/options/distance.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/Export.h>
// Targeting ../CosineSimilarityImpl.java


// Targeting ../CosineSimilarity.java


// Targeting ../PairwiseDistanceImpl.java


// Targeting ../PairwiseDistance.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/embedding.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/embedding.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/options/embedding.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <cstddef>
// Targeting ../EmbeddingImpl.java


// Targeting ../Embedding.java


// Targeting ../EmbeddingBagImpl.java


// Targeting ../EmbeddingBag.java


 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/fold.h

// #pragma once

// #include <torch/expanding_array.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/fold.h>
// #include <torch/nn/options/fold.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>
// Targeting ../FoldImpl.java


// Targeting ../Fold.java


// Targeting ../UnfoldImpl.java


// Targeting ../Unfold.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/linear.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/linear.h>
// #include <torch/nn/module.h>
// #include <torch/nn/options/linear.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <vector>
// Targeting ../IdentityImpl.java


// Targeting ../Identity.java


// Targeting ../LinearImpl.java


// Targeting ../Linear.java


// Targeting ../FlattenImpl.java


// Targeting ../Flatten.java


// Targeting ../UnflattenImpl.java


// Targeting ../Unflatten.java


// Targeting ../BilinearImpl.java


// Targeting ../Bilinear.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/loss.h

// #pragma once

// #include <torch/expanding_array.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/loss.h>
// #include <torch/nn/options/loss.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/Export.h>

// #include <cstddef>
// #include <vector>
// Targeting ../L1LossImpl.java


// Targeting ../L1Loss.java


// Targeting ../KLDivLossImpl.java


// Targeting ../KLDivLoss.java


// Targeting ../MSELossImpl.java


// Targeting ../MSELoss.java


// Targeting ../BCELossImpl.java


// Targeting ../BCELoss.java


// Targeting ../HingeEmbeddingLossImpl.java


// Targeting ../HingeEmbeddingLoss.java


// Targeting ../MultiMarginLossImpl.java


// Targeting ../MultiMarginLoss.java


// Targeting ../CosineEmbeddingLossImpl.java


// Targeting ../CosineEmbeddingLoss.java


// Targeting ../SmoothL1LossImpl.java


// Targeting ../SmoothL1Loss.java


// Targeting ../HuberLossImpl.java


// Targeting ../HuberLoss.java


// Targeting ../MultiLabelMarginLossImpl.java


// Targeting ../MultiLabelMarginLoss.java


// Targeting ../SoftMarginLossImpl.java


// Targeting ../SoftMarginLoss.java


// Targeting ../MultiLabelSoftMarginLossImpl.java


// Targeting ../MultiLabelSoftMarginLoss.java


// Targeting ../TripletMarginLossImpl.java


// Targeting ../TripletMarginLoss.java


// Targeting ../TripletMarginWithDistanceLossImpl.java


// Targeting ../TripletMarginWithDistanceLoss.java


// Targeting ../CTCLossImpl.java


// Targeting ../CTCLoss.java


// Targeting ../PoissonNLLLossImpl.java


// Targeting ../PoissonNLLLoss.java


// Targeting ../MarginRankingLossImpl.java


// Targeting ../MarginRankingLoss.java


// Targeting ../NLLLossImpl.java


// Targeting ../NLLLoss.java


// Targeting ../CrossEntropyLossImpl.java


// Targeting ../CrossEntropyLoss.java


// Targeting ../BCEWithLogitsLossImpl.java


// Targeting ../BCEWithLogitsLoss.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/padding.h

// #pragma once

// #include <torch/expanding_array.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/padding.h>

// #include <torch/csrc/Export.h>
// Targeting ../ReflectionPad1dImplBase.java


// Targeting ../ReflectionPad2dImplBase.java


// Targeting ../ReflectionPad3dImplBase.java


// Targeting ../ReflectionPad1dImpl.java


// Targeting ../ReflectionPad1d.java


// Targeting ../ReflectionPad2dImpl.java


// Targeting ../ReflectionPad2d.java


// Targeting ../ReflectionPad3dImpl.java


// Targeting ../ReflectionPad3d.java


// Targeting ../ReplicationPad1dImplBase.java


// Targeting ../ReplicationPad2dImplBase.java


// Targeting ../ReplicationPad3dImplBase.java


// Targeting ../ReplicationPad1dImpl.java


// Targeting ../ReplicationPad1d.java


// Targeting ../ReplicationPad2dImpl.java


// Targeting ../ReplicationPad2d.java


// Targeting ../ReplicationPad3dImpl.java


// Targeting ../ReplicationPad3d.java


// Targeting ../ZeroPad2dImpl.java


// Targeting ../ZeroPad2d.java


// Targeting ../ConstantPad1dImplBase.java


// Targeting ../ConstantPad2dImplBase.java


// Targeting ../ConstantPad3dImplBase.java


// Targeting ../ConstantPad1dImpl.java


// Targeting ../ConstantPad1d.java


// Targeting ../ConstantPad2dImpl.java


// Targeting ../ConstantPad2d.java


// Targeting ../ConstantPad3dImpl.java


// Targeting ../ConstantPad3d.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/pooling.h

// #pragma once

// #include <torch/expanding_array.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/pooling.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/options/pooling.h>

// #include <torch/csrc/Export.h>
// Targeting ../AvgPool1dImplBase.java


// Targeting ../AvgPool2dImplBase.java


// Targeting ../AvgPool3dImplBase.java


// Targeting ../AvgPool1dImpl.java


// Targeting ../AvgPool1d.java


// Targeting ../AvgPool2dImpl.java


// Targeting ../AvgPool2d.java


// Targeting ../AvgPool3dImpl.java


// Targeting ../AvgPool3d.java


// Targeting ../MaxPool1dImplBase.java


// Targeting ../MaxPool2dImplBase.java


// Targeting ../MaxPool3dImplBase.java


// Targeting ../MaxPool1dImpl.java


// Targeting ../MaxPool1d.java


// Targeting ../MaxPool2dImpl.java


// Targeting ../MaxPool2d.java


// Targeting ../MaxPool3dImpl.java


// Targeting ../MaxPool3d.java


// Targeting ../AdaptiveMaxPool1dImplBase.java


// Targeting ../AdaptiveMaxPool2dImplBase.java


// Targeting ../AdaptiveMaxPool3dImplBase.java


// Targeting ../AdaptiveMaxPool1dImpl.java


// Targeting ../AdaptiveMaxPool1d.java


// Targeting ../AdaptiveMaxPool2dImpl.java


// Targeting ../AdaptiveMaxPool2d.java


// Targeting ../AdaptiveMaxPool3dImpl.java


// Targeting ../AdaptiveMaxPool3d.java


// Targeting ../AdaptiveAvgPool1dImplBase.java


// Targeting ../AdaptiveAvgPool2dImplBase.java


// Targeting ../AdaptiveAvgPool3dImplBase.java


// Targeting ../AdaptiveAvgPool1dImpl.java


// Targeting ../AdaptiveAvgPool1d.java


// Targeting ../AdaptiveAvgPool2dImpl.java


// Targeting ../AdaptiveAvgPool2d.java


// Targeting ../AdaptiveAvgPool3dImpl.java


// Targeting ../AdaptiveAvgPool3d.java


// Targeting ../MaxUnpool1dImplBase.java


// Targeting ../MaxUnpool2dImplBase.java


// Targeting ../MaxUnpool3dImplBase.java


// Targeting ../MaxUnpool1dImpl.java


// Targeting ../MaxUnpool1d.java


// Targeting ../MaxUnpool2dImpl.java


// Targeting ../MaxUnpool2d.java


// Targeting ../MaxUnpool3dImpl.java


// Targeting ../MaxUnpool3d.java


// Targeting ../FractionalMaxPool2dImpl.java


// Targeting ../FractionalMaxPool2d.java


// Targeting ../FractionalMaxPool3dImpl.java


// Targeting ../FractionalMaxPool3d.java


// Targeting ../LPPool1dImplBase.java


// Targeting ../LPPool2dImplBase.java


// Targeting ../LPPool1dImpl.java


// Targeting ../LPPool1d.java


// Targeting ../LPPool2dImpl.java


// Targeting ../LPPool2d.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/rnn.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/modules/dropout.h>
// #include <torch/nn/options/rnn.h>
// #include <torch/nn/pimpl.h>
// #include <torch/nn/utils/rnn.h>
// #include <torch/types.h>

// #include <ATen/ATen.h>
// #include <c10/util/Exception.h>

// #include <cstddef>
// #include <functional>
// #include <memory>
// #include <vector>
// Targeting ../RNNImplBase.java


// Targeting ../LSTMImplBase.java


// Targeting ../GRUImplBase.java



// Targeting ../RNNImpl.java


// Targeting ../RNN.java


// Targeting ../LSTMImpl.java


// Targeting ../LSTM.java


// Targeting ../GRUImpl.java


// Targeting ../GRU.java



// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ RNNCellImplBase
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Targeting ../RNNCellImplBase.java


// Targeting ../LSTMCellImplBase.java


// Targeting ../GRUCellImplBase.java



// Targeting ../RNNCellImpl.java


// Targeting ../RNNCell.java


// Targeting ../LSTMCellImpl.java


// Targeting ../LSTMCell.java


// Targeting ../GRUCellImpl.java


// Targeting ../GRUCell.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/pixelshuffle.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/pixelshuffle.h>
// #include <torch/nn/options/pixelshuffle.h>

// #include <torch/csrc/Export.h>
// Targeting ../PixelShuffleImpl.java


// Targeting ../PixelShuffle.java


// Targeting ../PixelUnshuffleImpl.java


// Targeting ../PixelUnshuffle.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/upsampling.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/upsampling.h>
// #include <torch/nn/options/upsampling.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/Export.h>

// #include <cstddef>
// #include <ostream>
// Targeting ../UpsampleImpl.java


// Targeting ../Upsample.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/activation.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/activation.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/modules/linear.h>
// #include <torch/nn/options/activation.h>

// #include <torch/csrc/Export.h>
// Targeting ../ELUImpl.java


// Targeting ../ELU.java


// Targeting ../SELUImpl.java


// Targeting ../SELU.java


// Targeting ../HardshrinkImpl.java


// Targeting ../Hardshrink.java


// Targeting ../HardtanhImpl.java


// Targeting ../Hardtanh.java


// Targeting ../LeakyReLUImpl.java


// Targeting ../LeakyReLU.java


// Targeting ../LogSigmoidImpl.java


// Targeting ../LogSigmoid.java


// Targeting ../SoftmaxImpl.java


// Targeting ../Softmax.java


// Targeting ../SoftminImpl.java


// Targeting ../Softmin.java


// Targeting ../LogSoftmaxImpl.java


// Targeting ../LogSoftmax.java


// Targeting ../Softmax2dImpl.java


// Targeting ../Softmax2d.java


// Targeting ../PReLUImpl.java


// Targeting ../PReLU.java


// Targeting ../ReLUImpl.java


// Targeting ../ReLU.java


// Targeting ../ReLU6Impl.java


// Targeting ../ReLU6.java


// Targeting ../RReLUImpl.java


// Targeting ../RReLU.java


// Targeting ../CELUImpl.java


// Targeting ../CELU.java


// Targeting ../GLUImpl.java


// Targeting ../GLU.java


// Targeting ../GELUImpl.java


// Targeting ../GELU.java


// Targeting ../SiLUImpl.java


// Targeting ../SiLU.java


// Targeting ../MishImpl.java


// Targeting ../Mish.java


// Targeting ../SigmoidImpl.java


// Targeting ../Sigmoid.java


// Targeting ../SoftplusImpl.java


// Targeting ../Softplus.java


// Targeting ../SoftshrinkImpl.java


// Targeting ../Softshrink.java


// Targeting ../SoftsignImpl.java


// Targeting ../Softsign.java


// Targeting ../TanhImpl.java


// Targeting ../Tanh.java


// Targeting ../TanhshrinkImpl.java


// Targeting ../Tanhshrink.java


// Targeting ../ThresholdImpl.java


// Targeting ../Threshold.java


// Targeting ../MultiheadAttentionImpl.java


// Targeting ../MultiheadAttention.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/normalization.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/normalization.h>
// #include <torch/nn/modules/_functions.h>
// #include <torch/nn/options/normalization.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <vector>
// Targeting ../LayerNormImpl.java


// Targeting ../LayerNorm.java


// Targeting ../LocalResponseNormImpl.java


// Targeting ../LocalResponseNorm.java


// Targeting ../CrossMapLRN2dImpl.java


// Targeting ../CrossMapLRN2d.java


// Targeting ../GroupNormImpl.java


// Targeting ../GroupNorm.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/transformerlayer.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/activation.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/modules/dropout.h>
// #include <torch/nn/modules/linear.h>
// #include <torch/nn/modules/normalization.h>
// #include <torch/nn/options/transformerlayer.h>
// #include <torch/nn/pimpl.h>

// #include <torch/types.h>

// #include <ostream>
// Targeting ../TransformerEncoderLayerImpl.java


// Targeting ../TransformerEncoderLayer.java


// Targeting ../TransformerDecoderLayerImpl.java


// Targeting ../TransformerDecoderLayer.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/transformercoder.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/modules/container/any.h>
// #include <torch/nn/modules/container/modulelist.h>
// #include <torch/nn/options/transformercoder.h>
// #include <torch/nn/pimpl.h>

// #include <torch/types.h>

// #include <ostream>
// Targeting ../TransformerEncoderImpl.java


// Targeting ../TransformerEncoder.java


// Targeting ../TransformerDecoderImpl.java


// Targeting ../TransformerDecoder.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/transformer.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/options/transformer.h>
// #include <torch/nn/pimpl.h>

// #include <torch/types.h>

// #include <ostream>
// Targeting ../TransformerImpl.java


// Targeting ../Transformer.java



 // namespace nn
 // namespace torch


// Parsed from torch/optim.h

// #pragma once

// #include <torch/optim/adagrad.h>
// #include <torch/optim/adam.h>
// #include <torch/optim/adamw.h>
// #include <torch/optim/lbfgs.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/rmsprop.h>
// #include <torch/optim/sgd.h>

// #include <torch/optim/schedulers/lr_scheduler.h>
// #include <torch/optim/schedulers/step_lr.h>


// Parsed from torch/optim/optimizer.h

// #pragma once

// #include <ATen/Tensor.h>
// #include <c10/util/Exception.h>
// #include <c10/util/flat_hash_map.h>

// #include <torch/arg.h>
// #include <torch/csrc/Export.h>

// #include <algorithm>
// #include <functional>
// #include <iterator>
// #include <memory>
// #include <string>
// #include <vector>

// Forward declarations confuse Doxygen
// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace at
 // namespace serialize

// Targeting ../OptimizerParamState.java


// Targeting ../OptimizerCloneableAdagradParamState.java


// Targeting ../OptimizerCloneableAdamParamState.java


// Targeting ../OptimizerCloneableAdamWParamState.java


// Targeting ../OptimizerCloneableLBFGSParamState.java


// Targeting ../OptimizerCloneableRMSpropParamState.java


// Targeting ../OptimizerCloneableSGDParamState.java


// Targeting ../OptimizerOptions.java


// Targeting ../OptimizerCloneableAdagradOptions.java


// Targeting ../OptimizerCloneableAdamOptions.java


// Targeting ../OptimizerCloneableAdamWOptions.java


// Targeting ../OptimizerCloneableLBFGSOptions.java


// Targeting ../OptimizerCloneableRMSpropOptions.java


// Targeting ../OptimizerCloneableSGDOptions.java


// Targeting ../OptimizerParamGroup.java


// Targeting ../Optimizer.java



/* How do we decide whether to serialize undefined tensors or
  c10::nullopt values into the output archive?
Answer: we strictly follow the behavior of Python API. To be more specific:

For optimizer options:
a) For undefined tensor: currently no tensor is used as an options argument in
Python API, so we don't need to worry about it now. b) For c10::nullopt value:
we serialize c10::nullopt values into the output archive, to follow the exact
same behavior as Python API.

For optimizer param state:
a) For undefined tensor: in param state, undefined tensor in C++ impl is
equivalent to missing key in Python impl. Since we don't serialize missing keys
in Python API, we skip undefined tensors when serializing the param state. b)
For c10::nullopt value: in param state, c10::nullopt value in C++ impl is
equivalent to missing key in Python impl. Since we don't serialize missing keys
in Python API, we skip c10::nullopt values when serializing the param state. */

/** Serializes an {@code Optimizer} into an {@code OutputArchive}. */
@Namespace("torch::optim") public static native @ByRef @Name("operator <<") OutputArchive shiftLeft(
    @ByRef OutputArchive archive,
    @Const @ByRef Optimizer optimizer);

/** Deserializes a {@code Tensor} from an {@code InputArchive}. */
@Namespace("torch::optim") public static native @ByRef @Name("operator >>") InputArchive shiftRight(
    @ByRef InputArchive archive,
    @ByRef Optimizer optimizer);

 // namespace optim
 // namespace torch


// Parsed from torch/optim/serialize.h

// #pragma once

// #include <c10/util/irange.h>
// #include <torch/optim/optimizer.h>
// #include <torch/serialize/archive.h>
// #include <torch/types.h>
// #include <cstddef>
// #include <cstdint>
// #include <deque>
// #include <string>
// #include <vector>
// Utility function to save state


// Utility function to load state


// Utility function to save param_groups


// Utility function to load param_groups
// We take as input vector of pair of string and unique_ptr to optimizer options
// so that we can retain the state for each param by using the old tensor impl
// keys (saved during serialization) and map the new tensor impl keys to the
// correct state for each param

 // namespace detail

// Note: These functions are all called `serialize()` so they can be called
// inside a template where the archive type is a template type and can thus be
// passed such that the appropriate overload is selected.

/** Utility function to save a value of {@code int64_t} type. */


/** Utility function to load a value of {@code int64_t} type. */


/** Utility function to save a vector of step buffers. */


/** Utility function to load a vector of step buffers. */


// Utility function to save state and param_groups


// Utility function to load state and param_groups and update state


/** Utility function to save a vector of buffers. */


/** Utility function to load a vector of buffers. */


// #define _TORCH_OPTIM_SERIALIZE(name)
//   torch::optim::serialize(archive, #name, self.name)

// #define _TORCH_OPTIM_SERIALIZE_WITH_TEMPLATE_ARG(OptimizerName)
//   torch::optim::serialize<OptimizerName##ParamState, OptimizerName##Options>(
//       archive, self)

// #define _TORCH_OPTIM_SERIALIZE_TORCH_ARG(name)
//   {
//     auto ivalue = torch::IValue(name());
//     /* do not serialize if name is an undefined tensor*/
//     if (!(ivalue.isTensor() &&
//           ivalue.nsafeToTensorImpl() ==
//               at::UndefinedTensorImpl::singleton())) {
//       archive.write(#name, ivalue);
//     }
//   }

// #define _TORCH_OPTIM_SERIALIZE_TORCH_ARG_DEQUE(name)
//   {
//     c10::IValue ivalue = torch::IValue(deque_to_list(name()));
//     archive.write(#name, ivalue);
//   }

// #define _TORCH_OPTIM_DESERIALIZE_TORCH_ARG(T, name)
//   {
//     c10::IValue ivalue;
//     bool exists = archive.try_read(#name, ivalue);
//     if (exists) {
//       name(ivalue.to<T>());
//     } else {
//       bool is_tensor_type = std::is_base_of<torch::Tensor, T>::value;
//       TORCH_INTERNAL_ASSERT(is_tensor_type);
//     }
//   }

// #define _TORCH_OPTIM_DESERIALIZE_TORCH_ARG_OPTIONAL(T, name)
//   {
//     c10::IValue ivalue;
//     bool exists = archive.try_read(#name, ivalue);
//     if (exists) {
//       name(ivalue.toOptional<T>());
//     }
//   }

// #define _TORCH_OPTIM_DESERIALIZE_TORCH_ARG_DEQUE(T, name)
//   {
//     c10::IValue ivalue;
//     archive.read(#name, ivalue);
//     auto list = ivalue.to<c10::List<T::value_type>>();
//     name(list_to_deque(list));
//   }

 // namespace optim
 // namespace torch


// Parsed from torch/optim/adagrad.h

// #pragma once

// #include <torch/nn/pimpl.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>
// #include <torch/serialize/archive.h>
// #include <torch/types.h>

// #include <utility>
// #include <vector>
 // namespace serialize

// Targeting ../AdagradOptions.java


// Targeting ../AdagradParamState.java


// Targeting ../Adagrad.java


 // namespace optim
 // namespace torch


// Parsed from torch/optim/adam.h

// #pragma once

// #include <torch/nn/module.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>

// #include <utility>
// #include <vector>
 // namespace serialize

// Targeting ../AdamOptions.java


// Targeting ../AdamParamState.java


// Targeting ../Adam.java


 // namespace optim
 // namespace torch


// Parsed from torch/optim/adamw.h

// #pragma once

// #include <torch/nn/module.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>

// #include <utility>
// #include <vector>
 // namespace serialize

// Targeting ../AdamWOptions.java


// Targeting ../AdamWParamState.java


// Targeting ../AdamW.java


 // namespace optim
 // namespace torch


// Parsed from torch/optim/lbfgs.h

// #pragma once

// #include <torch/nn/module.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>
// #include <torch/serialize/archive.h>

// #include <deque>
// #include <functional>
// #include <memory>
// #include <vector>
// Targeting ../LBFGSOptions.java


// Targeting ../LBFGSParamState.java


// Targeting ../LBFGS.java


 // namespace optim
 // namespace torch


// Parsed from torch/optim/rmsprop.h

// #pragma once

// #include <torch/nn/module.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>
// #include <torch/serialize/archive.h>
// #include <torch/types.h>

// #include <functional>
// #include <memory>
// #include <string>
// #include <vector>
 // namespace serialize

// Targeting ../RMSpropOptions.java


// Targeting ../RMSpropParamState.java


// Targeting ../RMSprop.java


 // namespace optim
 // namespace torch


// Parsed from torch/optim/sgd.h

// #pragma once

// #include <torch/nn/module.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>
// #include <torch/serialize/archive.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <utility>
// #include <vector>
 // namespace serialize

// Targeting ../SGDOptions.java


// Targeting ../SGDParamState.java


// Targeting ../SGD.java


 // namespace optim
 // namespace torch


// Parsed from torch/optim/schedulers/lr_scheduler.h

// #pragma once

// #include <torch/optim/optimizer.h>

// #include <torch/csrc/Export.h>
// Targeting ../LRScheduler.java


 // namespace optim
 // namespace torch


// Parsed from torch/optim/schedulers/step_lr.h

// #pragma once

// #include <torch/optim/schedulers/lr_scheduler.h>
// Targeting ../StepLR.java


 // namespace optim
 // namespace torch


}

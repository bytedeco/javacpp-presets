// Targeted by JavaCPP version 1.5.6-SNAPSHOT: DO NOT EDIT THIS FILE

package org.bytedeco.pytorch.global;

import org.bytedeco.pytorch.*;

import org.bytedeco.pytorch.Allocator;
import org.bytedeco.pytorch.Function;
import org.bytedeco.pytorch.Module;
import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.bytedeco.javacpp.presets.javacpp.*;
import static org.bytedeco.openblas.global.openblas_nolapack.*;
import static org.bytedeco.openblas.global.openblas.*;

public class torch extends org.bytedeco.pytorch.presets.torch {
    static { Loader.load(); }

// Targeting ../BoolOptional.java


// Targeting ../LongOptional.java


// Targeting ../DoubleOptional.java


// Targeting ../SizeTOptional.java


// Targeting ../StringOptional.java


// Targeting ../LongVectorOptional.java


// Targeting ../DoubleVectorOptional.java


// Targeting ../SizeTVectorOptional.java


// Targeting ../TensorVectorOptional.java


// Targeting ../DeviceOptional.java


// Targeting ../LongArrayRefOptional.java


// Targeting ../DoubleArrayRefOptional.java


// Targeting ../LayoutOptional.java


// Targeting ../MemoryFormatOptional.java


// Targeting ../ScalarOptional.java


// Targeting ../ScalarTypeOptional.java


// Targeting ../DimnameOptional.java


// Targeting ../DimnameListOptional.java


// Targeting ../GeneratorOptional.java


// Targeting ../TensorOptional.java


// Targeting ../TensorListOptional.java


// Targeting ../LongExpandingArrayOptional.java


// Targeting ../DoubleExpandingArrayOptional.java


// Targeting ../IValueIValueDict.java


// Targeting ../TensorTensorOptional.java


// Targeting ../StringTensorDict.java


// Targeting ../StringModuleDict.java


// Targeting ../StringAnyModuleDict.java


// Targeting ../StringSharedModuleDict.java


// Targeting ../TensorDeque.java


// Targeting ../DimnameVector.java


// Targeting ../Bool2Vector.java


// Targeting ../LongVector.java


// Targeting ../DoubleVector.java


// Targeting ../SizeTVector.java


// Targeting ../StringVector.java


// Targeting ../StringLongVector.java


// Targeting ../QEngineVector.java


// Targeting ../ScalarTypeVector.java


// Targeting ../TensorImplVector.java


// Targeting ../TensorVector.java


// Targeting ../TensorIndexVector.java


// Targeting ../FunctionPreVector.java


// Targeting ../ModuleVector.java


// Targeting ../AnyModuleVector.java


// Targeting ../SharedModuleVector.java


// Targeting ../SharedAnyModuleVector.java


// Targeting ../StringSharedModulePairVector.java


// Targeting ../ExampleVector.java


// Targeting ../StringTensorPair.java


// Targeting ../IValueIValueMap.java


// Targeting ../StringFunctionMap.java


// Targeting ../IValueSet.java


// Parsed from c10/macros/cmake_macros.h

// #ifndef C10_MACROS_CMAKE_MACROS_H_
// #define C10_MACROS_CMAKE_MACROS_H_

// Automatically generated header file for the C10 library.
// Do not include this file directly. Instead, include c10/macros/Macros.h.

// #define C10_BUILD_SHARED_LIBS
/* #undef C10_USE_GLOG */
/* #undef C10_USE_GFLAGS */
// #define C10_USE_NUMA
/* #undef C10_USE_MSVC_STATIC_RUNTIME */

// Used by libtorch mobile build to enable features that are not enabled by
// caffe2 mobile build. Should only use it when necessary as we are committed
// to converging libtorch and caffe2 mobile builds and removing it eventually.
/* #undef FEATURE_TORCH_MOBILE */

// #endif // C10_MACROS_CMAKE_MACROS_H_


// Parsed from c10/macros/Export.h

// #ifndef C10_MACROS_EXPORT_H_
// #define C10_MACROS_EXPORT_H_

/* Header file to define the common scaffolding for exported symbols.
 *
 * Export is by itself a quite tricky situation to deal with, and if you are
 * hitting this file, make sure you start with the background here:
 * - Linux: https://gcc.gnu.org/wiki/Visibility
 * - Windows:
 * https://docs.microsoft.com/en-us/cpp/cpp/dllexport-dllimport?view=vs-2017
 *
 * Do NOT include this file directly. Instead, use c10/macros/Macros.h
 */

// You do not need to edit this part of file unless you are changing the core
// pytorch export abstractions.
//
// This part defines the C10 core export and import macros. This is controlled
// by whether we are building shared libraries or not, which is determined
// during build time and codified in c10/core/cmake_macros.h.
// When the library is built as a shared lib, EXPORT and IMPORT will contain
// visibility attributes. If it is being built as a static lib, then EXPORT
// and IMPORT basically have no effect.

// As a rule of thumb, you should almost NEVER mix static and shared builds for
// libraries that depend on c10. AKA, if c10 is built as a static library, we
// recommend everything dependent on c10 to be built statically. If c10 is built
// as a shared library, everything dependent on it should be built as shared. In
// the PyTorch project, all native libraries shall use the macro
// C10_BUILD_SHARED_LIB to check whether pytorch is building shared or static
// libraries.

// For build systems that do not directly depend on CMake and directly build
// from the source directory (such as Buck), one may not have a cmake_macros.h
// file at all. In this case, the build system is responsible for providing
// correct macro definitions corresponding to the cmake_macros.h.in file.
//
// In such scenarios, one should define the macro
//     C10_USING_CUSTOM_GENERATED_MACROS
// to inform this header that it does not need to include the cmake_macros.h
// file.

// #ifndef C10_USING_CUSTOM_GENERATED_MACROS
// #include <c10/macros/cmake_macros.h>
// #endif // C10_USING_CUSTOM_GENERATED_MACROS

// #ifdef _WIN32
// #else // _WIN32
// #if defined(__GNUC__)
// #define C10_EXPORT __attribute__((__visibility__("default")))
// #define C10_HIDDEN __attribute__((__visibility__("hidden")))
// #else // defined(__GNUC__)
// #define C10_EXPORT
// #define C10_HIDDEN
// #endif // defined(__GNUC__)
// #define C10_IMPORT C10_EXPORT
// #endif // _WIN32

// #ifdef NO_EXPORT
// #undef C10_EXPORT
// #define C10_EXPORT
// #endif

// Definition of an adaptive XX_API macro, that depends on whether you are
// building the library itself or not, routes to XX_EXPORT and XX_IMPORT.
// Basically, you will need to do this for each shared library that you are
// building, and the instruction is as follows: assuming that you are building
// a library called libawesome.so. You should:
// (1) for your cmake target (usually done by "add_library(awesome, ...)"),
//     define a macro called AWESOME_BUILD_MAIN_LIB using
//     target_compile_options.
// (2) define the AWESOME_API macro similar to the one below.
// And in the source file of your awesome library, use AWESOME_API to
// annotate public symbols.

// Here, for the C10 library, we will define the macro C10_API for both import
// and export.

// This one is being used by libc10.so
// #ifdef C10_BUILD_MAIN_LIB
// #define C10_API C10_EXPORT
// #else
// #define C10_API C10_IMPORT
// #endif

// This one is being used by libtorch.so
// #ifdef CAFFE2_BUILD_MAIN_LIB
// #define TORCH_API C10_EXPORT
// #else
// #define TORCH_API C10_IMPORT
// #endif

// NB: For now, HIP is overloaded to use the same macro, but ideally
// HIPify should translate TORCH_CUDA_API to TORCH_HIP_API
// JX: I removed the || defined(TORCH_HIP_BUILD_MAIN_LIB) check for TORCH_CUDA_*_API
// since TORCH_HIP_API seems properly initialized below
// libtorch_cuda_cu.so
// #ifdef TORCH_CUDA_CU_BUILD_MAIN_LIB
// #define TORCH_CUDA_CU_API C10_EXPORT
// #elif defined(BUILD_SPLIT_CUDA)
// #define TORCH_CUDA_CU_API C10_IMPORT
// #endif

// libtorch_cuda_cpp.so
// #ifdef TORCH_CUDA_CPP_BUILD_MAIN_LIB
// #define TORCH_CUDA_CPP_API C10_EXPORT
// #elif defined(BUILD_SPLIT_CUDA)
// #define TORCH_CUDA_CPP_API C10_IMPORT
// #endif

// libtorch_cuda.so (where torch_cuda_cu and torch_cuda_cpp are a part of the same api)
// #ifdef TORCH_CUDA_BUILD_MAIN_LIB
// #define TORCH_CUDA_CPP_API C10_EXPORT
// #define TORCH_CUDA_CU_API C10_EXPORT
// #elif !defined(BUILD_SPLIT_CUDA)
// #define TORCH_CUDA_CPP_API C10_IMPORT
// #define TORCH_CUDA_CU_API C10_IMPORT
// #endif

// #if defined(TORCH_HIP_BUILD_MAIN_LIB)
// #define TORCH_HIP_API C10_EXPORT
// #else
// #define TORCH_HIP_API C10_IMPORT
// #endif

// Enums only need to be exported on windows for non-CUDA files
// #if defined(_WIN32) && defined(__CUDACC__)
// #define C10_API_ENUM C10_API
// #else
// #define C10_API_ENUM
// #endif

// #endif // C10_MACROS_MACROS_H_


// Parsed from c10/macros/Macros.h

// #ifndef C10_MACROS_MACROS_H_
// #define C10_MACROS_MACROS_H_

/* Main entry for c10/macros.
 *
 * In your code, include c10/macros/Macros.h directly, instead of individual
 * files in this folder.
 */

// For build systems that do not directly depend on CMake and directly build
// from the source directory (such as Buck), one may not have a cmake_macros.h
// file at all. In this case, the build system is responsible for providing
// correct macro definitions corresponding to the cmake_macros.h.in file.
//
// In such scenarios, one should define the macro
//     C10_USING_CUSTOM_GENERATED_MACROS
// to inform this header that it does not need to include the cmake_macros.h
// file.

// #ifndef C10_USING_CUSTOM_GENERATED_MACROS
// #include <c10/macros/cmake_macros.h>
// #endif // C10_USING_CUSTOM_GENERATED_MACROS

// #include <c10/macros/Export.h>

// #if defined(__clang__)
//   #define __ubsan_ignore_float_divide_by_zero__ __attribute__((no_sanitize("float-divide-by-zero")))
//   #define __ubsan_ignore_undefined__ __attribute__((no_sanitize("undefined")))
//   #define __ubsan_ignore_signed_int_overflow__ __attribute__((no_sanitize("signed-integer-overflow")))
// #else
//   #define __ubsan_ignore_float_divide_by_zero__
//   #define __ubsan_ignore_undefined__
//   #define __ubsan_ignore_signed_int_overflow__
// #endif


// Detect address sanitizer as some stuff doesn't work with it
// #undef C10_ASAN_ENABLED

// for clang
// #if defined(__has_feature)
// #if ((__has_feature(address_sanitizer)))
public static final int C10_ASAN_ENABLED = 1;
// #endif
// #endif

// for gcc
// #if defined(__SANITIZE_ADDRESS__)
// #if __SANITIZE_ADDRESS__
// #if !defined(C10_ASAN_ENABLED)
// #endif
// #endif
// #endif

// #if !defined(C10_ASAN_ENABLED)
// #endif


// Disable the copy and assignment operator for a class. Note that this will
// disable the usage of the class in std containers.
// #define C10_DISABLE_COPY_AND_ASSIGN(classname)
//   classname(const classname&) = delete;
//   classname& operator=(const classname&) = delete

// #define C10_CONCATENATE_IMPL(s1, s2) s1##s2
// #define C10_CONCATENATE(s1, s2) C10_CONCATENATE_IMPL(s1, s2)

// #define C10_MACRO_EXPAND(args) args

// #define C10_STRINGIZE_IMPL(x) #x
// #define C10_STRINGIZE(x) C10_STRINGIZE_IMPL(x)

/**
 * C10_ANONYMOUS_VARIABLE(str) introduces an identifier starting with
 * str and ending with a number that varies with the line.
 */
// #ifdef __COUNTER__
// #else
// #define C10_UID __LINE__
// #define C10_ANONYMOUS_VARIABLE(str) C10_CONCATENATE(str, __LINE__)
// #endif


/** C10_NODISCARD - Warn if a type or return value is discarded. */

// Technically, we should check if __cplusplus > 201402L here, because
// [[nodiscard]] is only defined in C++17.  However, some compilers
// we care about don't advertise being C++17 (e.g., clang), but
// support the attribute anyway.  In fact, this is not just a good idea,
// it's the law: clang::warn_unused_result doesn't work on nvcc + clang
// and the best workaround for this case is to use [[nodiscard]]
// instead; see https://github.com/pytorch/pytorch/issues/13118
//
// Note to future editors: if you have noticed that a compiler is
// misbehaving (e.g., it advertises support, but the support doesn't
// actually work, or it is emitting warnings).  Some compilers which
// are strict about the matter include MSVC, which will complain:
//
//  error C2429: attribute 'nodiscard' requires compiler flag '/std:c++latest'
//
// Exhibits:
//  - MSVC 19.14: https://godbolt.org/z/Dzd7gn (requires /std:c++latest)
//  - Clang 8.0.0: https://godbolt.org/z/3PYL4Z (always advertises support)
//  - gcc 8.3: https://godbolt.org/z/4tLMQS (always advertises support)
// #define C10_NODISCARD
// #if defined(__has_cpp_attribute)
// # if __has_cpp_attribute(nodiscard)
// #  undef C10_NODISCARD
// #  define C10_NODISCARD [[nodiscard]]
// # endif
// Workaround for llvm.org/PR23435, since clang 3.6 and below emit a spurious
// error when __has_cpp_attribute is given a scoped attribute in C mode.
// #elif __cplusplus && defined(__has_cpp_attribute)
// # if __has_cpp_attribute(clang::warn_unused_result)
// TODO: It's possible this is still triggering https://github.com/pytorch/pytorch/issues/13118
// on Windows; if it is, better fix it.
// #  undef C10_NODISCARD
// #  define C10_NODISCARD [[clang::warn_unused_result]]
// # endif
// #endif

// suppress an unused variable.
// #if defined(_MSC_VER) && !defined(__clang__)
// #define C10_UNUSED __pragma(warning(suppress: 4100 4101))
// #else
// #define C10_UNUSED __attribute__((__unused__))
// #endif //_MSC_VER

// #define C10_RESTRICT __restrict

// Simply define the namespace, in case a dependent library want to refer to
// the c10 namespace but not any nontrivial files.
 // namespace c10
 
 

// Since C10 is the core library for caffe2 (and aten), we will simply reroute
// all abstractions defined in c10 to be available in caffe2 as well.
// This is only for backwards compatibility. Please use the symbols from the
// c10 namespace where possible.   

// WARNING!!! THIS IS A GIANT HACK!!!
// This line means you cannot simultaneously include c10/hip
// and c10/cuda and then use them from the at::cuda namespace.
// This is true in practice, because HIPIFY works inplace on
// files in ATen/cuda, so it assumes that c10::hip is available
// from at::cuda.  This namespace makes that happen.  When
// HIPIFY is no longer out-of-place, we can switch the cuda
// here to hip and everyone is happy. 

// C10_LIKELY/C10_UNLIKELY
//
// These macros provide parentheses, so you can use these macros as:
//
//    if C10_LIKELY(some_expr) {
//      ...
//    }
//
// NB: static_cast to boolean is mandatory in C++, because __builtin_expect
// takes a long argument, which means you may trigger the wrong conversion
// without it.
//
// #if defined(__GNUC__) || defined(__ICL) || defined(__clang__)
// #define C10_LIKELY(expr)    (__builtin_expect(static_cast<bool>(expr), 1))
// #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))
// #else
// #define C10_LIKELY(expr)    (expr)
// #define C10_UNLIKELY(expr)  (expr)
// #endif

/** C10_NOINLINE - Functions whose declaration is annotated with this will not
 *  be inlined. */
// #ifdef __GNUC__
// #define C10_NOINLINE __attribute__((__noinline__))
// #elif _MSC_VER
// #define C10_NOINLINE __declspec(noinline)
// #else
// #define C10_NOINLINE
// #endif

// #if __has_attribute(always_inline) || defined(__GNUC__)
// #define C10_ALWAYS_INLINE __attribute__((__always_inline__)) inline
// #elif defined(_MSC_VER)
// #else
// #define C10_ALWAYS_INLINE inline
// #endif

// #include <sstream>
// #include <string>

// #if defined(__CUDACC__) || defined(__HIPCC__)
// #else
// #define C10_HOST_DEVICE
// #define C10_HOST
// #define C10_DEVICE
// #endif

// #ifdef __HIP_PLATFORM_HCC__
// #define C10_HIP_HOST_DEVICE __host__ __device__
// #else
// #define C10_HIP_HOST_DEVICE
// #endif

// #ifdef __HIP_PLATFORM_HCC__
public static final int C10_WARP_SIZE = 64;
// #else
// #endif

// #if defined(_MSC_VER) && _MSC_VER <= 1900
// #endif

// CUDA_KERNEL_ASSERT checks the assertion
// even when NDEBUG is defined. This is useful for important assertions in CUDA
// code that would otherwise be suppressed when building Release.
// #if defined(__ANDROID__) || defined(__APPLE__) || defined(__HIP_PLATFORM_HCC__)
// Those platforms do not support assert()
// #define CUDA_KERNEL_ASSERT(cond)
// #elif defined(_MSC_VER)
// #else // __APPLE__, _MSC_VER
// #if defined(NDEBUG)
// #endif // NDEBUG
// #define CUDA_KERNEL_ASSERT(cond)
//   if (C10_UNLIKELY(!(cond))) {
//     __assert_fail(#cond, __FILE__, static_cast<unsigned int>(__LINE__),
//                   __func__);
//   }
// #endif // __APPLE__

// #ifdef __APPLE__
// #include <TargetConditionals.h>
// #endif

// #if defined(__ANDROID__)
// #elif (
//     defined(__APPLE__) &&
//     (TARGET_IPHONE_SIMULATOR || TARGET_OS_SIMULATOR || TARGET_OS_IPHONE))
public static final int C10_IOS = 1;
public static final int C10_MOBILE = 1;
// #endif // ANDROID / IOS

// Portable determination of whether type T is trivially copyable.
// Warning: __has_trivial_copy for GCC may not always detect the non-POD
// correctly. For example, T = std::unique_ptr may evaluate to true and be
// treated as POD. This can cause unexpected behavior.
// #if defined(__GNUG__) && __GNUC__ < 5
// #define C10_IS_TRIVIALLY_COPYABLE(T) __has_trivial_copy(T)
// #else
// #define C10_IS_TRIVIALLY_COPYABLE(T) std::is_trivially_copyable<T>::value
// #endif

// We need --expt-relaxed-constexpr in CUDA because of Eigen. This flag allows
// device code in CUDA to call host constexpr functions. Unfortunately,
// the CUDA compiler (at least for CUDA 9.0, 9.1 and 9.2) isn't compatible
// with many of the constexpr things we'd like to do and the device code
// compiler crashes when it sees one of these host-only functions.
// It works when nvcc builds host code, but not when it builds device code
// and notices it can call these constexpr functions from device code.
// As a workaround, we use C10_HOST_CONSTEXPR instead of constexpr for these
// functions. This enables constexpr when compiled on the host and applies
// __host__ when it is compiled on the device in an attempt to stop it from
// being called from device functions. Not sure if the latter works, but
// even if not, it not being constexpr anymore should be enough to stop
// it from being called from device code.
// TODO This occurred in CUDA 9 (9.0 to 9.2). Test if this is fixed in CUDA 10.
// #if defined(__CUDA_ARCH__)
// #define C10_HOST_CONSTEXPR __host__
// #define C10_HOST_CONSTEXPR_VAR
// #else
// #define C10_HOST_CONSTEXPR constexpr
// #define C10_HOST_CONSTEXPR_VAR constexpr
// #endif

// #if !defined(__clang__) && !defined(_MSC_VER) && defined(__GNUC__) &&
//     __GNUC__ < 6
// #define CONSTEXPR_EXCEPT_GCC5
public static final int IS_NOT_GCC5_CONSTEXPR = 0;
// #else
// #define CONSTEXPR_EXCEPT_GCC5 constexpr
// #endif

// #if defined(__CUDA_ARCH__)
// #if defined(_MSC_VER) && defined(__CUDACC__)
// #define CONSTEXPR_EXCEPT_WIN_CUDA const
// #define C10_HOST_CONSTEXPR_EXCEPT_WIN_CUDA __host__
// #else
// #define CONSTEXPR_EXCEPT_WIN_CUDA constexpr
// #define C10_HOST_CONSTEXPR_EXCEPT_WIN_CUDA __host__
// #endif
// #else
// #if defined(_MSC_VER) && defined(__CUDACC__)
// #define CONSTEXPR_EXCEPT_WIN_CUDA const
// #define C10_HOST_CONSTEXPR_EXCEPT_WIN_CUDA
// #else
// #define CONSTEXPR_EXCEPT_WIN_CUDA constexpr
// #define C10_HOST_CONSTEXPR_EXCEPT_WIN_CUDA constexpr
// #endif
// #endif

// #endif // C10_MACROS_MACROS_H_


// Parsed from c10/util/AlignOf.h

//===--- AlignOf.h - Portable calculation of type alignment -----*- C++ -*-===//
//
//                     The LLVM Compiler Infrastructure
//
// This file is distributed under the University of Illinois Open Source
// License. See LICENSE.TXT for details.
//
//===----------------------------------------------------------------------===//
//
// This file defines the AlignedCharArray and AlignedCharArrayUnion classes.
//
//===----------------------------------------------------------------------===//

// ATen: modified from llvm::AlignOf
// replaced LLVM_ALIGNAS with alignas

// #pragma once

// #include <cstddef>

/** \struct AlignedCharArray
 *  \brief Helper for building an aligned character array type.
 * 
 *  This template is used to explicitly build up a collection of aligned
 *  character array types. We have to build these up using a macro and explicit
 *  specialization to cope with MSVC (at least till 2015) where only an
 *  integer literal can be used to specify an alignment constraint. Once built
 *  up here, we can then begin to indirect between these using normal C++
 *  template parameters. */

// MSVC requires special handling here.
// #ifndef _MSC_VER

// #else // _MSC_VER

/** \brief Create a type with an aligned char buffer. */

// We provide special variations of this template for the most common
// alignments because __declspec(align(...)) doesn't actually work when it is
// a member of a by-value function argument in MSVC, even if the alignment
// request is something reasonably like 8-byte or 16-byte. Note that we can't
// even include the declspec with the union that forces the alignment because
// MSVC warns on the existence of the declspec despite the union member forcing
// proper alignment.

// The rest of these are provided with a __declspec(align(...)) and we simply
// can't pass them by-value as function arguments on MSVC.

// #define AT_ALIGNEDCHARARRAY_TEMPLATE_ALIGNMENT(x)
//   template <size_t Size>
//   struct AlignedCharArray<x, Size> {
//     __declspec(align(x)) char buffer[Size];
//   };

// #undef AT_ALIGNEDCHARARRAY_TEMPLATE_ALIGNMENT

// #endif // _MSC_VER
 // end namespace detail

/** \brief This union template exposes a suitably aligned and sized character
 *  array member which can hold elements of any of up to ten types.
 * 
 *  These types may be arrays, structs, or any other types. The goal is to
 *  expose a char array buffer member which can be used as suitable storage for
 *  a placement new of any of these types. Support for more than ten types can
 *  be added at the cost of more boilerplate. */
 // end namespace c10


// Parsed from c10/util/Deprecated.h

// #pragma once

/**
 * This file provides portable macros for marking declarations
 * as deprecated.  You should generally use C10_DEPRECATED,
 * except when marking 'using' declarations as deprecated,
 * in which case you should use C10_DEFINE_DEPRECATED_USING
 * (due to portability concerns).
 */

// Sample usage:
//
//    C10_DEPRECATED void bad_func();
//    struct C10_DEPRECATED BadStruct {
//      ...
//    };

// NB: In PyTorch, this block is not actually used at the moment
// because we are C++11.  However, aspirationally, we would like
// to use this version, because as of C++14 it is the correct and
// portable way to declare something deprecated.
// NB: __cplusplus doesn't work for MSVC, so for now MSVC always uses
// the "__declspec(deprecated)" implementation and not the C++14 "[[deprecated]]"
// attribute. We tried enabling "[[deprecated]]" for C++14 on MSVC, but
// ran into issues with some older MSVC versions.
// #if (defined(__cplusplus) && __cplusplus >= 201402L)
// # define C10_DEPRECATED [[deprecated]]
// # define C10_DEPRECATED_MESSAGE(message) [[deprecated(message)]]
// #elif defined(__GNUC__)
// # define C10_DEPRECATED __attribute__((deprecated))
// TODO Is there some way to implement this?
// # define C10_DEPRECATED_MESSAGE(message) __attribute__((deprecated))

// #elif defined(_MSC_VER)
// #else
// # warning "You need to implement C10_DEPRECATED for this compiler"
// # define C10_DEPRECATED
// #endif


// Sample usage:
//
//    C10_DEFINE_DEPRECATED_USING(BadType, int)
//
//   which is the portable version of
//
//    using BadType [[deprecated]] = int;

// technically [[deprecated]] syntax is from c++14 standard, but it works in
// many compilers.
// #if defined(__has_cpp_attribute)
// #if __has_cpp_attribute(deprecated) && !defined(__CUDACC__)
// # define C10_DEFINE_DEPRECATED_USING(TypeName, TypeThingy) using TypeName [[deprecated]] = TypeThingy;
// #endif
// #endif

// #if defined(_MSC_VER)
// #endif

// #if !defined(C10_DEFINE_DEPRECATED_USING) && defined(__GNUC__)
// nvcc has a bug where it doesn't understand __attribute__((deprecated))
// declarations even when the host compiler supports it. We'll only use this gcc
// attribute when not cuda, and when using a GCC compiler that doesn't support
// the c++14 syntax we checked for above (available in __GNUC__ >= 5)
// #if !defined(__CUDACC__)
// # define C10_DEFINE_DEPRECATED_USING(TypeName, TypeThingy) using TypeName __attribute__((deprecated)) = TypeThingy;
// #else
// using cuda + gcc < 5, neither deprecated syntax is available so turning off.
// # define C10_DEFINE_DEPRECATED_USING(TypeName, TypeThingy) using TypeName = TypeThingy;
// #endif
// #endif

// #if ! defined(C10_DEFINE_DEPRECATED_USING)
// # warning "You need to implement C10_DEFINE_DEPRECATED_USING for this compiler"
// # define C10_DEFINE_DEPRECATED_USING
// #endif


// Parsed from c10/util/StringUtil.h

// #ifndef C10_UTIL_STRINGUTIL_H_
// #define C10_UTIL_STRINGUTIL_H_

// #include <c10/macros/Macros.h>
// #include <c10/util/string_utils.h>

// #include <cstddef>
// #include <ostream>
// #include <sstream>
// #include <string>
// #include <vector>

// Obtains the base name from a full path.
@Namespace("c10::detail") public static native @StdString BytePointer StripBasename(@StdString BytePointer full_path);
@Namespace("c10::detail") public static native @StdString String StripBasename(@StdString String full_path);

@Namespace("c10::detail") public static native @StdString BytePointer ExcludeFileExtension(@StdString BytePointer full_path);
@Namespace("c10::detail") public static native @StdString String ExcludeFileExtension(@StdString String full_path);


@Namespace("c10::detail") public static native @Cast("std::ostream*") @ByRef Pointer _str(@Cast("std::ostream*") @ByRef Pointer ss);
// Targeting ../_str_wrapper.java



// For c10::str() with an empty argument list (which is common in our assert macros),
// we don't want to pay the binary size for constructing and destructing a stringstream
// or even constructing a string. Let's just return a reference to an empty string.

 // namespace detail

// Convert a list of string-like arguments into a single string.

// Replace all occurrences of "from" substring to "to" string.
// Returns number of replacements
@Namespace("c10") public static native @Cast("size_t") long ReplaceAll(@StdString @ByRef BytePointer s, @Cast("const char*") BytePointer from, @Cast("const char*") BytePointer to);
@Namespace("c10") public static native @Cast("size_t") long ReplaceAll(@StdString @ByRef BytePointer s, String from, String to);
// Targeting ../SourceLocation.java





// unix isprint but insensitive to locale
@Namespace("c10") public static native @Cast("bool") boolean isPrint(@Cast("char") byte s);

@Namespace("c10") public static native void printQuotedString(@Cast("std::ostream*") @ByRef Pointer stmt, @StdString BytePointer str);
@Namespace("c10") public static native void printQuotedString(@Cast("std::ostream*") @ByRef Pointer stmt, @StdString String str);

 // namespace c10

// #endif // C10_UTIL_STRINGUTIL_H_


// Parsed from c10/util/SmallVector.h

//===- llvm/ADT/SmallVector.h - 'Normally small' vectors --------*- C++ -*-===//
//
//                     The LLVM Compiler Infrastructure
//
// This file is distributed under the University of Illinois Open Source
// License. See LICENSE.TXT for details.
//
//===----------------------------------------------------------------------===//
//
// This file defines the SmallVector class.
//
//===----------------------------------------------------------------------===//

// ATen: modified from llvm::SmallVector.
// replaced report_bad_alloc_error with std::bad_alloc
// replaced isPodLike<T> with C10_IS_TRIVIALLY_COPYABLE (moved to Macros.h)
// replaced iterator_range constructor with inline Container&& constructor
// removed LLVM_NODISCARD and LLVM_ATTRIBUTE_ALWAYS_INLINE qualifiers
// removed LLVM_UNLIKELY

// #pragma once

// #include <c10/util/AlignOf.h>
// #include <c10/macros/Macros.h>

// #include <algorithm>
// #include <cassert>
// #include <cstddef>
// #include <cstdlib>
// #include <cstring>
// #include <initializer_list>
// #include <iterator>
// #include <memory>
// #include <new>
// #include <type_traits>
// #include <utility>

// From llvm/Support/MathExtras.h
@Namespace("c10::detail") public static native @Cast("uint64_t") long NextPowerOf2(@Cast("uint64_t") long A);


// Targeting ../SmallVectorBase.java



/** This is the part of SmallVectorTemplateBase which does not depend on whether
 *  the type T is a POD. The extra dummy template argument is used by ArrayRef
 *  to avoid unnecessarily requiring T to be complete. */

/** SmallVectorTemplateBase<isPodLike = false> - This is where we put method
 *  implementations that are designed to work with non-POD-like T's. */

// Define this out-of-line to dissuade the C++ compiler from inlining it.


/** SmallVectorTemplateBase<isPodLike = true> - This is where we put method
 *  implementations that are designed to work with POD-like T's. */

/** This class consists of common code factored out of the SmallVector class to
 *  reduce code duplication based on the SmallVector 'N' template parameter.
 *  Warning: C10_IS_TRIVIALLY_COPYABLE may not always detect non-POD
 *  type correctly. For example, std::unique_ptr may be treated as POD and cause
 *  memory leaks. */







/** Storage for the SmallVector elements which aren't contained in
 *  SmallVectorTemplateCommon. There are 'N-1' elements here. The remaining '1'
 *  element is in the base class. This is specialized for the N=1 and N=0 cases
 *  to avoid allocating unnecessary storage. */

/** This is a 'vector' (really, a variable-sized array), optimized
 *  for the case when the array is small.  It contains some number of elements
 *  in-place, which allows it to avoid heap allocation when the actual number of
 *  elements is below that threshold.  This allows normal "small" cases to be
 *  fast without losing generality for large inputs.
 * 
 *  Note that this does not attempt to be exception safe.
 *  */



 // end namespace c10

/** Implement std::swap in terms of SmallVector swap. */

/** Implement std::swap in terms of SmallVector swap. */

 // end namespace std


// Parsed from c10/util/Exception.h

// #ifndef C10_UTIL_EXCEPTION_H_
// #define C10_UTIL_EXCEPTION_H_

// #include <c10/macros/Macros.h>
// #include <c10/util/StringUtil.h>
// #include <c10/util/Deprecated.h>

// #include <cstddef>
// #include <exception>
// #include <ostream>
// #include <sstream>
// #include <string>
// #include <vector>

// #if defined(_MSC_VER) && _MSC_VER <= 1900
// #endif
// Targeting ../Error.java


// Targeting ../WarningHandler.java



// Note: [Verbatim Warnings]
// Warnings originating in C++ code can appear out-of-place to Python users:
// a user runs a line in Python, but the warning references a line in C++.
// Some parts of PyTorch, like the JIT, are cognizant of this mismatch
// and take care to map warnings back to the user's program, but most
// of PyTorch simply throws a context-free warning. To allow warning
// handlers to add context where appropriate, warn takes the
// "verbatim" flag. When this is false a warning handler might append
// the C++ warning to a Python warning message that relates the warning
// back to the user's program. Callers who have already accounted for
// context in their warnings should set verbatim to true so their warnings
// appear without modification.

/** Issue a warning with a given message. Dispatched to the current
 *  warning handler. */
@Namespace("c10::Warning") public static native void warn(@ByVal SourceLocation source_location,
    @StdString BytePointer msg,
    @Cast("bool") boolean verbatim);
@Namespace("c10::Warning") public static native void warn(@ByVal SourceLocation source_location,
    @StdString String msg,
    @Cast("bool") boolean verbatim);
/** Sets the global warning handler. This is not thread-safe, so it should
 *  generally be called once during initialization or while holding the GIL
 *  for programs that use python.
 *  User is responsible for keeping the WarningHandler alive until
 *  it is not needed. */
@Namespace("c10::Warning") public static native @NoException void set_warning_handler(WarningHandler handler);
/** Gets the global warning handler. */
@Namespace("c10::Warning") public static native @NoException WarningHandler get_warning_handler();


// Targeting ../IndexError.java


// Targeting ../ValueError.java


// Targeting ../TypeError.java


// Targeting ../EnforceFiniteError.java


// Targeting ../OnnxfiBackendSystemError.java



// A utility function to return an exception std::string by prepending its
// exception type before its what() content
@Namespace("c10") public static native @StdString BytePointer GetExceptionString(@Cast("const std::exception*") @ByRef Pointer e);

// Return x if it is non-empty; otherwise return y.
@Namespace("c10::detail") public static native @StdString BytePointer if_empty_then(@StdString BytePointer x, @StdString BytePointer y);
@Namespace("c10::detail") public static native @StdString String if_empty_then(@StdString String x, @StdString String y);




 // namespace c10

// Private helper macro for implementing TORCH_INTERNAL_ASSERT and TORCH_CHECK
//
// Note: In the debug build With MSVC, __LINE__ might be of long type (a.k.a int32_t),
// which is different from the definition of `SourceLocation` that requires
// unsigned int (a.k.a uint32_t) and may cause a compile error with the message:
// error C2397: conversion from 'long' to 'uint32_t' requires a narrowing conversion
// Here the static cast is used to pass the build.
// if this is used inside a lambda the __func__ macro expands to operator(),
// which isn't very useful, but hard to fix in a macro so suppressing the warning.
// #define C10_THROW_ERROR(err_type, msg)
//   throw ::c10::err_type({__func__, __FILE__, static_cast<uint32_t>(__LINE__)}, msg)

// Private helper macro for workaround MSVC misexpansion of nested macro
// invocations involving __VA_ARGS__.  See
// https://stackoverflow.com/questions/5134523/msvc-doesnt-expand-va-args-correctly
// #define C10_EXPAND_MSVC_WORKAROUND(x) x

// On nvcc, C10_UNLIKELY thwarts missing return statement analysis.  In cases
// where the unlikely expression may be a constant, use this macro to ensure
// return statement analysis keeps working (at the cost of not getting the
// likely/unlikely annotation on nvcc). https://github.com/pytorch/pytorch/issues/21418
//
// Currently, this is only used in the error reporting macros below.  If you
// want to use it more generally, move me to Macros.h
//
// TODO: Brian Vaughan observed that we might be able to get this to work on nvcc
// by writing some sort of C++ overload that distinguishes constexpr inputs
// from non-constexpr.  Since there isn't any evidence that losing C10_UNLIKELY
// in nvcc is causing us perf problems, this is not yet implemented, but this
// might be an interesting piece of C++ code for an intrepid bootcamper to
// write.
// #if defined(__CUDACC__)
// #define C10_UNLIKELY_OR_CONST(e) e
// #else
// #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)
// #endif


// ----------------------------------------------------------------------------
// Error reporting macros
// ----------------------------------------------------------------------------

// #ifdef STRIP_ERROR_MESSAGES
// #define TORCH_RETHROW(e, ...) throw
// #else
// #define TORCH_RETHROW(e, ...)
//   do {
//     e.add_context(::c10::str(__VA_ARGS__));
//     throw;
//   } while (false)
// #endif

// A utility macro to provide assert()-like functionality; that is, enforcement
// of internal invariants in code.  It supports an arbitrary number of extra
// arguments (evaluated only on failure), which will be printed in the assert
// failure message using operator<< (this is useful to print some variables
// which may be useful for debugging.)
//
// Usage:
//    TORCH_INTERNAL_ASSERT(should_be_true);
//    TORCH_INTERNAL_ASSERT(x == 0, "x = ", x);
//
// Assuming no bugs in PyTorch, the conditions tested by this macro should
// always be true; e.g., it should be possible to disable all of these
// conditions without changing observable user behavior.  If you would like to
// do error reporting for user input, please use TORCH_CHECK instead.
//
// NOTE: It is SAFE to use this macro in production code; on failure, this
// simply raises an exception, it does NOT unceremoniously quit the process
// (unlike assert()).
//
// #ifdef STRIP_ERROR_MESSAGES
// #define TORCH_INTERNAL_ASSERT(cond, ...)
//   if (C10_UNLIKELY_OR_CONST(!(cond))) {
//     C10_THROW_ERROR(Error,
//         #cond " INTERNAL ASSERT FAILED at"
//         C10_STRINGIZE(__FILE__)
//     );
//   }
// #else
// #define TORCH_INTERNAL_ASSERT(cond, ...)
//   if (C10_UNLIKELY_OR_CONST(!(cond))) {
//     C10_THROW_ERROR(Error, ::c10::str(
//         #cond " INTERNAL ASSERT FAILED at "
//         C10_STRINGIZE(__FILE__)
//         ":"
//         C10_STRINGIZE(__LINE__)
//         ", please report a bug to PyTorch. ",
//         ::c10::str(__VA_ARGS__)
//     ));
//   }
// #endif

// A utility macro to make it easier to test for error conditions from user
// input.  Like TORCH_INTERNAL_ASSERT, it supports an arbitrary number of extra
// arguments (evaluated only on failure), which will be printed in the error
// message using operator<< (e.g., you can pass any object which has
// operator<< defined.  Most objects in PyTorch have these definitions!)
//
// Usage:
//    TORCH_CHECK(should_be_true); // A default error message will be provided
//                                 // in this case; but we recommend writing an
//                                 // explicit error message, as it is more
//                                 // user friendly.
//    TORCH_CHECK(x == 0, "Expected x to be 0, but got ", x);
//
// On failure, this macro will raise an exception.  If this exception propagates
// to Python, it will convert into a Python RuntimeError.
//
// NOTE: It is SAFE to use this macro in production code; on failure, this
// simply raises an exception, it does NOT unceremoniously quit the process
// (unlike CHECK() from glog.)
//
// #define TORCH_CHECK_WITH(error_t, cond, ...)
//   TORCH_CHECK_WITH_MSG(error_t, cond, "", __VA_ARGS__)

// #ifdef STRIP_ERROR_MESSAGES
// #define TORCH_CHECK_MSG(cond, type, ...)
//   (#cond #type " CHECK FAILED at "
//    C10_STRINGIZE(__FILE__))
// #define TORCH_CHECK_WITH_MSG(error_t, cond, type, ...)
//   if (C10_UNLIKELY_OR_CONST(!(cond))) {
//     C10_THROW_ERROR(Error,
//         TORCH_CHECK_MSG(cond, type, __VA_ARGS__)
//     );
//   }
// #else
// #define TORCH_CHECK_MSG(cond, type, ...)
//   ::c10::detail::if_empty_then(
//       ::c10::str(__VA_ARGS__),
//       "Expected " #cond " to be true, but got false.  "
//       "(Could this error message be improved?  If so, "
//       "please report an enhancement request to PyTorch.)"
//   )
// #define TORCH_CHECK_WITH_MSG(error_t, cond, type, ...)
//   if (C10_UNLIKELY_OR_CONST(!(cond))) {
//     C10_THROW_ERROR(error_t,
//         TORCH_CHECK_MSG(cond, type, __VA_ARGS__)
//     );
//   }
// #endif

@Namespace("c10::detail") public static native void torchCheckFail(@Cast("const char*") BytePointer func, @Cast("const char*") BytePointer file, @Cast("uint32_t") int line, @StdString BytePointer msg);
@Namespace("c10::detail") public static native void torchCheckFail(String func, String file, @Cast("uint32_t") int line, @StdString String msg);

 // namespace detail
 // namespace 10

// #define TORCH_CHECK(cond, ...)
//   if (C10_UNLIKELY_OR_CONST(!(cond))) {
//     ::c10::detail::torchCheckFail(
//         __func__, __FILE__, static_cast<uint32_t>(__LINE__),
//         TORCH_CHECK_MSG(cond, "", __VA_ARGS__));
//   }

// An utility macro that does what `TORCH_CHECK` does if compiled in the host code,
// otherwise does nothing. Supposed to be used in the code shared between host and
// device code as an alternative for `TORCH_CHECK`.
// #if defined(__CUDACC__) || defined(__HIPCC__)
// #else
// #define TORCH_CHECK_IF_NOT_ON_CUDA(cond, ...) TORCH_CHECK(cond, __VA_ARGS__)
// #endif

// Debug only version of TORCH_INTERNAL_ASSERT. This macro only checks in debug
// build, and does nothing in release build.  It is appropriate to use
// in situations where you want to add an assert to a hotpath, but it is
// too expensive to run this assert on production builds.
// #ifdef NDEBUG
// Optimized version - generates no code.
// #define TORCH_INTERNAL_ASSERT_DEBUG_ONLY(...)
//   while (false)
//   C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(__VA_ARGS__))
// #else
// #define TORCH_INTERNAL_ASSERT_DEBUG_ONLY(...)
//   C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(__VA_ARGS__))
// #endif

// TODO: We're going to get a lot of similar looking string literals
// this way; check if this actually affects binary size.

// Like TORCH_CHECK, but raises IndexErrors instead of Errors.
// #define TORCH_CHECK_INDEX(cond, ...)
//   TORCH_CHECK_WITH_MSG(IndexError, cond, "INDEX", __VA_ARGS__)

// Like TORCH_CHECK, but raises ValueErrors instead of Errors.
// #define TORCH_CHECK_VALUE(cond, ...)
//   TORCH_CHECK_WITH_MSG(ValueError, cond, "VALUE", __VA_ARGS__)

// Like TORCH_CHECK, but raises TypeErrors instead of Errors.
// #define TORCH_CHECK_TYPE(cond, ...)
//   TORCH_CHECK_WITH_MSG(TypeError, cond, "TYPE", __VA_ARGS__)

// Report a warning to the user.  Accepts an arbitrary number of extra
// arguments which are concatenated into the warning message using operator<<
//
// #ifdef STRIP_ERROR_MESSAGES
// #define TORCH_WARN(...)
//   ::c10::Warning::warn({__func__, __FILE__, static_cast<uint32_t>(__LINE__)}, {}, false)
// #else
// #define TORCH_WARN(...)
//   ::c10::Warning::warn({__func__, __FILE__, static_cast<uint32_t>(__LINE__)}, ::c10::str(__VA_ARGS__), false)
// #endif

// Report a warning to the user only once.  Accepts an arbitrary number of extra
// arguments which are concatenated into the warning message using operator<<
//
// #ifdef STRIP_ERROR_MESSAGES
// #define TORCH_WARN_ONCE(...)
//   C10_UNUSED static const auto C10_ANONYMOUS_VARIABLE(torch_warn_once_) = [&] {
//     ::c10::Warning::warn({__func__, __FILE__, static_cast<uint32_t>(__LINE__)}, {}, false);
//     return true;
//   }()
// #else
// #define TORCH_WARN_ONCE(...)
//   C10_UNUSED static const auto C10_ANONYMOUS_VARIABLE(torch_warn_once_) = [&] {
//     ::c10::Warning::warn({__func__, __FILE__, static_cast<uint32_t>(__LINE__)}, ::c10::str(__VA_ARGS__), false);
//     return true;
//   }()
// #endif

// ----------------------------------------------------------------------------
// Deprecated macros
// ----------------------------------------------------------------------------

/*
// Deprecation disabled until we fix sites in our codebase
C10_DEPRECATED_MESSAGE("AT_ERROR(msg) is deprecated, use TORCH_CHECK(false, msg) instead.")
*/
@Namespace("c10::detail") public static native void deprecated_AT_ERROR();

/*
// Deprecation disabled until we fix sites in our codebase
C10_DEPRECATED_MESSAGE("AT_ASSERT is deprecated, if you mean to indicate an internal invariant failure, use " \
                       "TORCH_INTERNAL_ASSERT instead; if you mean to do user error checking, use " \
                       "TORCH_CHECK.  See https://github.com/pytorch/pytorch/issues/20287 for more details.")
*/
@Namespace("c10::detail") public static native void deprecated_AT_ASSERT();

/*
// Deprecation disabled until we fix sites in our codebase
C10_DEPRECATED_MESSAGE("AT_ASSERTM is deprecated, if you mean to indicate an internal invariant failure, use " \
                       "TORCH_INTERNAL_ASSERT instead; if you mean to do user error checking, use " \
                       "TORCH_CHECK.  See https://github.com/pytorch/pytorch/issues/20287 for more details.")
*/
@Namespace("c10::detail") public static native void deprecated_AT_ASSERTM();

 // namespace c10::detail

// Deprecated alias; this alias was deprecated because people kept mistakenly
// using it for user error checking.  Use TORCH_INTERNAL_ASSERT or TORCH_CHECK
// instead. See https://github.com/pytorch/pytorch/issues/20287 for more details.
// #define AT_ASSERT(...)
//   do {
//     ::c10::detail::deprecated_AT_ASSERT();
//     C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(__VA_ARGS__));
//   } while (false)

// Deprecated alias, like AT_ASSERT.  The new TORCH_INTERNAL_ASSERT macro supports
// both 0-ary and variadic calls, so having a separate message-accepting macro
// is not necessary.
//
// NB: we MUST include cond explicitly here, as MSVC will miscompile the macro
// expansion, shunting all of __VA_ARGS__ to cond.  An alternate workaround
// can be seen at
// https://stackoverflow.com/questions/5134523/msvc-doesnt-expand-va-args-correctly
// #define AT_ASSERTM(cond, ...)
//   do {
//     ::c10::detail::deprecated_AT_ASSERTM();
//     C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__));
//   } while (false)

// Deprecated alias; this alias was deprecated because it represents extra API
// surface that makes it hard for people to understand what macro to use.
// Use TORCH_CHECK(false, ...) or TORCH_INTERNAL_ASSERT(false, ...) to
// unconditionally fail at a line of code.
// #define AT_ERROR(...)
//   do {
//     ::c10::detail::deprecated_AT_ERROR();
//     C10_EXPAND_MSVC_WORKAROUND(TORCH_CHECK(false, ::c10::str(__VA_ARGS__)));
//   } while (false)

// #endif // C10_UTIL_EXCEPTION_H_


// Parsed from c10/util/ArrayRef.h

//===--- ArrayRef.h - Array Reference Wrapper -------------------*- C++ -*-===//
//
//                     The LLVM Compiler Infrastructure
//
// This file is distributed under the University of Illinois Open Source
// License. See LICENSE.TXT for details.
//
//===----------------------------------------------------------------------===//

// ATen: modified from llvm::ArrayRef.
// removed llvm-specific functionality
// removed some implicit const -> non-const conversions that rely on
// complicated std::enable_if meta-programming
// removed a bunch of slice variants for simplicity...

// #pragma once

// #include <c10/util/SmallVector.h>
// #include <c10/util/C++17.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Deprecated.h>

// #include <array>
// #include <iterator>
// #include <vector>
// Targeting ../ByteArrayRef.java


// Targeting ../ShortArrayRef.java


// Targeting ../IntArrayRef.java


// Targeting ../LongArrayRef.java


// Targeting ../FloatArrayRef.java


// Targeting ../DoubleArrayRef.java


// Targeting ../SizeTArrayRef.java


// Targeting ../BoolArrayRef.java


// Targeting ../HalfArrayRef.java


// Targeting ../BFloat16ArrayRef.java


// Targeting ../FloatComplexrrayRef.java


// Targeting ../DoubleComplexrrayRef.java


// Targeting ../ScalarTypeArrayRef.java


// Targeting ../IValueArrayRef.java


// Targeting ../DimnameArrayRef.java


// Targeting ../TensorArrayRef.java


// Targeting ../TensorArgArrayRef.java


// Targeting ../TensorIndexArrayRef.java





// WARNING: Template instantiation will NOT be willing to do an implicit
// conversions to get you to an c10::ArrayRef, which is why we need so
// many overloads.













// This alias is deprecated because it doesn't make ownership
// semantics obvious.  Use IntArrayRef instead!
 // namespace c10


// Parsed from c10/util/complex.h

// #pragma once

// #include <complex>
// #include <iostream>

// #include <c10/macros/Macros.h>

// #if defined(__CUDACC__) || defined(__HIPCC__)
// #endif

// c10::complex is an implementation of complex numbers that aims
// to work on all devices supported by PyTorch
//
// Most of the APIs duplicates std::complex
// Reference: https://en.cppreference.com/w/cpp/numeric/complex
//
// [NOTE: Complex Operator Unification]
// Operators currently use a mix of std::complex, thrust::complex, and c10::complex internally.
// The end state is that all operators will use c10::complex internally.  Until then, there may
// be some hacks to support all variants.
//
//
// [Note on Constructors]
//
// The APIs of constructors are mostly copied from C++ standard:
//   https://en.cppreference.com/w/cpp/numeric/complex/complex
//
// Since C++14, all constructors are constexpr in std::complex
//
// There are three types of constructors:
// - initializing from real and imag:
//     `constexpr complex( const T& re = T(), const T& im = T() );`
// - implicitly-declared copy constructor
// - converting constructors
//
// Converting constructors:
// - std::complex defines converting constructor between float/double/long double,
//   while we define converting constructor between float/double.
// - For these converting constructors, upcasting is implicit, downcasting is
//   explicit.
// - We also define explicit casting from std::complex/thrust::complex
//   - Note that the conversion from thrust is not constexpr, because
//     thrust does not define them as constexpr ????
//
//
// [Operator =]
//
// The APIs of operator = are mostly copied from C++ standard:
//   https://en.cppreference.com/w/cpp/numeric/complex/operator%3D
//
// Since C++20, all operator= are constexpr. Although we are not building with
// C++20, we also obey this behavior.
//
// There are three types of assign operator:
// - Assign a real value from the same scalar type
//   - In std, this is templated as complex& operator=(const T& x)
//     with specialization `complex& operator=(T x)` for float/double/long double
//     Since we only support float and double, on will use `complex& operator=(T x)`
// - Copy assignment operator and converting assignment operator
//   - There is no specialization of converting assignment operators, which type is
//     convertible is solely dependent on whether the scalar type is convertible
//
// In addition to the standard assignment, we also provide assignment operators with std and thrust
//
//
// [Casting operators]
//
// std::complex does not have casting operators. We define casting operators casting to std::complex and thrust::complex
//
//
// [Operator ""]
//
// std::complex has custom literals `i`, `if` and `il` defined in namespace `std::literals::complex_literals`.
// We define our own custom literals in the namespace `c10::complex_literals`. Our custom literals does not
// follow the same behavior as in std::complex, instead, we define _if, _id to construct float/double
// complex literals.
//
//
// [real() and imag()]
//
// In C++20, there are two overload of these functions, one it to return the real/imag, another is to set real/imag,
// they are both constexpr. We follow this design.
//
//
// [Operator +=,-=,*=,/=]
//
// Since C++20, these operators become constexpr. In our implementation, they are also constexpr.
//
// There are two types of such operators: operating with a real number, or operating with another complex number.
// For the operating with a real number, the generic template form has argument type `const T &`, while the overload
// for float/double/long double has `T`. We will follow the same type as float/double/long double in std.
//
// [Unary operator +-]
//
// Since C++20, they are constexpr. We also make them expr
//
// [Binary operators +-*/]
//
// Each operator has three versions (taking + as example):
// - complex + complex
// - complex + real
// - real + complex
//
// [Operator ==, !=]
//
// Each operator has three versions (taking == as example):
// - complex == complex
// - complex == real
// - real == complex
//
// Some of them are removed on C++20, but we decide to keep them
//
// [Operator <<, >>]
//
// These are implemented by casting to std::complex
//
//
//
// TODO(@zasdfgbnm): c10::complex<c10::Half> is not currently supported, because:
//  - lots of members and functions of c10::Half are not constexpr
//  - thrust::complex only support float and double









 // namespace complex_literals


// Define operators between integral scalars and c10::complex. std::complex does not support this when T is a
// floating-point number. This is useful because it saves a lot of "static_cast" when operate a complex and an integer.
// This makes the code both less verbose and potentially more efficient.
// #define COMPLEX_INTEGER_OP_TEMPLATE_CONDITION
//   typename std::enable_if_t<std::is_floating_point<fT>::value && std::is_integral<iT>::value, int> = 0

// #undef COMPLEX_INTEGER_OP_TEMPLATE_CONDITION
















 // namespace c10

// std functions
//
// The implementation of these functions also follow the design of C++20

// #if defined(__CUDACC__) || defined(__HIPCC__)
// #endif

// #ifdef __HIP_PLATFORM_HCC__
// #define ROCm_Bug(x)
// #else
// #define ROCm_Bug(x) x
// #endif

// #undef ROCm_Bug

// For std::conj, there are other versions of it:
//   constexpr std::complex<float> conj( float z );
//   template< class DoubleOrInteger >
//   constexpr std::complex<double> conj( DoubleOrInteger z );
//   constexpr std::complex<long double> conj( long double z );
// These are not implemented
// TODO(@zasdfgbnm): implement them as c10::conj

// Thrust does not have complex --> complex version of thrust::proj,
// so this function is not implemented at c10 right now.
// TODO(@zasdfgbnm): implement it by ourselves

// There is no c10 version of std::polar, because std::polar always
// returns std::complex. Use c10::polar instead;

 // namespace std

 // namespace c10

// #define C10_INTERNAL_INCLUDE_COMPLEX_REMAINING_H
// math functions are included in a separate file
// #include <c10/util/complex_math.h>
// utilities for complex types
// #include <c10/util/complex_utils.h>
// #undef C10_INTERNAL_INCLUDE_COMPLEX_REMAINING_H


// Parsed from c10/util/Half.h

// #pragma once

/** Defines the Half type (half-precision floating-point) including conversions
 *  to standard C types and basic arithmetic operations. Note that arithmetic
 *  operations are implemented by converting to floating point and
 *  performing the operation in float32, instead of using CUDA half intrinsics.
 *  Most uses of this type within ATen are memory bound, including the
 *  element-wise kernels, and the half intrinsics aren't efficient on all GPUs.
 *  If you are writing a compute bound kernel, you can use the CUDA half
 *  intrinsics directly on the Half type from device code. */

// #include <c10/macros/Macros.h>
// #include <c10/util/C++17.h>
// #include <c10/util/complex.h>

// #if defined(__cplusplus) && (__cplusplus >= 201103L)
// #include <cmath>
// #include <cstdint>
// #elif !defined(__OPENCL_VERSION__)
// #include <math.h>
// #include <stdint.h>
// #endif

// #ifdef _MSC_VER
// #include <intrin.h>
// #endif

// #include <complex>
// #include <cstring>
// #include <cstdint>
// #include <iosfwd>
// #include <limits>
// #include <sstream>
// #include <stdexcept>
// #include <string>
// #include <utility>

// #ifdef __CUDACC__
// #include <cuda_fp16.h>
// #endif

// #ifdef __HIPCC__
// #include <hip/hip_fp16.h>
// #endif

// Standard check for compiling CUDA with clang
// #if defined(__clang__) && defined(__CUDA__) && defined(__CUDA_ARCH__)
// #define C10_DEVICE_HOST_FUNCTION __device__ __host__
// #else
// #define C10_DEVICE_HOST_FUNCTION
// #endif

  @Namespace("c10::detail") public static native float fp32_from_bits(@Cast("uint32_t") int w);

  @Namespace("c10::detail") public static native @Cast("uint32_t") int fp32_to_bits(float f);

  /*
   * Convert a 16-bit floating-point number in IEEE half-precision format, in bit representation, to
   * a 32-bit floating-point number in IEEE single-precision format, in bit representation.
   *
   * @note The implementation doesn't use any floating-point operations.
   */
  @Namespace("c10::detail") public static native @Cast("uint32_t") int fp16_ieee_to_fp32_bits(@Cast("uint16_t") short h);

  /*
   * Convert a 16-bit floating-point number in IEEE half-precision format, in bit representation, to
   * a 32-bit floating-point number in IEEE single-precision format.
   *
   * @note The implementation relies on IEEE-like (no assumption about rounding mode and no operations on denormals)
   * floating-point operations and bitcasts between integer and floating-point variables.
   */
  @Namespace("c10::detail") public static native float fp16_ieee_to_fp32_value(@Cast("uint16_t") short h);

  /*
   * Convert a 32-bit floating-point number in IEEE single-precision format to a 16-bit floating-point number in
   * IEEE half-precision format, in bit representation.
   *
   * @note The implementation relies on IEEE-like (no assumption about rounding mode and no operations on denormals)
   * floating-point operations and bitcasts between integer and floating-point variables.
   */
  @Namespace("c10::detail") public static native @Cast("uint16_t") short fp16_ieee_from_fp32_value(float f);


// Targeting ../Half.java



// This is just a placeholder for whatever complex representation we
// end up deciding to use for half-precision complex numbers.

// In some versions of MSVC, there will be a compiler error when building.
// C4146: unary minus operator applied to unsigned type, result still unsigned
// C4804: unsafe use of type 'bool' in operation
// It can be addressed by disabling the following warning.
// #ifdef _MSC_VER
// #pragma warning( push )
// #pragma warning( disable : 4146 )
// #pragma warning( disable : 4804 )
// #pragma warning( disable : 4018 )
// #endif

// The overflow checks may involve float to int conversion which may
// trigger precision loss warning. Re-enable the warning once the code
// is fixed. See T58053069.
// #ifdef __clang__
// #pragma GCC diagnostic push
// #pragma GCC diagnostic ignored "-Wunknown-warning-option"
// #pragma GCC diagnostic ignored "-Wimplicit-int-float-conversion"
// #endif

// bool can be converted to any type.
// Without specializing on bool, in pytorch_linux_trusty_py2_7_9_build:
// `error: comparison of constant '255' with boolean expression is always false`
// for `f > limit::max()` below

// skip isnan and isinf check for integral types

// #ifdef __clang__
// #pragma GCC diagnostic pop
// #endif

// #ifdef _MSC_VER
// #pragma warning( pop )
// #endif



 // namespace c10

// #include <c10/util/Half-inl.h>


// Parsed from c10/util/qint32.h

// #pragma once
// #include <cstdint>

// #include <c10/macros/Macros.h>
// Targeting ../qint32.java



 // namespace c10


// Parsed from c10/util/qint8.h

// #pragma once
// #include <cstdint>

// #include <c10/macros/Macros.h>
// Targeting ../qint8.java



 // namespace c10


// Parsed from c10/util/quint8.h

// #pragma once
// #include <cstdint>

// #include <c10/macros/Macros.h>
// Targeting ../quint8.java



 // namespace c10


// Parsed from c10/util/BFloat16.h

// #pragma once

// Defines the bloat16 type (brain floating-point). This representation uses
// 1 bit for the sign, 8 bits for the exponent and 7 bits for the mantissa.

// #include <c10/macros/Macros.h>
// #include <cmath>
// #include <cstring>

// #if defined(CUDA_VERSION) && CUDA_VERSION >= 11000
// #endif
  @Namespace("c10::detail") public static native float f32_from_bits(@Cast("uint16_t") short src);

  @Namespace("c10::detail") public static native @Cast("uint16_t") short bits_from_f32(float src);

  @Namespace("c10::detail") public static native @Cast("uint16_t") short round_to_nearest_even(float src);

// Targeting ../BFloat16.java



 // namespace c10


// #include <c10/util/BFloat16-inl.h>


// Parsed from c10/util/quint4x2.h

// #pragma once
// #include <cstdint>

// #include <c10/macros/Macros.h>
// Targeting ../quint4x2.java



 // namespace c10


// Parsed from c10/util/ThreadLocalDebugInfo.h

// #pragma once

// #include <c10/macros/Export.h>
// #include <c10/util/Exception.h>

// #include <memory>
// #include <string>
// #include <unordered_map>

@Namespace("c10") public enum DebugInfoKind {
  PRODUCER_INFO((byte)(0)),
  MOBILE_RUNTIME_INFO((byte)(1)),
  PROFILER_STATE((byte)(2)),
  INFERENCE_CONTEXT((byte)(3)), // for inference usage

  TEST_INFO((byte)(4)), // used only in tests
  TEST_INFO_2((byte)(5));// used only in tests

    public final byte value;
    private DebugInfoKind(byte v) { this.value = v; }
    private DebugInfoKind(DebugInfoKind e) { this.value = e.value; }
    public DebugInfoKind intern() { for (DebugInfoKind e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../DebugInfoBase.java


// Targeting ../ThreadLocalDebugInfo.java


// Targeting ../DebugInfoGuard.java



 // namespace c10


// Parsed from c10/util/Type.h

// #ifndef C10_UTIL_TYPE_H_
// #define C10_UTIL_TYPE_H_

// #include <cstddef>
// #include <string>
// #include <typeinfo>

// #include <c10/macros/Macros.h>

/** Utility to demangle a C++ symbol name. */
@Namespace("c10") public static native @StdString BytePointer demangle(@Cast("const char*") BytePointer name);
@Namespace("c10") public static native @StdString String demangle(String name);

/** Returns the printable name of the type. */

 // namespace c10

// #endif // C10_UTIL_TYPE_H_


// Parsed from c10/util/TypeCast.h

// #pragma once
// #include <c10/core/ScalarType.h>
// #include <c10/util/Half.h>
// #include <c10/util/BFloat16.h>
// #include <c10/macros/Macros.h>

// #include <type_traits>

// Note: deliberately ignores undefined behavior, consistent with NumPy.
// PyTorch's type conversions can cause a variety of undefined behavior,
// including float to integral overflow and signed to unsigned integer overflow.
// Some of this undefined behavior is addressed below.

// Partial template instantiation for casting to uint8.
// Note: Converting from negative float values to unsigned integer types is
// undefined behavior in C++, and current CPU and GPU compilers exhibit
// divergent behavior. Casting from negative float values to signed
// integer types and then to unsigned integer types is not undefined,
// however, so this cast improves the consistency of type conversions
// to uint8 across compilers.
// Further note: Type conversions across compilers still have other undefined
// and divergent behavior.

// Dynamic type casting utils:
// - fetch_and_cast
// - cast_and_store
//
// fetch_and_cast fetch a value with dynamic type specified by a ScalarType
// from a void pointer and cast it to a static type.
//
// cast_and_store casts a static typed value into dynamic type specified
// by a ScalarType, and store it into a void pointer.
//
// NOTE:
//
// Dynamic casting allows us to support type promotion without blowing up
// the combination space: For example, without dynamic cast, in order to
// implement `add_` with type promotion, we would need something like
//
// AT_DISPATCH_ALL_TYPES(output.dtype(),
//    AT_DISPATCH_ALL_TYPES(input1.dtype(),
//       AT_DISPATCH_ALL_TYPES(input2.dtype(),
//           [](arg0_t a, arg1_t b) -> out_t { return a + b; }
//       )
//    )
// )
//
// If we support N dtypes, the above code would generate the a+b kernel for
// all the N * N * N different supported types, the compilation time and
// binary size would become horrible.
//
// Dynamic casting might sounds like a bad idea in terms of performance.
// Especially if you ever do it in a loop, you are going to do a billion tests.
// But in practice it is not as bad as it might look:
//
// - on CPU, this is a branch that always has the same outcome, therefore
//   hopefully the branch predictor could do the job pretty well
// - on GPU, these branches will not diverge, so we could still have the same
//   warp executing the same line of code
// - Most kernels, like `add`, are bandwidth bound, adding a few clock cycles to
//   check an integer does not hurt the performance much because the ALUs would
//   wait for load instructions anyway.
//
// For the discussion and benchmark, refer to:
// - https://github.com/pytorch/pytorch/pull/28343
// - https://github.com/pytorch/pytorch/pull/28344
// - https://github.com/pytorch/pytorch/pull/28345
//

// #ifdef C10_HOST_DEVICE
// #else
// #define ERROR_UNSUPPORTED_CAST TORCH_CHECK(false, "Unexpected scalar type");
// #endif

// Fetch a value with dynamic type src_type from ptr, and cast it to static type dest_t.
// #define FETCH_AND_CAST_CASE(type, scalartype) case ScalarType::scalartype: return static_cast_with_inter_type<dest_t, type>::apply(*(const type *)ptr);
@Namespace("c10") public static native @ByVal @Name("fetch_and_cast<c10::qint8>") qint8 fetch_and_cast_qint8(ScalarType src_type, @Const Pointer ptr);
@Namespace("c10") public static native @ByVal @Name("fetch_and_cast<c10::quint8>") quint8 fetch_and_cast_quint8(ScalarType src_type, @Const Pointer ptr);
@Namespace("c10") public static native @ByVal @Name("fetch_and_cast<c10::qint32>") qint32 fetch_and_cast_qint32(ScalarType src_type, @Const Pointer ptr);
@Namespace("c10") public static native @ByVal @Name("fetch_and_cast<c10::quint4x2>") quint4x2 fetch_and_cast_quint4x2(ScalarType src_type, @Const Pointer ptr);

// Cast a value with static type src_t into dynamic dest_type, and store it to ptr.
// #define CAST_AND_STORE_CASE(type, scalartype) case ScalarType::scalartype: *(type *)ptr = static_cast_with_inter_type<type, src_t>::apply(value); return;
@Namespace("c10") public static native @Name("cast_and_store<c10::qint8>") void cast_and_store_qint8(ScalarType dest_type, Pointer ptr, @ByVal qint8 value);
@Namespace("c10") public static native @Name("cast_and_store<c10::quint8>") void cast_and_store_quint8(ScalarType dest_type, Pointer ptr, @ByVal quint8 value);
@Namespace("c10") public static native @Name("cast_and_store<c10::qint32>") void cast_and_store_qint32(ScalarType dest_type, Pointer ptr, @ByVal qint32 value);
@Namespace("c10") public static native @Name("cast_and_store<c10::quint4x2>") void cast_and_store_quint4x2(ScalarType dest_type, Pointer ptr, @ByVal quint4x2 value);

// #define DEFINE_UNCASTABLE(T, scalartype_)
// template<>
// C10_HOST_DEVICE inline T fetch_and_cast<T>(const ScalarType src_type, const void *ptr) {
//   CUDA_KERNEL_ASSERT(ScalarType::scalartype_ == src_type);
//   return *(const T *)ptr;
// }
// template<>
// C10_HOST_DEVICE inline void cast_and_store<T>(const ScalarType dest_type, void *ptr, T value) {
//   CUDA_KERNEL_ASSERT(ScalarType::scalartype_ == dest_type);
//   *(T *)ptr = value;
// }



  // namespace c10

// Trigger tests for D25440771. TODO: Remove this line any time you want.


// Parsed from c10/util/Registry.h

// #ifndef C10_UTIL_REGISTRY_H_
// #define C10_UTIL_REGISTRY_H_

/**
 * Simple registry implementation that uses static variables to
 * register object creators during program initialization time.
 */

// NB: This Registry works poorly when you have other namespaces.
// Make all macro invocations from inside the at namespace.

// #include <algorithm>
// #include <cstdio>
// #include <cstdlib>
// #include <functional>
// #include <memory>
// #include <mutex>
// #include <string>
// #include <unordered_map>
// #include <vector>

// #include <c10/macros/Macros.h>
// #include <c10/util/Type.h>

@Namespace("c10") public static native @StdString BytePointer KeyStrRepr(@StdString BytePointer key);
@Namespace("c10") public static native @StdString String KeyStrRepr(@StdString String key);

@Namespace("c10") public enum RegistryPriority {
  REGISTRY_FALLBACK(1),
  REGISTRY_DEFAULT(2),
  REGISTRY_PREFERRED(3);

    public final int value;
    private RegistryPriority(int v) { this.value = v; }
    private RegistryPriority(RegistryPriority e) { this.value = e.value; }
    public RegistryPriority intern() { for (RegistryPriority e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

/**
 * \brief A template class that allows one to register classes by keys.
 *
 * The keys are usually a std::string specifying the name, but can be anything
 * that can be used in a std::map.
 *
 * You should most likely not use the Registry class explicitly, but use the
 * helper macros below to declare specific registries as well as registering
 * objects.
 */

/**
 * C10_DECLARE_TYPED_REGISTRY is a macro that expands to a function
 * declaration, as well as creating a convenient typename for its corresponding
 * registerer.
 */
// Note on C10_IMPORT and C10_EXPORT below: we need to explicitly mark DECLARE
// as import and DEFINE as export, because these registry macros will be used
// in downstream shared libraries as well, and one cannot use *_API - the API
// macro will be defined on a per-shared-library basis. Semantically, when one
// declares a typed registry it is always going to be IMPORT, and when one
// defines a registry (which should happen ONLY ONCE and ONLY IN SOURCE FILE),
// the instantiation unit is always going to be exported.
//
// The only unique condition is when in the same file one does DECLARE and
// DEFINE - in Windows compilers, this generates a warning that dllimport and
// dllexport are mixed, but the warning is fine and linker will be properly
// exporting the symbol. Same thing happens in the gflags flag declaration and
// definition caes.
// #define C10_DECLARE_TYPED_REGISTRY(
//     RegistryName, SrcType, ObjectType, PtrType, ...)
//   C10_IMPORT ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>*
//   RegistryName();
//   typedef ::c10::Registerer<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>
//       Registerer##RegistryName

// #define C10_DEFINE_TYPED_REGISTRY(
//     RegistryName, SrcType, ObjectType, PtrType, ...)
//   C10_EXPORT ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>*
//   RegistryName() {
//     static ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>*
//         registry = new ::c10::
//             Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>();
//     return registry;
//   }

// #define C10_DEFINE_TYPED_REGISTRY_WITHOUT_WARNING(
//     RegistryName, SrcType, ObjectType, PtrType, ...)
//   C10_EXPORT ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>*
//   RegistryName() {
//     static ::c10::Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>*
//         registry = new ::c10::
//             Registry<SrcType, PtrType<ObjectType>, ##__VA_ARGS__>(false);
//     return registry;
//   }

// Note(Yangqing): The __VA_ARGS__ below allows one to specify a templated
// creator with comma in its templated arguments.
// #define C10_REGISTER_TYPED_CREATOR(RegistryName, key, ...)
//   static Registerer##RegistryName C10_ANONYMOUS_VARIABLE(g_##RegistryName)(
//       key, RegistryName(), ##__VA_ARGS__);

// #define C10_REGISTER_TYPED_CREATOR_WITH_PRIORITY(
//     RegistryName, key, priority, ...)
//   static Registerer##RegistryName C10_ANONYMOUS_VARIABLE(g_##RegistryName)(
//       key, priority, RegistryName(), ##__VA_ARGS__);

// #define C10_REGISTER_TYPED_CLASS(RegistryName, key, ...)
//   static Registerer##RegistryName C10_ANONYMOUS_VARIABLE(g_##RegistryName)(
//       key,
//       RegistryName(),
//       Registerer##RegistryName::DefaultCreator<__VA_ARGS__>,
//       ::c10::demangle_type<__VA_ARGS__>());

// #define C10_REGISTER_TYPED_CLASS_WITH_PRIORITY(
//     RegistryName, key, priority, ...)
//   static Registerer##RegistryName C10_ANONYMOUS_VARIABLE(g_##RegistryName)(
//       key,
//       priority,
//       RegistryName(),
//       Registerer##RegistryName::DefaultCreator<__VA_ARGS__>,
//       ::c10::demangle_type<__VA_ARGS__>());

// C10_DECLARE_REGISTRY and C10_DEFINE_REGISTRY are hard-wired to use
// std::string as the key type, because that is the most commonly used cases.
// #define C10_DECLARE_REGISTRY(RegistryName, ObjectType, ...)
//   C10_DECLARE_TYPED_REGISTRY(
//       RegistryName, std::string, ObjectType, std::unique_ptr, ##__VA_ARGS__)

// #define C10_DEFINE_REGISTRY(RegistryName, ObjectType, ...)
//   C10_DEFINE_TYPED_REGISTRY(
//       RegistryName, std::string, ObjectType, std::unique_ptr, ##__VA_ARGS__)

// #define C10_DECLARE_SHARED_REGISTRY(RegistryName, ObjectType, ...)
//   C10_DECLARE_TYPED_REGISTRY(
//       RegistryName, std::string, ObjectType, std::shared_ptr, ##__VA_ARGS__)

// #define C10_DEFINE_SHARED_REGISTRY(RegistryName, ObjectType, ...)
//   C10_DEFINE_TYPED_REGISTRY(
//       RegistryName, std::string, ObjectType, std::shared_ptr, ##__VA_ARGS__)

// #define C10_DEFINE_SHARED_REGISTRY_WITHOUT_WARNING(
//     RegistryName, ObjectType, ...)
//   C10_DEFINE_TYPED_REGISTRY_WITHOUT_WARNING(
//       RegistryName, std::string, ObjectType, std::shared_ptr, ##__VA_ARGS__)

// C10_REGISTER_CREATOR and C10_REGISTER_CLASS are hard-wired to use std::string
// as the key
// type, because that is the most commonly used cases.
// #define C10_REGISTER_CREATOR(RegistryName, key, ...)
//   C10_REGISTER_TYPED_CREATOR(RegistryName, #key, __VA_ARGS__)

// #define C10_REGISTER_CREATOR_WITH_PRIORITY(RegistryName, key, priority, ...)
//   C10_REGISTER_TYPED_CREATOR_WITH_PRIORITY(
//       RegistryName, #key, priority, __VA_ARGS__)

// #define C10_REGISTER_CLASS(RegistryName, key, ...)
//   C10_REGISTER_TYPED_CLASS(RegistryName, #key, __VA_ARGS__)

// #define C10_REGISTER_CLASS_WITH_PRIORITY(RegistryName, key, priority, ...)
//   C10_REGISTER_TYPED_CLASS_WITH_PRIORITY(
//       RegistryName, #key, priority, __VA_ARGS__)

 // namespace c10

// #endif // C10_UTIL_REGISTRY_H_


// Parsed from c10/util/Flags.h

// #ifndef C10_UTIL_FLAGS_H_
// #define C10_UTIL_FLAGS_H_

/* Commandline flags support for C10.
 *
 * This is a portable commandline flags tool for c10, so we can optionally
 * choose to use gflags or a lightweight custom implementation if gflags is
 * not possible on a certain platform. If you have gflags installed, set the
 * macro C10_USE_GFLAGS will seamlessly route everything to gflags.
 *
 * To define a flag foo of type bool default to true, do the following in the
 * *global* namespace:
 *     C10_DEFINE_bool(foo, true, "An example.");
 *
 * To use it in another .cc file, you can use C10_DECLARE_* as follows:
 *     C10_DECLARE_bool(foo);
 *
 * In both cases, you can then access the flag via FLAGS_foo.
 *
 * It is recommended that you build with gflags. To learn more about the flags
 * usage, refer to the gflags page here:
 *
 * https://gflags.github.io/gflags/
 *
 * Note about Python users / devs: gflags is initiated from a C++ function
 * ParseCommandLineFlags, and is usually done in native binaries in the main
 * function. As Python does not have a modifiable main function, it is usually
 * difficult to change the flags after Python starts. Hence, it is recommended
 * that one sets the default value of the flags to one that's acceptable in
 * general - that will allow Python to run without wrong flags.
 */

// #include <string>

// #include <c10/macros/Macros.h>
// #include <c10/util/Registry.h>
/**
 * Sets the usage message when a commandline tool is called with "--help".
 */
@Namespace("c10") public static native void SetUsageMessage(@StdString BytePointer str);
@Namespace("c10") public static native void SetUsageMessage(@StdString String str);

/**
 * Returns the usage message for the commandline tool set by SetUsageMessage.
 */
@Namespace("c10") public static native @Cast("const char*") BytePointer UsageMessage();

/**
 * Parses the commandline flags.
 *
 * This command parses all the commandline arguments passed in via pargc
 * and argv. Once it is finished, partc and argv will contain the remaining
 * commandline args that c10 does not deal with. Note that following
 * convention, argv[0] contains the binary name and is not parsed.
 */
@Namespace("c10") public static native @Cast("bool") boolean ParseCommandLineFlags(IntPointer pargc, @Cast("char***") @ByPtrPtr PointerPointer pargv);
@Namespace("c10") public static native @Cast("bool") boolean ParseCommandLineFlags(IntBuffer pargc, @Cast("char***") @ByPtrPtr PointerPointer pargv);
@Namespace("c10") public static native @Cast("bool") boolean ParseCommandLineFlags(int[] pargc, @Cast("char***") @ByPtrPtr PointerPointer pargv);

/**
 * Checks if the commandline flags has already been passed.
 */
@Namespace("c10") public static native @Cast("bool") boolean CommandLineFlagsHasBeenParsed();

 // namespace c10

////////////////////////////////////////////////////////////////////////////////
// Below are gflags and non-gflags specific implementations.
// In general, they define the following macros for one to declare (use
// C10_DECLARE) or define (use C10_DEFINE) flags:
// C10_{DECLARE,DEFINE}_{int,int64,double,bool,string}
////////////////////////////////////////////////////////////////////////////////

// #ifdef C10_USE_GFLAGS

////////////////////////////////////////////////////////////////////////////////
// Begin gflags section: most functions are basically rerouted to gflags.
////////////////////////////////////////////////////////////////////////////////
// #include <gflags/gflags.h>

// C10 uses hidden visibility by default. However, in gflags, it only uses
// export on Windows platform (with dllexport) but not on linux/mac (with
// default visibility). As a result, to ensure that we are always exporting
// global variables, we will redefine the GFLAGS_DLL_DEFINE_FLAG macro if we
// are building C10 as a shared libray.
// This has to be done after the inclusion of gflags, because some early
// versions of gflags.h (e.g. 2.0 on ubuntu 14.04) directly defines the
// macros, so we need to do definition after gflags is done.
// #ifdef GFLAGS_DLL_DEFINE_FLAG
// #endif // GFLAGS_DLL_DEFINE_FLAG
// #ifdef GFLAGS_DLL_DECLARE_FLAG
// #endif // GFLAGS_DLL_DECLARE_FLAG
// #define GFLAGS_DLL_DEFINE_FLAG C10_EXPORT
// #define GFLAGS_DLL_DECLARE_FLAG C10_IMPORT

// gflags before 2.0 uses namespace google and after 2.1 uses namespace gflags.
// Using GFLAGS_GFLAGS_H_ to capture this change.
// #ifndef GFLAGS_GFLAGS_H_
// #endif // GFLAGS_GFLAGS_H_

// Motivation about the gflags wrapper:
// (1) We would need to make sure that the gflags version and the non-gflags
// version of C10 are going to expose the same flags abstraction. One should
// explicitly use FLAGS_flag_name to access the flags.
// (2) For flag names, it is recommended to start with c10_ to distinguish it
// from regular gflags flags. For example, do
//    C10_DEFINE_BOOL(c10_my_flag, true, "An example");
// to allow one to use FLAGS_c10_my_flag.
// (3) Gflags has a design issue that does not properly expose the global flags,
// if one builds the library with -fvisibility=hidden. The current gflags (as of
// Aug 2018) only deals with the Windows case using dllexport, and not the Linux
// counterparts. As a result, we will explciitly use C10_EXPORT to export the
// flags defined in C10. This is done via a global reference, so the flag
// itself is not duplicated - under the hood it is the same global gflags flag.
// #define C10_GFLAGS_DEF_WRAPPER(type, real_type, name, default_value, help_str)
//   DEFINE_##type(name, default_value, help_str);                                

// #define C10_DEFINE_int(name, default_value, help_str)
//   C10_GFLAGS_DEF_WRAPPER(int32, gflags::int32, name, default_value, help_str)
// #define C10_DEFINE_int32(name, default_value, help_str)
//   C10_DEFINE_int(name, default_value, help_str)
// #define C10_DEFINE_int64(name, default_value, help_str)
//   C10_GFLAGS_DEF_WRAPPER(int64, gflags::int64, name, default_value, help_str)
// #define C10_DEFINE_double(name, default_value, help_str)
//   C10_GFLAGS_DEF_WRAPPER(double, double, name, default_value, help_str)
// #define C10_DEFINE_bool(name, default_value, help_str)
//   C10_GFLAGS_DEF_WRAPPER(bool, bool, name, default_value, help_str)
// #define C10_DEFINE_string(name, default_value, help_str)
//   C10_GFLAGS_DEF_WRAPPER(string, ::fLS::clstring, name, default_value, help_str)

// DECLARE_typed_var should be used in header files and in the global namespace.
// #define C10_GFLAGS_DECLARE_WRAPPER(type, real_type, name)
//   DECLARE_##type(name);                                   

// #define C10_DECLARE_int(name)
//   C10_GFLAGS_DECLARE_WRAPPER(int32, gflags::int32, name)
// #define C10_DECLARE_int32(name) C10_DECLARE_int(name)
// #define C10_DECLARE_int64(name)
//   C10_GFLAGS_DECLARE_WRAPPER(int64, gflags::int64, name)
// #define C10_DECLARE_double(name)
//   C10_GFLAGS_DECLARE_WRAPPER(double, double, name)
// #define C10_DECLARE_bool(name) C10_GFLAGS_DECLARE_WRAPPER(bool, bool, name)
// #define C10_DECLARE_string(name)
//   C10_GFLAGS_DECLARE_WRAPPER(string, ::fLS::clstring, name)
// Targeting ../C10FlagParser.java





 // namespace c10

// The macros are defined outside the c10 namespace. In your code, you should
// write the C10_DEFINE_* and C10_DECLARE_* macros outside any namespace
// as well.

// #define C10_DEFINE_typed_var(type, name, default_value, help_str)
//   C10_EXPORT type FLAGS_##name = default_value;
//   namespace c10 {
//   namespace {
//   class C10FlagParser_##name : public C10FlagParser {
//    public:
//     explicit C10FlagParser_##name(const std::string& content) {
//       success_ = C10FlagParser::Parse<type>(content, &FLAGS_##name);
//     }
//   };
//   }
//   RegistererC10FlagsRegistry g_C10FlagsRegistry_##name(
//       #name,
//       C10FlagsRegistry(),
//       RegistererC10FlagsRegistry::DefaultCreator<C10FlagParser_##name>,
//       "(" #type ", default " #default_value ") " help_str);
//   }

// #define C10_DEFINE_int(name, default_value, help_str)
//   C10_DEFINE_typed_var(int, name, default_value, help_str)
// #define C10_DEFINE_int32(name, default_value, help_str)
//   C10_DEFINE_int(name, default_value, help_str)
// #define C10_DEFINE_int64(name, default_value, help_str)
//   C10_DEFINE_typed_var(int64_t, name, default_value, help_str)
// #define C10_DEFINE_double(name, default_value, help_str)
//   C10_DEFINE_typed_var(double, name, default_value, help_str)
// #define C10_DEFINE_bool(name, default_value, help_str)
//   C10_DEFINE_typed_var(bool, name, default_value, help_str)
// #define C10_DEFINE_string(name, default_value, help_str)
//   C10_DEFINE_typed_var(std::string, name, default_value, help_str)

// DECLARE_typed_var should be used in header files and in the global namespace.
// #define C10_DECLARE_typed_var(type, name) C10_IMPORT extern type FLAGS_##name

// #define C10_DECLARE_int(name) C10_DECLARE_typed_var(int, name)
// #define C10_DECLARE_int32(name) C10_DECLARE_int(name)
// #define C10_DECLARE_int64(name) C10_DECLARE_typed_var(int64_t, name)
// #define C10_DECLARE_double(name) C10_DECLARE_typed_var(double, name)
// #define C10_DECLARE_bool(name) C10_DECLARE_typed_var(bool, name)
// #define C10_DECLARE_string(name) C10_DECLARE_typed_var(std::string, name)

////////////////////////////////////////////////////////////////////////////////
// End non-gflags section.
////////////////////////////////////////////////////////////////////////////////

// #endif // C10_USE_GFLAGS

// #endif // C10_UTIL_FLAGS_H_


// Parsed from c10/util/Logging.h

// #ifndef C10_UTIL_LOGGING_H_
// #define C10_UTIL_LOGGING_H_

// #include <climits>
// #include <exception>
// #include <functional>
// #include <limits>
// #include <sstream>

// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Flags.h>
// #include <c10/util/StringUtil.h>

// CAFFE2_LOG_THRESHOLD is a compile time flag that would allow us to turn off
// logging at compile time so no logging message below that level is produced
// at all. The value should be between INT_MIN and CAFFE_FATAL.
// #ifndef CAFFE2_LOG_THRESHOLD
// If we have not defined the compile time log threshold, we keep all the
// log cases.
public static native @MemberGetter int CAFFE2_LOG_THRESHOLD();
public static final int CAFFE2_LOG_THRESHOLD = CAFFE2_LOG_THRESHOLD();
// #endif // CAFFE2_LOG_THRESHOLD

// Below are different implementations for glog and non-glog cases.
// #ifdef C10_USE_GLOG
// #include <c10/util/logging_is_google_glog.h>
// #else // !C10_USE_GLOG
// #include <c10/util/logging_is_not_google_glog.h>
// #endif // C10_USE_GLOG




// Some versions of GLOG support less-spammy version of LOG_EVERY_MS. If it's
// not available - just short-circuit to the always working one one.
// We define the C10_ name to avoid confusing other files
// #ifdef LOG_EVERY_MS
// #define C10_LOG_EVERY_MS(severity, ms) LOG_EVERY_MS(severity, ms)
// #else
// #define C10_LOG_EVERY_MS(severity, ms) LOG(severity)
// #endif

// Same for LOG_FIRST_N
// #ifdef LOG_FIRST_N
// #define C10_LOG_FIRST_N(severity, n) LOG_FIRST_N(severity, n)
// #else
// #define C10_LOG_FIRST_N(severity, n) LOG(severity)
// #endif

// Same for LOG_EVERY_N
// #ifdef LOG_EVERY_N
// #define C10_LOG_EVERY_N(severity, n) LOG_EVERY_N(severity, n)
// #else
// #define C10_LOG_EVERY_N(severity, n) LOG(severity)
// #endif

// Functions that we use for initialization.
@Namespace("c10") public static native @Cast("bool") boolean InitCaffeLogging(IntPointer argc, @Cast("char**") PointerPointer argv);
@Namespace("c10") public static native @Cast("bool") boolean InitCaffeLogging(IntPointer argc, @Cast("char**") @ByPtrPtr BytePointer argv);
@Namespace("c10") public static native @Cast("bool") boolean InitCaffeLogging(IntBuffer argc, @Cast("char**") @ByPtrPtr ByteBuffer argv);
@Namespace("c10") public static native @Cast("bool") boolean InitCaffeLogging(int[] argc, @Cast("char**") @ByPtrPtr byte[] argv);
@Namespace("c10") public static native void UpdateLoggingLevelsFromFlags();

@Namespace("c10") public static native void ThrowEnforceNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @StdString BytePointer msg,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @StdString BytePointer msg);
@Namespace("c10") public static native void ThrowEnforceNotMet(
    String file,
    int line,
    String condition,
    @StdString String msg,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceNotMet(
    String file,
    int line,
    String condition,
    @StdString String msg);

@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @StdString BytePointer msg,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    @Cast("const char*") BytePointer file,
    int line,
    @Cast("const char*") BytePointer condition,
    @StdString BytePointer msg);
@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    String file,
    int line,
    String condition,
    @StdString String msg,
    @Const Pointer caller/*=nullptr*/);
@Namespace("c10") public static native void ThrowEnforceFiniteNotMet(
    String file,
    int line,
    String condition,
    @StdString String msg);

@Namespace("c10") public static native @Cast("const bool") boolean IsUsingGoogleLogging();

/**
 * A utility to allow one to show log info to stderr after the program starts.
 *
 * This is similar to calling GLOG's --logtostderr, or setting caffe2_log_level
 * to smaller than INFO. You are recommended to only use this in a few sparse
 * cases, such as when you want to write a tutorial or something. Normally, use
 * the commandline flags to set the log level.
 */
@Namespace("c10") public static native void ShowLogInfoToStderr();

@Namespace("c10") public static native void SetStackTraceFetcher(@ByVal Fetcher fetcher);

// #define CAFFE_ENFORCE(condition, ...)
//   do {
//     if (C10_UNLIKELY(!(condition))) {
//       ::c10::ThrowEnforceNotMet(
//           __FILE__, __LINE__, #condition, ::c10::str(__VA_ARGS__));
//     }
//   } while (false)

// #define CAFFE_ENFORCE_FINITE(condition, ...)
//     do {
//       if (C10_UNLIKELY(!(condition))) {
//         ::c10::ThrowEnforceFiniteNotMet(
//           __FILE__, __LINE__, #condition, ::c10::str(__VA_ARGS__));
//       }
//     } while (false)

// #define CAFFE_ENFORCE_WITH_CALLER(condition, ...)
//   do {
//     if (C10_UNLIKELY(!(condition))) {
//       ::c10::ThrowEnforceNotMet(
//           __FILE__, __LINE__, #condition, ::c10::str(__VA_ARGS__), this);
//     }
//   } while (false)

// #define CAFFE_THROW(...)
//   ::c10::ThrowEnforceNotMet(__FILE__, __LINE__, "", ::c10::str(__VA_ARGS__))
// Targeting ../EnforceOK.java



// #define BINARY_COMP_HELPER(name, op)
//   template <typename T1, typename T2>
//   inline EnforceFailMessage name(const T1& x, const T2& y) {
//     if (x op y) {
//       return EnforceOK();
//     }
//     return c10::str(x, " vs ", y);
//   }
// #undef BINARY_COMP_HELPER

// #define CAFFE_ENFORCE_THAT_IMPL(condition, expr, ...)
//   do {
//     using namespace ::c10::enforce_detail;
//     const EnforceFailMessage& CAFFE_ENFORCE_THAT_IMPL_r_ = (condition);
//     if (C10_UNLIKELY(CAFFE_ENFORCE_THAT_IMPL_r_.bad())) {
//       ::c10::ThrowEnforceNotMet(
//           __FILE__,
//           __LINE__,
//           expr,
//           CAFFE_ENFORCE_THAT_IMPL_r_.get_message_and_free(
//               ::c10::str(__VA_ARGS__)));
//     }
//   } while (false)

// #define CAFFE_ENFORCE_THAT_IMPL_WITH_CALLER(condition, expr, ...)
//   do {
//     using namespace ::c10::enforce_detail;
//     const EnforceFailMessage& CAFFE_ENFORCE_THAT_IMPL_WITH_CALLER_r_ =
//         (condition);
//     if (C10_UNLIKELY(CAFFE_ENFORCE_THAT_IMPL_WITH_CALLER_r_.bad())) {
//       ::c10::ThrowEnforceNotMet(
//           __FILE__,
//           __LINE__,
//           expr,
//           CAFFE_ENFORCE_THAT_IMPL_WITH_CALLER_r_.get_message_and_free(
//               ::c10::str(__VA_ARGS__)),
//           this);
//     }
//   } while (false)
 // namespace enforce_detail

// #define CAFFE_ENFORCE_THAT(condition, ...)
//   CAFFE_ENFORCE_THAT_IMPL((condition), #condition, __VA_ARGS__)

// #define CAFFE_ENFORCE_EQ(x, y, ...)
//   CAFFE_ENFORCE_THAT_IMPL(Equals((x), (y)), #x " == " #y, __VA_ARGS__)
// #define CAFFE_ENFORCE_NE(x, y, ...)
//   CAFFE_ENFORCE_THAT_IMPL(NotEquals((x), (y)), #x " != " #y, __VA_ARGS__)
// #define CAFFE_ENFORCE_LE(x, y, ...)
//   CAFFE_ENFORCE_THAT_IMPL(LessEquals((x), (y)), #x " <= " #y, __VA_ARGS__)
// #define CAFFE_ENFORCE_LT(x, y, ...)
//   CAFFE_ENFORCE_THAT_IMPL(Less((x), (y)), #x " < " #y, __VA_ARGS__)
// #define CAFFE_ENFORCE_GE(x, y, ...)
//   CAFFE_ENFORCE_THAT_IMPL(GreaterEquals((x), (y)), #x " >= " #y, __VA_ARGS__)
// #define CAFFE_ENFORCE_GT(x, y, ...)
//   CAFFE_ENFORCE_THAT_IMPL(Greater((x), (y)), #x " > " #y, __VA_ARGS__)
// #define CAFFE_ENFORCE_EQ_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_THAT_IMPL_WITH_CALLER(
//       Equals((x), (y)), #x " == " #y, __VA_ARGS__)
// #define CAFFE_ENFORCE_NE_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_THAT_IMPL_WITH_CALLER(
//       NotEquals((x), (y)), #x " != " #y, __VA_ARGS__)
// #define CAFFE_ENFORCE_LE_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_THAT_IMPL_WITH_CALLER(
//       LessEquals((x), (y)), #x " <= " #y, __VA_ARGS__)
// #define CAFFE_ENFORCE_LT_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_THAT_IMPL_WITH_CALLER(Less((x), (y)), #x " < " #y, __VA_ARGS__)
// #define CAFFE_ENFORCE_GE_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_THAT_IMPL_WITH_CALLER(
//       GreaterEquals((x), (y)), #x " >= " #y, __VA_ARGS__)
// #define CAFFE_ENFORCE_GT_WITH_CALLER(x, y, ...)
//   CAFFE_ENFORCE_THAT_IMPL_WITH_CALLER(
//       Greater((x), (y)), #x " > " #y, __VA_ARGS__)

/**
 * Very lightweight logging for the first time API usage. It's beneficial for
 * tracking of individual functionality usage in larger applications.
 *
 * In order to ensure light-weightedness of logging, we utilize static variable
 * trick - LogAPIUsage will be invoked only once and further invocations will
 * just do an atomic check.
 *
 * Example:
 *   // Logs caller info with an arbitrary text event, if there is a usage.
 *   C10_LOG_API_USAGE_ONCE("my_api");
 */
// #define C10_LOG_API_USAGE_ONCE(...)
//   C10_UNUSED static bool C10_ANONYMOUS_VARIABLE(logFlag) =
//       ::c10::detail::LogAPIUsageFakeReturn(__VA_ARGS__);

// API usage logging capabilities
@Namespace("c10") public static native void SetAPIUsageLogger(@ByVal Logger logger);
@Namespace("c10") public static native void LogAPIUsage(@StdString BytePointer context);
@Namespace("c10") public static native void LogAPIUsage(@StdString String context);
// Targeting ../DDPLoggingData.java



@Namespace("c10") public static native void SetPyTorchDDPUsageLogger(@ByVal DataLogger logger);
@Namespace("c10") public static native void LogPyTorchDDPUsage(@Const @ByRef DDPLoggingData ddpData);
// Return value is needed to do the static variable initialization trick
@Namespace("c10::detail") public static native @Cast("bool") boolean LogAPIUsageFakeReturn(@StdString BytePointer context);
@Namespace("c10::detail") public static native @Cast("bool") boolean LogAPIUsageFakeReturn(@StdString String context);


 // namespace c10

// #endif // C10_UTIL_LOGGING_H_


// Parsed from c10/core/DeviceType.h

// #pragma once

// This is directly synchronized with caffe2/proto/caffe2.proto, but
// doesn't require me to figure out how to get Protobuf headers into
// ATen/core (which would require a lot more build system hacking.)
// If you modify me, keep me synchronized with that file.

// #include <c10/macros/Macros.h>

// #include <ostream>
// #include <functional>

@Namespace("c10") public enum DeviceType {
  CPU((byte)(0)),
  CUDA((byte)(1)), // CUDA.
  MKLDNN((byte)(2)), // Reserved for explicit MKLDNN
  OPENGL((byte)(3)), // OpenGL
  OPENCL((byte)(4)), // OpenCL
  IDEEP((byte)(5)), // IDEEP.
  HIP((byte)(6)), // AMD HIP
  FPGA((byte)(7)), // FPGA
  MSNPU((byte)(8)), // MSNPU
  XLA((byte)(9)), // XLA / TPU
  Vulkan((byte)(10)), // Vulkan
  Metal((byte)(11)), // Metal
  XPU((byte)(12)), // XPU
  // NB: If you add more devices:
  //  - Change the implementations of DeviceTypeName and isValidDeviceType
  //    in DeviceType.cpp
  //  - Change the number below
  COMPILE_TIME_MAX_DEVICE_TYPES((byte)(13));

    public final byte value;
    private DeviceType(byte v) { this.value = v; }
    private DeviceType(DeviceType e) { this.value = e.value; }
    public DeviceType intern() { for (DeviceType e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") @MemberGetter public static native DeviceType kCPU();
@Namespace("c10") @MemberGetter public static native DeviceType kCUDA();
@Namespace("c10") @MemberGetter public static native DeviceType kHIP();
@Namespace("c10") @MemberGetter public static native DeviceType kFPGA();
@Namespace("c10") @MemberGetter public static native DeviceType kMSNPU();
@Namespace("c10") @MemberGetter public static native DeviceType kXLA();
@Namespace("c10") @MemberGetter public static native DeviceType kVulkan();
@Namespace("c10") @MemberGetter public static native DeviceType kMetal();
@Namespace("c10") @MemberGetter public static native DeviceType kXPU();

// define explicit int constant
@Namespace("c10") @MemberGetter public static native int COMPILE_TIME_MAX_DEVICE_TYPES();

@Namespace("c10") public static native @StdString BytePointer DeviceTypeName(
    DeviceType d,
    @Cast("bool") boolean lower_case/*=false*/);
@Namespace("c10") public static native @StdString BytePointer DeviceTypeName(
    DeviceType d);
@Namespace("c10") public static native @StdString String DeviceTypeName(
    @Cast("c10::DeviceType") byte d,
    @Cast("bool") boolean lower_case/*=false*/);
@Namespace("c10") public static native @StdString String DeviceTypeName(
    @Cast("c10::DeviceType") byte d);

@Namespace("c10") public static native @Cast("bool") boolean isValidDeviceType(DeviceType d);
@Namespace("c10") public static native @Cast("bool") boolean isValidDeviceType(@Cast("c10::DeviceType") byte d);




// Targeting ../DeviceTypeHash.java


 // namespace std



// Parsed from c10/core/Device.h

// #pragma once

// #include <c10/core/DeviceType.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>

// #include <cstddef>
// #include <functional>
// #include <iosfwd>
// #include <string>

/** An index representing a specific device; e.g., the 1 in GPU 1.
 *  A DeviceIndex is not independently meaningful without knowing
 *  the DeviceType it is associated; try to use Device rather than
 *  DeviceIndex directly. */
// Targeting ../Device.java






// Targeting ../DeviceHash.java


 // namespace std


// Parsed from c10/core/DeviceGuard.h

// #pragma once

// #include <c10/core/impl/InlineDeviceGuard.h>
// Targeting ../DeviceGuard.java


// Targeting ../OptionalDeviceGuard.java



// Note [Whither the DeviceGuard boilerplate]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Design note: in principle, we could avoid these wrappers using:
//
// using DeviceGuard = impl::InlineDeviceGuard<impl::VirtualGuardImpl>;
// using OptionalDeviceGuard = impl::InlineOptionalDeviceGuard<impl::VirtualGuardImpl>;
//
// But the error messages are worse, and our users can't just look at the
// header file to find out what's going on.  Furthermore, for specializations
// like CUDAStreamGuard, it can be profitable to replace some interfaces with
// refined types (e.g., return CUDAStream instead of Stream).  So, we eat
// the boilerplate and write out the API explicitly.

 // namespace c10


// Parsed from c10/core/DispatchKey.h

// #pragma once

// #include <vector>
// #include <iostream>
// #include <string>
// #include <c10/macros/Macros.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>

// Semantically, a dispatch key identifies a possible "level" in our
// dispatch, for which a handler may be registered.  Traditional
// backends like CPU and CUDA get dispatch keys; however, so do
// "wrapping" layers like Variable (for autograd handling).
//
// In implementation terms, the dispatch key identifies a specific "bit" in a
// DispatchKeySet.  Higher bit indexes get handled by dispatching first (because
// we "count leading zeros" when we extract the highest priority dispatch
// key.)
//
// NOTE: Keep the list in sync with `DispatchKey` in tools/codegen/model.py
@Namespace("c10") public enum DispatchKey {

  // ~~~~~~~~~~~~~~~~~~~~~~~~~~ UNDEFINED ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ //
  // This is not a "real" tensor id, but it exists to give us a "nullopt"
  // element we can return for cases when a DispatchKeySet contains no elements.
  // You can think a more semantically accurate definition of DispatchKey is:
  //
  //    using DispatchKey = optional<RealDispatchKey>
  //
  // and Undefined == nullopt.  We didn't actually represent
  // it this way because optional<RealDispatchKey> would take two
  // words, when DispatchKey fits in eight bits.

  Undefined((byte)(0)),

  // Define an alias for Undefined to represent CatchAll (long term
  // this will get eliminated, but for now it's convenient)
  CatchAll((byte)(Undefined.value)),

  // ~~~~~~~~~~~~~~~~~~~~~~~~~~ BACKENDS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ //
  // A "backend" is colloquially used to refer to handlers for dispatch
  // which actually implement the numerics of an operation in question.
  //
  // Due to the nature of the enum, these backends are specified in
  // an ordered way, but for most backends this order is not semantically
  // meaningful (e.g., it's valid to reorder these backends without changing
  // semantics).  The only situation when backend ordering is meaningful
  // is when the backend participates in multiple dispatch with another
  // backend; e.g., CPU and SparseCPU (sparse must have
  // higher priority).

  // Here are backends which you think of as traditionally specifying
  // how to implement operations on some device.
  CPU((byte)(Undefined.value + 1)), // registered at build/aten/src/ATen/RegisterCPU.cpp
  CUDA((byte)(Undefined.value + 2)), // registered at build/aten/src/ATen/RegisterCUDA.cpp
  HIP((byte)(Undefined.value + 3)), // NB: I think this is not actually used, due to Note [Masquerading as
  // CUDA]
  FPGA((byte)(Undefined.value + 4)), // Xilinx support lives out of tree at
        // https://gitlab.com/pytorch-complex/vitis_kernels
  MSNPU((byte)(Undefined.value + 5)), // unused externally, but tested at
  // test/cpp_extensions/msnpu_extension.cpp
  XLA((byte)(Undefined.value + 6)), // lives out of tree at https://github.com/pytorch/xla
  Vulkan((byte)(Undefined.value + 7)),
  Metal((byte)(Undefined.value + 8)),
  XPU((byte)(Undefined.value + 9)), // For out of tree Intel's heterogeneous computing plug-in

  // These are Caffe2 device types which we grandfathered into
  // DispatchKey.
  // TODO: Caffe2-only DispatchKeys actually should be removed from this enum
  // and just simply be undispatchable.
  MKLDNN((byte)(Undefined.value + 10)), // (MKLDNN is treated as another "device" in Caffe2)
  OpenGL((byte)(Undefined.value + 11)),
  OpenCL((byte)(Undefined.value + 12)),
  IDEEP((byte)(Undefined.value + 13)),

  // Here are backends which specify more specialized operators
  // based on the dtype of the tensor.
  QuantizedCPU((byte)(Undefined.value + 14)), // registered at build/aten/src/ATen/RegisterQuantizedCPU.cpp
  QuantizedCUDA((byte)(Undefined.value + 15)), // registered at build/aten/src/ATen/RegisterQuantizedCUDA.cpp
  QuantizedXPU((byte)(Undefined.value + 16)), // For out of tree Intel's heterogeneous computing plug-in
  ComplexCPU((byte)(Undefined.value + 17)), // lives out of tree at
  // https://gitlab.com/pytorch-complex/pytorch-cpu-strided-complex
  ComplexCUDA((byte)(Undefined.value + 18)), // and
  // https://gitlab.com/pytorch-complex/pytorch-cuda-strided-complex
  // tested at test/cpp_extensions/complex_registration_extension.cpp
  // TODO: Remove Complex dispatch keys when Complex is moved in tree

  // This backend is to support custom RNGs; it lets you go
  // to a different kernel if you pass in a generator that is not a
  // traditional CPUGeneratorImpl/CUDAGeneratorImpl.  To make use of this
  // key:
  //  1) set it as a second parameter of at::Generator constructor call in
  //     the user-defined PRNG class.
  //  2) use it as a dispatch key while registering custom kernels
  //     (templatized kernels specialized for user-defined PRNG class)
  // intended for out of tree use; tested by aten/src/ATen/test/rng_test.cpp
  CustomRNGKeyId((byte)(Undefined.value + 19)),

  // Here are backends which specify more specialized operators
  // based on the layout of the tensor.  Note that the sparse backends
  // are one case where ordering matters: sparse multi-dispatches with
  // the corresponding dense tensors, and must be handled before them.
  MkldnnCPU((byte)(Undefined.value + 20)), // registered at build/aten/src/ATen/RegisterMkldnnCPU.cpp
  // NB: not to be confused with MKLDNN, which is Caffe2 only
  SparseCPU((byte)(Undefined.value + 21)), // registered at build/aten/src/ATen/RegisterSparseCPU.cpp
  SparseCUDA((byte)(Undefined.value + 22)), // registered at build/aten/src/ATen/RegisterSparseCUDA.cpp
  SparseHIP((byte)(Undefined.value + 23)), // TODO: I think this is not actually used, due to Note
  // [Masquerading as CUDA]
  SparseXPU((byte)(Undefined.value + 24)), // For out of tree Intel's heterogeneous computing plug-in

  NestedTensor((byte)(Undefined.value + 25)), // lives out of tree at https://github.com/pytorch/nestedtensor
  // Here are reserved backends for user-defined backends, see Note [Private use
  // DispatchKey]
  // To see some example about how to use this, check out MSNPU
  PrivateUse1((byte)(Undefined.value + 26)),
  PrivateUse2((byte)(Undefined.value + 27)),
  PrivateUse3((byte)(Undefined.value + 28)),

  // Define an alias key to represent end of backend dispatch keys.
  // If you add new backend keys after PrivateUse3, please also update it here.
  EndOfBackendKeys((byte)(PrivateUse3.value)),

  // The meta function characterizes how an operation affects the metadata of a
  // tensor (shape, dtype) without doing any of the actual computation.  A
  // meta tensor can be used to dry run operators without actually doing
  // any computation, e.g., add on two meta tensors would give you another
  // meta tensor with the output shape and dtype, but wouldn't actually
  // add anything.  A meta implementation typically would look something like:
  //
  //  Tensor meta::add(const Tensor& self, const Tensor& other) {
  //    TORCH_CHECK(self.size().equals(other.size()));
  //    return at::empty_like(self, self.size());
  //  }
  //
  // The meta function would get invoked if you ran an operator passing
  // in meta tensors.  The call stack in such a case would look something like
  // this:
  //
  //  at::add(x: Meta, y: Meta) {
  //    return [dispatch] meta::add(x: Meta, y: Meta) {
  //      output_shape = ...
  //      [dispatch] meta::empty(output_shape) {
  //        return ... meta tensor with output_shape but no data allocated ...
  //      }
  //    }
  //  }
  //
  // Meta functions have an important secondary function, which is they can
  // be used as tensor "allocators".  A typical backend implementation should
  // be implemented in this way:
  //
  //  Tensor cpu::add(const Tensor& self, const Tensor& other) {
  //    Tensor result = meta::add(self, other);
  //    // ... do the actual computation into result ...
  //    return result;
  //  }
  //
  // In this case, the internal at::empty_like invocation would dispatch to the
  // CPU factory function, not the meta factory function.  The call stack in
  // this case looks like:
  //
  //  at::add(x: CPU, y: CPU) {
  //    return [dispatch] cpu::add(x: CPU, y: CPU) {
  //      output = [direct] meta::add(x: CPU, y: CPU) {
  //        output_shape = ...
  //        [dispatch] cpu::empty(output_shape)
  //      }
  //      ... compute on output ...
  //      return output;
  //    }
  //  }
  //
  Meta((byte)(PrivateUse3.value + 1)),

  // In some situations, it is not immediately obvious what the correct
  // backend for function is, because the function in question doesn't
  // have any "tensor" arguments.  In this case, a BackendSelect function
  // can be registered to implement the custom determination of the
  // correct backend.
  BackendSelect((byte)(PrivateUse3.value + 2)),

  // The named dispatch key is set for any tensors with named dimensions.
  // Although we have a dispatch key for named tensors, for historical reasons,
  // this dispatch key doesn't do any of the substantive functionality for named
  // tensor (though, hypothetically, it could!)  At the moment, it's just
  // responsible for letting us give good error messages when operations
  // don't support named tensors.
  //
  // NB: If you ever consider moving named tensor functionality into
  // this dispatch key, note that it might be necessary add another dispatch
  // key that triggers before composite operators, in case a composite operator
  // has named dimension propagation that doesn't match that of its
  // constituent parts.
  Named((byte)(PrivateUse3.value + 3)),

  // Note [Alias Dispatch Key : Autograd]
  // All backends are oblivious to autograd; autograd is handled as a
  // layer which happens on top of all backends. It inspects the autograd
  // metadata of all inputs, determines what autograd metadata should be
  // constructed by the output, and otherwise defers to the backend to
  // actually do the numeric computation.  Autograd contains
  // the bulk of this logic.

  // Autograd is now an alias dispatch key which by default maps to all
  // backend-specific autograd keys.
  // Backend-specific allow backends to override the default kernel registered
  // to Autograd key as needed.
  // For example, XLA wants to define autograd for einsum directly.
  // Registering a custom autograd implementation at the XLA key won't work
  // because we process Autograd before XLA.  This key has higher priority and
  // gets processed first.  You generally should NOT redispatch after handling
  // autograd here (since that would result in execution of the Autograd
  // operator, which you're trying to skip).  In AutogradXLA implementations,
  // you are responsible for handling autograd yourself, or deferring to other
  // operators which support autograd.

  // Currently we only have backend-specific autograd keys for CPU/CUDA/XLA and
  // reserved user-defined backends. All other in-tree backends share the
  // AutogradOther key. We can add specific autograd key for those backends
  // upon request.
  AutogradOther((byte)(PrivateUse3.value + 4)),
  AutogradCPU((byte)(PrivateUse3.value + 5)),
  AutogradCUDA((byte)(PrivateUse3.value + 6)),
  AutogradXLA((byte)(PrivateUse3.value + 7)),
  AutogradNestedTensor((byte)(PrivateUse3.value + 8)), // lives out of tree at
                        // https://github.com/pytorch/nestedtensor
  AutogradXPU((byte)(PrivateUse3.value + 9)),
  // Here are some reserved pre-autograd keys for user-defined backends, see
  // Note [Private use DispatchKey]
  AutogradPrivateUse1((byte)(PrivateUse3.value + 10)),
  AutogradPrivateUse2((byte)(PrivateUse3.value + 11)),
  AutogradPrivateUse3((byte)(PrivateUse3.value + 12)),

  Tracer((byte)(PrivateUse3.value + 13)),

  // Autocasting precedes VariableTypeId, to ensure casts are autograd-exposed
  // and inputs are saved for backward in the post-autocast type.
  Autocast((byte)(PrivateUse3.value + 14)),

  // ~~~~~~~~~~~~~~~~~~~~~~~~~~~ WRAPPERS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ //
  // There are a number of alternative modes which may want to handle before
  // autograd; for example, error checking, tracing, profiling or vmap.  They
  // go here.

  // This is the dispatch key for BatchedTensorImpl, which is used to implement
  // batching rules for vmap.
  Batched((byte)(PrivateUse3.value + 15)),

  // When we are inside a vmap, all tensors dispatch on this key.
  // See Note: [DispatchKey::VmapMode usage] for more details.
  VmapMode((byte)(PrivateUse3.value + 16)),

  // TESTING: This is intended to be a generic testing tensor type id.
  // Don't use it for anything real; its only acceptable use is within a single
  // process test.  Use it by creating a TensorImpl with this DispatchKey, and
  // then registering operators to operate on this type id.  See
  // aten/src/ATen/core/dispatch/backend_fallback_test.cpp for a usage example.
  TESTING_ONLY_GenericWrapper((byte)(PrivateUse3.value + 17)),

  // TESTING: This is intended to be a generic testing tensor type id.
  // Don't use it for anything real; its only acceptable use is within a ingle
  // process test.  Use it by toggling the mode on and off via
  // TESTING_ONLY_tls_generic_mode_set_enabled and then registering operators
  // to operate on this type id.  See
  // aten/src/ATen/core/dispatch/backend_fallback_test.cpp
  // for a usage example
  TESTING_ONLY_GenericMode((byte)(PrivateUse3.value + 18)),

  // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FIN ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ //
  NumDispatchKeys((byte)(PrivateUse3.value + 19)), // Sentinel, end of runtime keys.

  // ~~~~~~~~~~~~~~~~~~~~~~ Alias Dispatch Keys ~~~~~~~~~~~~~~~~~~~~~~~~~~ //
  // Alias dispatch keys are synthetic dispatch keys which map to multiple
  // runtime dispatch keys. Alisa keys have precedence, but they are always
  // lower precedence than runtime keys. You can register a kernel to an
  // alias key, the kernel might be populated to the mapped runtime keys
  // during dispatch table computation.
  // If a runtime dispatch key has multiple kernels from alias keys, which
  // kernel wins is done based on the precedence of alias keys (but runtime
  // keys always have precedence over alias keys).
  // Alias keys won't be directly called during runtime.

  // See Note [Alias Dispatch Key : Autograd]
  Autograd((byte)(PrivateUse3.value + 20)),
  Math((byte)(PrivateUse3.value + 21)), // registered at build/aten/src/ATen/RegisterMath.cpp
  DefaultBackend((byte)(PrivateUse3.value + 22)), // registered at
                  // build/aten/src/ATen/RegisterDefaultBackend.cpp

  // Define an alias key to represent end of alias dispatch keys.
  // If you add new alias keys after Autograd, please also update it here.
  EndOfAliasKeys((byte)(DefaultBackend.value)), //

  // ~~~~~~~~~~~~~~~~~~~~~~~~~ BC ALIASES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ //
  // The aliases exist for backwards compatibility reasons, they shouldn't
  // be used
  CPUTensorId((byte)(CPU.value)),
  CUDATensorId((byte)(CUDA.value)),
  PrivateUse1_PreAutograd((byte)(AutogradPrivateUse1.value)),
  PrivateUse2_PreAutograd((byte)(AutogradPrivateUse2.value)),
  PrivateUse3_PreAutograd((byte)(AutogradPrivateUse3.value));

    public final byte value;
    private DispatchKey(byte v) { this.value = v; }
    private DispatchKey(DispatchKey e) { this.value = e.value; }
    public DispatchKey intern() { for (DispatchKey e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

// Note [Private use DispatchKey]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Private use tensor IDs are preallocated tensor type IDs for use in user
// applications.  Similar to private use fields in HTTP, they can be used
// by end users for experimental or private applications, without needing
// to "standardize" the tensor ID (which would be done by submitting a PR
// to PyTorch to add your type ID).
//
// Private use tensor IDs are appropriate to use if you want to experiment
// with adding a new tensor type (without having to patch PyTorch first) or
// have a private, non-distributed application that needs to make use of a
// new tensor type.  Private use tensor IDs are NOT appropriate to use for
// libraries intended to be distributed to further users: please contact
// the PyTorch developers to get a type ID registered in this case.
//
// We provide two classes of private user tensor id: regular DispatchKeys
// and Autograd DispatchKeys.  DispatchKeys serve the role of ordinary "backend"
// DispatchKeys; if you were adding support for a new type of accelerator, you
// would use a backend DispatchKey, and ideally automatically reuse AutogradOther
// definitions already defined in PyTorch.  AutogradPrivateUse DispatchKeys serve
// as "wrapper" DispatchKeys: they are only necessary for tensors that compose
// multiple internal tensors, and for cases when the built-in autograd formulas
// for operators are not appropriate.

@Namespace("c10") public static native @Cast("const char*") BytePointer toString(DispatchKey arg0);
@Namespace("c10") public static native String toString(@Cast("c10::DispatchKey") byte arg0);


@Namespace("c10") public static native DispatchKey getAutogradKeyFromBackend(DispatchKey t);
@Namespace("c10") public static native @Cast("c10::DispatchKey") byte getAutogradKeyFromBackend(@Cast("c10::DispatchKey") byte t);

// These are some convenience identifiers for dispatch keys which are
// shorter to type than their long counterparts.  Note that some of these
// dispatch keys directly correspond to DeviceType; and most APIs that
// accept DispatchKey also accept DeviceType; e.g.,
// torch::dispatch(torch::kCPU, ...) is also valid.
@Namespace("c10") @MemberGetter public static native DispatchKey kAutograd();

// Check if a DispatchKey is an alias mapping to other runtime keys.
@Namespace("c10") public static native @Cast("bool") boolean isAliasDispatchKey(DispatchKey k);
@Namespace("c10") public static native @Cast("bool") boolean isAliasDispatchKey(@Cast("c10::DispatchKey") byte k);
 // namespace c10
  // Expose the constant, but not the TYPE (DispatchKey is an implementation
  // detail!)


// NB: You really shouldn't use this instance; this enum is guaranteed
// to be pretty small so a regular array should be acceptable.



// Parsed from c10/core/DispatchKeySet.h

// #pragma once

// #include <c10/core/DispatchKey.h>
// #include <c10/util/llvmMathExtras.h>
// #include <c10/util/Exception.h>
// #include <ostream>
// Targeting ../DispatchKeySet.java



@Namespace("c10") public static native @StdString BytePointer toString(@ByVal DispatchKeySet arg0);


// autograd_dispatch_keyset should include all runtime autograd keys.
// Alias key DispatchKey::Autograd maps to autograd_dispatch_keyset.
// NB: keys in this set also get associated with Math
@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet autograd_dispatch_keyset();

// backend dispatch keys that map to DispatchKey::AutogradOther
// NB: keys in this set also get associated with Math
@Namespace("c10") @MemberGetter public static native @Const @ByRef DispatchKeySet autogradother_backends();

// true if t is a backend dispatch key
@Namespace("c10") public static native @Cast("bool") boolean isBackendDispatchKey(DispatchKey t);
@Namespace("c10") public static native @Cast("bool") boolean isBackendDispatchKey(@Cast("c10::DispatchKey") byte t);

// Resolve alias dispatch key to DispatchKeySet if applicable
@Namespace("c10") public static native @ByVal DispatchKeySet getRuntimeDispatchKeySet(DispatchKey t);
@Namespace("c10") public static native @ByVal DispatchKeySet getRuntimeDispatchKeySet(@Cast("c10::DispatchKey") byte t);

// Returns a DispatchKeySet of all backend keys mapped to Autograd dispatch key t,
// DispatchKeySet is empty if t is not alias of DispatchKey::Autograd.
@Namespace("c10") public static native @ByVal DispatchKeySet getBackendKeySetFromAutograd(DispatchKey t);
@Namespace("c10") public static native @ByVal DispatchKeySet getBackendKeySetFromAutograd(@Cast("c10::DispatchKey") byte t);

// This API exists because we have a use case for checking
// getRuntimeDispatchKeySet(alias).has(DispatchKey::Undefined)
// in OperatorEntry.cpp but we disallow it in has() API.
@Namespace("c10") public static native @Cast("bool") boolean isIncludedInAlias(DispatchKey k, DispatchKey alias);
@Namespace("c10") public static native @Cast("bool") boolean isIncludedInAlias(@Cast("c10::DispatchKey") byte k, @Cast("c10::DispatchKey") byte alias);

// Historically, every tensor only had a single DispatchKey, and it was always
// something like CPU, and there wasn't any of this business where TLS
// could cause the DispatchKey of a tensor to change.  But we still have some
// legacy code that is still using DispatchKey for things like instanceof
// checks; if at all possible, refactor the code to stop using DispatchKey in
// those cases.
@Namespace("c10") public static native DispatchKey legacyExtractDispatchKey(@ByVal DispatchKeySet s);



// Parsed from c10/core/Backend.h

// #pragma once

// #include <c10/core/DeviceType.h>
// #include <c10/core/DispatchKey.h>
// #include <c10/core/DispatchKeySet.h>
// #include <c10/util/Exception.h>

// #include <stdexcept>

/**
 * This legacy enum class defines the set of backends supported by old school,
 * code generated Type-based ATen.  A "backend" in this sense roughly
 * corresponds to the cartesian product of (device type, layout), but restricted
 * only to combinations which we actually have kernels for.  Backend does NOT
 * include dtype.
 *
 * The reason we are sunsetting this enum class is because it doesn't allow for
 * open registration; e.g., if you want to add SparseXLA, you'd have to
 * edit this enum; you wouldn't be able to do it out of tree.  DispatchKey is
 * the replacement for Backend which supports open registration.
 *
 * NB: The concept of 'Backend' here disagrees with the notion of backend
 * exposed to users in torch.backends.  Backend here is something like "CPU"
 * or "SparseCUDA"; backend in torch.backends is something like "MKL" or
 * "CUDNN".
 */
@Namespace("c10") public enum Backend {
  CPU(0),
  CUDA(1),
  HIP(2),
  FPGA(3),
  XPU(4),
  SparseCPU(5),
  SparseCUDA(6),
  SparseHIP(7),
  SparseXPU(8),
  MSNPU(9),
  XLA(10),
  Vulkan(11),
  Metal(12),
  QuantizedCPU(13),
  QuantizedCUDA(14),
  QuantizedXPU(15),
  Undefined(16),
  MkldnnCPU(17),
  NumOptions(18);

    public final int value;
    private Backend(int v) { this.value = v; }
    private Backend(Backend e) { this.value = e.value; }
    public Backend intern() { for (Backend e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") public static native Backend toSparse(Backend b);
@Namespace("c10") public static native @Cast("c10::Backend") int toSparse(@Cast("c10::Backend") int b);

@Namespace("c10") public static native Backend toDense(Backend b);
@Namespace("c10") public static native @Cast("c10::Backend") int toDense(@Cast("c10::Backend") int b);

@Namespace("c10") public static native Backend dispatchKeyToBackend(DispatchKey t);
@Namespace("c10") public static native @Cast("c10::Backend") int dispatchKeyToBackend(@Cast("c10::DispatchKey") byte t);

@Namespace("c10") public static native DispatchKey backendToDispatchKey(Backend b);
@Namespace("c10") public static native @Cast("c10::DispatchKey") byte backendToDispatchKey(@Cast("c10::Backend") int b);

@Namespace("c10") public static native DeviceType backendToDeviceType(Backend b);
@Namespace("c10") public static native @Cast("c10::DeviceType") byte backendToDeviceType(@Cast("c10::Backend") int b);

@Namespace("c10") public static native Backend backendToCPU(Backend b);
@Namespace("c10") public static native @Cast("c10::Backend") int backendToCPU(@Cast("c10::Backend") int b);

@Namespace("c10") public static native Backend backendToXPU(Backend b);
@Namespace("c10") public static native @Cast("c10::Backend") int backendToXPU(@Cast("c10::Backend") int b);

@Namespace("c10") public static native Backend backendToCUDA(Backend b);
@Namespace("c10") public static native @Cast("c10::Backend") int backendToCUDA(@Cast("c10::Backend") int b);

@Namespace("c10") public static native Backend backendToHIP(Backend b);
@Namespace("c10") public static native @Cast("c10::Backend") int backendToHIP(@Cast("c10::Backend") int b);

// TODO: This probably shouldn't actually be static inline
@Namespace("c10") public static native @Cast("const char*") BytePointer toString(Backend b);
@Namespace("c10") public static native String toString(@Cast("c10::Backend") int b);

@Namespace("c10") public static native @Cast("bool") boolean isSparse(Backend b);
@Namespace("c10") public static native @Cast("bool") boolean isSparse(@Cast("c10::Backend") int b);

 // namespace c10


// Parsed from c10/core/CopyBytes.h

// #pragma once

// #include <c10/core/Device.h>
// Targeting ../CopyBytesFunction.java


// Targeting ../_CopyBytesFunctionRegisterer.java



// #define REGISTER_COPY_BYTES_FUNCTION(from, to, ...)
//   namespace {
//   static _CopyBytesFunctionRegisterer C10_ANONYMOUS_VARIABLE(
//       g_copy_function)(from, to, __VA_ARGS__);
//   }

/*
 * WARNING: Implementations for this function are currently registered from
 * ATen and caffe2, not yet from c10. Don't use this if not either ATen
 * or caffe2 is present as well.
 * We can't move them yet, because the CUDA implementations aren't unified yet
 * between ATen and caffe2.
 * We're planning to move the implementations into c10/backend/xxx
 * to make c10 self contained again.
 */
@Namespace("c10") public static native void CopyBytes(
    @Cast("size_t") long nbytes,
    @Const Pointer src,
    @ByVal Device src_device,
    Pointer dst,
    @ByVal Device dst_device,
    @Cast("bool") boolean async);
 // namespace c10


// Parsed from c10/core/Layout.h

// #pragma once

// #include <c10/core/Backend.h>
// #include <c10/util/Exception.h>

// #include <iostream>
@Namespace("c10") public enum Layout { Strided((byte)(0)), Sparse((byte)(1)), Mkldnn((byte)(2)), NumOptions((byte)(3));

    public final byte value;
    private Layout(byte v) { this.value = v; }
    private Layout(Layout e) { this.value = e.value; }
    public Layout intern() { for (Layout e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") public static native Layout layout_from_backend(Backend backend);
@Namespace("c10") public static native @Cast("c10::Layout") byte layout_from_backend(@Cast("c10::Backend") int backend);



 // namespace c10


// Parsed from c10/core/MemoryFormat.h

// #pragma once

// #include <c10/core/Backend.h>
// #include <c10/util/Exception.h>
// #include <c10/util/ArrayRef.h>

// #include <iostream>

// Memory format is not the property of a Tensor. It is the way to tell an
// operator how the result should be organized in memory and nothing more. That
// means memory format should never be used as return value for any tensor state
// interrogation functions (internally and externally).
//
// Possible options are:
//  Preserve:
//    If any of the input tensors is in channels_last format, operator output
//    should be in channels_last format
//
//  Contiguous:
//    Regardless of input tensors format, the output should be contiguous Tensor.
//
//  ChannelsLast:
//    Regardless of input tensors format, the output should be in channels_last format.
@Namespace("c10") public enum MemoryFormat { Contiguous((byte)(0)), Preserve((byte)(1)), ChannelsLast((byte)(2)), ChannelsLast3d((byte)(3));

    public final byte value;
    private MemoryFormat(byte v) { this.value = v; }
    private MemoryFormat(MemoryFormat e) { this.value = e.value; }
    public MemoryFormat intern() { for (MemoryFormat e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

// If you are seeing this, it means that this call site was not checked if
// the memory format could be preserved, and it was switched to old default
// behaviour of contiguous
// #define LEGACY_CONTIGUOUS_MEMORY_FORMAT c10::get_contiguous_memory_format()

@Namespace("c10") public static native MemoryFormat get_contiguous_memory_format();



// Note: Hardcoded the channel last stride indices here to get better performance
@Namespace("c10") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector get_channels_last_strides_2d(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("c10") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector get_channels_last_strides_2d(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... sizes);

@Namespace("c10") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector get_channels_last_strides_3d(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("c10") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector get_channels_last_strides_3d(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... sizes);

// NOTE:
// Below are Helper functions for is_channels_last_strides_xd.
// 1. Please do not combine these helper functions, each helper function handles
// exactly one case of sizes + memory_format, by doing this, the strides indices
// will be a constant array and we can access it using constant index number,
// the compiler will fully unroll the loop on strides indices to gain a better
// performance.
// 2. No error check in helper function, caller ensures the correctness of the input
// 3. All helper functions have similar comments, only 1st helper function is commented here.
@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_2d_s4(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides);
@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_2d_s4(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] sizes, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... strides);

@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_3d_s5(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides);
@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_3d_s5(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] sizes, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... strides);

// Note [Ambiguous is_channels_last_strides_xd]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// The flaw of carrying memory_format implicitly through strides is very hard
// to WAR properly. issue #24090
// Without the history of permutation, we can't infer the memory_format of a
// tensor from the snapshot of its size & stride
// e.g.
//
// 1. We can NOT specify the memory_format of N111 tensor through strides in a
//  meaningful way;
//
// 2. Two path that ended up with identical size/stride
//  N11W contiguous tensor sliced at w-dimension becomes [N,1,1,1]@[W,W,W,W]
//  NC11 channels_last tensor sliced at c-dimension becomes [N,1,1,1]@[C,C,C,C]
//    So if we see a tensor [N,1,1,1]@[X,X,X,X], there's no way for us to infer
//    the memory_format of the original tensor.
//
// Due to the limitations, our temporary WAR `is_channels_last_strides` does the
// best effort to infer whether the original memory_format of a tensor is
// at::MemoryFormat::ChannelsLast. The two objectives of this function (ordered
// by their importance):
//   1. Ensure that normal shape manipulation does not accidentally change the
//      MemoryFormat of an existing tensor.
//   2. Allows user to mark MemoryFormat::ChannelsLast to tensors;
//
// The function does so via checking strides of the tensor, including strides of
// size-1 dimensions. Although conventionally PyTorch implies no restriction on
// trivial stride (stride for size-1 dimension).
//
// Note that this approach is a compromise. We did not solve the problem
// completely. Many cases we will not be able to infer the correct memory
// format.
// The implementation of `is_channels_last_strides` is to serve the objectives:
// MemoryFormat::ChannelsLast has to be explicitly opted-in (no accidental
// conversion); Best effort to maintain the ChannelsLast flag.
//
// Due to the fact that this is not a bulletproof solution, through testing
// (aten/src/ATen/test/memory_format_test.cpp)
//   a. we ensure that the common tasks are supported;
//   a. we identify corner cases where the implementation compromises on.
//
// By the time accumulated permutation is enabled to replace implicit
// memory_format through strides, we should be updating our tests and fix the
// issues in our tests.
//
// We use Channels Last 2d as an example above.
// This is a general problem for all the is_channels_last_strides_xd implementation.
// Please check the helper functions (is_channels_last_strides_*d_s*) for more details.

@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_2d(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides);
@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_2d(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] sizes, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... strides);

@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_3d(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides);
@Namespace("c10") public static native @Cast("bool") boolean is_channels_last_strides_3d(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] sizes, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... strides);

 // namespace c10


// Parsed from c10/core/QEngine.h

// #pragma once

// #include <c10/core/DeviceType.h>
// #include <c10/core/DispatchKey.h>
// #include <c10/util/Exception.h>

/**
 * QEngine is an enum that is used to select the engine to run quantized ops.
 * Keep this enum in sync with get_qengine_id() in
 * torch/backends/quantized/__init__.py
 */
@Namespace("c10") public enum QEngine {
  NoQEngine((byte)(0)),
  FBGEMM((byte)(1)),
  QNNPACK((byte)(2));

    public final byte value;
    private QEngine(byte v) { this.value = v; }
    private QEngine(QEngine e) { this.value = e.value; }
    public QEngine intern() { for (QEngine e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") public static native @StdString BytePointer toString(QEngine qengine);

 // namespace c10


// Parsed from c10/core/QScheme.h

// #pragma once

// #include <c10/core/DeviceType.h>
// #include <c10/util/Exception.h>

/**
 * QScheme is an enum that specifies the type of quantization. This has a one
 * to one correspondence with Quantizer
 * Please refer to ATen/quantized/Quantizer.h to see the Quantizers classes.
 * Keep this file in sync with torch/nn/_qscheme.py
 */
@Namespace("c10") public enum QScheme {
  PER_TENSOR_AFFINE((byte)(0)),
  PER_CHANNEL_AFFINE((byte)(1)),
  PER_TENSOR_SYMMETRIC((byte)(2)),
  PER_CHANNEL_SYMMETRIC((byte)(3)),
  PER_CHANNEL_AFFINE_FLOAT_QPARAMS((byte)(4)),
  COMPILE_TIME_NUM_QSCHEMES((byte)(5));

    public final byte value;
    private QScheme(byte v) { this.value = v; }
    private QScheme(QScheme e) { this.value = e.value; }
    public QScheme intern() { for (QScheme e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
@Namespace("c10") @MemberGetter public static native int COMPILE_TIME_NUM_QSCHEMES();

@Namespace("c10") public static native @StdString BytePointer toString(QScheme qscheme);

 // namespace c10


// Parsed from c10/core/Stream.h

// #pragma once

// #include <c10/core/Device.h>

/** An index representing a specific stream.  A StreamId is not independently
 *  meaningful without knowing the Device it is associated with; try to
 *  use Stream rather than StreamId directly.
 * 
 *  StreamIds are opaque; they are assigned by some DeviceType-specific
 *  numbering system which is not visible to the user.  HOWEVER, we
 *  guarantee that StreamId 0 is always a valid stream, and corresponds
 *  to some sort of "default" stream. */
// Targeting ../Stream.java






// Targeting ../StreamHash.java


 // namespace std


// Parsed from c10/core/ScalarType.h

// #pragma once

// #include <c10/util/ArrayRef.h>
// #include <c10/util/complex.h>
// #include <c10/util/Half.h>
// #include <c10/util/qint32.h>
// #include <c10/util/qint8.h>
// #include <c10/util/quint8.h>
// #include <c10/util/BFloat16.h>
// #include <c10/util/quint4x2.h>
// #include <c10/util/Optional.h>

// #include <complex>
// #include <cstdint>
// #include <iostream>

// For the macros below:
// NB: If you want to macro some code for all non-QInt scalar types (i.e. types
// with complete information, you probably want one of the
// AT_FORALL_SCALAR_TYPES / AT_FORALL_SCALAR_TYPES_AND
// macros below, which are designed to behave similarly to the Dispatch macros
// with the same name.

// NB: Order matters for this macro; it is relied upon in
// _promoteTypesLookup and the serialization format.
// #define AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_AND_QINTS(_)
//   _(uint8_t, Byte) /* 0 */
//   _(int8_t, Char) /* 1 */
//   _(int16_t, Short) /* 2 */
//   _(int, Int) /* 3 */
//   _(int64_t, Long) /* 4 */
//   _(at::Half, Half) /* 5 */
//   _(float, Float) /* 6 */
//   _(double, Double) /* 7 */
//   _(c10::complex<c10::Half>, ComplexHalf) /* 8 */
//   _(c10::complex<float>, ComplexFloat) /* 9 */
//   _(c10::complex<double>, ComplexDouble) /* 10 */
//   _(bool, Bool) /* 11 */
//   _(c10::qint8, QInt8) /* 12 */
//   _(c10::quint8, QUInt8) /* 13 */
//   _(c10::qint32, QInt32) /* 14 */
//   _(at::BFloat16, BFloat16) /* 15 */
//   _(c10::quint4x2, QUInt4x2) /* 16 */


// If you want to support ComplexHalf for real, add ComplexHalf
// into this macro (and change the name).  But beware: convert()
// doesn't work for all the conversions you need...
// #define AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_EXCEPT_COMPLEX_HALF(_)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(at::Half, Half)
//   _(float, Float)
//   _(double, Double)
//   _(c10::complex<float>, ComplexFloat)
//   _(c10::complex<double>, ComplexDouble)
//   _(bool, Bool)
//   _(at::BFloat16, BFloat16)


@Namespace("c10") public enum ScalarType {
  Byte((byte)(0)), /* 0 */
  Char((byte)(1)), /* 1 */
  Short((byte)(2)), /* 2 */
  Int((byte)(3)), /* 3 */
  Long((byte)(4)), /* 4 */
  Half((byte)(5)), /* 5 */
  Float((byte)(6)), /* 6 */
  Double((byte)(7)), /* 7 */
  ComplexHalf((byte)(8)), /* 8 */
  ComplexFloat((byte)(9)), /* 9 */
  ComplexDouble((byte)(10)), /* 10 */
  Bool((byte)(11)), /* 11 */
  QInt8((byte)(12)), /* 12 */
  QUInt8((byte)(13)), /* 13 */
  QInt32((byte)(14)), /* 14 */
  BFloat16((byte)(15)), /* 15 */
  QUInt4x2((byte)(16)),
      Undefined((byte)(17)),
  NumOptions((byte)(18));

    public final byte value;
    private ScalarType(byte v) { this.value = v; }
    private ScalarType(ScalarType e) { this.value = e.value; }
    public ScalarType intern() { for (ScalarType e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("c10") @MemberGetter public static native @Cast("const uint16_t") short NumScalarTypes();

// These are used to map ScalarTypes to C++ types.

// #define SPECIALIZE_ScalarTypeToCPPType(cpp_type, scalar_type)
// template<>
// struct ScalarTypeToCPPType<c10::ScalarType::scalar_type> {
//   using type = cpp_type;
// 
//   /* This is a workaround for the CUDA bug which prevents */
//   /* ::detail::ScalarTypeToCType<T>::type being used directly due to */
//   /* ambiguous reference which can't to be resolved. For some reason it */
//   /* cant pick between at::detail and at::cuda::detail. */
//   /* For repro example, please see: */
//   /* https://gist.github.com/izdeby/952ae7cf256ddb740a73776d39a7e7ba */
//   /* TODO: remove once the bug is fixed. */
//   static type t;
// }; /* 0 */ /* 1 */ /* 2 */ /* 3 */ /* 4 */ /* 5 */ /* 6 */ /* 7 */ /* 8 */ /* 9 */ /* 10 */ /* 11 */ /* 12 */ /* 13 */ /* 14 */ /* 15 */ /* 16 */

// #undef SPECIALIZE_ScalarTypeToCPPType

 // namespace impl

// #define SPECIALIZE_CppTypeToScalarType(cpp_type, scalar_type)
//   template<>
//   struct CppTypeToScalarType<cpp_type>:
//     std::integral_constant<c10::ScalarType,
//                            c10::ScalarType::scalar_type>
//   {}; /* 0 */ /* 1 */ /* 2 */ /* 3 */ /* 4 */ /* 5 */ /* 6 */ /* 7 */ /* 8 */ /* 9 */ /* 10 */ /* 11 */ /* 12 */ /* 13 */ /* 14 */ /* 15 */ /* 16 */

// #undef SPECIALIZE_CppTypeToScalarType

// #define AT_FORALL_INT_TYPES(_)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)

// #define AT_FORALL_SCALAR_TYPES(_)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(float, Float)
//   _(double, Double)

// #define AT_FORALL_SCALAR_TYPES_AND(SCALARTYPE, _)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(float, Float)
//   _(double, Double)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<::c10::ScalarType::SCALARTYPE>::t), SCALARTYPE)

// #define AT_FORALL_SCALAR_TYPES_AND2(SCALARTYPE1, SCALARTYPE2, _)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(float, Float)
//   _(double, Double)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<::c10::ScalarType::SCALARTYPE1>::t), SCALARTYPE1)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<::c10::ScalarType::SCALARTYPE2>::t), SCALARTYPE2)

// #define AT_FORALL_SCALAR_TYPES_AND3(SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, _)
//   _(uint8_t, Byte)
//   _(int8_t, Char)
//   _(int16_t, Short)
//   _(int, Int)
//   _(int64_t, Long)
//   _(float, Float)
//   _(double, Double)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<::c10::ScalarType::SCALARTYPE1>::t), SCALARTYPE1)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<::c10::ScalarType::SCALARTYPE2>::t), SCALARTYPE2)
//   _(decltype(::c10::impl::ScalarTypeToCPPType<::c10::ScalarType::SCALARTYPE3>::t), SCALARTYPE3)

// #define AT_FORALL_QINT_TYPES(_)
//   _(c10::qint8, QInt8)
//   _(c10::quint8, QUInt8)
//   _(c10::qint32, QInt32)
//   _(c10::quint4x2, QUInt4x2)

// #define AT_FORALL_COMPLEX_TYPES(_)
//   _(c10::complex<float>, ComplexFloat)
//   _(c10::complex<double>, ComplexDouble)

// #define DEFINE_CONSTANT(_, name)
//   constexpr ScalarType k##name = ScalarType::name;

@Namespace("c10") @MemberGetter public static native ScalarType kByte(); /* 0 */
  @Namespace("c10") @MemberGetter public static native ScalarType kChar(); /* 1 */
  @Namespace("c10") @MemberGetter public static native ScalarType kShort(); /* 2 */
  @Namespace("c10") @MemberGetter public static native ScalarType kInt(); /* 3 */
  @Namespace("c10") @MemberGetter public static native ScalarType kLong(); /* 4 */
  @Namespace("c10") @MemberGetter public static native ScalarType kHalf(); /* 5 */
  @Namespace("c10") @MemberGetter public static native ScalarType kFloat(); /* 6 */
  @Namespace("c10") @MemberGetter public static native ScalarType kDouble(); /* 7 */
  @Namespace("c10") @MemberGetter public static native ScalarType kComplexHalf(); /* 8 */
  @Namespace("c10") @MemberGetter public static native ScalarType kComplexFloat(); /* 9 */
  @Namespace("c10") @MemberGetter public static native ScalarType kComplexDouble(); /* 10 */
  @Namespace("c10") @MemberGetter public static native ScalarType kBool(); /* 11 */
  @Namespace("c10") @MemberGetter public static native ScalarType kQInt8(); /* 12 */
  @Namespace("c10") @MemberGetter public static native ScalarType kQUInt8(); /* 13 */
  @Namespace("c10") @MemberGetter public static native ScalarType kQInt32(); /* 14 */
  @Namespace("c10") @MemberGetter public static native ScalarType kBFloat16(); /* 15 */
  @Namespace("c10") @MemberGetter public static native ScalarType kQUInt4x2(); /* 16 */
// #undef DEFINE_CONSTANT

@Namespace("c10") public static native @Cast("const char*") BytePointer toString(ScalarType t);

@Namespace("c10") public static native @Cast("size_t") long elementSize(ScalarType t);

@Namespace("c10") public static native @Cast("bool") @Deprecated boolean isIntegralType(ScalarType t);

@Namespace("c10") public static native @Cast("bool") boolean isIntegralType(ScalarType t, @Cast("bool") boolean includeBool);

@Namespace("c10") public static native @Cast("bool") boolean isFloatingType(ScalarType t);

@Namespace("c10") public static native @Cast("bool") boolean isComplexType(ScalarType t);

@Namespace("c10") public static native @Cast("bool") boolean isQIntType(ScalarType t);

@Namespace("c10") public static native ScalarType toQIntType(ScalarType t);

@Namespace("c10") public static native ScalarType toUnderlying(ScalarType t);

@Namespace("c10") public static native @Cast("bool") boolean isSignedType(ScalarType t);

@Namespace("c10") public static native @Cast("bool") boolean isUnderlying(ScalarType type, ScalarType qtype);

@Namespace("c10") public static native ScalarType toValueType(ScalarType t);

@Namespace("c10") public static native ScalarType toComplexType(ScalarType t);

// see tensor_attributes.rst for detailed explanation and examples
// of casting rules.
@Namespace("c10") public static native @Cast("bool") boolean canCast(ScalarType from, ScalarType to);

@Namespace("c10") public static native ScalarType promoteTypes(ScalarType a, ScalarType b);



 // namespace c10


// Parsed from c10/core/ScalarTypeToTypeMeta.h

// #pragma once

// #include <c10/core/ScalarType.h>
// #include <c10/util/typeid.h>

// these just expose TypeMeta/ScalarType bridge functions in c10
// TODO move to typeid.h (or codemod away) when TypeMeta et al
// are moved from caffe2 to c10 (see note at top of typeid.h)

/**
 * convert ScalarType enum values to TypeMeta handles
 */
@Namespace("c10") public static native @ByVal @Cast("caffe2::TypeMeta*") Pointer scalarTypeToTypeMeta(ScalarType scalar_type);

/**
 * convert TypeMeta handles to ScalarType enum values
 */
@Namespace("c10") public static native ScalarType typeMetaToScalarType(@ByVal @Cast("caffe2::TypeMeta*") Pointer dtype);

/**
 * typeMetaToScalarType(), lifted to optional
 */
@Namespace("c10") public static native @ByVal ScalarTypeOptional optTypeMetaToScalarType(@ByVal @Cast("c10::optional<caffe2::TypeMeta>*") Pointer type_meta);

/**
 * convenience: equality across TypeMeta/ScalarType conversion
 */








 // namespace c10


// Parsed from c10/core/Scalar.h

// #pragma once

// #include <assert.h>
// #include <stdint.h>
// #include <stdexcept>
// #include <string>
// #include <utility>
// #include <type_traits>

// #include <c10/core/ScalarType.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/Half.h>
// #include <c10/util/TypeCast.h>
// Targeting ../Scalar.java



// define the scalar.to<int64_t>() specializations


// #define DEFINE_TO(T, name)
//   template <>
//   inline T Scalar::to<T>() const {
//     return to##name();
//   }

  
  
  
  
  
  
  
  
  
  
  
// #undef DEFINE_TO

 // namespace c10


// Parsed from c10/core/Allocator.h

// #pragma once

// #include <stddef.h>
// #include <memory>

// #include <c10/core/Device.h>
// #include <c10/util/Exception.h>
// #include <c10/util/ThreadLocalDebugInfo.h>
// #include <c10/util/UniqueVoidPtr.h>
// Targeting ../DataPtr.java



// NB: Device is NOT tested for here; a CUDA nullptr is as much a nullptr as a
// CPU nullptr





// Targeting ../Allocator.java



// This context is used to generate DataPtr which have arbitrary
// std::function deleters associated with them.  In some user facing
// functions, we give a (user-friendly) interface for constructing
// tensors from external data which take an arbitrary std::function
// deleter.  Grep for InefficientStdFunctionContext to find these
// occurrences.
//
// This context is inefficient because we have to do a dynamic
// allocation InefficientStdFunctionContext, on top of the dynamic
// allocation which is implied by std::function itself.

/** Set the allocator for DeviceType {@code t}. The passed in allocator pointer is
 *  expected to have static lifetime; this function does NOT take ownership
 *  of the raw pointer. (The reason for this is to prevent existing pointers
 *  to an allocator of a particular device from being invalidated when
 *  SetAllocator is called.)
 *
 *  Also note that this is not thread-safe, and we assume this function will
 *  only be called during initialization.
 *
 *  The 'priority' flag is introduced when we want to overwrite the default
 *  allocator, since the allocators are set statically. The default priority
 *  is 0, which means the lowest. Only higher or equal priority can overwrite
 *  existing ones.
 */
@Namespace("c10") public static native void SetAllocator(DeviceType t, Allocator alloc, @Cast("uint8_t") byte priority/*=0*/);
@Namespace("c10") public static native void SetAllocator(DeviceType t, Allocator alloc);
@Namespace("c10") public static native void SetAllocator(@Cast("c10::DeviceType") byte t, Allocator alloc, @Cast("uint8_t") byte priority/*=0*/);
@Namespace("c10") public static native void SetAllocator(@Cast("c10::DeviceType") byte t, Allocator alloc);
@Namespace("c10") public static native Allocator GetAllocator(DeviceType t);
@Namespace("c10") public static native Allocator GetAllocator(@Cast("c10::DeviceType") byte t);

// #define REGISTER_ALLOCATOR(t, f)
//   namespace {
//   static AllocatorRegisterer<t> g_allocator_d(f);
//   }
// Targeting ../MemoryReportingInfoBase.java



@Namespace("c10") public static native @Cast("bool") boolean memoryProfilingEnabled();
@Namespace("c10") public static native void reportMemoryUsageToProfiler(Pointer ptr, @Cast("int64_t") long alloc_size, @ByVal Device device);

 // namespace c10


// Parsed from c10/core/DefaultDtype.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/core/ScalarType.h>
 // namespace caffe2
@Namespace("c10") public static native void set_default_dtype(@ByVal @Cast("caffe2::TypeMeta*") Pointer dtype);
@Namespace("c10") public static native @ByVal @Cast("const caffe2::TypeMeta*") Pointer get_default_dtype();
@Namespace("c10") public static native ScalarType get_default_dtype_as_scalartype();
@Namespace("c10") public static native @ByVal @Cast("const caffe2::TypeMeta*") Pointer get_default_complex_dtype();
 // namespace c10


// Parsed from c10/core/StorageImpl.h

// #pragma once

// #include <c10/core/Allocator.h>
// #include <c10/core/ScalarType.h>

// #include <c10/util/intrusive_ptr.h>
// Targeting ../StorageImpl.java


 // namespace c10


// Parsed from c10/core/Storage.h

// #pragma once

// #include <c10/core/StorageImpl.h>
// Targeting ../Storage.java



 // namespace c10


// Parsed from c10/core/TensorOptions.h

// #pragma once

// #include <c10/core/DefaultDtype.h>
// #include <c10/core/Backend.h>
// #include <c10/core/Layout.h>
// #include <c10/core/ScalarType.h>
// #include <c10/core/ScalarTypeToTypeMeta.h>
// #include <c10/core/Device.h>
// #include <c10/core/MemoryFormat.h>
// #include <c10/core/DispatchKeySet.h>

// #include <c10/util/Optional.h>
// #include <c10/util/C++17.h>
// #include <c10/macros/Macros.h>

// #include <cstddef>
// #include <iosfwd>
// #include <utility>

@Namespace("c10") public static native DispatchKey computeDispatchKey(@ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device);

@Namespace("c10") public static native ScalarType dtype_or_default(@ByVal ScalarTypeOptional dtype);

@Namespace("c10") public static native @ByVal @Cast("caffe2::TypeMeta*") Pointer dtype_or_default(@ByVal @Cast("c10::optional<caffe2::TypeMeta>*") Pointer dtype);

@Namespace("c10") public static native Layout layout_or_default(@ByVal LayoutOptional layout);

@Namespace("c10") public static native @ByVal Device device_or_default(@ByVal DeviceOptional device);


///
///
///
///
///
///
///
///
///
///
///
///
///
///
///
///
///
@Namespace("c10") public static native @Cast("bool") boolean pinned_memory_or_default(@ByVal BoolOptional pinned_memory);
// Targeting ../TensorOptions.java



// We should aspire to fit in one machine-size word; but a size greater than two
// words is too much.  (We are doing terribly on 32-bit archs, where we require
// three machine size words to store tensor options.  Eek!)

/** Convenience function that returns a {@code TensorOptions} object with the {@code dtype}
 *  set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions dtype(@ByVal @Cast("caffe2::TypeMeta*") Pointer dtype);

// legacy function to support ScalarType
@Namespace("c10") public static native @ByVal TensorOptions dtype(ScalarType dtype);

/** Convenience function that returns a {@code TensorOptions} object with the {@code layout}
 *  set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions layout(Layout layout);
@Namespace("c10") public static native @ByVal TensorOptions layout(@Cast("c10::Layout") byte layout);

/** Convenience function that returns a {@code TensorOptions} object with the {@code device}
 *  set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions device(@ByVal Device device);

/** Convenience function that returns a {@code TensorOptions} object with the
 *  {@code device} set to CUDA and the {@code device_index} set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions device_index(short device_index);

/** Convenience function that returns a {@code TensorOptions} object with the
 *  {@code requires_grad} set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions requires_grad(@Cast("bool") boolean requires_grad/*=true*/);

/** Convenience function that returns a {@code TensorOptions} object with the
 *  {@code memory_format} set to the given one. */
@Namespace("c10") public static native @ByVal TensorOptions memory_format(MemoryFormat memory_format);
@Namespace("c10") public static native @ByVal TensorOptions memory_format(@Cast("c10::MemoryFormat") byte memory_format);



@Namespace("c10") public static native @StdString BytePointer toString(@Const @ByVal TensorOptions options);

// This is intended to be a centralized location by which we can determine
// what an appropriate DispatchKey for a tensor is.

// We deliberately ignore handling AutogradCPU/CUDA/XLA... keys to
// avoid adding asymmetry in device <--> Autograd dispatch key mapping.
@Namespace("c10") public static native DeviceType computeDeviceType(DispatchKey tid);
@Namespace("c10") public static native @Cast("c10::DeviceType") byte computeDeviceType(@Cast("c10::DispatchKey") byte tid);

 // namespace c10


// Parsed from c10/core/TensorImpl.h

// #pragma once

// #include <algorithm>
// #include <atomic>
// #include <memory>
// #include <numeric>

// #include <c10/core/Backend.h>
// #include <c10/core/MemoryFormat.h>
// #include <c10/core/Storage.h>
// #include <c10/core/TensorOptions.h>
// #include <c10/core/DispatchKeySet.h>
// #include <c10/core/impl/LocalDispatchKeySet.h>
// #include <c10/core/impl/SizesAndStrides.h>
// #include <c10/core/CopyBytes.h>


// #include <c10/util/Exception.h>
// #include <c10/util/Optional.h>
// #include <c10/util/Flags.h>
// #include <c10/util/Logging.h>
// #include <c10/util/python_stub.h>

// A global boolean variable to control whether we free memory when a Tensor
// is shrunk to a smaller size. As a result, a Tensor is always going to
// keep the memory allocated for its maximum capacity reshaped to so far.
//
// This parameter is respected "upper-case" methods which call Resize()
// (e.g., CopyFrom, ResizeLike); it is NOT respected by Tensor::resize_
// or ShrinkTo, both of which guarantee to never to free memory.


// Since we can have high variance in blob memory allocated across different
// inputs in the same run, we will shrink the blob only if the memory gain
// is larger than this flag in bytes.  This only applies to functions which
// respect caffe2_keep_on_shrink.



/**
 * A utility function to convert vector<int> to vector<int64_t>.
 */
@Namespace("c10") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector ToVectorint64_t(@ByVal @Cast("c10::ArrayRef<int>*") IntArrayRef src);

/**
 * Return product of all dimensions starting from k
 */
@Namespace("c10") public static native @Cast("int64_t") long size_from_dim_(int k, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("c10") public static native @Cast("int64_t") long size_from_dim_(int k, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dims);

// Product of all dims up to k (not including dims[k])
@Namespace("c10") public static native @Cast("int64_t") long size_to_dim_(int k, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("c10") public static native @Cast("int64_t") long size_to_dim_(int k, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dims);

// Product of all dims between k and l (not including dims[k] and dims[l])
@Namespace("c10") public static native @Cast("int64_t") long size_between_dim_(int k, int l, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("c10") public static native @Cast("int64_t") long size_between_dim_(int k, int l, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dims);

// Wrap around axis_index if it is negative, s.t., -1 is the last dim
@Namespace("c10") public static native int canonical_axis_index_(int axis_index, int ndims);
// Targeting ../PlacementDtor.java


// Targeting ../PlacementDeleteContext.java


// Targeting ../AutogradMetaInterface.java


// Targeting ../AutogradMetaFactory.java



@Namespace("c10::impl") public static native void SetAutogradMetaFactory(AutogradMetaFactory factory);
@Namespace("c10::impl") public static native AutogradMetaFactory GetAutogradMetaFactory();
// Targeting ../AutogradMetaFactoryRegisterer.java




// Targeting ../NamedTensorMetaInterface.java


// Targeting ../VariableVersion.java



/**
 * NOTE: Some TensorImpl methods are small and not overridden in the
 * PyTorch codebase itself, but may theoretically need to be
 * overridden by third-party TensorImpl subclasses. This macro allows
 * users that need maximum performance and don't need these extension
 * points to disable them with a build-time flag. (In particular,
 * XLA's XLATensorImpl currently overrides these methods, so we can't
 * enable this flag by default.)
 */
// #ifdef C10_DISABLE_TENSORIMPL_EXTENSIBILITY
// #define TENSORIMPL_MAYBE_VIRTUAL
// #else
// #define TENSORIMPL_MAYBE_VIRTUAL virtual
// Targeting ../TensorImpl.java



// Note [TensorImpl size constraints]
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Changed the size of TensorImpl?  If the size went down, good for
// you!  Adjust the documentation below and the expected size.
// Did it go up?  Read on...
//
// Struct size matters.  In some production systems at Facebook, we have
// 400M live tensors during a training run.  Do the math: every 64-bit
// word you add to Tensor is an extra 3.2 gigabytes in RAM.
//
// If you are a Facebook employee, you can check if the run in question
// has tipped you over the point using the command here:
// https://fburl.com/q5enpv98
//
// For reference, we OOMed at 160 bytes (20 words) per TensorImpl.
// This is not counting overhead from strides out-of-line allocation and
// StorageImpl space and this is from before we inlined sizes and strides
// directly into TensorImpl as SmallVectors.
//
// Our memory usage on 32-bit systems is suboptimal, but we're not checking
// for it at the moment (to help avoid rage inducing cycles when the
// 32-bit number is wrong).
//
// Current breakdown:
//
//    vtable pointer
//    strong refcount           TODO: pack these into one word
//    weak refcount
//    storage pointer
//    autograd metadata pointer
//    version counter pointer
//    PyObject pointer
//    SizesAndStrides size/pointer
//    SizesAndStrides sizes (pre-allocated 0)
//    SizesAndStrides sizes (pre-allocated 1)
//    SizesAndStrides sizes (pre-allocated 2)
//    SizesAndStrides sizes (pre-allocated 3)
//    SizesAndStrides sizes (pre-allocated 4)
//    SizesAndStrides strides (pre-allocated 0)
//    SizesAndStrides strides (pre-allocated 1)
//    SizesAndStrides strides (pre-allocated 2)
//    SizesAndStrides strides (pre-allocated 3)
//    SizesAndStrides strides (pre-allocated 4)
//    storage offset
//    numel
//    data type
//    (optional) device
//    tensor type id
//    miscellaneous bitfield
//
 // namespace c10


// Parsed from c10/core/UndefinedTensorImpl.h

// #pragma once

// #include <c10/core/TensorImpl.h>
// Targeting ../UndefinedTensorImpl.java



 // namespace c10


// Parsed from c10/core/WrapDimMinimal.h

// #pragma once

// #include <c10/util/Exception.h>

@Namespace("c10") public static native @Cast("int64_t") long maybe_wrap_dim(@Cast("int64_t") long dim, @Cast("int64_t") long dim_post_expr, @Cast("bool") boolean wrap_scalar/*=true*/);
@Namespace("c10") public static native @Cast("int64_t") long maybe_wrap_dim(@Cast("int64_t") long dim, @Cast("int64_t") long dim_post_expr);




// Parsed from ATen/core/aten_interned_strings.h

// #pragma once

// ATen symbols correspond exactly to operators defined in ATen.  Every
// symbol here corresponds exactly to an ATen operation which is defined
// in Declarations.yaml; attributes are in one-to-one correspondence with
// their ATen name.
//
// To explicitly use interned strings as symbols in your code, you must add
// them to this list.

// #define FORALL_ATEN_BASE_SYMBOLS(_)
// _(aten, __and__)
// _(aten, __iand__)
// _(aten, __ilshift__)
// _(aten, __ior__)
// _(aten, __irshift__)
// _(aten, __ixor__)
// _(aten, __lshift__)
// _(aten, __or__)
// _(aten, __rshift__)
// _(aten, __xor__)
// _(aten, _abs)
// _(aten, _addmv)
// _(aten, _addr)
// _(aten, _amp_foreach_non_finite_check_and_unscale_)
// _(aten, _amp_update_scale)
// _(aten, _arange)
// _(aten, _argmax)
// _(aten, _argmin)
// _(aten, _baddbmm_mkl)
// _(aten, _cast_Byte)
// _(aten, _cast_Char)
// _(aten, _cast_Double)
// _(aten, _cast_Float)
// _(aten, _cast_Half)
// _(aten, _cast_Int)
// _(aten, _cast_Long)
// _(aten, _cast_Short)
// _(aten, _cat)
// _(aten, _ceil)
// _(aten, _clamp_max)
// _(aten, _clamp_min)
// _(aten, _convolution)
// _(aten, _convolution_double_backward)
// _(aten, convolution_overrideable)
// _(aten, convolution_backward_overrideable)
// _(aten, _convolution_nogroup)
// _(aten, _copy_ignoring_overlaps)
// _(aten, _cos)
// _(aten, _cosh)
// _(aten, _ctc_loss)
// _(aten, _ctc_loss_backward)
// _(aten, _cudnn_ctc_loss)
// _(aten, _cudnn_init_dropout_state)
// _(aten, _cudnn_rnn)
// _(aten, _cudnn_rnn_backward)
// _(aten, _cudnn_rnn_flatten_weight)
// _(aten, _cufft_clear_plan_cache)
// _(aten, _cufft_get_plan_cache_max_size)
// _(aten, _cufft_get_plan_cache_size)
// _(aten, _cufft_set_plan_cache_max_size)
// _(aten, _cumprod)
// _(aten, _cumsum)
// _(aten, _denseDims)
// _(aten, _dimI)
// _(aten, _dimV)
// _(aten, _dim_arange)
// _(aten, _dirichlet_grad)
// _(aten, _dot)
// _(aten, _embedding_bag)
// _(aten, _embedding_bag_backward)
// _(aten, _embedding_bag_dense_backward)
// _(aten, _embedding_bag_sparse_backward)
// _(aten, _erf)
// _(aten, _erfc)
// _(aten, _exp)
// _(aten, _exp2)
// _(aten, _expm1)
// _(aten, _fft_with_size)
// _(aten, _fill)
// _(aten, _floor)
// _(aten, _fused_dropout)
// _(aten, _indexCopy)
// _(aten, _indices)
// _(aten, _ldexp)
// _(aten, _linspace)
// _(aten, _local_scalar)
// _(aten, _local_scalar_dense)
// _(aten, _log)
// _(aten, _log10)
// _(aten, _log1p)
// _(aten, _log2)
// _(aten, _logspace)
// _(aten, _lu_with_info)
// _(aten, _masked_scale)
// _(aten, _mm)
// _(aten, _mv)
// _(aten, _nnz)
// _(aten, _nansum)
// _(aten, _pack_padded_sequence)
// _(aten, _pack_padded_sequence_backward)
// _(aten, _pad_packed_sequence)
// _(aten, _pdist_backward)
// _(aten, _pdist_forward)
// _(aten, _prod)
// _(aten, _prodall)
// _(aten, _range)
// _(aten, _reshape_from_tensor)
// _(aten, _round)
// _(aten, _rsqrt)
// _(aten, _s_where)
// _(aten, _shape_as_tensor)
// _(aten, _sigmoid)
// _(aten, _sigmoid_forward)
// _(aten, _sin)
// _(aten, _sinh)
// _(aten, _sparseDims)
// _(aten, _sparse_add)
// _(aten, _sparse_addmm)
// _(aten, _sparse_coo_tensor_with_dims)
// _(aten, _sparse_coo_tensor_with_dims_and_tensors)
// _(aten, _sparse_coo_tensor_unsafe)
// _(aten, _sparse_dense_add)
// _(aten, _sparse_div_scalar)
// _(aten, _sparse_div_zerodim)
// _(aten, _sparse_mul)
// _(aten, _sparse_mul_scalar)
// _(aten, _sparse_mul_zerodim)
// _(aten, _sparse_sum)
// _(aten, _sqrt)
// _(aten, _square)
// _(aten, _standard_gamma)
// _(aten, _standard_gamma_grad)
// _(aten, _sum)
// _(aten, _sum_cuda)
// _(aten, _tan)
// _(aten, _tanh)
// _(aten, _tanh_forward)
// _(aten, _th_get_device)
// _(aten, _th_kthvalue)
// _(aten, _th_mode)
// _(aten, _th_prod)
// _(aten, _th_sigmoid)
// _(aten, _th_std)
// _(aten, _th_sum)
// _(aten, _th_tanh)
// _(aten, _th_var)
// _(aten, _thnn_fused_gru_cell)
// _(aten, _thnn_fused_gru_cell_backward)
// _(aten, _thnn_fused_lstm_cell)
// _(aten, _thnn_fused_lstm_cell_backward)
// _(aten, _trilinear)
// _(aten, _trunc)
// _(aten, _unique)
// _(aten, _unique_dim)
// _(aten, _unsafe_view)
// _(aten, _validate_sparse_coo_tensor_args)
// _(aten, _values)
// _(aten, _weight_norm)
// _(aten, _weight_norm_cuda_interface)
// _(aten, _weight_norm_cuda_interface_backward)
// _(aten, _weight_norm_differentiable_backward)
// _(aten, abs)
// _(aten, adaptive_avg_pool1d)
// _(aten, adaptive_avg_pool2d)
// _(aten, adaptive_avg_pool2d_backward)
// _(aten, adaptive_avg_pool2d_forward)
// _(aten, adaptive_avg_pool3d)
// _(aten, adaptive_avg_pool3d_backward)
// _(aten, adaptive_avg_pool3d_forward)
// _(aten, adaptive_max_pool1d)
// _(aten, adaptive_max_pool2d)
// _(aten, adaptive_max_pool2d_backward)
// _(aten, adaptive_max_pool2d_forward)
// _(aten, adaptive_max_pool3d)
// _(aten, adaptive_max_pool3d_backward)
// _(aten, adaptive_max_pool3d_forward)
// _(aten, add)
// _(aten, add_)
// _(aten, addbmm)
// _(aten, addcdiv)
// _(aten, addcmul)
// _(aten, addmm)
// _(aten, addmv)
// _(aten, addr)
// _(aten, affine_grid_generator)
// _(aten, affine_grid_generator_backward)
// _(aten, alias)
// _(aten, all)
// _(aten, allclose)
// _(aten, alpha_dropout)
// _(aten, any)
// _(aten, arange)
// _(aten, argmax)
// _(aten, argmin)
// _(aten, as_strided)
// _(aten, as_tensor)
// _(aten, atan2)
// _(aten, atleast_1d)
// _(aten, atleast_2d)
// _(aten, atleast_3d)
// _(aten, avg_pool1d)
// _(aten, avg_pool2d)
// _(aten, avg_pool2d_backward)
// _(aten, avg_pool2d_forward)
// _(aten, avg_pool3d)
// _(aten, avg_pool3d_backward)
// _(aten, avg_pool3d_forward)
// _(aten, baddbmm)
// _(aten, bartlett_window)
// _(aten, batch_norm)
// _(aten, bernoulli)
// _(aten, bilinear)
// _(aten, binary_cross_entropy)
// _(aten, binary_cross_entropy_backward)
// _(aten, binary_cross_entropy_forward)
// _(aten, binary_cross_entropy_with_logits)
// _(aten, binary_cross_entropy_with_logits_backward)
// _(aten, binary_cross_entropy_with_logits_target_backward)
// _(aten, bincount)
// _(aten, blackman_window)
// _(aten, block_diag)
// _(aten, bmm)
// _(aten, broadcast_tensors)
// _(aten, broadcast_to)
// _(aten, cartesian_prod)
// _(aten, cat)
// _(aten, cauchy)
// _(aten, ceil)
// _(aten, celu)
// _(aten, chain_matmul)
// _(aten, cholesky)
// _(aten, cholesky_inverse)
// _(aten, cholesky_solve)
// _(aten, chunk)
// _(aten, clamp_max)
// _(aten, clamp_min)
// _(aten, clone)
// _(aten, coalesce)
// _(aten, combinations)
// _(aten, _conj)
// _(aten, conj)
// _(aten, complex)
// _(aten, copysign)
// _(aten, polar)
// _(aten, constant_pad_nd)
// _(aten, contiguous)
// _(aten, conv1d)
// _(aten, conv2d)
// _(aten, conv3d)
// _(aten, conv_tbc)
// _(aten, conv_tbc_backward)
// _(aten, conv_transpose1d)
// _(aten, convolution)
// _(aten, copy_sparse_to_sparse)
// _(aten, cos)
// _(aten, cosh)
// _(aten, cosine_embedding_loss)
// _(aten, cosine_similarity)
// _(aten, count_nonzero)
// _(aten, cross)
// _(aten, std_mean)
// _(aten, var_mean)
// _(aten, ctc_loss)
// _(aten, cudnn_affine_grid_generator)
// _(aten, cudnn_affine_grid_generator_backward)
// _(aten, cudnn_batch_norm)
// _(aten, cudnn_batch_norm_backward)
// _(aten, cudnn_convolution)
// _(aten, cudnn_convolution_backward)
// _(aten, cudnn_convolution_backward_bias)
// _(aten, cudnn_convolution_backward_input)
// _(aten, cudnn_convolution_backward_weight)
// _(aten, cudnn_convolution_transpose)
// _(aten, cudnn_convolution_transpose_backward)
// _(aten, cudnn_convolution_transpose_backward_bias)
// _(aten, cudnn_convolution_transpose_backward_input)
// _(aten, cudnn_convolution_transpose_backward_weight)
// _(aten, cudnn_grid_sampler)
// _(aten, cudnn_grid_sampler_backward)
// _(aten, cudnn_is_acceptable)
// _(aten, cummax)
// _(aten, cummin)
// _(aten, cumprod)
// _(aten, cumsum)
// _(aten, data_ptr)
// _(aten, deg2rad)
// _(aten, detach)
// _(aten, diag)
// _(aten, diag_embed)
// _(aten, diagflat)
// _(aten, diagonal)
// _(aten, fill_diagonal_)
// _(aten, diff)
// _(aten, digamma)
// _(aten, dim)
// _(aten, dist)
// _(aten, dot)
// _(aten, dropout)
// _(aten, dstack)
// _(aten, eig)
// _(aten, einsum)
// _(aten, elu)
// _(aten, elu_backward)
// _(aten, elu_forward)
// _(aten, embedding)
// _(aten, embedding_backward)
// _(aten, embedding_bag)
// _(aten, embedding_dense_backward)
// _(aten, embedding_renorm)
// _(aten, embedding_sparse_backward)
// _(aten, empty)
// _(aten, empty_like)
// _(aten, empty_strided)
// _(aten, eq)
// _(aten, equal)
// _(aten, erf)
// _(aten, erfc)
// _(aten, erfinv)
// _(aten, exp)
// _(aten, expand)
// _(aten, expand_as)
// _(aten, expm1)
// _(aten, exponential)
// _(aten, eye)
// _(aten, feature_alpha_dropout)
// _(aten, feature_dropout)
// _(aten, fft)
// _(aten, fill)
// _(aten, flatten)
// _(aten, flip)
// _(aten, fliplr)
// _(aten, flipud)
// _(aten, floor)
// _(aten, fmod)
// _(aten, fmod_)
// _(aten, fmax)
// _(aten, fmin)
// _(aten, frac)
// _(aten, fractional_max_pool2d)
// _(aten, fractional_max_pool2d_backward)
// _(aten, fractional_max_pool2d_forward)
// _(aten, frobenius_norm)
// _(aten, full)
// _(aten, full_like)
// _(aten, gather)
// _(aten, gcd)
// _(aten, gelu)
// _(aten, geometric)
// _(aten, geqrf)
// _(aten, get_device)
// _(aten, glu)
// _(aten, glu_backward)
// _(aten, glu_forward)
// _(aten, grid_sampler)
// _(aten, grid_sampler_2d)
// _(aten, grid_sampler_2d_backward)
// _(aten, grid_sampler_3d)
// _(aten, grid_sampler_3d_backward)
// _(aten, group_norm)
// _(aten, gru)
// _(aten, gru_cell)
// _(aten, hamming_window)
// _(aten, hann_window)
// _(aten, hardshrink)
// _(aten, hardshrink_backward)
// _(aten, hardsigmoid)
// _(aten, hardsigmoid_backward)
// _(aten, hardtanh)
// _(aten, hardtanh_backward)
// _(aten, hardtanh_forward)
// _(aten, heaviside)
// _(aten, hinge_embedding_loss)
// _(aten, histc)
// _(aten, hspmm)
// _(aten, hstack)
// _(aten, hypot)
// _(aten, i0)
// _(aten, i0_)
// _(aten, igamma)
// _(aten, igamma_)
// _(aten, igammac)
// _(aten, igammac_)
// _(aten, ifft)
// _(aten, index)
// _(aten, index_add)
// _(aten, index_copy)
// _(aten, index_fill)
// _(aten, index_put)
// _(aten, index_select)
// _(aten, indices)
// _(aten, inner)
// _(aten, instance_norm)
// _(aten, inverse)
// _(aten, irfft)
// _(aten, is_coalesced)
// _(aten, is_complex)
// _(aten, is_contiguous)
// _(aten, is_cuda)
// _(aten, is_distributed)
// _(aten, is_floating_point)
// _(aten, is_nonzero)
// _(aten, is_same_size)
// _(aten, is_set_to)
// _(aten, is_signed)
// _(aten, is_sparse)
// _(aten, isclose)
// _(aten, isreal)
// _(aten, istft)
// _(aten, isposinf)
// _(aten, isneginf)
// _(aten, kaiser_window)
// _(aten, kl_div)
// _(aten, kl_div_backward)
// _(aten, kthvalue)
// _(aten, l1_loss)
// _(aten, l1_loss_backward)
// _(aten, l1_loss_forward)
// _(aten, layer_norm)
// _(aten, lcm)
// _(aten, leaky_relu)
// _(aten, leaky_relu_backward)
// _(aten, leaky_relu_forward)
// _(aten, lerp)
// _(aten, lgamma)
// _(aten, linear)
// _(aten, linspace)
// _(aten, log)
// _(aten, log10)
// _(aten, log1p)
// _(aten, log2)
// _(aten, log_normal)
// _(aten, log_sigmoid)
// _(aten, log_sigmoid_backward)
// _(aten, log_sigmoid_forward)
// _(aten, log_softmax)
// _(aten, _log_softmax)
// _(aten, _log_softmax_backward_data)
// _(aten, logcumsumexp)
// _(aten, logdet)
// _(aten, logit)
// _(aten, logspace)
// _(aten, logsumexp)
// _(aten, xlogy)
// _(aten, lstm)
// _(aten, lstm_cell)
// _(aten, lstsq)
// _(aten, lu_solve)
// _(aten, margin_ranking_loss)
// _(aten, masked_fill)
// _(aten, masked_scatter)
// _(aten, masked_select)
// _(aten, matmul)
// _(aten, matrix_power)
// _(aten, matrix_rank)
// _(aten, matrix_exp)
// _(aten, max)
// _(aten, max_pool1d)
// _(aten, max_pool1d_with_indices)
// _(aten, max_pool2d)
// _(aten, max_pool2d_with_indices)
// _(aten, max_pool2d_with_indices_backward)
// _(aten, max_pool2d_with_indices_forward)
// _(aten, max_pool3d)
// _(aten, max_pool3d_with_indices)
// _(aten, max_pool3d_with_indices_backward)
// _(aten, max_pool3d_with_indices_forward)
// _(aten, max_unpool2d)
// _(aten, max_unpool2d_backward)
// _(aten, max_unpool2d_forward)
// _(aten, max_unpool3d)
// _(aten, max_unpool3d_backward)
// _(aten, max_unpool3d_forward)
// _(aten, max_values)
// _(aten, mean)
// _(aten, median)
// _(aten, nanmedian)
// _(aten, meshgrid)
// _(aten, min)
// _(aten, min_values)
// _(aten, miopen_batch_norm)
// _(aten, miopen_batch_norm_backward)
// _(aten, miopen_convolution)
// _(aten, miopen_convolution_backward)
// _(aten, miopen_convolution_backward_bias)
// _(aten, miopen_convolution_backward_input)
// _(aten, miopen_convolution_backward_weight)
// _(aten, miopen_convolution_transpose)
// _(aten, miopen_convolution_transpose_backward)
// _(aten, miopen_convolution_transpose_backward_input)
// _(aten, miopen_convolution_transpose_backward_weight)
// _(aten, miopen_depthwise_convolution)
// _(aten, miopen_depthwise_convolution_backward)
// _(aten, miopen_depthwise_convolution_backward_input)
// _(aten, miopen_depthwise_convolution_backward_weight)
// _(aten, miopen_rnn)
// _(aten, miopen_rnn_backward)
// _(aten, mkldnn_convolution)
// _(aten, mkldnn_convolution_backward)
// _(aten, mkldnn_convolution_backward_input)
// _(aten, mkldnn_convolution_backward_weights)
// _(aten, mm)
// _(aten, mode)
// _(aten, mse_loss)
// _(aten, mse_loss_backward)
// _(aten, mse_loss_forward)
// _(aten, msort)
// _(aten, multi_margin_loss)
// _(aten, multi_margin_loss_backward)
// _(aten, multi_margin_loss_forward)
// _(aten, multilabel_margin_loss)
// _(aten, multilabel_margin_loss_backward)
// _(aten, multilabel_margin_loss_forward)
// _(aten, multinomial)
// _(aten, mv)
// _(aten, mvlgamma)
// _(aten, nansum)
// _(aten, nan_to_num)
// _(aten, narrow)
// _(aten, narrow_copy)
// _(aten, native_batch_norm)
// _(aten, native_batch_norm_backward)
// _(aten, native_clone)
// _(aten, native_get_device)
// _(aten, native_norm)
// _(aten, native_pow)
// _(aten, native_resize_as)
// _(aten, native_tensor)
// _(aten, native_zero)
// _(aten, nextafter)
// _(aten, bitwise_and)
// _(aten, bitwise_not)
// _(aten, bitwise_or)
// _(aten, bitwise_xor)
// _(aten, element_size)
// _(aten, nll_loss)
// _(aten, nll_loss2d)
// _(aten, nll_loss2d_backward)
// _(aten, nll_loss2d_forward)
// _(aten, nll_loss_backward)
// _(aten, nll_loss_forward)
// _(aten, nonzero)
// _(aten, nonzero_numpy)
// _(aten, norm)
// _(aten, norm_except_dim)
// _(aten, normal)
// _(aten, nuclear_norm)
// _(aten, numel)
// _(aten, ones)
// _(aten, ones_like)
// _(aten, orgqr)
// _(aten, ormqr)
// _(aten, pairwise_distance)
// _(aten, _euclidean_dist)
// _(aten, pdist)
// _(aten, cdist)
// _(aten, permute)
// _(aten, pin_memory)
// _(aten, pinverse)
// _(aten, pixel_shuffle)
// _(aten, pixel_unshuffle)
// _(aten, poisson)
// _(aten, polygamma)
// _(aten, pow)
// _(aten, float_power)
// _(aten, prelu)
// _(aten, prelu_backward)
// _(aten, prod)
// _(aten, put)
// _(aten, qr)
// _(aten, quantile)
// _(aten, nanquantile)
// _(aten, rad2deg)
// _(aten, rand)
// _(aten, rand_like)
// _(aten, randint)
// _(aten, randint_like)
// _(aten, randn)
// _(aten, randn_like)
// _(aten, random)
// _(aten, randperm)
// _(aten, range)
// _(aten, ravel)
// _(aten, reciprocal)
// _(aten, reflection_pad1d)
// _(aten, reflection_pad1d_backward)
// _(aten, reflection_pad1d_forward)
// _(aten, reflection_pad2d)
// _(aten, reflection_pad2d_backward)
// _(aten, reflection_pad2d_forward)
// _(aten, relu)
// _(aten, remainder)
// _(aten, renorm)
// _(aten, repeat)
// _(aten, replication_pad1d)
// _(aten, replication_pad1d_backward)
// _(aten, replication_pad1d_forward)
// _(aten, replication_pad2d)
// _(aten, replication_pad2d_backward)
// _(aten, replication_pad2d_forward)
// _(aten, replication_pad3d)
// _(aten, replication_pad3d_backward)
// _(aten, replication_pad3d_forward)
// _(aten, reshape)
// _(aten, reshape_as)
// _(aten, resize)
// _(aten, resize_)
// _(aten, resize_as)
// _(aten, resize_as_)
// _(aten, rfft)
// _(aten, rnn_relu)
// _(aten, rnn_relu_cell)
// _(aten, rnn_tanh)
// _(aten, rnn_tanh_cell)
// _(aten, rot90)
// _(aten, round)
// _(aten, rrelu)
// _(aten, rrelu_with_noise)
// _(aten, rrelu_with_noise_backward)
// _(aten, rrelu_with_noise_forward)
// _(aten, rsqrt)
// _(aten, scatter)
// _(aten, scatter_add)
// _(aten, select)
// _(aten, selu)
// _(aten, set)
// _(aten, sigmoid)
// _(aten, sign)
// _(aten, signbit)
// _(aten, silu)
// _(aten, sgn)
// _(aten, sin)
// _(aten, sinc)
// _(aten, sinh)
// _(aten, size)
// _(aten, sizes)
// _(aten, slice)
// _(aten, slogdet)
// _(aten, smm)
// _(aten, smooth_l1_loss)
// _(aten, smooth_l1_loss_backward)
// _(aten, smooth_l1_loss_forward)
// _(aten, soft_margin_loss)
// _(aten, soft_margin_loss_backward)
// _(aten, soft_margin_loss_forward)
// _(aten, softmax)
// _(aten, _softmax)
// _(aten, _softmax_backward_data)
// _(aten, softplus)
// _(aten, softplus_backward)
// _(aten, softplus_forward)
// _(aten, softshrink)
// _(aten, softshrink_backward)
// _(aten, softshrink_forward)
// _(aten, solve)
// _(aten, sort)
// _(aten, sparse_coo_tensor)
// _(aten, sparse_mask)
// _(aten, sparse_resize)
// _(aten, sparse_resize_and_clear)
// _(aten, split)
// _(aten, split_with_sizes)
// _(aten, sqrt)
// _(aten, square)
// _(aten, squeeze)
// _(aten, sspaddmm)
// _(aten, stack)
// _(aten, std)
// _(aten, stft)
// _(aten, storage_offset)
// _(aten, stride)
// _(aten, strides)
// _(aten, rsub)
// _(aten, sum)
// _(aten, sum_to_size)
// _(aten, svd)
// _(aten, symeig)
// _(aten, t)
// _(aten, take)
// _(aten, tan)
// _(aten, tanh)
// _(aten, tensor)
// _(aten, tensordot)
// _(aten, tensor_split)
// _(aten, th_clone)
// _(aten, th_norm)
// _(aten, th_pow)
// _(aten, th_resize_as)
// _(aten, th_tensor)
// _(aten, th_zero)
// _(aten, thnn_conv2d)
// _(aten, thnn_conv2d_backward)
// _(aten, thnn_conv2d_forward)
// _(aten, tile)
// _(aten, slow_conv3d)
// _(aten, slow_conv3d_backward)
// _(aten, slow_conv3d_forward)
// _(aten, thnn_conv_depthwise2d)
// _(aten, thnn_conv_depthwise2d_backward)
// _(aten, thnn_conv_depthwise2d_forward)
// _(aten, slow_conv_dilated2d)
// _(aten, slow_conv_dilated2d_backward)
// _(aten, slow_conv_dilated3d)
// _(aten, slow_conv_dilated3d_backward)
// _(aten, slow_conv_transpose2d)
// _(aten, slow_conv_transpose2d_backward)
// _(aten, slow_conv_transpose3d)
// _(aten, slow_conv_transpose3d_backward)
// _(aten, threshold)
// _(aten, threshold_backward)
// _(aten, to)
// _(aten, to_sparse)
// _(aten, to_dense)
// _(aten, topk)
// _(aten, trace)
// _(aten, triangular_solve)
// _(aten, tril)
// _(aten, triplet_margin_loss)
// _(aten, triu)
// _(aten, type_as)
// _(aten, unbind)
// _(aten, unfold)
// _(aten, uniform)
// _(aten, unsafe_chunk)
// _(aten, unsafe_split)
// _(aten, unsafe_split_with_sizes)
// _(aten, unsqueeze)
// _(aten, upsample_bilinear2d)
// _(aten, upsample_bilinear2d_backward)
// _(aten, upsample_bilinear2d_forward)
// _(aten, upsample_bicubic2d)
// _(aten, upsample_bicubic2d_backward)
// _(aten, upsample_bicubic2d_forward)
// _(aten, upsample_linear1d)
// _(aten, upsample_linear1d_backward)
// _(aten, upsample_linear1d_forward)
// _(aten, upsample_nearest1d)
// _(aten, upsample_nearest1d_backward)
// _(aten, upsample_nearest1d_forward)
// _(aten, upsample_nearest2d)
// _(aten, upsample_nearest2d_backward)
// _(aten, upsample_nearest2d_forward)
// _(aten, upsample_nearest3d)
// _(aten, upsample_nearest3d_backward)
// _(aten, upsample_nearest3d_forward)
// _(aten, upsample_trilinear3d)
// _(aten, upsample_trilinear3d_backward)
// _(aten, upsample_trilinear3d_forward)
// _(aten, values)
// _(aten, vander)
// _(aten, var)
// _(aten, view)
// _(aten, view_as)
// _(aten, where)
// _(aten, zero)
// _(aten, zeros)
// _(aten, zeros_like)
// _(aten, real)
// _(aten, imag)
// _(aten, view_as_real)
// _(aten, view_as_complex) 
/* nothing */

// #define FORALL_ATTR_BASE_SYMBOLS(_)
// _(attr, A)
// _(attr, C)
// _(attr, H)
// _(attr, LU_data)
// _(attr, LU_pivots)
// _(attr, N)
// _(attr, W)
// _(attr, accumulate)
// _(attr, align_corners)
// _(attr, alpha)
// _(attr, anchor)
// _(attr, argmaxes)
// _(attr, atol)
// _(attr, b_hh)
// _(attr, b_ih)
// _(attr, bag_size)
// _(attr, base)
// _(attr, batch1)
// _(attr, batch2)
// _(attr, batch_first)
// _(attr, batch_sizes)
// _(attr, benchmark)
// _(attr, beta)
// _(attr, bias)
// _(attr, bias_defined)
// _(attr, bidirectional)
// _(attr, bins)
// _(attr, blank)
// _(attr, buffer)
// _(attr, ceil_mode)
// _(attr, checked_signal_sizes)
// _(attr, chunks)
// _(attr, columns)
// _(attr, column_stack)
// _(attr, complex_input)
// _(attr, complex_output)
// _(attr, condition)
// _(attr, count_include_pad)
// _(attr, cudnn_enable)
// _(attr, cudnn_enabled)
// _(attr, cx)
// _(attr, cy)
// _(attr, data)
// _(attr, dense_dim)
// _(attr, descending)
// _(attr, deterministic)
// _(attr, device)
// _(attr, diagonal)
// _(attr, dilation)
// _(attr, dim)
// _(attr, dim0)
// _(attr, dim1)
// _(attr, dim2)
// _(attr, dimension)
// _(attr, dims)
// _(attr, dims_other)
// _(attr, dims_self)
// _(attr, divisor_override)
// _(attr, dropout)
// _(attr, dropout_seed)
// _(attr, dropout_state)
// _(attr, dtype)
// _(attr, eigenvectors)
// _(attr, end)
// _(attr, end_dim)
// _(attr, eps)
// _(attr, epsilon)
// _(attr, equal_nan)
// _(attr, equation)
// _(attr, expand1)
// _(attr, expand2)
// _(attr, expand3)
// _(attr, exponent)
// _(attr, exponential_average_factor)
// _(attr, fgrad_input)
// _(attr, fill_value)
// _(attr, finput)
// _(attr, from)
// _(attr, g)
// _(attr, gO)
// _(attr, generator)
// _(attr, ggI)
// _(attr, ggW)
// _(attr, ggb)
// _(attr, grad)
// _(attr, gradOutput)
// _(attr, grad_bias)
// _(attr, grad_cy)
// _(attr, grad_hy)
// _(attr, grad_input)
// _(attr, grad_out)
// _(attr, grad_output)
// _(attr, grad_w)
// _(attr, grad_weight)
// _(attr, grid)
// _(attr, groups)
// _(attr, has_bias)
// _(attr, has_biases)
// _(attr, hidden_bias)
// _(attr, hidden_gates)
// _(attr, hidden_size)
// _(attr, high)
// _(attr, hop_length)
// _(attr, hx)
// _(attr, i1)
// _(attr, i2)
// _(attr, i3)
// _(attr, ignore_index)
// _(attr, implicit)
// _(attr, index)
// _(attr, indices)
// _(attr, info)
// _(attr, input)
// _(attr, input1)
// _(attr, input2)
// _(attr, input3)
// _(attr, input_bias)
// _(attr, input_gates)
// _(attr, input_lengths)
// _(attr, input_scale)
// _(attr, input_size)
// _(attr, interpolation_mode)
// _(attr, inverse)
// _(attr, is_target)
// _(attr, k)
// _(attr, keepdim)
// _(attr, kernel_size)
// _(attr, lambd)
// _(attr, largest)
// _(attr, layout)
// _(attr, left)
// _(attr, length)
// _(attr, lengths)
// _(attr, like)
// _(attr, log_alpha)
// _(attr, log_probs)
// _(attr, low)
// _(attr, lower)
// _(attr, lu)
// _(attr, m)
// _(attr, margin)
// _(attr, mask)
// _(attr, mat)
// _(attr, mat1)
// _(attr, mat2)
// _(attr, max)
// _(attr, max_indices)
// _(attr, max_norm)
// _(attr, max_size)
// _(attr, max_val)
// _(attr, max_values)
// _(attr, maximum_indices)
// _(attr, maxnorm)
// _(attr, maximum)
// _(attr, mean)
// _(attr, median)
// _(attr, nanmedian)
// _(attr, min)
// _(attr, min_indices)
// _(attr, min_val)
// _(attr, minlength)
// _(attr, minimum)
// _(attr, mode)
// _(attr, momentum)
// _(attr, n)
// _(attr, n_fft)
// _(attr, neg_log_likelihood)
// _(attr, negative)
// _(attr, negative_slope)
// _(attr, noise)
// _(attr, non_blocking)
// _(attr, norm_type)
// _(attr, normalized)
// _(attr, normalized_shape)
// _(attr, num_groups)
// _(attr, num_layers)
// _(attr, num_samples)
// _(attr, num_weights)
// _(attr, offset)
// _(attr, offset2bag)
// _(attr, offsets)
// _(attr, ones)
// _(attr, onesided)
// _(attr, options)
// _(attr, other)
// _(attr, output)
// _(attr, output_mask)
// _(attr, output_padding)
// _(attr, output_size)
// _(attr, output_sizes)
// _(attr, p)
// _(attr, pad)
// _(attr, padding)
// _(attr, padding_idx)
// _(attr, padding_mode)
// _(attr, padding_value)
// _(attr, params)
// _(attr, pdist)
// _(attr, cdist)
// _(attr, std_mean)
// _(attr, var_mean)
// _(attr, periodic)
// _(attr, pivot)
// _(attr, pivots)
// _(attr, pooledHeight)
// _(attr, pooledWidth)
// _(attr, positive)
// _(attr, pow)
// _(attr, random_samples)
// _(attr, rcond)
// _(attr, reduction)
// _(attr, repeats)
// _(attr, replacement)
// _(attr, res1)
// _(attr, res2)
// _(attr, res3)
// _(attr, reserve)
// _(attr, result)
// _(attr, return_inverse)
// _(attr, rois)
// _(attr, rtol)
// _(attr, running_mean)
// _(attr, running_var)
// _(attr, save_mean)
// _(attr, save_std)
// _(attr, save_var)
// _(attr, saved_g)
// _(attr, saved_norms)
// _(attr, saved_v)
// _(attr, scale)
// _(attr, scale_grad_by_freq)
// _(attr, self)
// _(attr, self_size)
// _(attr, self_ty)
// _(attr, shape)
// _(attr, sigma)
// _(attr, signal_ndim)
// _(attr, signal_sizes)
// _(attr, size)
// _(attr, solution)
// _(attr, some)
// _(attr, sorted)
// _(attr, source)
// _(attr, sparse)
// _(attr, sparse_dim)
// _(attr, sparse_dtype)
// _(attr, spatialScale)
// _(attr, split_size)
// _(attr, split_sizes)
// _(attr, src)
// _(attr, start)
// _(attr, start_dim)
// _(attr, std)
// _(attr, step)
// _(attr, steps)
// _(attr, storage)
// _(attr, storageOffset)
// _(attr, storage_offset)
// _(attr, stride)
// _(attr, sumdim)
// _(attr, swap)
// _(attr, symmetric)
// _(attr, target)
// _(attr, target_lengths)
// _(attr, targets)
// _(attr, tensor)
// _(attr, tensor1)
// _(attr, tensor2)
// _(attr, tensors)
// _(attr, the_template)
// _(attr, theta)
// _(attr, threshold)
// _(attr, to)
// _(attr, tol)
// _(attr, total)
// _(attr, total_length)
// _(attr, total_weight)
// _(attr, train)
// _(attr, training)
// _(attr, transpose)
// _(attr, transposed)
// _(attr, unbiased)
// _(attr, unitriangular)
// _(attr, unroll_dim)
// _(attr, upper)
// _(attr, upscale_factor)
// _(attr, use_input_stats)
// _(attr, v)
// _(attr, value)
// _(attr, values)
// _(attr, vec)
// _(attr, vec1)
// _(attr, vec2)
// _(attr, w_hh)
// _(attr, w_ih)
// _(attr, weight)
// _(attr, weight_arr)
// _(attr, weight_buf)
// _(attr, weight_size)
// _(attr, weight_stride0)
// _(attr, weights)
// _(attr, win_length)
// _(attr, window)
// _(attr, window_length)
// _(attr, workspace)
// _(attr, x)
// _(attr, x1)
// _(attr, x2)


// Parsed from ATen/core/interned_strings.h

// #pragma once
// #include <vector>
// #include <cstdint>
// #include <string>
// #include <unordered_map>
// #include <algorithm>

// #include <c10/macros/Macros.h>

// #if !defined(C10_MOBILE) || defined(FEATURE_TORCH_MOBILE)
// #include <ATen/core/aten_interned_strings.h>
// #endif

// #if !defined(C10_MOBILE) || defined(FEATURE_TORCH_MOBILE)
// #define FORALL_NS_SYMBOLS(_)
//   _(namespaces, prim)
//   _(namespaces, aten)
//   _(namespaces, cuda)
//   _(namespaces, onnx)
//   _(namespaces, attr)
//   _(namespaces, scope)
//   _(namespaces, user)
//   _(namespaces, _caffe2)
//   _(namespaces, dimname)
//   _(namespaces, namespaces)
//   _(prim, Assign)
//   _(prim, BroadcastingChunk)
//   _(prim, BroadcastSizes)
//   _(prim, ReductionSizes)
//   _(prim, Constant)
//   _(prim, ChunkSizes)
//   _(prim, Drop)
//   _(prim, Eval)
//   _(prim, Expand) /* onnx */
//   _(prim, FusionGroup)
//   _(prim, CudaFusionGroup)
//   _(prim, CudaFusionGuard)
//   _(prim, FunctionalGraph)
//   _(prim, DifferentiableGraph)
//   _(prim, TensorExprGroup)
//   _(prim, StaticSubgraph)
//   _(prim, If)
//   _(prim, Jump) /* debug */
//   _(prim, JumpNZ) /* debug */
//   _(prim, JumpZ) /* debug */
//   _(prim, Load)
//   _(prim, Loop)
//   _(prim, Param)
//   _(prim, PackPadded) /* onnx */
//   _(prim, PadPacked) /* onnx */
//   _(prim, Placeholder) /* debug */
//   _(prim, Print)
//   _(prim, PythonOp)
//   _(prim, IgnoredPythonOp)
//   _(prim, Reverse)
//   _(prim, Return)
//   _(prim, ReturnStmt)
//   _(prim, BreakStmt)
//   _(prim, ContinueStmt)
//   _(prim, ComprehensionScope)
//   _(prim, Store)
//   _(prim, AutogradZero)
//   _(prim, AutogradAnyNonZero)
//   _(prim, AutogradAllNonZero)
//   _(prim, AutogradAllZero)
//   _(prim, Starred)
//   _(prim, TupleConstruct)
//   _(prim, TupleUnpack)
//   _(prim, TupleIndex)
//   _(prim, TupleSlice)
//   _(prim, ListConstruct)
//   _(prim, ListUnpack)
//   _(prim, DictConstruct)
//   _(prim, ModuleDictIndex)
//   _(prim, EnumName)
//   _(prim, EnumValue)
//   _(prim, StringIndex)
//   _(prim, NumToTensor)
//   _(prim, Uninitialized)
//   _(prim, With)
//   _(prim, Enter)
//   _(prim, Exit)
//   _(aten, Bool)
//   _(aten, Int)
//   _(aten, FloatImplicit)
//   _(aten, IntImplicit)
//   _(aten, ScalarImplicit)
//   _(aten, Float)
//   _(aten, str)
//   _(aten, Delete)
//   _(prim, device)
//   _(prim, dtype)
//   _(prim, layout)
//   _(prim, id)
//   _(prim, requires_grad)
//   _(prim, MakeTestTensor) /* test */
//   _(prim, AutogradAdd)
//   _(prim, GradOf)
//   _(aten, grad)
//   _(aten, backward)
//   _(prim, Guard)
//   _(prim, BailOut)
//   _(prim, TypeCheck)
//   _(prim, RequiresGradCheck)
//   _(prim, FallbackGraph)
//   _(prim, FusedConcat)
//   _(prim, ConstantChunk)
//   _(prim, MMTreeReduce)
//   _(prim, MMBatchSide)
//   _(prim, list)
//   _(prim, min)
//   _(prim, max)
//   _(prim, abs)
//   _(aten, divmod)
//   _(prim, zip)
//   _(prim, enumerate)
//   _(prim, range)
//   _(prim, rangelist)
//   _(prim, isinstance)
//   _(prim, tolist)
//   _(prim, unchecked_cast)
//   _(aten, _grad_sum_to_size)
//   _(aten, _size_if_not_equal)
//   _(aten, _ncf_unsqueeze)
//   _(aten, warn)
//   _(aten, sorted)
//   _(aten, floordiv)
//   _(aten, __range_length)
//   _(aten, __derive_index)
//   _(aten, __round_to_zero_floordiv)
//   _(aten, is_scripting)
//   _(aten, _unwrap_optional)
//   _(prim, fork)
//   _(prim, forkClosure)
//   _(prim, RaiseException)
//   _(prim, Closure)
//   _(prim, CreateObject)
//   _(prim, SetAttr)
//   _(prim, GetAttr)
//   _(prim, HasAttr)
//   _(prim, profile)
//   _(prim, profile_ivalue)
//   _(prim, AddStatValue)
//   _(prim, TimePoint)
//   _(prim, CallFunction)
//   _(prim, CallMethod)
//   _(prim, LoopContinuation)
//   _(prim, annotate)
//   _(prim, TracedModuleForward)
//   _(prim, TracedFork)
//   _(prim, TracedAttr)
//   _(prim, rpc_async)
//   _(prim, rpc_sync)
//   _(prim, rpc_remote)
//   _(prim, is_cuda)
//   _(aten, abs_)
//   _(aten, absolute)
//   _(aten, absolute_)
//   _(aten, acos)
//   _(aten, acos_)
//   _(aten, arccos)
//   _(aten, arccos_)
//   _(aten, acosh)
//   _(aten, acosh_)
//   _(aten, arccosh)
//   _(aten, arccosh_)
//   _(aten, asin)
//   _(aten, asin_)
//   _(aten, arcsin)
//   _(aten, arcsin_)
//   _(aten, asinh)
//   _(aten, asinh_)
//   _(aten, arcsinh)
//   _(aten, arcsinh_)
//   _(aten, atan)
//   _(aten, atan_)
//   _(aten, arctan)
//   _(aten, arctan_)
//   _(aten, atanh)
//   _(aten, atanh_)
//   _(aten, arctanh)
//   _(aten, arctanh_)
//   _(aten, clamp)
//   _(aten, clamp_)
//   _(aten, clip)
//   _(aten, clip_)
//   _(aten, det)
//   _(aten, linalg_det)
//   _(aten, linalg_norm)
//   _(aten, append)
//   _(aten, item)
//   _(aten, format)
//   _(aten, percentFormat)
//   _(aten, __not__)
//   _(aten, __is__)
//   _(aten, __isnot__)
//   _(aten, copy)
//   _(aten, copy_)
//   _(aten, div)
//   _(aten, div_)
//   _(aten, divide)
//   _(aten, divide_)
//   _(aten, true_divide)
//   _(aten, true_divide_)
//   _(aten, t_)
//   _(aten, addbmm_)
//   _(aten, addcdiv_)
//   _(aten, addcmul_)
//   _(aten, addmv_)
//   _(aten, addr_)
//   _(aten, baddbmm_)
//   _(aten, ge)
//   _(aten, ge_)
//   _(aten, greater_equal)
//   _(aten, greater_equal_)
//   _(aten, gt)
//   _(aten, gt_)
//   _(aten, greater)
//   _(aten, greater_)
//   _(aten, le)
//   _(aten, le_)
//   _(aten, less_equal)
//   _(aten, less_equal_)
//   _(aten, lerp_)
//   _(aten, lt)
//   _(aten, lt_)
//   _(aten, less)
//   _(aten, less_)
//   _(aten, isnan)
//   _(aten, mul)
//   _(aten, mul_)
//   _(aten, multiply)
//   _(aten, multiply_)
//   _(aten, ne)
//   _(aten, ne_)
//   _(aten, not_equal)
//   _(aten, not_equal_)
//   _(aten, _ger)
//   _(aten, ger)
//   _(aten, outer)
//   _(aten, transpose)
//   _(aten, transpose_)
//   _(aten, unsqueeze_)
//   _(aten, __getitem__)
//   _(aten, _set_item)
//   _(aten, manual_seed)
//   _(aten, set_)
//   _(aten, index_put_)
//   _(aten, device)
//   _(aten, hash)
//   _(aten, len)
//   _(aten, list)
//   _(aten, wait)
//   _(aten, save)
//   _(aten, sub)
//   _(aten, sub_)
//   _(aten, subtract)
//   _(aten, subtract_)
//   _(aten, keys)
//   _(aten, ord)
//   _(aten, chr)
//   _(aten, hex)
//   _(aten, oct)
//   _(aten, clear)
//   _(aten, trunc)
//   _(aten, trunc_)
//   _(aten, fix)
//   _(aten, fix_)
//   _(aten, neg)
//   _(aten, neg_)
//   _(aten, negative)
//   _(aten, negative_)
//   _(aten, setdefault)
//   _(aten, bin)
//   _(aten, pop)
//   _(aten, insert)
//   _(aten, vstack)
//   _(aten, row_stack)
//   _(prim, unchecked_unwrap_optional)
//   _(aten, __contains__)
//   _(prim, BailoutTemplate)
//   _(prim, grad)
//   _(aten, zero_)
//   _(aten, fill_)
//   _(aten, masked_fill_)
//   _(cuda, _set_device)
//   _(cuda, set_stream)
//   _(cuda, _current_device)
//   _(aten, swapaxes)
//   _(aten, swapaxes_)
//   _(aten, swapdims)
//   _(aten, swapdims_)
//   _(aten, movedim)
//   _(aten, moveaxis)
//   _(aten, has_torch_function)
//   FORALL_ATEN_BASE_SYMBOLS(_)
//   _(onnx, Add)
//   _(onnx, Concat)
//   _(onnx, Constant)
//   _(onnx, ConstantFill)
//   _(onnx, Div)
//   _(onnx, GRU)
//   _(onnx, Gather)
//   _(onnx, Gemm)
//   _(onnx, LSTM)
//   _(onnx, Mul)
//   _(onnx, Pow)
//   _(onnx, RNN)
//   _(onnx, Shape)
//   _(onnx, Size)
//   _(onnx, Slice)
//   _(onnx, Squeeze)
//   _(onnx, Sub)
//   _(onnx, Transpose)
//   _(onnx, Unsqueeze)
//   _(onnx, Loop)
//   _(onnx, If)
//   _(onnx, Reshape)
//   _(onnx, Expand)
//   _(onnx, Equal)
//   _(onnx, Greater)
//   _(onnx, GreaterOrEqual)
//   _(onnx, Less)
//   _(onnx, LessOrEqual)
//   _(onnx, Not)
//   _(onnx, ATen)
//   _(onnx, Split)
//   _(onnx, ConstantOfShape)
//   _(onnx, Cast)
//   _(onnx, Mod)
//   _(onnx, Sqrt)
//   _(onnx, SplitToSequence)
//   _(onnx, SequenceAt)
//   _(onnx, SequenceConstruct)
//   _(onnx, SequenceEmpty)
//   _(onnx, SequenceInsert)
//   _(onnx, SequenceErase)
//   _(onnx, ConcatFromSequence)
//   _(onnx, Identity)
//   _(onnx, SoftmaxCrossEntropyLoss)
//   _(onnx, NegativeLogLikelihoodLoss)
//   _(onnx, LogSoftmax)
//   _(onnx, ReduceL1)
//   _(onnx, ReduceL2)
//   _(onnx, Conv)
//   _(onnx, BatchNormalization)
//   _(onnx, ReduceProd)
//   FORALL_ATTR_BASE_SYMBOLS(_)
//   _(attr, Subgraph)
//   _(attr, ReverseSubgraph)
//   _(attr, f_real_outputs)
//   _(attr, df_input_vjps)
//   _(attr, df_input_captured_inputs)
//   _(attr, df_input_captured_outputs)
//   _(attr, df_output_vjps)
//   _(attr, axes)
//   _(attr, axis)
//   _(attr, broadcast)
//   _(attr, direction)
//   _(attr, ends)
//   _(attr, inplace)
//   _(attr, input_as_shape)
//   _(attr, is_zero)
//   _(attr, num_none)
//   _(attr, num_present)
//   _(attr, perm)
//   _(attr, sizes)
//   _(attr, starts)
//   _(attr, profiled_type)
//   _(attr, transA)
//   _(attr, transB)
//   _(attr, name)
//   _(attr, a)
//   _(attr, b)
//   _(attr, beg)
//   _(attr, idx)
//   _(attr, split)
//   _(attr, slot)
//   _(attr, kinds)
//   _(attr, types)
//   _(attr, scope)
//   _(attr, keepdims)
//   _(attr, cache_id)
//   _(attr, new_axis)
//   _(attr, warn_id)
// #else
// #define FORALL_NS_SYMBOLS(_)
//   _(namespaces, prim)
//   _(namespaces, aten)
//   _(namespaces, cuda)
//   _(namespaces, onnx)
//   _(namespaces, attr)
//   _(namespaces, scope)
//   _(namespaces, user)
//   _(namespaces, _caffe2)
//   _(namespaces, dimname)
//   _(namespaces, namespaces)
// #endif

// 'prim' symbols are synthetic operators that occur only in the IR
// and don't have corresponding implementations in ATen.

// 'onnx' symbols correspond to ONNX operators.  Their semantics
// are defined in https://github.com/onnx/onnx/blob/master/docs/Operators.md
// The particular version we are targeting is specified by '_onnx_opset_version'
// in torch.onnx.symbolic_helper
//
// In general, most ONNX operators won't get an entry here, because they
// are handled from the Python end.  However, you may occasionally need
// to intern an ONNX symbol here so that you can conveniently write an
// optimization on ONNX operations.

// 'attr' symbols are attribute keys.  They are shared between both ONNX and ATen
// operators (you disambiguate their meaning by looking at the operator itself).
// In general, you only need to define attribute keys that are used by
// onnx or prim; ATen attributes are automatically generated in FORALL_ATTR_BASE_SYMBOLS.

// Note [Symbol allocation]
// ~~~~~~~~~~~~~~~~~~~~~~~~
//
//  1. Symbol namespace is split up into namespaces.
//
//  2. The intended access pattern for built-in symbols is onnx::MatMul
//  in the c10 namespace (this is a Symbol).
//

// Built-in constant definition strategy:
// - Enum is the most convenient way to generate a contiguous sequence
//   of numbers for an identifier.
// - However, an enum gives you a fresh type.  We want onnx::MatMul to
//   be type Symbol, not some random enum type!
// - Therefore, after using enums to generate the sequence of integers,
//   we then declare constexpr Symbols to get everything the actual Symbol
//   type we want.  Symbols must be constexpr to be valid to be "case"ed on.


// Targeting ../Symbol.java





@Namespace("c10") public enum _keys {
    namespaces_prim(0),
  namespaces_aten(1),
  namespaces_cuda(2),
  namespaces_onnx(3),
  namespaces_attr(4),
  namespaces_scope(5),
  namespaces_user(6),
  namespaces__caffe2(7),
  namespaces_dimname(8),
  namespaces_namespaces(9),
  prim_Assign(10),
  prim_BroadcastingChunk(11),
  prim_BroadcastSizes(12),
  prim_ReductionSizes(13),
  prim_Constant(14),
  prim_ChunkSizes(15),
  prim_Drop(16),
  prim_Eval(17),
  prim_Expand(18), /* onnx */
  prim_FusionGroup(19),
  prim_CudaFusionGroup(20),
  prim_CudaFusionGuard(21),
  prim_FunctionalGraph(22),
  prim_DifferentiableGraph(23),
  prim_TensorExprGroup(24),
  prim_StaticSubgraph(25),
  prim_If(26),
  prim_Jump(27), /* debug */
  prim_JumpNZ(28), /* debug */
  prim_JumpZ(29), /* debug */
  prim_Load(30),
  prim_Loop(31),
  prim_Param(32),
  prim_PackPadded(33), /* onnx */
  prim_PadPacked(34), /* onnx */
  prim_Placeholder(35), /* debug */
  prim_Print(36),
  prim_PythonOp(37),
  prim_IgnoredPythonOp(38),
  prim_Reverse(39),
  prim_Return(40),
  prim_ReturnStmt(41),
  prim_BreakStmt(42),
  prim_ContinueStmt(43),
  prim_ComprehensionScope(44),
  prim_Store(45),
  prim_AutogradZero(46),
  prim_AutogradAnyNonZero(47),
  prim_AutogradAllNonZero(48),
  prim_AutogradAllZero(49),
  prim_Starred(50),
  prim_TupleConstruct(51),
  prim_TupleUnpack(52),
  prim_TupleIndex(53),
  prim_TupleSlice(54),
  prim_ListConstruct(55),
  prim_ListUnpack(56),
  prim_DictConstruct(57),
  prim_ModuleDictIndex(58),
  prim_EnumName(59),
  prim_EnumValue(60),
  prim_StringIndex(61),
  prim_NumToTensor(62),
  prim_Uninitialized(63),
  prim_With(64),
  prim_Enter(65),
  prim_Exit(66),
  aten_Bool(67),
  aten_Int(68),
  aten_FloatImplicit(69),
  aten_IntImplicit(70),
  aten_ScalarImplicit(71),
  aten_Float(72),
  aten_str(73),
  aten_Delete(74),
  prim_device(75),
  prim_dtype(76),
  prim_layout(77),
  prim_id(78),
  prim_requires_grad(79),
  prim_MakeTestTensor(80), /* test */
  prim_AutogradAdd(81),
  prim_GradOf(82),
  aten_grad(83),
  aten_backward(84),
  prim_Guard(85),
  prim_BailOut(86),
  prim_TypeCheck(87),
  prim_RequiresGradCheck(88),
  prim_FallbackGraph(89),
  prim_FusedConcat(90),
  prim_ConstantChunk(91),
  prim_MMTreeReduce(92),
  prim_MMBatchSide(93),
  prim_list(94),
  prim_min(95),
  prim_max(96),
  prim_abs(97),
  aten_divmod(98),
  prim_zip(99),
  prim_enumerate(100),
  prim_range(101),
  prim_rangelist(102),
  prim_isinstance(103),
  prim_tolist(104),
  prim_unchecked_cast(105),
  aten__grad_sum_to_size(106),
  aten__size_if_not_equal(107),
  aten__ncf_unsqueeze(108),
  aten_warn(109),
  aten_sorted(110),
  aten_floordiv(111),
  aten___range_length(112),
  aten___derive_index(113),
  aten___round_to_zero_floordiv(114),
  aten_is_scripting(115),
  aten__unwrap_optional(116),
  prim_fork(117),
  prim_forkClosure(118),
  prim_RaiseException(119),
  prim_Closure(120),
  prim_CreateObject(121),
  prim_SetAttr(122),
  prim_GetAttr(123),
  prim_HasAttr(124),
  prim_profile(125),
  prim_profile_ivalue(126),
  prim_AddStatValue(127),
  prim_TimePoint(128),
  prim_CallFunction(129),
  prim_CallMethod(130),
  prim_LoopContinuation(131),
  prim_annotate(132),
  prim_TracedModuleForward(133),
  prim_TracedFork(134),
  prim_TracedAttr(135),
  prim_rpc_async(136),
  prim_rpc_sync(137),
  prim_rpc_remote(138),
  prim_is_cuda(139),
  aten_abs_(140),
  aten_absolute(141),
  aten_absolute_(142),
  aten_acos(143),
  aten_acos_(144),
  aten_arccos(145),
  aten_arccos_(146),
  aten_acosh(147),
  aten_acosh_(148),
  aten_arccosh(149),
  aten_arccosh_(150),
  aten_asin(151),
  aten_asin_(152),
  aten_arcsin(153),
  aten_arcsin_(154),
  aten_asinh(155),
  aten_asinh_(156),
  aten_arcsinh(157),
  aten_arcsinh_(158),
  aten_atan(159),
  aten_atan_(160),
  aten_arctan(161),
  aten_arctan_(162),
  aten_atanh(163),
  aten_atanh_(164),
  aten_arctanh(165),
  aten_arctanh_(166),
  aten_clamp(167),
  aten_clamp_(168),
  aten_clip(169),
  aten_clip_(170),
  aten_det(171),
  aten_linalg_det(172),
  aten_linalg_norm(173),
  aten_append(174),
  aten_item(175),
  aten_format(176),
  aten_percentFormat(177),
  aten___not__(178),
  aten___is__(179),
  aten___isnot__(180),
  aten_copy(181),
  aten_copy_(182),
  aten_div(183),
  aten_div_(184),
  aten_divide(185),
  aten_divide_(186),
  aten_true_divide(187),
  aten_true_divide_(188),
  aten_t_(189),
  aten_addbmm_(190),
  aten_addcdiv_(191),
  aten_addcmul_(192),
  aten_addmv_(193),
  aten_addr_(194),
  aten_baddbmm_(195),
  aten_ge(196),
  aten_ge_(197),
  aten_greater_equal(198),
  aten_greater_equal_(199),
  aten_gt(200),
  aten_gt_(201),
  aten_greater(202),
  aten_greater_(203),
  aten_le(204),
  aten_le_(205),
  aten_less_equal(206),
  aten_less_equal_(207),
  aten_lerp_(208),
  aten_lt(209),
  aten_lt_(210),
  aten_less(211),
  aten_less_(212),
  aten_isnan(213),
  aten_mul(214),
  aten_mul_(215),
  aten_multiply(216),
  aten_multiply_(217),
  aten_ne(218),
  aten_ne_(219),
  aten_not_equal(220),
  aten_not_equal_(221),
  aten__ger(222),
  aten_ger(223),
  aten_outer(224),
  aten_transpose(225),
  aten_transpose_(226),
  aten_unsqueeze_(227),
  aten___getitem__(228),
  aten__set_item(229),
  aten_manual_seed(230),
  aten_set_(231),
  aten_index_put_(232),
  aten_device(233),
  aten_hash(234),
  aten_len(235),
  aten_list(236),
  aten_wait(237),
  aten_save(238),
  aten_sub(239),
  aten_sub_(240),
  aten_subtract(241),
  aten_subtract_(242),
  aten_keys(243),
  aten_ord(244),
  aten_chr(245),
  aten_hex(246),
  aten_oct(247),
  aten_clear(248),
  aten_trunc(249),
  aten_trunc_(250),
  aten_fix(251),
  aten_fix_(252),
  aten_neg(253),
  aten_neg_(254),
  aten_negative(255),
  aten_negative_(256),
  aten_setdefault(257),
  aten_bin(258),
  aten_pop(259),
  aten_insert(260),
  aten_vstack(261),
  aten_row_stack(262),
  prim_unchecked_unwrap_optional(263),
  aten___contains__(264),
  prim_BailoutTemplate(265),
  prim_grad(266),
  aten_zero_(267),
  aten_fill_(268),
  aten_masked_fill_(269),
  cuda__set_device(270),
  cuda_set_stream(271),
  cuda__current_device(272),
  aten_swapaxes(273),
  aten_swapaxes_(274),
  aten_swapdims(275),
  aten_swapdims_(276),
  aten_movedim(277),
  aten_moveaxis(278),
  aten_has_torch_function(279),
  aten___and__(280),
aten___iand__(281),
aten___ilshift__(282),
aten___ior__(283),
aten___irshift__(284),
aten___ixor__(285),
aten___lshift__(286),
aten___or__(287),
aten___rshift__(288),
aten___xor__(289),
aten__abs(290),
aten__addmv(291),
aten__addr(292),
aten__amp_foreach_non_finite_check_and_unscale_(293),
aten__amp_update_scale(294),
aten__arange(295),
aten__argmax(296),
aten__argmin(297),
aten__baddbmm_mkl(298),
aten__cast_Byte(299),
aten__cast_Char(300),
aten__cast_Double(301),
aten__cast_Float(302),
aten__cast_Half(303),
aten__cast_Int(304),
aten__cast_Long(305),
aten__cast_Short(306),
aten__cat(307),
aten__ceil(308),
aten__clamp_max(309),
aten__clamp_min(310),
aten__convolution(311),
aten__convolution_double_backward(312),
aten_convolution_overrideable(313),
aten_convolution_backward_overrideable(314),
aten__convolution_nogroup(315),
aten__copy_ignoring_overlaps(316),
aten__cos(317),
aten__cosh(318),
aten__ctc_loss(319),
aten__ctc_loss_backward(320),
aten__cudnn_ctc_loss(321),
aten__cudnn_init_dropout_state(322),
aten__cudnn_rnn(323),
aten__cudnn_rnn_backward(324),
aten__cudnn_rnn_flatten_weight(325),
aten__cufft_clear_plan_cache(326),
aten__cufft_get_plan_cache_max_size(327),
aten__cufft_get_plan_cache_size(328),
aten__cufft_set_plan_cache_max_size(329),
aten__cumprod(330),
aten__cumsum(331),
aten__denseDims(332),
aten__dimI(333),
aten__dimV(334),
aten__dim_arange(335),
aten__dirichlet_grad(336),
aten__dot(337),
aten__embedding_bag(338),
aten__embedding_bag_backward(339),
aten__embedding_bag_dense_backward(340),
aten__embedding_bag_sparse_backward(341),
aten__erf(342),
aten__erfc(343),
aten__exp(344),
aten__exp2(345),
aten__expm1(346),
aten__fft_with_size(347),
aten__fill(348),
aten__floor(349),
aten__fused_dropout(350),
aten__indexCopy(351),
aten__indices(352),
aten__ldexp(353),
aten__linspace(354),
aten__local_scalar(355),
aten__local_scalar_dense(356),
aten__log(357),
aten__log10(358),
aten__log1p(359),
aten__log2(360),
aten__logspace(361),
aten__lu_with_info(362),
aten__masked_scale(363),
aten__mm(364),
aten__mv(365),
aten__nnz(366),
aten__nansum(367),
aten__pack_padded_sequence(368),
aten__pack_padded_sequence_backward(369),
aten__pad_packed_sequence(370),
aten__pdist_backward(371),
aten__pdist_forward(372),
aten__prod(373),
aten__prodall(374),
aten__range(375),
aten__reshape_from_tensor(376),
aten__round(377),
aten__rsqrt(378),
aten__s_where(379),
aten__shape_as_tensor(380),
aten__sigmoid(381),
aten__sigmoid_forward(382),
aten__sin(383),
aten__sinh(384),
aten__sparseDims(385),
aten__sparse_add(386),
aten__sparse_addmm(387),
aten__sparse_coo_tensor_with_dims(388),
aten__sparse_coo_tensor_with_dims_and_tensors(389),
aten__sparse_coo_tensor_unsafe(390),
aten__sparse_dense_add(391),
aten__sparse_div_scalar(392),
aten__sparse_div_zerodim(393),
aten__sparse_mul(394),
aten__sparse_mul_scalar(395),
aten__sparse_mul_zerodim(396),
aten__sparse_sum(397),
aten__sqrt(398),
aten__square(399),
aten__standard_gamma(400),
aten__standard_gamma_grad(401),
aten__sum(402),
aten__sum_cuda(403),
aten__tan(404),
aten__tanh(405),
aten__tanh_forward(406),
aten__th_get_device(407),
aten__th_kthvalue(408),
aten__th_mode(409),
aten__th_prod(410),
aten__th_sigmoid(411),
aten__th_std(412),
aten__th_sum(413),
aten__th_tanh(414),
aten__th_var(415),
aten__thnn_fused_gru_cell(416),
aten__thnn_fused_gru_cell_backward(417),
aten__thnn_fused_lstm_cell(418),
aten__thnn_fused_lstm_cell_backward(419),
aten__trilinear(420),
aten__trunc(421),
aten__unique(422),
aten__unique_dim(423),
aten__unsafe_view(424),
aten__validate_sparse_coo_tensor_args(425),
aten__values(426),
aten__weight_norm(427),
aten__weight_norm_cuda_interface(428),
aten__weight_norm_cuda_interface_backward(429),
aten__weight_norm_differentiable_backward(430),
aten_abs(431),
aten_adaptive_avg_pool1d(432),
aten_adaptive_avg_pool2d(433),
aten_adaptive_avg_pool2d_backward(434),
aten_adaptive_avg_pool2d_forward(435),
aten_adaptive_avg_pool3d(436),
aten_adaptive_avg_pool3d_backward(437),
aten_adaptive_avg_pool3d_forward(438),
aten_adaptive_max_pool1d(439),
aten_adaptive_max_pool2d(440),
aten_adaptive_max_pool2d_backward(441),
aten_adaptive_max_pool2d_forward(442),
aten_adaptive_max_pool3d(443),
aten_adaptive_max_pool3d_backward(444),
aten_adaptive_max_pool3d_forward(445),
aten_add(446),
aten_add_(447),
aten_addbmm(448),
aten_addcdiv(449),
aten_addcmul(450),
aten_addmm(451),
aten_addmv(452),
aten_addr(453),
aten_affine_grid_generator(454),
aten_affine_grid_generator_backward(455),
aten_alias(456),
aten_all(457),
aten_allclose(458),
aten_alpha_dropout(459),
aten_any(460),
aten_arange(461),
aten_argmax(462),
aten_argmin(463),
aten_as_strided(464),
aten_as_tensor(465),
aten_atan2(466),
aten_atleast_1d(467),
aten_atleast_2d(468),
aten_atleast_3d(469),
aten_avg_pool1d(470),
aten_avg_pool2d(471),
aten_avg_pool2d_backward(472),
aten_avg_pool2d_forward(473),
aten_avg_pool3d(474),
aten_avg_pool3d_backward(475),
aten_avg_pool3d_forward(476),
aten_baddbmm(477),
aten_bartlett_window(478),
aten_batch_norm(479),
aten_bernoulli(480),
aten_bilinear(481),
aten_binary_cross_entropy(482),
aten_binary_cross_entropy_backward(483),
aten_binary_cross_entropy_forward(484),
aten_binary_cross_entropy_with_logits(485),
aten_binary_cross_entropy_with_logits_backward(486),
aten_binary_cross_entropy_with_logits_target_backward(487),
aten_bincount(488),
aten_blackman_window(489),
aten_block_diag(490),
aten_bmm(491),
aten_broadcast_tensors(492),
aten_broadcast_to(493),
aten_cartesian_prod(494),
aten_cat(495),
aten_cauchy(496),
aten_ceil(497),
aten_celu(498),
aten_chain_matmul(499),
aten_cholesky(500),
aten_cholesky_inverse(501),
aten_cholesky_solve(502),
aten_chunk(503),
aten_clamp_max(504),
aten_clamp_min(505),
aten_clone(506),
aten_coalesce(507),
aten_combinations(508),
aten__conj(509),
aten_conj(510),
aten_complex(511),
aten_copysign(512),
aten_polar(513),
aten_constant_pad_nd(514),
aten_contiguous(515),
aten_conv1d(516),
aten_conv2d(517),
aten_conv3d(518),
aten_conv_tbc(519),
aten_conv_tbc_backward(520),
aten_conv_transpose1d(521),
aten_convolution(522),
aten_copy_sparse_to_sparse(523),
aten_cos(524),
aten_cosh(525),
aten_cosine_embedding_loss(526),
aten_cosine_similarity(527),
aten_count_nonzero(528),
aten_cross(529),
aten_std_mean(530),
aten_var_mean(531),
aten_ctc_loss(532),
aten_cudnn_affine_grid_generator(533),
aten_cudnn_affine_grid_generator_backward(534),
aten_cudnn_batch_norm(535),
aten_cudnn_batch_norm_backward(536),
aten_cudnn_convolution(537),
aten_cudnn_convolution_backward(538),
aten_cudnn_convolution_backward_bias(539),
aten_cudnn_convolution_backward_input(540),
aten_cudnn_convolution_backward_weight(541),
aten_cudnn_convolution_transpose(542),
aten_cudnn_convolution_transpose_backward(543),
aten_cudnn_convolution_transpose_backward_bias(544),
aten_cudnn_convolution_transpose_backward_input(545),
aten_cudnn_convolution_transpose_backward_weight(546),
aten_cudnn_grid_sampler(547),
aten_cudnn_grid_sampler_backward(548),
aten_cudnn_is_acceptable(549),
aten_cummax(550),
aten_cummin(551),
aten_cumprod(552),
aten_cumsum(553),
aten_data_ptr(554),
aten_deg2rad(555),
aten_detach(556),
aten_diag(557),
aten_diag_embed(558),
aten_diagflat(559),
aten_diagonal(560),
aten_fill_diagonal_(561),
aten_diff(562),
aten_digamma(563),
aten_dim(564),
aten_dist(565),
aten_dot(566),
aten_dropout(567),
aten_dstack(568),
aten_eig(569),
aten_einsum(570),
aten_elu(571),
aten_elu_backward(572),
aten_elu_forward(573),
aten_embedding(574),
aten_embedding_backward(575),
aten_embedding_bag(576),
aten_embedding_dense_backward(577),
aten_embedding_renorm(578),
aten_embedding_sparse_backward(579),
aten_empty(580),
aten_empty_like(581),
aten_empty_strided(582),
aten_eq(583),
aten_equal(584),
aten_erf(585),
aten_erfc(586),
aten_erfinv(587),
aten_exp(588),
aten_expand(589),
aten_expand_as(590),
aten_expm1(591),
aten_exponential(592),
aten_eye(593),
aten_feature_alpha_dropout(594),
aten_feature_dropout(595),
aten_fft(596),
aten_fill(597),
aten_flatten(598),
aten_flip(599),
aten_fliplr(600),
aten_flipud(601),
aten_floor(602),
aten_fmod(603),
aten_fmod_(604),
aten_fmax(605),
aten_fmin(606),
aten_frac(607),
aten_fractional_max_pool2d(608),
aten_fractional_max_pool2d_backward(609),
aten_fractional_max_pool2d_forward(610),
aten_frobenius_norm(611),
aten_full(612),
aten_full_like(613),
aten_gather(614),
aten_gcd(615),
aten_gelu(616),
aten_geometric(617),
aten_geqrf(618),
aten_get_device(619),
aten_glu(620),
aten_glu_backward(621),
aten_glu_forward(622),
aten_grid_sampler(623),
aten_grid_sampler_2d(624),
aten_grid_sampler_2d_backward(625),
aten_grid_sampler_3d(626),
aten_grid_sampler_3d_backward(627),
aten_group_norm(628),
aten_gru(629),
aten_gru_cell(630),
aten_hamming_window(631),
aten_hann_window(632),
aten_hardshrink(633),
aten_hardshrink_backward(634),
aten_hardsigmoid(635),
aten_hardsigmoid_backward(636),
aten_hardtanh(637),
aten_hardtanh_backward(638),
aten_hardtanh_forward(639),
aten_heaviside(640),
aten_hinge_embedding_loss(641),
aten_histc(642),
aten_hspmm(643),
aten_hstack(644),
aten_hypot(645),
aten_i0(646),
aten_i0_(647),
aten_igamma(648),
aten_igamma_(649),
aten_igammac(650),
aten_igammac_(651),
aten_ifft(652),
aten_index(653),
aten_index_add(654),
aten_index_copy(655),
aten_index_fill(656),
aten_index_put(657),
aten_index_select(658),
aten_indices(659),
aten_inner(660),
aten_instance_norm(661),
aten_inverse(662),
aten_irfft(663),
aten_is_coalesced(664),
aten_is_complex(665),
aten_is_contiguous(666),
aten_is_cuda(667),
aten_is_distributed(668),
aten_is_floating_point(669),
aten_is_nonzero(670),
aten_is_same_size(671),
aten_is_set_to(672),
aten_is_signed(673),
aten_is_sparse(674),
aten_isclose(675),
aten_isreal(676),
aten_istft(677),
aten_isposinf(678),
aten_isneginf(679),
aten_kaiser_window(680),
aten_kl_div(681),
aten_kl_div_backward(682),
aten_kthvalue(683),
aten_l1_loss(684),
aten_l1_loss_backward(685),
aten_l1_loss_forward(686),
aten_layer_norm(687),
aten_lcm(688),
aten_leaky_relu(689),
aten_leaky_relu_backward(690),
aten_leaky_relu_forward(691),
aten_lerp(692),
aten_lgamma(693),
aten_linear(694),
aten_linspace(695),
aten_log(696),
aten_log10(697),
aten_log1p(698),
aten_log2(699),
aten_log_normal(700),
aten_log_sigmoid(701),
aten_log_sigmoid_backward(702),
aten_log_sigmoid_forward(703),
aten_log_softmax(704),
aten__log_softmax(705),
aten__log_softmax_backward_data(706),
aten_logcumsumexp(707),
aten_logdet(708),
aten_logit(709),
aten_logspace(710),
aten_logsumexp(711),
aten_xlogy(712),
aten_lstm(713),
aten_lstm_cell(714),
aten_lstsq(715),
aten_lu_solve(716),
aten_margin_ranking_loss(717),
aten_masked_fill(718),
aten_masked_scatter(719),
aten_masked_select(720),
aten_matmul(721),
aten_matrix_power(722),
aten_matrix_rank(723),
aten_matrix_exp(724),
aten_max(725),
aten_max_pool1d(726),
aten_max_pool1d_with_indices(727),
aten_max_pool2d(728),
aten_max_pool2d_with_indices(729),
aten_max_pool2d_with_indices_backward(730),
aten_max_pool2d_with_indices_forward(731),
aten_max_pool3d(732),
aten_max_pool3d_with_indices(733),
aten_max_pool3d_with_indices_backward(734),
aten_max_pool3d_with_indices_forward(735),
aten_max_unpool2d(736),
aten_max_unpool2d_backward(737),
aten_max_unpool2d_forward(738),
aten_max_unpool3d(739),
aten_max_unpool3d_backward(740),
aten_max_unpool3d_forward(741),
aten_max_values(742),
aten_mean(743),
aten_median(744),
aten_nanmedian(745),
aten_meshgrid(746),
aten_min(747),
aten_min_values(748),
aten_miopen_batch_norm(749),
aten_miopen_batch_norm_backward(750),
aten_miopen_convolution(751),
aten_miopen_convolution_backward(752),
aten_miopen_convolution_backward_bias(753),
aten_miopen_convolution_backward_input(754),
aten_miopen_convolution_backward_weight(755),
aten_miopen_convolution_transpose(756),
aten_miopen_convolution_transpose_backward(757),
aten_miopen_convolution_transpose_backward_input(758),
aten_miopen_convolution_transpose_backward_weight(759),
aten_miopen_depthwise_convolution(760),
aten_miopen_depthwise_convolution_backward(761),
aten_miopen_depthwise_convolution_backward_input(762),
aten_miopen_depthwise_convolution_backward_weight(763),
aten_miopen_rnn(764),
aten_miopen_rnn_backward(765),
aten_mkldnn_convolution(766),
aten_mkldnn_convolution_backward(767),
aten_mkldnn_convolution_backward_input(768),
aten_mkldnn_convolution_backward_weights(769),
aten_mm(770),
aten_mode(771),
aten_mse_loss(772),
aten_mse_loss_backward(773),
aten_mse_loss_forward(774),
aten_msort(775),
aten_multi_margin_loss(776),
aten_multi_margin_loss_backward(777),
aten_multi_margin_loss_forward(778),
aten_multilabel_margin_loss(779),
aten_multilabel_margin_loss_backward(780),
aten_multilabel_margin_loss_forward(781),
aten_multinomial(782),
aten_mv(783),
aten_mvlgamma(784),
aten_nansum(785),
aten_nan_to_num(786),
aten_narrow(787),
aten_narrow_copy(788),
aten_native_batch_norm(789),
aten_native_batch_norm_backward(790),
aten_native_clone(791),
aten_native_get_device(792),
aten_native_norm(793),
aten_native_pow(794),
aten_native_resize_as(795),
aten_native_tensor(796),
aten_native_zero(797),
aten_nextafter(798),
aten_bitwise_and(799),
aten_bitwise_not(800),
aten_bitwise_or(801),
aten_bitwise_xor(802),
aten_element_size(803),
aten_nll_loss(804),
aten_nll_loss2d(805),
aten_nll_loss2d_backward(806),
aten_nll_loss2d_forward(807),
aten_nll_loss_backward(808),
aten_nll_loss_forward(809),
aten_nonzero(810),
aten_nonzero_numpy(811),
aten_norm(812),
aten_norm_except_dim(813),
aten_normal(814),
aten_nuclear_norm(815),
aten_numel(816),
aten_ones(817),
aten_ones_like(818),
aten_orgqr(819),
aten_ormqr(820),
aten_pairwise_distance(821),
aten__euclidean_dist(822),
aten_pdist(823),
aten_cdist(824),
aten_permute(825),
aten_pin_memory(826),
aten_pinverse(827),
aten_pixel_shuffle(828),
aten_pixel_unshuffle(829),
aten_poisson(830),
aten_polygamma(831),
aten_pow(832),
aten_float_power(833),
aten_prelu(834),
aten_prelu_backward(835),
aten_prod(836),
aten_put(837),
aten_qr(838),
aten_quantile(839),
aten_nanquantile(840),
aten_rad2deg(841),
aten_rand(842),
aten_rand_like(843),
aten_randint(844),
aten_randint_like(845),
aten_randn(846),
aten_randn_like(847),
aten_random(848),
aten_randperm(849),
aten_range(850),
aten_ravel(851),
aten_reciprocal(852),
aten_reflection_pad1d(853),
aten_reflection_pad1d_backward(854),
aten_reflection_pad1d_forward(855),
aten_reflection_pad2d(856),
aten_reflection_pad2d_backward(857),
aten_reflection_pad2d_forward(858),
aten_relu(859),
aten_remainder(860),
aten_renorm(861),
aten_repeat(862),
aten_replication_pad1d(863),
aten_replication_pad1d_backward(864),
aten_replication_pad1d_forward(865),
aten_replication_pad2d(866),
aten_replication_pad2d_backward(867),
aten_replication_pad2d_forward(868),
aten_replication_pad3d(869),
aten_replication_pad3d_backward(870),
aten_replication_pad3d_forward(871),
aten_reshape(872),
aten_reshape_as(873),
aten_resize(874),
aten_resize_(875),
aten_resize_as(876),
aten_resize_as_(877),
aten_rfft(878),
aten_rnn_relu(879),
aten_rnn_relu_cell(880),
aten_rnn_tanh(881),
aten_rnn_tanh_cell(882),
aten_rot90(883),
aten_round(884),
aten_rrelu(885),
aten_rrelu_with_noise(886),
aten_rrelu_with_noise_backward(887),
aten_rrelu_with_noise_forward(888),
aten_rsqrt(889),
aten_scatter(890),
aten_scatter_add(891),
aten_select(892),
aten_selu(893),
aten_set(894),
aten_sigmoid(895),
aten_sign(896),
aten_signbit(897),
aten_silu(898),
aten_sgn(899),
aten_sin(900),
aten_sinc(901),
aten_sinh(902),
aten_size(903),
aten_sizes(904),
aten_slice(905),
aten_slogdet(906),
aten_smm(907),
aten_smooth_l1_loss(908),
aten_smooth_l1_loss_backward(909),
aten_smooth_l1_loss_forward(910),
aten_soft_margin_loss(911),
aten_soft_margin_loss_backward(912),
aten_soft_margin_loss_forward(913),
aten_softmax(914),
aten__softmax(915),
aten__softmax_backward_data(916),
aten_softplus(917),
aten_softplus_backward(918),
aten_softplus_forward(919),
aten_softshrink(920),
aten_softshrink_backward(921),
aten_softshrink_forward(922),
aten_solve(923),
aten_sort(924),
aten_sparse_coo_tensor(925),
aten_sparse_mask(926),
aten_sparse_resize(927),
aten_sparse_resize_and_clear(928),
aten_split(929),
aten_split_with_sizes(930),
aten_sqrt(931),
aten_square(932),
aten_squeeze(933),
aten_sspaddmm(934),
aten_stack(935),
aten_std(936),
aten_stft(937),
aten_storage_offset(938),
aten_stride(939),
aten_strides(940),
aten_rsub(941),
aten_sum(942),
aten_sum_to_size(943),
aten_svd(944),
aten_symeig(945),
aten_t(946),
aten_take(947),
aten_tan(948),
aten_tanh(949),
aten_tensor(950),
aten_tensordot(951),
aten_tensor_split(952),
aten_th_clone(953),
aten_th_norm(954),
aten_th_pow(955),
aten_th_resize_as(956),
aten_th_tensor(957),
aten_th_zero(958),
aten_thnn_conv2d(959),
aten_thnn_conv2d_backward(960),
aten_thnn_conv2d_forward(961),
aten_tile(962),
aten_slow_conv3d(963),
aten_slow_conv3d_backward(964),
aten_slow_conv3d_forward(965),
aten_thnn_conv_depthwise2d(966),
aten_thnn_conv_depthwise2d_backward(967),
aten_thnn_conv_depthwise2d_forward(968),
aten_slow_conv_dilated2d(969),
aten_slow_conv_dilated2d_backward(970),
aten_slow_conv_dilated3d(971),
aten_slow_conv_dilated3d_backward(972),
aten_slow_conv_transpose2d(973),
aten_slow_conv_transpose2d_backward(974),
aten_slow_conv_transpose3d(975),
aten_slow_conv_transpose3d_backward(976),
aten_threshold(977),
aten_threshold_backward(978),
aten_to(979),
aten_to_sparse(980),
aten_to_dense(981),
aten_topk(982),
aten_trace(983),
aten_triangular_solve(984),
aten_tril(985),
aten_triplet_margin_loss(986),
aten_triu(987),
aten_type_as(988),
aten_unbind(989),
aten_unfold(990),
aten_uniform(991),
aten_unsafe_chunk(992),
aten_unsafe_split(993),
aten_unsafe_split_with_sizes(994),
aten_unsqueeze(995),
aten_upsample_bilinear2d(996),
aten_upsample_bilinear2d_backward(997),
aten_upsample_bilinear2d_forward(998),
aten_upsample_bicubic2d(999),
aten_upsample_bicubic2d_backward(1000),
aten_upsample_bicubic2d_forward(1001),
aten_upsample_linear1d(1002),
aten_upsample_linear1d_backward(1003),
aten_upsample_linear1d_forward(1004),
aten_upsample_nearest1d(1005),
aten_upsample_nearest1d_backward(1006),
aten_upsample_nearest1d_forward(1007),
aten_upsample_nearest2d(1008),
aten_upsample_nearest2d_backward(1009),
aten_upsample_nearest2d_forward(1010),
aten_upsample_nearest3d(1011),
aten_upsample_nearest3d_backward(1012),
aten_upsample_nearest3d_forward(1013),
aten_upsample_trilinear3d(1014),
aten_upsample_trilinear3d_backward(1015),
aten_upsample_trilinear3d_forward(1016),
aten_values(1017),
aten_vander(1018),
aten_var(1019),
aten_view(1020),
aten_view_as(1021),
aten_where(1022),
aten_zero(1023),
aten_zeros(1024),
aten_zeros_like(1025),
aten_real(1026),
aten_imag(1027),
aten_view_as_real(1028),
aten_view_as_complex(1029),
/* nothing */
  onnx_Add(1030),
  onnx_Concat(1031),
  onnx_Constant(1032),
  onnx_ConstantFill(1033),
  onnx_Div(1034),
  onnx_GRU(1035),
  onnx_Gather(1036),
  onnx_Gemm(1037),
  onnx_LSTM(1038),
  onnx_Mul(1039),
  onnx_Pow(1040),
  onnx_RNN(1041),
  onnx_Shape(1042),
  onnx_Size(1043),
  onnx_Slice(1044),
  onnx_Squeeze(1045),
  onnx_Sub(1046),
  onnx_Transpose(1047),
  onnx_Unsqueeze(1048),
  onnx_Loop(1049),
  onnx_If(1050),
  onnx_Reshape(1051),
  onnx_Expand(1052),
  onnx_Equal(1053),
  onnx_Greater(1054),
  onnx_GreaterOrEqual(1055),
  onnx_Less(1056),
  onnx_LessOrEqual(1057),
  onnx_Not(1058),
  onnx_ATen(1059),
  onnx_Split(1060),
  onnx_ConstantOfShape(1061),
  onnx_Cast(1062),
  onnx_Mod(1063),
  onnx_Sqrt(1064),
  onnx_SplitToSequence(1065),
  onnx_SequenceAt(1066),
  onnx_SequenceConstruct(1067),
  onnx_SequenceEmpty(1068),
  onnx_SequenceInsert(1069),
  onnx_SequenceErase(1070),
  onnx_ConcatFromSequence(1071),
  onnx_Identity(1072),
  onnx_SoftmaxCrossEntropyLoss(1073),
  onnx_NegativeLogLikelihoodLoss(1074),
  onnx_LogSoftmax(1075),
  onnx_ReduceL1(1076),
  onnx_ReduceL2(1077),
  onnx_Conv(1078),
  onnx_BatchNormalization(1079),
  onnx_ReduceProd(1080),
  attr_A(1081),
attr_C(1082),
attr_H(1083),
attr_LU_data(1084),
attr_LU_pivots(1085),
attr_N(1086),
attr_W(1087),
attr_accumulate(1088),
attr_align_corners(1089),
attr_alpha(1090),
attr_anchor(1091),
attr_argmaxes(1092),
attr_atol(1093),
attr_b_hh(1094),
attr_b_ih(1095),
attr_bag_size(1096),
attr_base(1097),
attr_batch1(1098),
attr_batch2(1099),
attr_batch_first(1100),
attr_batch_sizes(1101),
attr_benchmark(1102),
attr_beta(1103),
attr_bias(1104),
attr_bias_defined(1105),
attr_bidirectional(1106),
attr_bins(1107),
attr_blank(1108),
attr_buffer(1109),
attr_ceil_mode(1110),
attr_checked_signal_sizes(1111),
attr_chunks(1112),
attr_columns(1113),
attr_column_stack(1114),
attr_complex_input(1115),
attr_complex_output(1116),
attr_condition(1117),
attr_count_include_pad(1118),
attr_cudnn_enable(1119),
attr_cudnn_enabled(1120),
attr_cx(1121),
attr_cy(1122),
attr_data(1123),
attr_dense_dim(1124),
attr_descending(1125),
attr_deterministic(1126),
attr_device(1127),
attr_diagonal(1128),
attr_dilation(1129),
attr_dim(1130),
attr_dim0(1131),
attr_dim1(1132),
attr_dim2(1133),
attr_dimension(1134),
attr_dims(1135),
attr_dims_other(1136),
attr_dims_self(1137),
attr_divisor_override(1138),
attr_dropout(1139),
attr_dropout_seed(1140),
attr_dropout_state(1141),
attr_dtype(1142),
attr_eigenvectors(1143),
attr_end(1144),
attr_end_dim(1145),
attr_eps(1146),
attr_epsilon(1147),
attr_equal_nan(1148),
attr_equation(1149),
attr_expand1(1150),
attr_expand2(1151),
attr_expand3(1152),
attr_exponent(1153),
attr_exponential_average_factor(1154),
attr_fgrad_input(1155),
attr_fill_value(1156),
attr_finput(1157),
attr_from(1158),
attr_g(1159),
attr_gO(1160),
attr_generator(1161),
attr_ggI(1162),
attr_ggW(1163),
attr_ggb(1164),
attr_grad(1165),
attr_gradOutput(1166),
attr_grad_bias(1167),
attr_grad_cy(1168),
attr_grad_hy(1169),
attr_grad_input(1170),
attr_grad_out(1171),
attr_grad_output(1172),
attr_grad_w(1173),
attr_grad_weight(1174),
attr_grid(1175),
attr_groups(1176),
attr_has_bias(1177),
attr_has_biases(1178),
attr_hidden_bias(1179),
attr_hidden_gates(1180),
attr_hidden_size(1181),
attr_high(1182),
attr_hop_length(1183),
attr_hx(1184),
attr_i1(1185),
attr_i2(1186),
attr_i3(1187),
attr_ignore_index(1188),
attr_implicit(1189),
attr_index(1190),
attr_indices(1191),
attr_info(1192),
attr_input(1193),
attr_input1(1194),
attr_input2(1195),
attr_input3(1196),
attr_input_bias(1197),
attr_input_gates(1198),
attr_input_lengths(1199),
attr_input_scale(1200),
attr_input_size(1201),
attr_interpolation_mode(1202),
attr_inverse(1203),
attr_is_target(1204),
attr_k(1205),
attr_keepdim(1206),
attr_kernel_size(1207),
attr_lambd(1208),
attr_largest(1209),
attr_layout(1210),
attr_left(1211),
attr_length(1212),
attr_lengths(1213),
attr_like(1214),
attr_log_alpha(1215),
attr_log_probs(1216),
attr_low(1217),
attr_lower(1218),
attr_lu(1219),
attr_m(1220),
attr_margin(1221),
attr_mask(1222),
attr_mat(1223),
attr_mat1(1224),
attr_mat2(1225),
attr_max(1226),
attr_max_indices(1227),
attr_max_norm(1228),
attr_max_size(1229),
attr_max_val(1230),
attr_max_values(1231),
attr_maximum_indices(1232),
attr_maxnorm(1233),
attr_maximum(1234),
attr_mean(1235),
attr_median(1236),
attr_nanmedian(1237),
attr_min(1238),
attr_min_indices(1239),
attr_min_val(1240),
attr_minlength(1241),
attr_minimum(1242),
attr_mode(1243),
attr_momentum(1244),
attr_n(1245),
attr_n_fft(1246),
attr_neg_log_likelihood(1247),
attr_negative(1248),
attr_negative_slope(1249),
attr_noise(1250),
attr_non_blocking(1251),
attr_norm_type(1252),
attr_normalized(1253),
attr_normalized_shape(1254),
attr_num_groups(1255),
attr_num_layers(1256),
attr_num_samples(1257),
attr_num_weights(1258),
attr_offset(1259),
attr_offset2bag(1260),
attr_offsets(1261),
attr_ones(1262),
attr_onesided(1263),
attr_options(1264),
attr_other(1265),
attr_output(1266),
attr_output_mask(1267),
attr_output_padding(1268),
attr_output_size(1269),
attr_output_sizes(1270),
attr_p(1271),
attr_pad(1272),
attr_padding(1273),
attr_padding_idx(1274),
attr_padding_mode(1275),
attr_padding_value(1276),
attr_params(1277),
attr_pdist(1278),
attr_cdist(1279),
attr_std_mean(1280),
attr_var_mean(1281),
attr_periodic(1282),
attr_pivot(1283),
attr_pivots(1284),
attr_pooledHeight(1285),
attr_pooledWidth(1286),
attr_positive(1287),
attr_pow(1288),
attr_random_samples(1289),
attr_rcond(1290),
attr_reduction(1291),
attr_repeats(1292),
attr_replacement(1293),
attr_res1(1294),
attr_res2(1295),
attr_res3(1296),
attr_reserve(1297),
attr_result(1298),
attr_return_inverse(1299),
attr_rois(1300),
attr_rtol(1301),
attr_running_mean(1302),
attr_running_var(1303),
attr_save_mean(1304),
attr_save_std(1305),
attr_save_var(1306),
attr_saved_g(1307),
attr_saved_norms(1308),
attr_saved_v(1309),
attr_scale(1310),
attr_scale_grad_by_freq(1311),
attr_self(1312),
attr_self_size(1313),
attr_self_ty(1314),
attr_shape(1315),
attr_sigma(1316),
attr_signal_ndim(1317),
attr_signal_sizes(1318),
attr_size(1319),
attr_solution(1320),
attr_some(1321),
attr_sorted(1322),
attr_source(1323),
attr_sparse(1324),
attr_sparse_dim(1325),
attr_sparse_dtype(1326),
attr_spatialScale(1327),
attr_split_size(1328),
attr_split_sizes(1329),
attr_src(1330),
attr_start(1331),
attr_start_dim(1332),
attr_std(1333),
attr_step(1334),
attr_steps(1335),
attr_storage(1336),
attr_storageOffset(1337),
attr_storage_offset(1338),
attr_stride(1339),
attr_sumdim(1340),
attr_swap(1341),
attr_symmetric(1342),
attr_target(1343),
attr_target_lengths(1344),
attr_targets(1345),
attr_tensor(1346),
attr_tensor1(1347),
attr_tensor2(1348),
attr_tensors(1349),
attr_the_template(1350),
attr_theta(1351),
attr_threshold(1352),
attr_to(1353),
attr_tol(1354),
attr_total(1355),
attr_total_length(1356),
attr_total_weight(1357),
attr_train(1358),
attr_training(1359),
attr_transpose(1360),
attr_transposed(1361),
attr_unbiased(1362),
attr_unitriangular(1363),
attr_unroll_dim(1364),
attr_upper(1365),
attr_upscale_factor(1366),
attr_use_input_stats(1367),
attr_v(1368),
attr_value(1369),
attr_values(1370),
attr_vec(1371),
attr_vec1(1372),
attr_vec2(1373),
attr_w_hh(1374),
attr_w_ih(1375),
attr_weight(1376),
attr_weight_arr(1377),
attr_weight_buf(1378),
attr_weight_size(1379),
attr_weight_stride0(1380),
attr_weights(1381),
attr_win_length(1382),
attr_window(1383),
attr_window_length(1384),
attr_workspace(1385),
attr_x(1386),
attr_x1(1387),
attr_x2(1388),
  attr_Subgraph(1389),
  attr_ReverseSubgraph(1390),
  attr_f_real_outputs(1391),
  attr_df_input_vjps(1392),
  attr_df_input_captured_inputs(1393),
  attr_df_input_captured_outputs(1394),
  attr_df_output_vjps(1395),
  attr_axes(1396),
  attr_axis(1397),
  attr_broadcast(1398),
  attr_direction(1399),
  attr_ends(1400),
  attr_inplace(1401),
  attr_input_as_shape(1402),
  attr_is_zero(1403),
  attr_num_none(1404),
  attr_num_present(1405),
  attr_perm(1406),
  attr_sizes(1407),
  attr_starts(1408),
  attr_profiled_type(1409),
  attr_transA(1410),
  attr_transB(1411),
  attr_name(1412),
  attr_a(1413),
  attr_b(1414),
  attr_beg(1415),
  attr_idx(1416),
  attr_split(1417),
  attr_slot(1418),
  attr_kinds(1419),
  attr_types(1420),
  attr_scope(1421),
  attr_keepdims(1422),
  attr_cache_id(1423),
  attr_new_axis(1424),
  attr_warn_id(1425),
    num_symbols(1426);

    public final int value;
    private _keys(int v) { this.value = v; }
    private _keys(_keys e) { this.value = e.value; }
    public _keys intern() { for (_keys e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

// #define DEFINE_SYMBOL(s)
//   constexpr Symbol s(static_cast<unique_t>(_keys::s));

// #undef DEFINE_SYMBOL

// #define DEFINE_SYMBOL(ns, s)
//   namespace ns { constexpr Symbol s(static_cast<unique_t>(_keys::ns##_##s)); }
@Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol prim(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol aten(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol cuda(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol onnx(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol attr(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol scope(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol user(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol _caffe2(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol dimname(); 
  @Namespace("c10::namespaces") @MemberGetter public static native @Const @ByRef Symbol namespaces(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Assign(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol BroadcastingChunk(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol BroadcastSizes(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ReductionSizes(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Constant(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ChunkSizes(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Drop(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Eval(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Expand();  /* onnx */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol FusionGroup(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol CudaFusionGroup(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol CudaFusionGuard(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol FunctionalGraph(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol DifferentiableGraph(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TensorExprGroup(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol StaticSubgraph(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol If(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Jump();  /* debug */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol JumpNZ();  /* debug */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol JumpZ();  /* debug */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Load(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Loop(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Param(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol PackPadded();  /* onnx */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol PadPacked();  /* onnx */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Placeholder();  /* debug */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Print(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol PythonOp(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol IgnoredPythonOp(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Reverse(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Return(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ReturnStmt(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol BreakStmt(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ContinueStmt(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ComprehensionScope(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Store(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol AutogradZero(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol AutogradAnyNonZero(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol AutogradAllNonZero(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol AutogradAllZero(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Starred(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TupleConstruct(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TupleUnpack(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TupleIndex(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TupleSlice(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ListConstruct(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ListUnpack(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol DictConstruct(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ModuleDictIndex(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol EnumName(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol EnumValue(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol StringIndex(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol NumToTensor(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Uninitialized(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol With(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Enter(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Exit(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol Bool(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol Int(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol FloatImplicit(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol IntImplicit(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ScalarImplicit(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol Float(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol str(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol Delete(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol device(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol dtype(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol layout(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol id(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol requires_grad(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol MakeTestTensor();  /* test */
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol AutogradAdd(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol GradOf(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol grad(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol backward(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Guard(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol BailOut(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TypeCheck(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol RequiresGradCheck(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol FallbackGraph(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol FusedConcat(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol ConstantChunk(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol MMTreeReduce(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol MMBatchSide(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol list(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol min(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol max(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol abs(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol divmod(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol zip(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol enumerate(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol range(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol rangelist(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol isinstance(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol tolist(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol unchecked_cast(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _grad_sum_to_size(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _size_if_not_equal(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _ncf_unsqueeze(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol warn(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sorted(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol floordiv(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __range_length(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __derive_index(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __round_to_zero_floordiv(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_scripting(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _unwrap_optional(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol fork(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol forkClosure(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol RaiseException(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol Closure(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol CreateObject(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol SetAttr(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol GetAttr(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol HasAttr(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol profile(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol profile_ivalue(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol AddStatValue(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TimePoint(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol CallFunction(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol CallMethod(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol LoopContinuation(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol annotate(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TracedModuleForward(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TracedFork(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol TracedAttr(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol rpc_async(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol rpc_sync(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol rpc_remote(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol is_cuda(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol abs_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol absolute(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol absolute_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol acos(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol acos_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arccos(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arccos_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol acosh(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol acosh_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arccosh(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arccosh_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol asin(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol asin_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arcsin(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arcsin_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol asinh(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol asinh_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arcsinh(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arcsinh_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atan(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atan_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arctan(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arctan_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atanh(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atanh_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arctanh(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arctanh_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol clamp(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol clamp_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol clip(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol clip_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol det(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_det(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linalg_norm(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol append(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol item(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol format(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol percentFormat(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __not__(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __is__(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __isnot__(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol copy(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol copy_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol div(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol div_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol divide(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol divide_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol true_divide(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol true_divide_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol t_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addbmm_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addcdiv_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addcmul_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addmv_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addr_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol baddbmm_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ge(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ge_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol greater_equal(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol greater_equal_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gt(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gt_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol greater(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol greater_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol le(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol le_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol less_equal(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol less_equal_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lerp_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lt(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lt_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol less(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol less_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol isnan(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mul(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mul_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multiply(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multiply_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ne(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ne_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol not_equal(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol not_equal_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _ger(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ger(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol outer(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol transpose(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol transpose_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unsqueeze_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __getitem__(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _set_item(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol manual_seed(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol set_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_put_();  
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hash(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol len();  
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef @Name("wait") Symbol _wait(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol save(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sub(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sub_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol subtract(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol subtract_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol keys(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ord(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol chr(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hex(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol oct(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol clear(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol trunc(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol trunc_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fix(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fix_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol neg(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol neg_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol negative(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol negative_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol setdefault(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bin(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pop(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol insert(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol vstack(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol row_stack(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol unchecked_unwrap_optional(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __contains__(); 
  @Namespace("c10::prim") @MemberGetter public static native @Const @ByRef Symbol BailoutTemplate();  
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol zero_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fill_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol masked_fill_(); 
  @Namespace("c10::cuda") @MemberGetter public static native @Const @ByRef Symbol _set_device(); 
  @Namespace("c10::cuda") @MemberGetter public static native @Const @ByRef Symbol set_stream(); 
  @Namespace("c10::cuda") @MemberGetter public static native @Const @ByRef Symbol _current_device(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol swapaxes(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol swapaxes_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol swapdims(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol swapdims_(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol movedim(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol moveaxis(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol has_torch_function(); 
  @Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __and__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __iand__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __ilshift__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __ior__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __irshift__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __ixor__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __lshift__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __or__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __rshift__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol __xor__(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _abs(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _addmv(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _addr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _amp_foreach_non_finite_check_and_unscale_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _amp_update_scale(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _arange(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _argmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _argmin(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _baddbmm_mkl(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Byte(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Char(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Double(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Float(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Half(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Int(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Long(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cast_Short(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cat(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _ceil(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _clamp_max(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _clamp_min(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _convolution(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _convolution_double_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol convolution_overrideable(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol convolution_backward_overrideable(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _convolution_nogroup(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _copy_ignoring_overlaps(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cos(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cosh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _ctc_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _ctc_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cudnn_ctc_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cudnn_init_dropout_state(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cudnn_rnn(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cudnn_rnn_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cudnn_rnn_flatten_weight(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cufft_clear_plan_cache(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cufft_get_plan_cache_max_size(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cufft_get_plan_cache_size(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cufft_set_plan_cache_max_size(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cumprod(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _cumsum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _denseDims(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _dimI(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _dimV(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _dim_arange(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _dirichlet_grad(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _dot(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _embedding_bag(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _embedding_bag_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _embedding_bag_dense_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _embedding_bag_sparse_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _erf(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _erfc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _exp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _exp2(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _expm1(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fft_with_size(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fill(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _floor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _fused_dropout(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _indexCopy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _indices(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _ldexp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _linspace(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _local_scalar(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _local_scalar_dense(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _log(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _log10(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _log1p(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _log2(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _logspace(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _lu_with_info(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _masked_scale(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _mm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _mv(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _nnz(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _nansum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _pack_padded_sequence(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _pack_padded_sequence_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _pad_packed_sequence(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _pdist_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _pdist_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _prod(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _prodall(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _range(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _reshape_from_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _round(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _rsqrt(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _s_where(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _shape_as_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sigmoid(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sigmoid_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sin(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sinh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparseDims(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_add(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_addmm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_coo_tensor_with_dims(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_coo_tensor_with_dims_and_tensors(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_coo_tensor_unsafe(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_dense_add(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_div_scalar(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_div_zerodim(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_mul(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_mul_scalar(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_mul_zerodim(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sparse_sum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sqrt(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _square(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _standard_gamma(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _standard_gamma_grad(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _sum_cuda(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _tan(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _tanh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _tanh_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _th_get_device(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _th_kthvalue(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _th_mode(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _th_prod(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _th_sigmoid(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _th_std(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _th_sum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _th_tanh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _th_var(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _thnn_fused_gru_cell(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _thnn_fused_gru_cell_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _thnn_fused_lstm_cell(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _thnn_fused_lstm_cell_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _trilinear(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _trunc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _unique(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _unique_dim(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _unsafe_view(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _validate_sparse_coo_tensor_args(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _values(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _weight_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _weight_norm_cuda_interface(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _weight_norm_cuda_interface_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _weight_norm_differentiable_backward();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_avg_pool1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_avg_pool2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_avg_pool2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_avg_pool2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_avg_pool3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_avg_pool3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_avg_pool3d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_max_pool1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_max_pool2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_max_pool2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_max_pool2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_max_pool3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_max_pool3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol adaptive_max_pool3d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol add(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol add_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addbmm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addcdiv(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addcmul(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addmm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addmv(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol addr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol affine_grid_generator(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol affine_grid_generator_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol alias(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol all(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol allclose(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol alpha_dropout(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol any(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol arange(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol argmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol argmin(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol as_strided(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol as_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atan2(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atleast_1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atleast_2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol atleast_3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol avg_pool1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol avg_pool2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol avg_pool2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol avg_pool2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol avg_pool3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol avg_pool3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol avg_pool3d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol baddbmm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bartlett_window(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol batch_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bernoulli(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bilinear(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol binary_cross_entropy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol binary_cross_entropy_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol binary_cross_entropy_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol binary_cross_entropy_with_logits(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol binary_cross_entropy_with_logits_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol binary_cross_entropy_with_logits_target_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bincount(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol blackman_window(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol block_diag(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bmm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol broadcast_tensors(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol broadcast_to(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cartesian_prod(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cat(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cauchy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ceil(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol celu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol chain_matmul(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cholesky(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cholesky_inverse(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cholesky_solve(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol chunk(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol clamp_max(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol clamp_min(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef @Name("clone") Symbol _clone(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol coalesce(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol combinations(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _conj(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conj(); 
 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol copysign(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol polar(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol constant_pad_nd(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol contiguous(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conv1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conv2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conv3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conv_tbc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conv_tbc_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol conv_transpose1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol convolution(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol copy_sparse_to_sparse(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cos(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cosh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cosine_embedding_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cosine_similarity(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol count_nonzero(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cross(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol std_mean(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol var_mean(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ctc_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_affine_grid_generator(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_affine_grid_generator_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_batch_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_batch_norm_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution_backward_bias(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution_backward_input(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution_backward_weight(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution_transpose(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution_transpose_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution_transpose_backward_bias(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution_transpose_backward_input(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_convolution_transpose_backward_weight(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_grid_sampler(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_grid_sampler_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cudnn_is_acceptable(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cummax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cummin(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cumprod(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cumsum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol data_ptr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol deg2rad(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol detach(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol diag(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol diag_embed(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol diagflat(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol diagonal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fill_diagonal_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol diff(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol digamma(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol dim(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol dist(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol dot(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol dropout(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol dstack(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol eig(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol einsum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol elu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol elu_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol elu_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol embedding(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol embedding_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol embedding_bag(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol embedding_dense_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol embedding_renorm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol embedding_sparse_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol empty(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol empty_like(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol empty_strided(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol eq(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol equal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol erf(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol erfc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol erfinv(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol exp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol expand(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol expand_as(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol expm1(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol exponential(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol eye(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol feature_alpha_dropout(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol feature_dropout(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fft();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol flatten(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol flip(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fliplr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol flipud(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol floor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fmod(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fmod_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fmin(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol frac(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fractional_max_pool2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fractional_max_pool2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol fractional_max_pool2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol frobenius_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol full(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol full_like(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gather(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gcd(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gelu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol geometric(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol geqrf(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol get_device(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol glu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol glu_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol glu_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol grid_sampler(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol grid_sampler_2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol grid_sampler_2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol grid_sampler_3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol grid_sampler_3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol group_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gru(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol gru_cell(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hamming_window(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hann_window(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardshrink(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardshrink_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardsigmoid(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardsigmoid_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardtanh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardtanh_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hardtanh_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol heaviside(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hinge_embedding_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol histc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hspmm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hstack(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol hypot(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol i0(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol i0_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol igamma(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol igamma_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol igammac(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol igammac_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ifft(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_add(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_fill(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_put(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol index_select(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol indices(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol inner(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol instance_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol inverse(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol irfft(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_coalesced(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_complex(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_contiguous();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_distributed(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_floating_point(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_nonzero(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_same_size(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_set_to(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_signed(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol is_sparse(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol isclose(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol isreal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol istft(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol isposinf(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol isneginf(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol kaiser_window(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol kl_div(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol kl_div_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol kthvalue(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol l1_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol l1_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol l1_loss_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol layer_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lcm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol leaky_relu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol leaky_relu_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol leaky_relu_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lerp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lgamma(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linear(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol linspace(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log10(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log1p(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log2(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log_normal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log_sigmoid(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log_sigmoid_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log_sigmoid_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol log_softmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _log_softmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _log_softmax_backward_data(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logcumsumexp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logdet(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logit(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logspace(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol logsumexp(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol xlogy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lstm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lstm_cell(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lstsq(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol lu_solve(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol margin_ranking_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol masked_fill(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol masked_scatter(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol masked_select(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol matmul(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol matrix_power(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol matrix_rank(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol matrix_exp();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool1d_with_indices(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool2d_with_indices(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool2d_with_indices_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool2d_with_indices_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool3d_with_indices(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool3d_with_indices_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_pool3d_with_indices_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_unpool2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_unpool2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_unpool2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_unpool3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_unpool3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_unpool3d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol max_values(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mean(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol median(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nanmedian(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol meshgrid();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol min_values(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_batch_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_batch_norm_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_convolution(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_convolution_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_convolution_backward_bias(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_convolution_backward_input(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_convolution_backward_weight(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_convolution_transpose(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_convolution_transpose_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_convolution_transpose_backward_input(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_convolution_transpose_backward_weight(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_depthwise_convolution(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_depthwise_convolution_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_depthwise_convolution_backward_input(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_depthwise_convolution_backward_weight(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_rnn(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol miopen_rnn_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mkldnn_convolution(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mkldnn_convolution_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mkldnn_convolution_backward_input(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mkldnn_convolution_backward_weights(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mode(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mse_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mse_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mse_loss_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol msort(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multi_margin_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multi_margin_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multi_margin_loss_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multilabel_margin_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multilabel_margin_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multilabel_margin_loss_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol multinomial(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mv(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol mvlgamma(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nansum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nan_to_num(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol narrow(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol narrow_copy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_batch_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_batch_norm_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_clone(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_get_device(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_pow(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_resize_as(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol native_zero(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nextafter(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bitwise_and(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bitwise_not(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bitwise_or(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol bitwise_xor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol element_size(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nll_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nll_loss2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nll_loss2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nll_loss2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nll_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nll_loss_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nonzero(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nonzero_numpy(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol norm_except_dim(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol normal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nuclear_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol numel(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ones(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ones_like(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol orgqr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ormqr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pairwise_distance(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _euclidean_dist(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pdist(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol cdist(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol permute(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pin_memory(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pinverse(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pixel_shuffle(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pixel_unshuffle(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol poisson(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol polygamma(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol pow(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol float_power(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol prelu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol prelu_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol prod(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol put(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol qr(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol quantile(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol nanquantile(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rad2deg(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rand(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rand_like(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol randint(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol randint_like(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol randn(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol randn_like(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol random(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol randperm();  
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol ravel(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reciprocal(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reflection_pad1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reflection_pad1d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reflection_pad1d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reflection_pad2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reflection_pad2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reflection_pad2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol relu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol remainder(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol renorm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol repeat(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad1d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad1d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol replication_pad3d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reshape(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol reshape_as(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol resize(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol resize_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol resize_as(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol resize_as_(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rfft(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rnn_relu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rnn_relu_cell(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rnn_tanh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rnn_tanh_cell(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rot90(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol round(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rrelu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rrelu_with_noise(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rrelu_with_noise_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rrelu_with_noise_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rsqrt(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol scatter(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol scatter_add(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol select(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol selu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol set(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sigmoid(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sign(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol signbit(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol silu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sgn(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sin(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sinc(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sinh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol size(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sizes(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slice(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slogdet(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol smm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol smooth_l1_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol smooth_l1_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol smooth_l1_loss_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol soft_margin_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol soft_margin_loss_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol soft_margin_loss_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol softmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _softmax(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol _softmax_backward_data(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol softplus(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol softplus_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol softplus_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol softshrink(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol softshrink_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol softshrink_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol solve(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sort(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sparse_coo_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sparse_mask(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sparse_resize(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sparse_resize_and_clear(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol split(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol split_with_sizes(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sqrt(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol square(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol squeeze(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sspaddmm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol stack(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol std(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol stft(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol storage_offset(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol stride(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol strides(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol rsub(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sum(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol sum_to_size(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol svd(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol symeig(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol t(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol take(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tan(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tanh(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tensordot(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tensor_split(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol th_clone(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol th_norm(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol th_pow(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol th_resize_as(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol th_tensor(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol th_zero(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol thnn_conv2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol thnn_conv2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol thnn_conv2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tile(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv3d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol thnn_conv_depthwise2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol thnn_conv_depthwise2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol thnn_conv_depthwise2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv_dilated2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv_dilated2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv_dilated3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv_dilated3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv_transpose2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv_transpose2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv_transpose3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol slow_conv_transpose3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol threshold(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol threshold_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol to(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol to_sparse(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol to_dense(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol topk(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol trace(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol triangular_solve(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol tril(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol triplet_margin_loss(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol triu(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol type_as(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unbind(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unfold(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol uniform(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unsafe_chunk(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unsafe_split(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unsafe_split_with_sizes(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol unsqueeze(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_bilinear2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_bilinear2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_bilinear2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_bicubic2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_bicubic2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_bicubic2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_linear1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_linear1d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_linear1d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest1d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest1d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest1d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest2d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest2d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest2d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_nearest3d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_trilinear3d(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_trilinear3d_backward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol upsample_trilinear3d_forward(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol values(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol vander(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol var(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol view(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol view_as(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol where(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef @Name("zero") Symbol _zero(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol zeros(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol zeros_like(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol real(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol imag(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol view_as_real(); 
@Namespace("c10::aten") @MemberGetter public static native @Const @ByRef Symbol view_as_complex(); 
/* nothing */
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Add(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Concat();  
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol ConstantFill(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Div(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol GRU(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Gather(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Gemm(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol LSTM(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Mul(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Pow(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol RNN(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Shape(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Size(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Slice(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Squeeze(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Sub(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Transpose(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Unsqueeze();   
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Reshape();  
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Equal(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Greater(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol GreaterOrEqual(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Less(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol LessOrEqual(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Not(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol ATen(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Split(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol ConstantOfShape(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Cast(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Mod(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Sqrt(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol SplitToSequence(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol SequenceAt(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol SequenceConstruct(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol SequenceEmpty(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol SequenceInsert(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol SequenceErase(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol ConcatFromSequence(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Identity(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol SoftmaxCrossEntropyLoss(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol NegativeLogLikelihoodLoss(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol LogSoftmax(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol ReduceL1(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol ReduceL2(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol Conv(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol BatchNormalization(); 
  @Namespace("c10::onnx") @MemberGetter public static native @Const @ByRef Symbol ReduceProd(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol A(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol C(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol H(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol LU_data(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol LU_pivots(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol N(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol W(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol accumulate(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol align_corners(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol alpha(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol anchor(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol argmaxes(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol atol(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol b_hh(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol b_ih(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol bag_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol base(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol batch1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol batch2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol batch_first(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol batch_sizes(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol benchmark(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol beta(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol bias(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol bias_defined(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol bidirectional(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol bins(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol blank(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol buffer(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ceil_mode(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol checked_signal_sizes(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol chunks(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol columns(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol column_stack(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol complex_input(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol complex_output(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol condition(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol count_include_pad(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cudnn_enable(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cudnn_enabled(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cx(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cy(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol data(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dense_dim(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol descending(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol deterministic();   
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dilation();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dim0(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dim1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dim2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dimension(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dims(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dims_other(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dims_self(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol divisor_override();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dropout_seed(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol dropout_state();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol eigenvectors(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol end(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol end_dim(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol eps(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol epsilon(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol equal_nan(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol equation(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol expand1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol expand2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol expand3(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol exponent(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol exponential_average_factor(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol fgrad_input(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol fill_value(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol finput(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol from(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol g(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol gO(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol generator(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ggI(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ggW(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ggb();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol gradOutput(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_bias(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_cy(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_hy(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_input(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_out(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_output(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_w(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grad_weight(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol grid(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol groups(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol has_bias(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol has_biases(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol hidden_bias(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol hidden_gates(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol hidden_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol high(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol hop_length(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol hx(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol i1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol i2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol i3(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ignore_index(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol implicit();   
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol info(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input3(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input_bias(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input_gates(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input_lengths(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input_scale(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol interpolation_mode();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol is_target(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol k(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol keepdim(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol kernel_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol lambd(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol largest();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol left(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol length(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol lengths(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol like(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol log_alpha(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol log_probs(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol low(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol lower(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol lu(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol m(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol margin(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol mask(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol mat(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol mat1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol mat2();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol max_indices(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol max_norm(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol max_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol max_val();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol maximum_indices(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol maxnorm(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol maximum();     
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol min_indices(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol min_val(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol minlength(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol minimum();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol momentum(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol n(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol n_fft(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol neg_log_likelihood();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol negative_slope(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol noise(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol non_blocking(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol norm_type(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol normalized(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol normalized_shape(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol num_groups(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol num_layers(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol num_samples(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol num_weights(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol offset(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol offset2bag(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol offsets();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol onesided(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol options(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol other(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol output(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol output_mask(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol output_padding(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol output_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol output_sizes(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol p(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol pad(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol padding(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol padding_idx(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol padding_mode(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol padding_value(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol params();     
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol periodic(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol pivot(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol pivots(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol pooledHeight(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol pooledWidth(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol positive();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol random_samples(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol rcond(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol reduction(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol repeats(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol replacement(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol res1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol res2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol res3(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol reserve(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol result(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol return_inverse(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol rois(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol rtol(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol running_mean(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol running_var(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol save_mean(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol save_std(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol save_var(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol saved_g(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol saved_norms(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol saved_v(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol scale(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol scale_grad_by_freq(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol self(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol self_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol self_ty(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol shape(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol sigma(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol signal_ndim(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol signal_sizes();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol solution(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol some();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol source(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol sparse(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol sparse_dim(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol sparse_dtype(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol spatialScale(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol split_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol split_sizes(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol src(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol start(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol start_dim();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol step(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol steps(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol storage(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol storageOffset();   
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol sumdim(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol swap(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol symmetric(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol target(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol target_lengths(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol targets();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol tensor1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol tensor2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol tensors(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol the_template(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol theta();   
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol tol(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol total(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol total_length(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol total_weight(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol train(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol training();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol transposed(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol unbiased(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol unitriangular(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol unroll_dim(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol upper(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol upscale_factor(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol use_input_stats(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol v(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol value();  
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol vec(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol vec1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol vec2(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol w_hh(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol w_ih(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weight(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weight_arr(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weight_buf(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weight_size(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weight_stride0(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol weights(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol win_length(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol window(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol window_length(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol workspace(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol x(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol x1(); 
@Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol x2(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol Subgraph(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ReverseSubgraph(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol f_real_outputs(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol df_input_vjps(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol df_input_captured_inputs(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol df_input_captured_outputs(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol df_output_vjps(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol axes(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol axis(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol broadcast(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol direction(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol ends(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol inplace(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol input_as_shape(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol is_zero(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol num_none(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol num_present(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol perm();  
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol starts(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol profiled_type(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol transA(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol transB(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol name(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol a(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol b(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol beg(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol idx();  
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol slot(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol kinds(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol types();  
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol keepdims(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol cache_id(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol new_axis(); 
  @Namespace("c10::attr") @MemberGetter public static native @Const @ByRef Symbol warn_id(); 
// #undef DEFINE_SYMBOL




















// Targeting ../SymbolHash.java





// Parsed from ATen/core/grad_mode.h

// #pragma once

// #include <c10/macros/Macros.h>
// Targeting ../GradMode.java


// Targeting ../AutoGradMode.java


// Targeting ../NoGradGuard.java






// Parsed from ATen/core/ATenGeneral.h

// #pragma once

// #include <c10/macros/Macros.h>


// Parsed from ATen/core/Dimname.h

// #pragma once

// #include <ATen/core/interned_strings.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Optional.h>
// #include <ostream>

@Namespace("at") public enum NameType { BASIC((byte)(0)), WILDCARD((byte)(1));

    public final byte value;
    private NameType(byte v) { this.value = v; }
    private NameType(NameType e) { this.value = e.value; }
    public NameType intern() { for (NameType e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
// Targeting ../Dimname.java



@Namespace("at") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Dimname dimname);

@Namespace("at") public static native @Cast("bool") @Name("operator ==") boolean equals(@Const @ByRef Dimname lhs, @Const @ByRef Dimname rhs);

@Namespace("at") public static native @Cast("bool") @Name("operator !=") boolean notEquals(@Const @ByRef Dimname lhs, @Const @ByRef Dimname rhs);

 // namespace at


// Parsed from ATen/core/DimVector.h

// #pragma once

// #include <c10/util/SmallVector.h>
// #include <stdint.h>

@Namespace("at") @MemberGetter public static native @Cast("const size_t") long kDimVectorStaticSize();

/** A container for sizes or strides */

 // namespace at


// Parsed from ATen/core/Generator.h

// #pragma once

// #include <stdint.h>
// #include <mutex>
// #include <deque>
// #include <atomic>
// #include <typeinfo>
// #include <utility>
// #include <cstddef>

// #include <c10/util/Exception.h>
// #include <c10/util/C++17.h>
// #include <c10/util/intrusive_ptr.h>
// #include <c10/core/Device.h>
// #include <c10/core/DispatchKeySet.h>

// For the record I don't think this is a correct pimpl idiom.
// Including Impl header in interface header defeats the purpose
// because you can't change Impl private members without forcing
// everything that included the interface to rebuild.
// Impl should be forward-declared in the interface header instead.
// #include <c10/core/GeneratorImpl.h>
// Targeting ../Generator.java



/**
 * Helper function for checking the validity of new random generator
 * state. Right now following conditions are checked:
 * 
 * - The new state tensor must be a torch.ByteTensor
 * - Data of the new state tensor must be contiguous
 */
@Namespace("at::detail") public static native void check_rng_state(@Const @ByRef TensorImpl new_state);

 // namespace detail

 // namespace at


// Parsed from ATen/core/List.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/util/TypeTraits.h>
// #include <c10/util/TypeList.h>
// #include <c10/util/intrusive_ptr.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Optional.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <vector>

// Targeting ../Type.java


// Targeting ../ListImpl.java







// Targeting ../ListElementConstReferenceTraits.java



// this wraps vector::iterator to make sure user code can't rely
// on it being the type of the underlying vector.
  

// Parsed from ATen/core/NamedTensor.h

// #pragma once

// #include <ATen/core/Dimname.h>
// #include <c10/core/TensorImpl.h>
// #include <c10/util/C++17.h>
// Targeting ../NamedTensorMeta.java


// Targeting ../NamesMode.java


// Targeting ../NoNamesGuard.java






// Sets the names of `tensor` to be `names`.
@Namespace("at") public static native @ByRef Tensor internal_set_names_inplace(@ByRef Tensor tensor, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByRef Tensor internal_set_names_inplace(@ByRef Tensor tensor, @StdMove DimnameVector names, @Cast("bool") boolean validate_names);

@Namespace("at") @MemberGetter public static native @Cast("const size_t") long kMaxNamedTensorDim();



// Some helper functions on TensorImpl. Useful for working with names in TH.
// XXX: Ideally these would exist as methods on TensorImpl
@Namespace("at::impl") public static native void internal_set_names_inplace(TensorImpl impl, @ByVal DimnameListOptional names, @Cast("bool") boolean validate_names);
@Namespace("at::impl") public static native void internal_set_names_inplace(TensorImpl impl, @StdMove DimnameVector names, @Cast("bool") boolean validate_names);



// Returns true if the tensor's names exist and are not all 'None'.
// Returns false if the tensor's names don't exist (were not allocated),
// or if all names are 'None'.
// We treat not-allocated-names the same as allocated names that are all 'None'.
@Namespace("at::impl") public static native @Cast("bool") boolean has_names(@Const TensorImpl impl);

// Returns the names of the tensor's dimensions.
// Unnamed tensors are treated as having 'None' in all dimension; this method
// would return a DimnameList of all 'None's for an unnamed tensor.
@Namespace("at::impl") public static native @ByVal DimnameArrayRef get_names(@Const TensorImpl impl);

// This is more of an implementation detail; one should use impl::get_names /
// Tensor::names() whenever possible because it provides a cleaner API.
// Returns the names of the tensor if they have been allocated; returns nullopt
// instead if the haven't been. The names of a tensor are not allocated if a
// tensor is constructed with names=None.
@Namespace("at::impl") public static native @ByVal DimnameListOptional get_opt_names(@Const TensorImpl impl);


 // namespace impl

 // namespace at


// Parsed from ATen/core/Reduction.h

// #pragma once

// NB: Keep this in sync with Reduction class in torch/nn/_reduction.py
// These constants control the reduction behavior of loss functions.
// Ideally, this would be a scoped enum, but jit doesn't support that
@Namespace("at::Reduction") public enum Reduction {
  None(0),             // Do not reduce
  Mean(1),             // (Possibly weighted) mean of losses
  Sum(2),              // Sum losses
  END(3);

    public final int value;
    private Reduction(int v) { this.value = v; }
    private Reduction(Reduction e) { this.value = e.value; }
    public Reduction intern() { for (Reduction e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}
 // namespace Reduction
 // namespace at


// Parsed from ATen/core/Scalar.h

// #include <c10/core/Scalar.h>


// Parsed from ATen/core/TensorAccessor.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/util/Deprecated.h>
// #include <stdint.h>
// #include <cstddef>

// The PtrTraits argument to the TensorAccessor/GenericPackedTensorAccessor
// is used to enable the __restrict__ keyword/modifier for the data
// passed to cuda.

// #if defined(__CUDACC__) || defined(__HIPCC__)
// #endif

// TensorAccessorBase and TensorAccessor are used for both CPU and CUDA tensors.
// For CUDA tensors it is used in device code (only). This means that we restrict ourselves
// to functions and types available there (e.g. IntArrayRef isn't).

// The PtrTraits argument is only relevant to cuda to support `__restrict__` pointers.

// The `TensorAccessor` is typically instantiated for CPU `Tensor`s using
// `Tensor.accessor<T, N>()`.
// For CUDA `Tensor`s, `GenericPackedTensorAccessor` is used on the host and only
// indexing on the device uses `TensorAccessor`s.


// GenericPackedTensorAccessorBase and GenericPackedTensorAccessor are used on for CUDA `Tensor`s on the host
// and as
// In contrast to `TensorAccessor`s, they copy the strides and sizes on instantiation (on the host)
// in order to transfer them on the device when calling kernels.
// On the device, indexing of multidimensional tensors gives to `TensorAccessor`s.
// Use RestrictPtrTraits as PtrTraits if you want the tensor's data pointer to be marked as __restrict__.
// Instantiation from data, sizes, strides is only needed on the host and std::copy isn't available
// on the device, so those functions are host only.


// Can't put this directly into the macro function args because of commas
// #define AT_X GenericPackedTensorAccessor<T, N, PtrTraits, index_t>

// Old name for `GenericPackedTensorAccessor`
// #undef AT_X
 // namespace at


// Parsed from ATen/core/TensorBody.h

// #pragma once

// #include <c10/core/Device.h>
// #include <c10/core/Layout.h>
// #include <c10/core/MemoryFormat.h>
// #include <c10/core/QScheme.h>
// #include <c10/core/Stream.h>
// #include <c10/core/Scalar.h>
// #include <c10/core/ScalarType.h>
// #include <c10/core/ScalarTypeToTypeMeta.h>
// #include <c10/core/Storage.h>
// #include <ATen/core/TensorAccessor.h>
// #include <c10/core/TensorImpl.h>
// #include <c10/core/UndefinedTensorImpl.h>
// #include <c10/core/WrapDimMinimal.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Deprecated.h>
// #include <c10/util/Optional.h>
// #include <c10/util/intrusive_ptr.h>
// #include <ATen/core/DeprecatedTypePropertiesRegistry.h>
// #include <ATen/core/DeprecatedTypeProperties.h>
// #include <ATen/core/NamedTensor.h>
// #include <ATen/core/QuantizerBase.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>


// Targeting ../DeprecatedTypeProperties.java


 // namespace at
 // namespace indexing

// Targeting ../Node.java



 // namespace torch::autograd
@Namespace("at::impl") public static native @Cast("bool") boolean variable_excluded_from_dispatch();

// Targeting ../Tensor.java



// For "multiple ... operators specified" warnings, closing brace of class
// declaration must be included between pragma push & pop
// #ifdef _MSC_VER
// #pragma warning( pop )
// #endif






// Helper creator for Tensor class which doesn't requires the users to pass
// in an intrusive_ptr instead it just converts the argument passed to
// requested intrusive_ptr type.

 // namespace detail

@Namespace("at") public static native DispatchKey legacyExtractDispatchKey(@Const @ByRef Tensor t);

 // namespace at


// Parsed from ATen/core/Tensor.h

// #pragma once

// #include <ATen/core/TensorBody.h>


// Parsed from ATen/core/Formatting.h

// #pragma once

// #include <c10/core/Scalar.h>
// #include <ATen/core/Tensor.h>
// #include <iostream>



@Namespace("at") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef DeprecatedTypeProperties t);
@Namespace("at") public static native @Cast("std::ostream*") @ByRef Pointer print(
    @Cast("std::ostream*") @ByRef Pointer stream,
    @Const @ByRef Tensor tensor,
    @Cast("int64_t") long linesize);
@Namespace("at") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Tensor t);
@Namespace("at") public static native void print(@Const @ByRef Tensor t, @Cast("int64_t") long linesize/*=80*/);
@Namespace("at") public static native void print(@Const @ByRef Tensor t);

@Namespace("at") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @ByVal Scalar s);




// Parsed from ATen/core/UnsafeFromTH.h

// #include <ATen/core/Tensor.h>

@Namespace("at") public static native @ByVal Tensor unsafeTensorFromTH(Pointer th_pointer, @Cast("bool") boolean retain);

@Namespace("at") public static native @Cast({"", "c10::Storage&&"}) @StdMove Storage unsafeStorageFromTH(Pointer th_pointer, @Cast("bool") boolean retain);




// Parsed from ATen/core/blob.h

// #pragma once

// #include <cstddef>
// #include <sstream>
// #include <type_traits>
// #include <typeinfo>
// #include <vector>

// #include <c10/util/intrusive_ptr.h>
// #include <c10/util/typeid.h>
// #include <c10/macros/Macros.h>
// Targeting ../Blob.java



@Namespace("caffe2") public static native void swap(@ByRef Blob lhs, @ByRef Blob rhs);

@Namespace("caffe2") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @Const @ByRef Blob v);

 // namespace caffe2


// Parsed from ATen/core/functional.h

// #pragma once

// #include <vector>
// #include <c10/util/ArrayRef.h>

// The passed in function must take T by value (T), or by
// const reference (const T&); taking T by non-const reference
// will result in an error like:
//
//    error: no type named 'type' in 'class std::result_of<foobar::__lambda(T)>'
//
// No explicit template parameters are required.

// Overload for explicit function and ArrayRef

// C++ forbids taking an address of a constructor, so here's a workaround...
// Overload for constructor (R) application

 // namespace c10


// Parsed from ATen/core/ivalue.h

// #pragma once

// #include <ATen/core/TensorBody.h>
// #include <ATen/core/blob.h>
// #include <c10/util/C++17.h>
// #include <c10/util/intrusive_ptr.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <typeindex>
// Targeting ../CustomClassHolder.java


// Targeting ../Function.java


 // namespace jit

// Targeting ../IValueIValueDict.java


// Targeting ../ClassType.java


// Targeting ../RRefInterface.java



@Namespace("c10") public static native @Cast("bool") boolean _fastEqualsForContainer(@Const @ByRef IValue lhs, @Const @ByRef IValue rhs);

@Namespace("c10") public static native Function checkObjectSortSchema(
    @SharedPtr ClassType t,
    @Cast("std::stringstream*") @ByRef Pointer why_not);

// A comparator that checks ordering of two IValues of same type.

@Namespace("c10") public static native @ByVal @Cast("c10::IValueComparator*") Pointer getLessThanComparator(@Const @ByRef IValue v);
@Namespace("c10") public static native @ByVal @Cast("c10::IValueComparator*") Pointer getGreaterThanComparator(@Const @ByRef IValue v);
// Targeting ../Tuple.java


// Targeting ../Future.java


// Targeting ../ConstantString.java


// Targeting ../GenericDict.java


// Targeting ../Object.java


// Targeting ../PyObjectHolder.java


// Targeting ../EnumHolder.java


// Targeting ../ComplexHolder.java


 // namespace ivalue

// This is an owning wrapper for a c10::optional<std::vector<T>>
// that can be implicitly converted to a (non-owning) optional<ArrayRef<T>>.
// Its purpose is to be used in generated code to keep the vector alive
// either until the end of a statement (as a temporary), or as a saved arg
// in autograd.
// Targeting ../Capsule.java



// IValue is the generic tagged union used by the interpreter to hold
// all value types.
// It is a 16-byte object with an 8-byte payload and an 8-byte tag.
// The tag is currently 4 bytes to determine the type, and 1 byte
// to mark whether that type is a subtype of c10::intrusive_ptr_target and needs
// retain/release calls.


///
///
///
///
///
// #define TORCH_FORALL_TAGS(_)
//   _(None)
//   _(Tensor)
//   _(Storage)
//   _(Double)
//   _(ComplexDouble)
//   _(Int)
//   _(Bool)
//   _(Tuple)
//   _(String)
//   _(Blob)
//   _(GenericList)
//   _(GenericDict)
//   _(Future)
//   _(Device)
//   _(Stream)
//   _(Object)
//   _(PyObject)
//   _(Uninitialized)
//   _(Capsule)
//   _(RRef)
//   _(Quantizer)
//   _(Generator)
//   _(Enum)
// Targeting ../IValue.java


// Targeting ../WeakIValue.java


// Targeting ../StrongTypePtr.java



@Namespace("c10") public static native @ByRef StringFunctionMap getClassConverter();
 // namespace c10

// #include <ATen/core/ivalue_inl.h>


// Parsed from ATen/core/operator_name.h

// #pragma once

// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Optional.h>
// #include <c10/util/string_view.h>
// #include <string>
// #include <utility>
// #include <ostream>
// Targeting ../OperatorName.java


// Targeting ../OperatorNameView.java







@Namespace("c10") public static native @StdString BytePointer toString(@Const @ByRef OperatorName opName);


 // namespace c10



// Parsed from ATen/core/qualified_name.h

// #pragma once

// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>
// #include <c10/util/StringUtil.h>
// #include <string>
// Targeting ../QualifiedName.java


 // namespace c10
 // namespace std


// Parsed from ATen/Config.h

// #pragma once

// Test these using #if AT_MKL_ENABLED(), not #ifdef, so that it's
// obvious if you forgot to include Config.h
//    c.f. https://stackoverflow.com/questions/33759787/generating-an-error-if-checked-boolean-macro-is-not-defined
//
// DO NOT put the macros for CUDA libraries in this file; they belong in cuda/CUDAConfig.h

// #define AT_MKLDNN_ENABLED() 1
// #define AT_MKL_ENABLED() 0
// #define AT_FFTW_ENABLED() 1
// #define AT_NNPACK_ENABLED() 1
// #define CAFFE2_STATIC_LINK_CUDA() 0
// #define AT_BUILD_WITH_BLAS() 1
public static final int AT_PARALLEL_OPENMP = 1;
public static final int AT_PARALLEL_NATIVE = 0;
public static final int AT_PARALLEL_NATIVE_TBB = 0;


// Parsed from ATen/Device.h

// #pragma once
// #include <c10/core/Device.h>


// Parsed from ATen/DeviceGuard.h

// #pragma once

// #include <c10/core/DeviceGuard.h>
// #include <ATen/core/Tensor.h>
// #include <c10/core/ScalarType.h> // TensorList whyyyyy

// Are you here because you're wondering why DeviceGuard(tensor) no
// longer works?  For code organization reasons, we have temporarily(?)
// removed this constructor from DeviceGuard.  The new way to
// spell it is:
//
//    OptionalDeviceGuard guard(device_of(tensor));

/** Return the Device of a Tensor, if the Tensor is defined. */
@Namespace("at") public static native @ByVal DeviceOptional device_of(@Const @ByRef Tensor t);

/** Return the Device of a TensorList, if the list is non-empty and
 *  the first Tensor is defined.  (This function implicitly assumes
 *  that all tensors in the list have the same device.) */
@Namespace("at") public static native @ByVal DeviceOptional device_of(@ByVal TensorArrayRef t);

 // namespace at


// Parsed from ATen/DimVector.h

// #pragma once
// #include <ATen/core/DimVector.h>


// Parsed from ATen/Dispatch.h

// #pragma once

// #include <ATen/core/DeprecatedTypeProperties.h>
// #include <ATen/core/Tensor.h>
// #include <ATen/record_function.h>
// #include <c10/macros/Macros.h>
// #include <c10/util/Exception.h>
// #include <c10/util/Half.h>
// #include <c10/util/Metaprogramming.h>
// #include <c10/util/complex.h>
// #include <c10/util/string_view.h>

// #ifdef XPLAT_MOBILE_BUILD
// #include <ATen/selected_mobile_ops.h>
// #else
/**
 * The method should_include_kernel_dtype() returns true/false
 * based on whether the switching code for a specific dtype should be
 * included based on build time constants generated from tracing model
 * execution. This method will be implmeneted via code-generation and
 * included in this file when code-gen is ready.
 */
@Namespace("at") public static native @Cast("const bool") boolean should_include_kernel_dtype(
  @Cast("const char*") BytePointer kernel_tag_str,
  ScalarType scalar_type
);
@Namespace("at") public static native @Cast("const bool") boolean should_include_kernel_dtype(
  String kernel_tag_str,
  ScalarType scalar_type
);

// #endif

/**
 * In the Facebook internal build (using BUCK), this macro is enabled by
 * passing in -c pt.enable_record_kernel_dtype=1 when building the tracer
 * binary.
 */
// #if defined ENABLE_RECORD_KERNEL_FUNCTION_DTYPE
// #define RECORD_KERNEL_FUNCTION_DTYPE(NAME, enum_type)
//   {RECORD_FUNCTION_WITH_SCOPE(
//     at::RecordScope::KERNEL_FUNCTION_DTYPE,
//     std::string(NAME) + "$" + toString(enum_type),
//     {});}
// #else
// #define RECORD_KERNEL_FUNCTION_DTYPE(NAME, enum_type)
// #endif

// #define AT_PRIVATE_CASE_TYPE_USING_HINT(NAME, enum_type, type, HINT, ...)
//   case enum_type: {
//     at::guts::if_constexpr<(!at::should_include_kernel_dtype(NAME, enum_type))>(
//       [&] {
//         AT_ERROR("dtype '", toString(enum_type), "' not selected for kernel tag ", #NAME);
//       }
//     );
//     using HINT = type;
//     return __VA_ARGS__();
//   }

// #define AT_PRIVATE_CASE_TYPE(NAME, enum_type, type, ...)
//   AT_PRIVATE_CASE_TYPE_USING_HINT(NAME, enum_type, type, scalar_t, __VA_ARGS__)

// Workaround for C10_UNUSED because CUDA 10.1 and below fails to handle unused
// attribute in the type aliasing context. Keep name long and verbose to avoid
// macro collisions.
// #if defined(__CUDACC__) && CUDA_VERSION <= 10100
// #define C10_UNUSED_DISPATCH_CUDA_WORKAROUND
// #else
// #define C10_UNUSED_DISPATCH_CUDA_WORKAROUND C10_UNUSED
// #endif // defined(__CUDACC__) && CUDA_VERSION <= 10100

// #define AT_QINT_PRIVATE_CASE_TYPE(
//     enum_type, type, underlying_enum, underlying_type, ...)
//   case enum_type: {
//     using scalar_t = type;
//     using underlying_t C10_UNUSED_DISPATCH_CUDA_WORKAROUND =
//         scalar_t::underlying;
//     const auto& SCALAR_TYPE C10_UNUSED_DISPATCH_CUDA_WORKAROUND = enum_type;
//     const auto& UNDERLYING_TYPE C10_UNUSED_DISPATCH_CUDA_WORKAROUND =
//         toUnderlying(enum_type);
//     (void)SCALAR_TYPE;  /* Suppress unused-var compiler warning */
//     /* TODO: Use [[maybe-unused]] when C++17 becomes the standard */
//     return __VA_ARGS__();
//   }

// #define AT_QINT_SUB_BYTE_PRIVATE_CASE_TYPE(
//     enum_type, type, underlying_type, bitwidth, qmin, qmax, ...)
//   case enum_type: {
//     using scalar_t = type;
//     using underlying_t C10_UNUSED_DISPATCH_CUDA_WORKAROUND =
//         scalar_t::underlying;
//     const auto& SCALAR_TYPE C10_UNUSED_DISPATCH_CUDA_WORKAROUND = enum_type;
//     const auto& UNDERLYING_TYPE C10_UNUSED_DISPATCH_CUDA_WORKAROUND =
//         toUnderlying(enum_type);
//     int bit_width = bitwidth;
//     int64_t quant_min = qmin;
//     int64_t quant_max = qmax;
//     return __VA_ARGS__();
//   }

@Namespace("detail") public static native ScalarType scalar_type(ScalarType s);

@Namespace("detail") public static native @Deprecated ScalarType scalar_type(@Const @ByRef DeprecatedTypeProperties t);

@Namespace("detail") public static native @Deprecated void deprecated_AT_DISPATCH_ALL_TYPES_AND_HALF();

@Namespace("detail") public static native @Deprecated void deprecated_AT_DISPATCH_ALL_TYPES_AND_HALF_AND_COMPLEX();

 // namespace detail

// The AT_DISPATCH_* family of macros provides the ability to
// conveniently generate specializations of a kernel over all of the
// dtypes we care about in PyTorch.  We call it "dispatch" because
// we are "dispatching" to the correct, dtype-specific kernel.
//
// A standard usage looks like:
//
//      AT_DISPATCH_ALL_TYPES(self.scalar_type(), "op_name", [&] {
//          // Your code here, with 'scalar_t' now defined to
//          // be the dtype in question
//      })
//
// There are many variations of this macro, so it's important to
// understand exactly /which/ dtypes you want to get instantiated, as
// well as what the "default" set is.
//
// The default set of dtypes that are instantiated (e.g., by
// AT_DISPATCH_ALL_TYPES) are floating point types (float, double),
// and integral types (int32_t, int64_t, int16_t, int8_t, uint8_t),
// but NOT booleans (bool), half-precision floats (Half) or
// complex number (c10::complex<float>, c10::complex<double>).
// This "cut" is somewhat historical (the default types are the
// ones that TH historically supported), but it also reflects the
// fact that the non-default types are "poorly" behaved (booleans
// are NOT integers mod 2, half precision operations ~essentially
// don't exist on CPU, complex numbers are an experimental application).
//
// Here are the questions you should generally ask to decide which
// dispatch you want:
//
// 1. Is this an integral or floating point specific operation?
//    (If so, you'll want one of the FLOATING or INTEGRAL macros.)
//
// 2. Should half be supported?  (If you're on CPU, the answer is almost
//    definitely no.  If you do want support, use one of the AND_HALF
//    macros)
//
// Much rarer situations:
//
// 3. Should bool be supported?  (You often have to write your kernel
//    differently if arithmetic operations are involved.)  If so,
//    Use AT_DISPATCH_ALL_TYPES_AND along with ScalarType::Bool
//
// 4. Should complex be supported?  The answer is almost always no,
//    unless you are working on "generic" code that should work on
//    all dtypes.
//
// Parameters:
// -----------
//
// 1. The NAME argument is a "tag" that is used to trace and then
//    conditionally compile fragments of the case statements such
//    that the kernel functions are specialized only for the dtypes
//    that are needed. The NAME parameter *must* be a build time
//    cons char* (can't be std::string, etc...)
//
// Please ensure that the NAME is unique for every implementation
// or you run the risk of over-including code for the kernel
// functions. There is no risk of missing out on any code, so
// it's mostly a risk of a Type-2 error, and not a Type-1 error.
//

// NB: the the_type variable is not used, but we have kept it for
// backwards compatibility.  It's probably not used by anyone though;
// but we're just being safe (and it doesn't hurt.)  Note we must
// use it to shut up warnings about unused store.

// #define AT_DISPATCH_FLOATING_TYPES(TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_FLOATING_TYPES_AND_HALF(TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_FLOATING_TYPES_AND(SCALARTYPE, TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME,
//           SCALARTYPE,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE>::t),
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(TYPE), "'");
//     }
//   }()

// #define AT_DISPATCH_FLOATING_TYPES_AND2(
//     SCALARTYPE1, SCALARTYPE2, TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE1,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE1>::t),
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE2,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE2>::t),
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(TYPE), "'");
//     }
//   }()

// #define AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES(TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           at::ScalarType::ComplexDouble,
//           c10::complex<double>,
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           at::ScalarType::ComplexFloat,
//           c10::complex<float>,
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(
//     SCALARTYPE, TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           at::ScalarType::ComplexDouble, c10::complex<double>, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           at::ScalarType::ComplexFloat, c10::complex<float>, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE>::t),
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(
//     SCALARTYPE1, SCALARTYPE2, TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           at::ScalarType::ComplexDouble,
//           c10::complex<double>,
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           at::ScalarType::ComplexFloat,
//           c10::complex<float>,
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE1,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE1>::t),
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE2,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE2>::t),
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_INTEGRAL_TYPES(TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Byte, uint8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Char, int8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Int, int32_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Long, int64_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Short, int16_t, __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_INTEGRAL_TYPES_AND(SCALARTYPE, TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Byte, uint8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Char, int8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Int, int32_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Long, int64_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Short, int16_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME,
//           SCALARTYPE,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE>::t),
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_ALL_TYPES(TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op  */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Byte, uint8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Char, int8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Int, int32_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Long, int64_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Short, int16_t, __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_COMPLEX_TYPES(TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           at::ScalarType::ComplexFloat,
//           c10::complex<float>,
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           at::ScalarType::ComplexDouble,
//           c10::complex<double>,
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_QINT_TYPES(TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_QINT_PRIVATE_CASE_TYPE(
//           at::kQInt8, at::qint8, at::kChar, int8_t, __VA_ARGS__)
//       AT_QINT_PRIVATE_CASE_TYPE(
//           at::kQUInt8, at::quint8, at::kByte, uint8_t, __VA_ARGS__)
//       AT_QINT_PRIVATE_CASE_TYPE(
//           at::kQInt32, at::qint32, at::kInt, int, __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(TYPE), "'");
//     }
//   }()

// #define AT_DISPATCH_QINT_AND_SUB_BYTE_TYPES(TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_QINT_SUB_BYTE_PRIVATE_CASE_TYPE(
//           at::kQInt8, at::qint8, int8_t, CHAR_BIT, SCHAR_MIN, SCHAR_MAX, __VA_ARGS__)
//       AT_QINT_SUB_BYTE_PRIVATE_CASE_TYPE(
//           at::kQUInt8, at::quint8, uint8_t, CHAR_BIT, 0, UCHAR_MAX, __VA_ARGS__)
//       AT_QINT_SUB_BYTE_PRIVATE_CASE_TYPE(
//           at::kQInt32, at::qint32, int, CHAR_BIT * sizeof(int), INT_MIN, INT_MAX, __VA_ARGS__)
//       AT_QINT_SUB_BYTE_PRIVATE_CASE_TYPE(
//           at::kQUInt4x2, at::quint4x2, uint8_t, 4, 0, 15, __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(TYPE), "'");
//     }
//   }()

// #define AT_DISPATCH_ALL_TYPES_AND_COMPLEX(TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op*/
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Byte, uint8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Char, int8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Int, int32_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Long, int64_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Short, int16_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME,
//           at::ScalarType::ComplexFloat, c10::complex<float>, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME,
//           at::ScalarType::ComplexDouble, c10::complex<double>, __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_ALL_TYPES_AND(SCALARTYPE, TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op*/
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Byte, uint8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Char, int8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Int, int32_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Long, int64_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Short, int16_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE>::t),
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND(SCALARTYPE, TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op*/
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Byte, uint8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Char, int8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Int, int32_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Long, int64_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Short, int16_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           at::ScalarType::ComplexFloat,
//           c10::complex<float>,
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           at::ScalarType::ComplexDouble,
//           c10::complex<double>,
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE>::t),
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_ALL_TYPES_AND2(SCALARTYPE1, SCALARTYPE2, TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op*/
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Byte, uint8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Char, int8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Int, int32_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Long, int64_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Short, int16_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE1,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE1>::t),
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE2,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE2>::t),
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(
//     SCALARTYPE1, SCALARTYPE2, TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op*/
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Byte, uint8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Char, int8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Int, int32_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Long, int64_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Short, int16_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME, at::ScalarType::ComplexFloat, c10::complex<float>, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME, at::ScalarType::ComplexDouble, c10::complex<double>, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE1,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE1>::t),
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE2,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE2>::t),
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_ALL_TYPES_AND3(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op*/
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Byte, uint8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Char, int8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Int, int32_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Long, int64_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Short, int16_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE1,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE1>::t),
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE2,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE2>::t),
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE3,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE3>::t),
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(
//     SCALARTYPE1, SCALARTYPE2, SCALARTYPE3, TYPE, NAME, ...)
//   [&] {
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op*/
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Byte, uint8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Char, int8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Int, int32_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Long, int64_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Short, int16_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME, at::ScalarType::ComplexFloat, c10::complex<float>, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME, at::ScalarType::ComplexDouble, c10::complex<double>, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE1,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE1>::t),
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE2,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE2>::t),
//           __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(
//           NAME,
//           SCALARTYPE3,
//           decltype(c10::impl::ScalarTypeToCPPType<SCALARTYPE3>::t),
//           __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()

// #define AT_DISPATCH_INDEX_TYPES(TYPE, NAME, ...)
//   [&] {
//     const auto& the_index_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _it = ::detail::scalar_type(the_index_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _it)
//     switch (_it) {
//       AT_PRIVATE_CASE_TYPE_USING_HINT(NAME, at::ScalarType::Int, int32_t, index_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE_USING_HINT(NAME, at::ScalarType::Long, int64_t, index_t, __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_it), "'");
//     }
//   }()

// ----------------------------------------------------------------------------
// DEPRECATED MACROS, DON'T USE THESE
// ----------------------------------------------------------------------------

// #define AT_DISPATCH_ALL_TYPES_AND_HALF(TYPE, NAME, ...)
//   [&] {
//     detail::deprecated_AT_DISPATCH_ALL_TYPES_AND_HALF();
//     const auto& the_type = TYPE;
//     /* don't use TYPE again in case it is an expensive or side-effect op */
//     at::ScalarType _st = ::detail::scalar_type(the_type);
//     RECORD_KERNEL_FUNCTION_DTYPE(NAME, _st);
//     switch (_st) {
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Byte, uint8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Char, int8_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Int, int32_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Long, int64_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Short, int16_t, __VA_ARGS__)
//       AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)
//       default:
//         AT_ERROR(#NAME, " not implemented for '", toString(_st), "'");
//     }
//   }()


// Parsed from ATen/Formatting.h

// #include <ATen/core/Formatting.h>


// Parsed from ATen/Generator.h

// #pragma once
// #include <ATen/core/Generator.h>


// Parsed from ATen/Parallel.h

// #pragma once
// #include <ATen/Config.h>
// #include <ATen/core/ivalue.h>
// #include <c10/macros/Macros.h>

@Namespace("at") public static native @Cast("int64_t") long divup(@Cast("int64_t") long x, @Cast("int64_t") long y);

// Called during new thread initialization
@Namespace("at") public static native void init_num_threads();

// Sets the number of threads to be used in parallel region
@Namespace("at") public static native void set_num_threads(int arg0);

// Returns the maximum number of threads that may be used in a parallel region
@Namespace("at") public static native int get_num_threads();

// Returns the current thread number (starting from 0)
// in the current parallel region, or 0 in the sequential region
@Namespace("at") public static native int get_thread_num();

// Checks whether the code runs in parallel region
@Namespace("at") public static native @Cast("bool") boolean in_parallel_region();

// Initialise num_threads lazily at first parallel call
@Namespace("at::internal") public static native void lazy_init_num_threads();



/*
parallel_for

begin: index at which to start applying user function

end: index at which to stop applying user function

grain_size: number of elements per chunk. impacts the degree of parallelization

f: user function applied in parallel to the chunks, signature:
  void f(int64_t begin, int64_t end)

Warning: parallel_for does NOT copy thread local
states from the current thread to the worker threads.
This means for example that Tensor operations CANNOT be used in the
body of your function, only data pointers.
*/

/*
parallel_reduce

begin: index at which to start applying reduction

end: index at which to stop applying reduction

grain_size: number of elements per chunk. impacts number of elements in
intermediate results tensor and degree of parallelization.

ident: identity for binary combination function sf. sf(ident, x) needs to return
x.

f: function for reduction over a chunk. f needs to be of signature scalar_t
f(int64_t partial_begin, int64_t partial_end, scalar_t identifiy)

sf: function to combine two partial results. sf needs to be of signature
scalar_t sf(scalar_t x, scalar_t y)

For example, you might have a tensor of 10000 entires and want to sum together
all the elements. Parallel_reduce with a grain_size of 2500 will then allocate
an intermediate result tensor with 4 elements. Then it will execute the function
"f" you provide and pass the beginning and end index of these chunks, so
0-2499, 2500-4999, etc. and the combination identity. It will then write out
the result from each of these chunks into the intermediate result tensor. After
that it'll reduce the partial results from each chunk into a single number using
the combination function sf and the identity ident. For a total summation this
would be "+" and 0 respectively. This is similar to tbb's approach [1], where
you need to provide a function to accumulate a subrange, a function to combine
two partial results and an identity.

Warning: parallel_reduce does NOT copy thread local
states from the current thread to the worker threads.
This means for example that Tensor operations CANNOT be used in the
body of your function, only data pointers.

[1] https://software.intel.com/en-us/node/506154
*/

// Returns a detailed string describing parallelization settings
@Namespace("at") public static native @StdString BytePointer get_parallel_info();

// Sets number of threads used for inter-op parallelism
@Namespace("at") public static native void set_num_interop_threads(int arg0);

// Returns the number of threads used for inter-op parallelism
@Namespace("at") public static native int get_num_interop_threads();

// Launches inter-op parallel task
@Namespace("at") public static native void launch(@ByVal Func func);

 // namespace internal

// Launches intra-op parallel task
@Namespace("at") public static native void intraop_launch(@ByVal Func func);

// Launches intra-op parallel task, returns a future
@Namespace("at") public static native @SharedPtr Future intraop_launch_future(
    @ByVal Func func);

// Returns number of intra-op threads used by default
@Namespace("at") public static native int intraop_default_num_threads();

 // namespace at

// #if AT_PARALLEL_OPENMP
// #include <ATen/ParallelOpenMP.h>
// #elif AT_PARALLEL_NATIVE
// #include <ATen/ParallelNative.h>
// #elif AT_PARALLEL_NATIVE_TBB
// #include <ATen/ParallelNativeTBB.h>
// #endif


// Parsed from ATen/Utils.h

// #pragma once

// #include <ATen/core/ATenGeneral.h>
// #include <ATen/core/Generator.h>
// #include <c10/core/StorageImpl.h>
// #include <c10/core/UndefinedTensorImpl.h>

// #include <c10/core/ScalarType.h>
// #include <ATen/Formatting.h>
// #include <c10/util/ArrayRef.h>
// #include <c10/util/Exception.h>

// #include <algorithm>
// #include <sstream>
// #include <typeinfo>
// #include <numeric>
// #include <memory>

// #define AT_DISALLOW_COPY_AND_ASSIGN(TypeName)
//   TypeName(const TypeName&) = delete;
//   void operator=(const TypeName&) = delete

@Namespace("at") public static native int _crash_if_asan(int arg0);

// TODO: This unwrapping code is ONLY used for TH bindings; once TH goes
// away, we can delete this function
@Namespace("at") public static native TensorImpl checked_dense_tensor_unwrap(@Const @ByRef Tensor expr, @Cast("const char*") BytePointer name, int pos, @Cast("const char*") BytePointer api, @Cast("bool") boolean allowNull, DeviceType device_type, ScalarType scalar_type);
@Namespace("at") public static native TensorImpl checked_dense_tensor_unwrap(@Const @ByRef Tensor expr, String name, int pos, String api, @Cast("bool") boolean allowNull, @Cast("c10::DeviceType") byte device_type, ScalarType scalar_type);

// Converts a TensorList (i.e. ArrayRef<Tensor> to vector of TensorImpl*)
// NB: This is ONLY used by legacy TH bindings, and ONLY used by cat.
// Once cat is ported entirely to ATen this can be deleted!
@Namespace("at") public static native @ByVal TensorImplVector checked_dense_tensor_list_unwrap(@ByVal TensorArrayRef tensors, @Cast("const char*") BytePointer name, int pos, DeviceType device_type, ScalarType scalar_type);
@Namespace("at") public static native @ByVal TensorImplVector checked_dense_tensor_list_unwrap(@ByVal TensorArrayRef tensors, String name, int pos, @Cast("c10::DeviceType") byte device_type, ScalarType scalar_type);

@Namespace("at") public static native @Cast("int64_t") long sum_intlist(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef list);
@Namespace("at") public static native @Cast("int64_t") long sum_intlist(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... list);

//std::accumulate infers return type from `init` type, so if `init` type is not enough to hold the result, computation can overflow
//the next 2 functions set `init` type to int64_t to avoid overflow.
/**
 * Utility function to static cast input Generator* to
 * the backend generator type (CPU/CUDAGeneratorImpl etc.)
 */

/**
 * Utility function used in tensor implementations, which
 * supplies the default generator to tensors, if an input generator
 * is not supplied. The input Generator* is also static casted to
 * the backend generator type (CPU/CUDAGeneratorImpl etc.)
 */

@Namespace("at") public static native void check_size_nonnegative(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native void check_size_nonnegative(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at::detail") public static native @ByVal Tensor empty_cpu(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype_opt, @ByVal LayoutOptional layout_opt,
                 @ByVal DeviceOptional device_opt, @ByVal BoolOptional pin_memory_opt, @ByVal MemoryFormatOptional memory_format_opt);
@Namespace("at::detail") public static native @ByVal Tensor empty_cpu(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype_opt, @ByVal LayoutOptional layout_opt,
                 @ByVal DeviceOptional device_opt, @ByVal BoolOptional pin_memory_opt, @ByVal MemoryFormatOptional memory_format_opt);
 // namespace detail


 // at


// Parsed from ATen/TracerMode.h

// #pragma once

// #include <c10/core/impl/LocalDispatchKeySet.h>
// #include <c10/macros/Macros.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>

// NOTE [Tracing Mode Switches]
//
// Historically, tracing function was controlled by two switches:
//
// - `AutoNonVariableTypeMode` guard
//
//    Tracing function used to be script-generated inside `VariableType_*.cpp`
//    kernels, sharing the same `Autograd` dispatch key with autograd function.
//    Therefore, before tracing function was moved out of VariableType,
//    `AutoNonVariableTypeMode` guard can also disable tracing as a side effect
//    of disabling `Autograd` dispatching.
//
// - `setTracingState()` API in `torch/csrc/jit/frontend/tracer.h`
//
//    It stores tracing data in a `TracingState` object in TLS. If the
//    `TracingState` object in TLS is `null`, then tracing is paused.
//
//    The `TracingState` object is created in `tracer::trace()` - the main
//    entrance of tracing function. It's temporarily set to `null` inside
//    generated VariableType (now TraceType) to bypass tracing for intermediate
//    ops (ops being called by other ops). After the intermediate op call
//    finishes it's set back to the original `TracingState` object.
//
//    The `TracingState` obect in TLS can also be read/written via its Python
//    binding in `python_tracer.cpp`, and `get/setTracingState()` C++ APIs,
//    which are also exposed as `TORCH_API`.
//
// Two new switches were introduced since tracing function was moved out of
// VariableType:
//
// - `tracer::impl::set_dispatch_enabled()` API
//
//    Unlike the special `Autograd` dispatch key which is included in dispatch
//    key set by default, `Tracer` dispatch key is off by default. The
//    dispatching switch can be toggled via this new API.
//
// - `tracer::impl::NoTracerDispatchMode` guard
//
//    It's used to cover the old semantics of `AutoNonVariableTypeMode` after
//    tracing was moved out of VariableType.
//
// Before tracing function was moved out of VariableType, tracing was enabled
// when the following conditions are satisfied:
//
//    1) `TracingState` object in TLS != null;
//       - Either inside the execution scope of `tracer::trace()`, or
//       - Eagerly called `setTracingState()` with non-null object.
//    2) Not inside `AutoNonVariableTypeMode` scope;
//
// After:
//
//    1) `TracingState` object in TLS != null;
//    2) Has called `tracer::impl::set_dispatch_enabled(true)`;
//    3) Not inside `tracer::impl::NonDispatchGuard` scope;
//
// [TODOs]
//
// - `setTracingState()` v.s. `tracer::impl::set_dispatch_enabled()`
//
//   Currently `set_dispatch_enabled()` is set/unset inside `setTracingState()`
//   to keep the semantics exactly the same as before - it's confusing to keep
//   both switches, though. We should consider simplifying/limiting the exposed
//   `setTracingState()` Python/C++ APIs (and other APIs calling it) so that
//   these two can be unified.
//
// - `AutoNonVariableTypeMode` v.s. `tracer::impl::NoTracerDispatchMode`
//
//   We don't need to always set both guards together to keep semantics
//   unchanged. For the follow use cases of `AutoNonVariableTypeMode` we don't
//   need set the new tracer guard:
//
//   * Script-generated VariableType kernels. The guard is not necessary as
//     tracing is already disabled explicitly by `setTracingState(null)` in
//     generated TraceType kernels - we could keep it as is or use the new guard
//     instead.
//
//   * Custom ops. Will be handled by fallback kernel for `Tracer`.
//
//   * Functions that are not likely to be called in tracing context (no python
//     binding / not an operator), e.g.: all mobile forward() wrappers, test
//     binaries, and etc.
//
//   * Where new threads are spawned, e.g.: ATen/native/ConvolutionMM2d.cpp.
//     It's not necessary as tracing is off by default.
//
//   For the rest of cases we might need have both:
//
//   * Functions that might be reachable from eager mode python (especially
//     factory methods), e.g.:
//     `internal_new_from_data()` in `torch/csrc/utils/tensor_new.cpp`.
//     Without the new guard it will add `aten::empty` to the traced graph.
//
//   * Some manually maintained functions, e.g.:
//     `torch/csrc/autograd/VariableTypeManual.cpp`.
//     Set the new guard if it's not obvious whether `setTracingState(null)`
//     has been called before it reaches the `AutoNonVariableTypeMode` guard.
//
//   We might need tweak the usage of the new guard to optimize/fix things.
//   It should only affect the correctness of tracing function, because the
//   guard is essentially no-op when the master `setTracingState()` switch is
//   off.
// TODO: move this from `at::` to `jit::torch::` after
// `aten/src/ATen/cpp_custom_type_hack.h` is removed.

@Namespace("at::tracer::impl") public static native @Cast("bool") boolean is_dispatch_enabled();

@Namespace("at::tracer::impl") public static native void set_dispatch_enabled(@Cast("bool") boolean enabled);
// Targeting ../NoTracerDispatchMode.java



 // namespace impl
 // namespace tracer
 // namespace at


// Parsed from ATen/WrapDimUtils.h

// #pragma once

// #include <c10/core/WrapDimMinimal.h>
// #include <c10/core/TensorImpl.h>
// #include <ATen/core/Tensor.h>

@Namespace("at") public static native @Cast("int64_t") long maybe_wrap_dim(@Cast("int64_t") long dim, TensorImpl tensor);

@Namespace("at") public static native @Cast("int64_t") long maybe_wrap_dim(@Cast("int64_t") long dim, @ByVal TensorArrayRef tensors);

@Namespace("at") public static native @Cast("int64_t") long maybe_wrap_dim(@Cast("int64_t") long dim, @Cast("std::vector<int64_t>*") @StdVector LongVector tensor_sizes);

// wrap each dim in the dims array, taking dim_post_expr as the true number of dimensions
@Namespace("at") public static native void maybe_wrap_dims_n(@Cast("int64_t*") LongPointer dims, @Cast("int64_t") long ndims, @Cast("int64_t") long dim_post_expr);
@Namespace("at") public static native void maybe_wrap_dims_n(@Cast("int64_t*") LongBuffer dims, @Cast("int64_t") long ndims, @Cast("int64_t") long dim_post_expr);
@Namespace("at") public static native void maybe_wrap_dims_n(@Cast("int64_t*") long[] dims, @Cast("int64_t") long ndims, @Cast("int64_t") long dim_post_expr);

// Wrap each dim in a contiguous container, taking dim_post_expr as the true number of dimensions
// E.g. could also be std::array or c10::SmallVector

// previously, size [0] tensors were the only possible empty tensors; thus, it wasn't possible
// to cat empty tensors unless all the other tensors were 1-dimensional, so we allowed these tensors
// to be "skipped" (both for wrap dimension behavior and dimension size checking).
// We maintain this behavior for backwards compatibility, but only for this specific size
// (i.e. other empty sizes are not skipped).
@Namespace("at") public static native @Cast("int64_t") long legacy_cat_wrap_dim(@Cast("int64_t") long dim, @Cast("std::vector<int64_t>*") @StdVector LongVector tensor_sizes);

@Namespace("at") public static native @Cast("int64_t") long legacy_cat_wrap_dim(@Cast("int64_t") long dim, @ByVal TensorArrayRef tensors);

// wrap negative dims in a vector
@Namespace("at") public static native void wrap_all_dims(@Cast("std::vector<int64_t>*") @ByRef LongVector dims_to_wrap, @Cast("int64_t") long tensor_total_dims);




// Parsed from ATen/Tensor.h

// #pragma once

// #include <ATen/core/TensorBody.h>


// Parsed from ATen/TensorGeometry.h

// #pragma once

// #include <ATen/WrapDimUtils.h>
// #include <ATen/core/Tensor.h>
// Targeting ../TensorGeometry.java



 // namespace at


// Parsed from ATen/TensorNames.h

// #pragma once

// #include <ATen/WrapDimUtils.h>
// Targeting ../TensorName.java


// Targeting ../TensorNames.java




 // namespace at::namedinference


// Parsed from ATen/TensorUtils.h

// #pragma once

// #include <ATen/Tensor.h>
// #include <ATen/TensorGeometry.h>
// #include <ATen/Utils.h>

// These functions are NOT in Utils.h, because this file has a dep on Tensor.h
// Targeting ../TensorArg.java


// Targeting ../TensorGeometryArg.java



// A string describing which function did checks on its input
// arguments.
// TODO: Consider generalizing this into a call stack.

// The undefined convention: singular operators assume their arguments
// are defined, but functions which take multiple tensors will
// implicitly filter out undefined tensors (to make it easier to perform
// tests which should apply if the tensor is defined, and should not
// otherwise.)
//
// NB: This means that the n-ary operators take lists of TensorArg,
// not TensorGeometryArg, because the Tensor to TensorGeometry
// conversion will blow up if you have undefined tensors.

@Namespace("at") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer out, @ByVal TensorGeometryArg t);
@Namespace("at") public static native void checkDim(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef Tensor tensor,
    @Cast("const char*") BytePointer name,
    int pos,
    @Cast("int64_t") long dim);
@Namespace("at") public static native void checkDim(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef Tensor tensor,
    String name,
    int pos,
    @Cast("int64_t") long dim);
@Namespace("at") public static native void checkDim(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim);
@Namespace("at") public static native void checkDim(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim);
// NB: this is an inclusive-exclusive range
@Namespace("at") public static native void checkDimRange(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim_start,
    @Cast("int64_t") long dim_end);
@Namespace("at") public static native void checkDimRange(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim_start,
    @Cast("int64_t") long dim_end);
@Namespace("at") public static native void checkSameDim(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t1,
    @Const @ByRef TensorGeometryArg t2);
@Namespace("at") public static native void checkSameDim(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t1,
    @Const @ByRef TensorGeometryArg t2);
@Namespace("at") public static native void checkContiguous(@Cast("at::CheckedFrom") BytePointer c, @Const @ByRef TensorGeometryArg t);
@Namespace("at") public static native void checkContiguous(@Cast("at::CheckedFrom") String c, @Const @ByRef TensorGeometryArg t);
@Namespace("at") public static native void checkAllContiguous(@Cast("at::CheckedFrom") BytePointer c, @ByVal TensorArgArrayRef ts);
@Namespace("at") public static native void checkAllContiguous(@Cast("at::CheckedFrom") String c, @ByVal TensorArgArrayRef ts);
@Namespace("at") public static native void checkSize(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("at") public static native void checkSize(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t,
    @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... sizes);
@Namespace("at") public static native void checkSize(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim,
    @Cast("int64_t") long size);
@Namespace("at") public static native void checkSize(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long dim,
    @Cast("int64_t") long size);
@Namespace("at") public static native void checkNumel(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long numel);
@Namespace("at") public static native void checkNumel(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorGeometryArg t,
    @Cast("int64_t") long numel);

@Namespace("at") public static native void checkAllSameNumel(@Cast("at::CheckedFrom") BytePointer c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkAllSameNumel(@Cast("at::CheckedFrom") String c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkScalarType(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorArg t,
    ScalarType s);
@Namespace("at") public static native void checkScalarType(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorArg t,
    ScalarType s);
@Namespace("at") public static native void checkScalarTypes(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorArg t,
    @ByVal ScalarTypeArrayRef l);
@Namespace("at") public static native void checkScalarTypes(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorArg t,
    @ByVal ScalarTypeArrayRef l);
@Namespace("at") public static native void checkSameGPU(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkSameGPU(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkAllSameGPU(@Cast("at::CheckedFrom") BytePointer c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkAllSameGPU(@Cast("at::CheckedFrom") String c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkSameType(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkSameType(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkAllSameType(@Cast("at::CheckedFrom") BytePointer c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkAllSameType(@Cast("at::CheckedFrom") String c, @ByVal TensorArgArrayRef tensors);
@Namespace("at") public static native void checkSameSize(
    @Cast("at::CheckedFrom") BytePointer c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkSameSize(
    @Cast("at::CheckedFrom") String c,
    @Const @ByRef TensorArg t1,
    @Const @ByRef TensorArg t2);
@Namespace("at") public static native void checkDefined(@Cast("at::CheckedFrom") BytePointer c, @Const @ByRef TensorArg t);
@Namespace("at") public static native void checkDefined(@Cast("at::CheckedFrom") String c, @Const @ByRef TensorArg t);
@Namespace("at") public static native void checkAllDefined(@Cast("at::CheckedFrom") BytePointer c, @ByVal TensorArgArrayRef t);
@Namespace("at") public static native void checkAllDefined(@Cast("at::CheckedFrom") String c, @ByVal TensorArgArrayRef t);

// FixMe: does TensorArg slow things down?
@Namespace("at") public static native void checkBackend(
    @Cast("at::CheckedFrom") BytePointer c,
    @ByVal TensorArrayRef t,
    @ByVal Backend backend);
@Namespace("at") public static native void checkBackend(
    @Cast("at::CheckedFrom") String c,
    @ByVal TensorArrayRef t,
    @ByVal Backend backend);

@Namespace("at") public static native void checkDeviceType(
    @Cast("at::CheckedFrom") BytePointer c,
    @ByVal TensorArrayRef tensors,
    @ByVal DeviceType device_type);
@Namespace("at") public static native void checkDeviceType(
    @Cast("at::CheckedFrom") String c,
    @ByVal TensorArrayRef tensors,
    @ByVal DeviceType device_type);

@Namespace("at") public static native void checkLayout(@Cast("at::CheckedFrom") BytePointer c, @Const @ByRef Tensor t, Layout layout);
@Namespace("at") public static native void checkLayout(@Cast("at::CheckedFrom") String c, @Const @ByRef Tensor t, @Cast("c10::Layout") byte layout);

@Namespace("at") public static native void checkLayout(@Cast("at::CheckedFrom") BytePointer c, @ByVal TensorArrayRef tensors, @ByVal Layout layout);
@Namespace("at") public static native void checkLayout(@Cast("at::CheckedFrom") String c, @ByVal TensorArrayRef tensors, @ByVal Layout layout);

// Methods for getting data_ptr if tensor is defined
@Namespace("at") public static native Pointer maybe_data_ptr(@Const @ByRef Tensor tensor);
@Namespace("at") public static native Pointer maybe_data_ptr(@Const @ByRef TensorArg tensor);

// Return if the tensor geometry represented by `sizes` and `strides` is contiguous
// Although we cache is_contiguous in tensor now, this is till useful because it
// allows checking if a particular geometry is contiguous without explicitly
// constructing a tensor, e.g., when you want to choose a kernel strategy based
// on whether a subgeometry is contiguous.
@Namespace("at") public static native @Cast("bool") boolean geometry_is_contiguous(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides);
@Namespace("at") public static native @Cast("bool") boolean geometry_is_contiguous(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] sizes, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... strides);

// Correspond to THCUNN_check_dim_size/THNN_check_dim_size
@Namespace("at") public static native void check_dim_size(
    @Const @ByRef Tensor tensor,
    @Cast("int64_t") long dim,
    @Cast("int64_t") long dim_size,
    @Cast("int64_t") long size);
@Namespace("at::detail") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector defaultStrides(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("at::detail") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector defaultStrides(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... sizes);
@Namespace("at::detail") public static native @Cast("size_t") long computeStorageNbytes(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides, @Cast("size_t") long itemsize);
@Namespace("at::detail") public static native @Cast("size_t") long computeStorageNbytes(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] sizes, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] strides, @Cast("size_t") long itemsize);
@Namespace("at::detail") public static native @ByVal LongVectorOptional computeStride(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef oldshape,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef oldstride,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef newshape);
@Namespace("at::detail") public static native @ByVal LongVectorOptional computeStride(
    @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] oldshape,
    @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] oldstride,
    @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... newshape);
 // namespace detail
 // namespace at


// Parsed from ATen/Context.h

// #pragma once

// #include <ATen/core/ATenGeneral.h>
// #include <ATen/Tensor.h>
// #include <ATen/Utils.h>
// #include <ATen/core/ATenGeneral.h>
// #include <ATen/core/Generator.h>
// #include <ATen/CPUGeneratorImpl.h>
// #include <ATen/core/LegacyTypeDispatch.h>
// #include <ATen/detail/CUDAHooksInterface.h>
// #include <ATen/detail/HIPHooksInterface.h>
// #include <c10/util/Exception.h>
// #include <c10/core/impl/DeviceGuardImplInterface.h>
// #include <c10/core/QEngine.h>

// #include <memory>
// #include <mutex>
// #include <cstdint>
// Targeting ../Context.java



@Namespace("at") public static native @ByRef Context globalContext();

@Namespace("at") public static native void init();

@Namespace("at") public static native Allocator getCPUAllocator();

@Namespace("at") public static native @ByRef DeprecatedTypeProperties getDeprecatedTypeProperties(Backend p, ScalarType s);
@Namespace("at") public static native @ByRef DeprecatedTypeProperties getDeprecatedTypeProperties(@Cast("c10::Backend") int p, ScalarType s);

@Namespace("at") public static native @ByRef DeprecatedTypeProperties CPU(ScalarType s);

@Namespace("at") public static native @ByRef DeprecatedTypeProperties CUDA(ScalarType s);

@Namespace("at") public static native @ByRef DeprecatedTypeProperties HIP(ScalarType s);

@Namespace("at") public static native @Cast("bool") boolean hasCUDA();

@Namespace("at") public static native @Cast("bool") boolean hasHIP();

@Namespace("at") public static native @Cast("bool") boolean hasXLA();

// Despite its name, this function returns the number of *CUDA* GPUs.
@Namespace("at") public static native @Cast("size_t") long getNumGPUs();

@Namespace("at") public static native @Cast("bool") boolean hasOpenMP();

@Namespace("at") public static native @Cast("bool") boolean hasMKL();

@Namespace("at") public static native @Cast("bool") boolean hasLAPACK();

@Namespace("at") public static native @Cast("bool") boolean hasMAGMA();

@Namespace("at") public static native @Cast("bool") boolean hasMKLDNN();

@Namespace("at") public static native void manual_seed(@Cast("uint64_t") long seed);
// Targeting ../NoTF32Guard.java



 // namespace at


// Parsed from ATen/ExpandUtils.h

// #pragma once

// #include <ATen/Tensor.h>
// #include <c10/util/Exception.h>

// #include <functional>
// #include <sstream>
// #include <tuple>

@Namespace("at") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector infer_size(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef a, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef b);
@Namespace("at") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector infer_size(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] a, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... b);
@Namespace("at") public static native @ByVal @Cast("std::tuple<std::vector<int64_t>,std::vector<int64_t> >*") LongVector inferExpandGeometry(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef tensor_sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef tensor_strides,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("at") public static native @ByVal @Cast("std::tuple<std::vector<int64_t>,std::vector<int64_t> >*") LongVector inferExpandGeometry(
    @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] tensor_sizes,
    @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] tensor_strides,
    @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... sizes);

@Namespace("at") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector infer_dense_strides(
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef tensor_sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef tensor_strides);
@Namespace("at") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector infer_dense_strides(
    @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] tensor_sizes,
    @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... tensor_strides);

// True if input shapes are expandable
// NOTE: infer_size did a similar check, please keep them sync if change is needed
@Namespace("at") public static native @Cast("bool") boolean are_expandable(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape1, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape2);
@Namespace("at") public static native @Cast("bool") boolean are_expandable(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] shape1, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... shape2);

// avoid copy-construction of Tensor by using a reference_wrapper.

@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor>*") Tensor expand_inplace(@Const @ByRef Tensor tensor, @Const @ByRef Tensor to_expand);

@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor>*") Tensor expand_inplace(@Const @ByRef Tensor tensor, @Const @ByRef Tensor to_expand, @Cast("const char*") BytePointer api_name);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor>*") Tensor expand_inplace(@Const @ByRef Tensor tensor, @Const @ByRef Tensor to_expand, String api_name);

@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor expand_inplace(@Const @ByRef Tensor tensor, @Const @ByRef Tensor to_expand1, @Const @ByRef Tensor to_expand2);

@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor expand_inplace(@Const @ByRef Tensor tensor, @Const @ByRef Tensor to_expand1, @Const @ByRef Tensor to_expand2,
                                                 @Cast("const char*") BytePointer api_name);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor expand_inplace(@Const @ByRef Tensor tensor, @Const @ByRef Tensor to_expand1, @Const @ByRef Tensor to_expand2,
                                                 String api_name);

@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor expand_outplace(@Const @ByRef Tensor to_expand1, @Const @ByRef Tensor to_expand2);

@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor expand_outplace(@Const @ByRef Tensor to_expand1, @Const @ByRef Tensor to_expand2, @Cast("const char*") BytePointer api_name);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor expand_outplace(@Const @ByRef Tensor to_expand1, @Const @ByRef Tensor to_expand2, String api_name);

@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor expand_outplace(@Const @ByRef Tensor to_expand1,
                                                          @Const @ByRef Tensor to_expand2,
                                                          @Const @ByRef Tensor to_expand3);

@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor expand_outplace(@Const @ByRef Tensor to_expand1,
                                                          @Const @ByRef Tensor to_expand2,
                                                          @Const @ByRef Tensor to_expand3,
                                                          @Cast("const char*") BytePointer api_name);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor expand_outplace(@Const @ByRef Tensor to_expand1,
                                                          @Const @ByRef Tensor to_expand2,
                                                          @Const @ByRef Tensor to_expand3,
                                                          String api_name);

@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor>*") Tensor expand_size(@Const @ByRef Tensor to_expand, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor>*") Tensor expand_size(@Const @ByRef Tensor to_expand, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... sizes);

@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor>*") Tensor expand_size(@Const @ByRef Tensor to_expand, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes, @Cast("const char*") BytePointer api_name);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor>*") Tensor expand_size(@Const @ByRef Tensor to_expand, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] sizes, String api_name);

@Namespace("at") public static native @StdMove TensorVector expand_outplace(@ByVal TensorArrayRef to_expand);

// Sums `tensor` repeatedly to produce a tensor of shape `shape`.
// Precondition: is_expandable_to(shape, tensor.sizes()) must be true
@Namespace("at") public static native @ByVal Tensor sum_to(@ByVal Tensor tensor, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape);
@Namespace("at") public static native @ByVal Tensor sum_to(@ByVal Tensor tensor, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... shape);

// True if `shape` can be broadcasted to `desired`
@Namespace("at") public static native @Cast("bool") boolean is_expandable_to(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef desired);
@Namespace("at") public static native @Cast("bool") boolean is_expandable_to(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] shape, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... desired);




// Parsed from ATen/Functions.h

// #pragma once

// @generated by tools/codegen/gen.py from Functions.h

// #include <c10/core/Scalar.h>
// #include <ATen/Tensor.h>
// #include <c10/core/Storage.h>
// #include <ATen/core/Generator.h>
// #include <c10/util/Deprecated.h>
// #include <ATen/DeviceGuard.h>
// #include <c10/core/TensorOptions.h>
// #include <ATen/core/Reduction.h>
// #include <c10/util/Optional.h>
// #include <ATen/TensorUtils.h>
// #include <ATen/Context.h>
// #include <ATen/TracerMode.h>
// #include <ATen/core/op_registration/hacky_wrapper_for_legacy_signatures.h>

// These functions are defined in ATen/Utils.cpp.
// #define TENSOR(T, S)
//   TORCH_API Tensor tensor(ArrayRef<T> values, const TensorOptions& options);
//   inline Tensor tensor(
//       std::initializer_list<T> values, const TensorOptions& options) {
//     return at::tensor(ArrayRef<T>(values), options);
//   }
//   inline Tensor tensor(T value, const TensorOptions& options) {
//     return at::tensor(ArrayRef<T>(value), options);
//   }
//   inline Tensor tensor(ArrayRef<T> values) {
//     return at::tensor(std::move(values), at::dtype(k##S));
//   }
//   inline Tensor tensor(std::initializer_list<T> values) {
//     return at::tensor(ArrayRef<T>(values));
//   }
//   inline Tensor tensor(T value) {
//     return at::tensor(ArrayRef<T>(value));
//   }
@Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<uint8_t>*") ByteArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("uint8_t") byte value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<uint8_t>*") ByteArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("uint8_t") byte value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<int16_t>*") ShortArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(short value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<int16_t>*") ShortArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(short value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<int>*") IntArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(int value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<int>*") IntArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(int value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("int64_t") long value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... values);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("int64_t") long value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal FloatArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(float value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal FloatArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(float value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal DoubleArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(double value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal DoubleArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(double value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BoolArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("decltype(::c10::impl::ScalarTypeToCPPType<::c10::ScalarType::Bool>::t)") boolean value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BoolArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@Cast("decltype(::c10::impl::ScalarTypeToCPPType<::c10::ScalarType::Bool>::t)") boolean value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal HalfArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal Half value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal HalfArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal Half value); 
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BFloat16ArrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BFloat16 value, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BFloat16ArrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal BFloat16 value);
@Namespace("at") public static native @ByVal Tensor tensor(@ByVal FloatComplexrrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal FloatComplexrrayRef values);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal DoubleComplexrrayRef values, @Const @ByRef TensorOptions options);
  @Namespace("at") public static native @ByVal Tensor tensor(@ByVal DoubleComplexrrayRef values);
// #undef TENSOR

@Namespace("at") public static native @ByVal Tensor _cast_Byte(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Byte(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor _cast_Char(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Char(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor _cast_Double(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Double(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor _cast_Float(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Float(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor _cast_Int(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Int(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor _cast_Long(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Long(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor _cast_Short(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Short(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor _cast_Half(@Const @ByRef Tensor self, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _cast_Half(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor _make_dual(@Const @ByRef Tensor primal, @Const @ByRef Tensor tangent, @Cast("int64_t") long level);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _unpack_dual(@Const @ByRef Tensor dual, @Cast("int64_t") long level);
@Namespace("at") public static native @StdMove TensorVector align_tensors(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @Cast("bool") boolean _use_cudnn_ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Cast("int64_t") long blank);
@Namespace("at") public static native @Cast("bool") boolean _use_cudnn_ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Cast("int64_t") long blank);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _cudnn_ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Cast("int64_t") long blank, @Cast("bool") boolean deterministic, @Cast("bool") boolean zero_infinity);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _cudnn_ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Cast("int64_t") long blank, @Cast("bool") boolean deterministic, @Cast("bool") boolean zero_infinity);
@Namespace("at") public static native @Cast("bool") boolean _use_cudnn_rnn_flatten_weight();
@Namespace("at") public static native @ByVal Tensor _cudnn_rnn_flatten_weight(@ByVal TensorArrayRef weight_arr, @Cast("int64_t") long weight_stride0, @Cast("int64_t") long input_size, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, @Cast("bool") boolean bidirectional);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor>*") Tensor _cudnn_rnn(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef TensorOptional weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor>*") Tensor _cudnn_rnn(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef TensorOptional weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Const @ByRef TensorOptional dropout_state);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor,std::vector<at::Tensor> >*") Tensor _cudnn_rnn_backward(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor,std::vector<at::Tensor> >*") Tensor _cudnn_rnn_backward(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long proj_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal Tensor _cudnn_init_dropout_state(double dropout, @Cast("bool") boolean train, @Cast("int64_t") long dropout_seed, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _cudnn_init_dropout_state(double dropout, @Cast("bool") boolean train, @Cast("int64_t") long dropout_seed, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @Cast("int64_t") long _debug_has_internal_overlap(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _fused_dropout(@Const @ByRef Tensor self, double p, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _fused_dropout(@Const @ByRef Tensor self, double p);
@Namespace("at") public static native @ByVal Tensor _masked_scale(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, double scale);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _sobol_engine_draw(@Const @ByRef Tensor quasi, @Cast("int64_t") long n, @Const @ByRef Tensor sobolstate, @Cast("int64_t") long dimension, @Cast("int64_t") long num_generated, @ByVal ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor _sobol_engine_ff_(@ByRef Tensor self, @Cast("int64_t") long n, @Const @ByRef Tensor sobolstate, @Cast("int64_t") long dimension, @Cast("int64_t") long num_generated);
@Namespace("at") public static native @ByRef Tensor _sobol_engine_scramble_(@ByRef Tensor self, @Const @ByRef Tensor ltm, @Cast("int64_t") long dimension);
@Namespace("at") public static native @ByRef Tensor _sobol_engine_initialize_state_(@ByRef Tensor self, @Cast("int64_t") long dimension);
@Namespace("at") public static native @ByVal Tensor _reshape_from_tensor(@Const @ByRef Tensor self, @Const @ByRef Tensor shape);
@Namespace("at") public static native @ByVal Tensor _shape_as_tensor(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor dropout(@Const @ByRef Tensor input, double p, @Cast("bool") boolean train);
@Namespace("at") public static native @ByRef Tensor dropout_(@ByRef Tensor self, double p, @Cast("bool") boolean train);
@Namespace("at") public static native @ByVal Tensor feature_dropout(@Const @ByRef Tensor input, double p, @Cast("bool") boolean train);
@Namespace("at") public static native @ByRef Tensor feature_dropout_(@ByRef Tensor self, double p, @Cast("bool") boolean train);
@Namespace("at") public static native @ByVal Tensor alpha_dropout(@Const @ByRef Tensor input, double p, @Cast("bool") boolean train);
@Namespace("at") public static native @ByRef Tensor alpha_dropout_(@ByRef Tensor self, double p, @Cast("bool") boolean train);
@Namespace("at") public static native @ByVal Tensor feature_alpha_dropout(@Const @ByRef Tensor input, double p, @Cast("bool") boolean train);
@Namespace("at") public static native @ByRef Tensor feature_alpha_dropout_(@ByRef Tensor self, double p, @Cast("bool") boolean train);
@Namespace("at") public static native @ByVal Tensor abs(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor abs_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor abs_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor abs_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor absolute(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor absolute_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor absolute_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor angle(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor angle_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor angle_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor view_as_real(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor view_as_complex(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor sgn(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor sgn_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor sgn_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor real(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor imag(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor conj(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor conj_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor conj_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor _conj(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor acos(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor acos_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor acos_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor acos_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor arccos(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor arccos_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor arccos_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor arccos_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor avg_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/);
@Namespace("at") public static native @ByVal Tensor avg_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor avg_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/);
@Namespace("at") public static native @ByVal Tensor avg_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... output_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor adaptive_max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor adaptive_max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... output_size);
@Namespace("at") public static native @ByVal Tensor add(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor add(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor add_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor add_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor add_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal Scalar alpha, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor _add_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor _add_relu(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor _add_relu_(@ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor _add_relu_(@ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor _add_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor _add_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor _add_relu_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal Scalar alpha, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor add(@Const @ByRef Tensor self, @ByVal Scalar other, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor add(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByVal Tensor addmv(@Const @ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec, @ByVal(nullValue = "c10::Scalar(1)") Scalar beta, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor addmv(@Const @ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec);
@Namespace("at") public static native @ByRef Tensor addmv_(@ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec, @ByVal(nullValue = "c10::Scalar(1)") Scalar beta, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor addmv_(@ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec);
@Namespace("at") public static native @ByRef Tensor addmv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec, @ByVal(nullValue = "c10::Scalar(1)") Scalar beta, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor addmv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec);
@Namespace("at") public static native @ByRef Tensor addmv_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec, @ByVal Scalar beta, @ByVal Scalar alpha, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _addmv_impl_(@ByRef Tensor self, @Const @ByRef Tensor self2, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec, @ByVal(nullValue = "c10::Scalar(1)") Scalar beta, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor _addmv_impl_(@ByRef Tensor self, @Const @ByRef Tensor self2, @Const @ByRef Tensor mat, @Const @ByRef Tensor vec);
@Namespace("at") public static native @ByVal Tensor addr(@Const @ByRef Tensor self, @Const @ByRef Tensor vec1, @Const @ByRef Tensor vec2, @ByVal(nullValue = "c10::Scalar(1)") Scalar beta, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor addr(@Const @ByRef Tensor self, @Const @ByRef Tensor vec1, @Const @ByRef Tensor vec2);
@Namespace("at") public static native @ByRef Tensor addr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor vec1, @Const @ByRef Tensor vec2, @ByVal(nullValue = "c10::Scalar(1)") Scalar beta, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor addr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor vec1, @Const @ByRef Tensor vec2);
@Namespace("at") public static native @ByRef Tensor addr_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor vec1, @Const @ByRef Tensor vec2, @ByVal Scalar beta, @ByVal Scalar alpha, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor affine_grid_generator(@Const @ByRef Tensor theta, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor affine_grid_generator(@Const @ByRef Tensor theta, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor affine_grid_generator_backward(@Const @ByRef Tensor grad, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor affine_grid_generator_backward(@Const @ByRef Tensor grad, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor all(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor all(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByRef Tensor all_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor all_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByRef Tensor all_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor all(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor all(@Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByRef Tensor all_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor all_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByRef Tensor all_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @Cast("bool") boolean allclose(@Const @ByRef Tensor self, @Const @ByRef Tensor other, double rtol/*=1e-05*/, double atol/*=1e-08*/, @Cast("bool") boolean equal_nan/*=false*/);
@Namespace("at") public static native @Cast("bool") boolean allclose(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByVal Tensor any(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor any(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByRef Tensor any_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor any_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByRef Tensor any_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor any(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor any(@Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByRef Tensor any_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor any_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByRef Tensor any_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor arange(@ByVal Scalar end, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor arange(@ByVal Scalar end);
@Namespace("at") public static native @ByVal Tensor arange(@ByVal Scalar end, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor arange(@ByVal Scalar start, @ByVal Scalar end, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor arange(@ByVal Scalar start, @ByVal Scalar end);
@Namespace("at") public static native @ByVal Tensor arange(@ByVal Scalar start, @ByVal Scalar end, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor arange(@ByVal Scalar start, @ByVal Scalar end, @ByVal Scalar step, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor arange(@ByVal Scalar start, @ByVal Scalar end, @ByVal Scalar step);
@Namespace("at") public static native @ByVal Tensor arange(@ByVal Scalar start, @ByVal Scalar end, @ByVal Scalar step, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByRef Tensor arange_out(@ByRef Tensor out, @ByVal Scalar end);
@Namespace("at") public static native @ByRef Tensor arange_outf(@ByVal Scalar end, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor arange_out(@ByRef Tensor out, @ByVal Scalar start, @ByVal Scalar end, @ByVal(nullValue = "c10::Scalar(1)") Scalar step);
@Namespace("at") public static native @ByRef Tensor arange_out(@ByRef Tensor out, @ByVal Scalar start, @ByVal Scalar end);
@Namespace("at") public static native @ByRef Tensor arange_outf(@ByVal Scalar start, @ByVal Scalar end, @ByVal Scalar step, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor _dim_arange(@Const @ByRef Tensor like, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal Tensor argmax(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor argmax(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor argmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor argmax_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor argmax_outf(@Const @ByRef Tensor self, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor argmin(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor argmin(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor argmin_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor argmin_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor argmin_outf(@Const @ByRef Tensor self, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor acosh(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor acosh_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor acosh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor acosh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor arccosh(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor arccosh_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor arccosh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor arccosh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor asinh(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor asinh_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor asinh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor asinh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor arcsinh(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor arcsinh_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor arcsinh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor arcsinh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor atanh(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor atanh_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor atanh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor atanh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor arctanh(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor arctanh_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor arctanh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor arctanh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor as_strided(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByVal Tensor as_strided(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor as_strided(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByVal Tensor as_strided(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... stride);
@Namespace("at") public static native @ByRef Tensor as_strided_(@ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByRef Tensor as_strided_(@ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor as_strided_(@ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional storage_offset);
@Namespace("at") public static native @ByRef Tensor as_strided_(@ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... stride);
@Namespace("at") public static native @ByVal Tensor asin(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor asin_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor asin_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor asin_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor arcsin(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor arcsin_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor arcsin_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor arcsin_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor atan(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor atan_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor atan_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor atan_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor arctan(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor arctan_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor arctan_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor arctan_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor atleast_1d(@Const @ByRef Tensor self);
@Namespace("at") public static native @StdMove TensorVector atleast_1d(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor atleast_2d(@Const @ByRef Tensor self);
@Namespace("at") public static native @StdMove TensorVector atleast_2d(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor atleast_3d(@Const @ByRef Tensor self);
@Namespace("at") public static native @StdMove TensorVector atleast_3d(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor baddbmm(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @ByVal(nullValue = "c10::Scalar(1)") Scalar beta, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor baddbmm(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2);
@Namespace("at") public static native @ByRef Tensor _baddbmm_mkl_(@ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @ByVal(nullValue = "c10::Scalar(1)") Scalar beta, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor _baddbmm_mkl_(@ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2);
@Namespace("at") public static native @ByRef Tensor baddbmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @ByVal(nullValue = "c10::Scalar(1)") Scalar beta, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor baddbmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2);
@Namespace("at") public static native @ByRef Tensor baddbmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @ByVal Scalar beta, @ByVal Scalar alpha, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length);
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
@Namespace("at") public static native @ByVal Tensor bartlett_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor batch_norm(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double momentum, double eps, @Cast("bool") boolean cudnn_enabled);
@Namespace("at") public static native @ByVal Tensor quantized_batch_norm(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef Tensor mean, @Const @ByRef Tensor var, double eps, double output_scale, @Cast("int64_t") long output_zero_point);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,int64_t>*") Tensor _batch_norm_impl_index(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double momentum, double eps, @Cast("bool") boolean cudnn_enabled);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor _batch_norm_impl_index_backward(@Cast("int64_t") long impl_index, @Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_var_transform, @Cast("bool") boolean train, double eps, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask, @Const @ByRef Tensor reservedSpace);
@Namespace("at") public static native @ByVal Tensor bernoulli(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor bernoulli(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor bernoulli_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor bernoulli_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor bernoulli_outf(@Const @ByRef Tensor self, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor bernoulli(@Const @ByRef Tensor self, double p, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor bernoulli(@Const @ByRef Tensor self, double p);
@Namespace("at") public static native @ByVal Tensor bilinear(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias);
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy(@Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByRef Tensor binary_cross_entropy_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy_with_logits(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional pos_weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy_with_logits(@Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy_with_logits_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional pos_weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor binary_cross_entropy_with_logits_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByVal Tensor bincount(@Const @ByRef Tensor self, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weights, @Cast("int64_t") long minlength/*=0*/);
@Namespace("at") public static native @ByVal Tensor bincount(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor bitwise_not(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor bitwise_not_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor bitwise_not_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor copysign(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor copysign_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor copysign_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor copysign(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByVal Tensor logical_not(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor logical_not_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor logical_not_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor logical_xor(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor logical_xor_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor logical_xor_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor logical_and(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor logical_and_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor logical_and_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor logical_or(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor logical_or_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor logical_or_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length);
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
@Namespace("at") public static native @ByVal Tensor blackman_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor bmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat2);
@Namespace("at") public static native @ByVal Tensor _bmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat2, @Cast("bool") boolean deterministic/*=false*/);
@Namespace("at") public static native @ByVal Tensor _bmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat2);
@Namespace("at") public static native @ByRef Tensor bmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat2);
@Namespace("at") public static native @ByRef Tensor bmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat2, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _bmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat2, @Cast("bool") boolean deterministic/*=false*/);
@Namespace("at") public static native @ByRef Tensor _bmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat2);
@Namespace("at") public static native @ByRef Tensor _bmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat2, @Cast("bool") boolean deterministic, @ByRef Tensor out);
@Namespace("at") public static native @StdMove TensorVector broadcast_tensors(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor broadcast_to(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor broadcast_to(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at") public static native @ByVal Tensor cat(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor cat(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor cat_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByRef Tensor cat_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor cat_outf(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor cat(@ByVal TensorArrayRef tensors, @ByVal Dimname dim);
@Namespace("at") public static native @ByRef Tensor cat_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @ByVal Dimname dim);
@Namespace("at") public static native @ByRef Tensor cat_outf(@ByVal TensorArrayRef tensors, @ByVal Dimname dim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor block_diag(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor ceil(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor ceil_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor ceil_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor ceil_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor chain_matmul(@ByVal TensorArrayRef matrices);
@Namespace("at") public static native @StdMove TensorVector unsafe_chunk(@Const @ByRef Tensor self, @Cast("int64_t") long chunks, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @StdMove TensorVector unsafe_chunk(@Const @ByRef Tensor self, @Cast("int64_t") long chunks);
@Namespace("at") public static native @StdMove TensorVector chunk(@Const @ByRef Tensor self, @Cast("int64_t") long chunks, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @StdMove TensorVector chunk(@Const @ByRef Tensor self, @Cast("int64_t") long chunks);
@Namespace("at") public static native @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @Cast("int64_t") long sections, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @Cast("int64_t") long sections);
@Namespace("at") public static native @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef indices, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef indices);
@Namespace("at") public static native @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] indices, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... indices);
@Namespace("at") public static native @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor_indices_or_sections, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @StdMove TensorVector tensor_split(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor_indices_or_sections);
@Namespace("at") public static native @ByVal Tensor clamp(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::Scalar>(c10::nullopt)") ScalarOptional min, @ByVal(nullValue = "c10::optional<c10::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByVal Tensor clamp(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor clamp_(@ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::Scalar>(c10::nullopt)") ScalarOptional min, @ByVal(nullValue = "c10::optional<c10::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByRef Tensor clamp_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor clamp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::Scalar>(c10::nullopt)") ScalarOptional min, @ByVal(nullValue = "c10::optional<c10::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByRef Tensor clamp_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor clamp_outf(@Const @ByRef Tensor self, @ByVal ScalarOptional min, @ByVal ScalarOptional max, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor clamp_max(@Const @ByRef Tensor self, @ByVal Scalar max);
@Namespace("at") public static native @ByRef Tensor clamp_max_(@ByRef Tensor self, @ByVal Scalar max);
@Namespace("at") public static native @ByRef Tensor clamp_max_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Scalar max);
@Namespace("at") public static native @ByRef Tensor clamp_max_outf(@Const @ByRef Tensor self, @ByVal Scalar max, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor clamp_min(@Const @ByRef Tensor self, @ByVal Scalar min);
@Namespace("at") public static native @ByRef Tensor clamp_min_(@ByRef Tensor self, @ByVal Scalar min);
@Namespace("at") public static native @ByRef Tensor clamp_min_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Scalar min);
@Namespace("at") public static native @ByRef Tensor clamp_min_outf(@Const @ByRef Tensor self, @ByVal Scalar min, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor clip(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::Scalar>(c10::nullopt)") ScalarOptional min, @ByVal(nullValue = "c10::optional<c10::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByVal Tensor clip(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor clip_(@ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::Scalar>(c10::nullopt)") ScalarOptional min, @ByVal(nullValue = "c10::optional<c10::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByRef Tensor clip_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor clip_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::Scalar>(c10::nullopt)") ScalarOptional min, @ByVal(nullValue = "c10::optional<c10::Scalar>(c10::nullopt)") ScalarOptional max);
@Namespace("at") public static native @ByRef Tensor clip_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor clip_outf(@Const @ByRef Tensor self, @ByVal ScalarOptional min, @ByVal ScalarOptional max, @ByRef Tensor out);
@Namespace("at") public static native @Cast("bool") boolean cudnn_is_acceptable(@Const @ByRef Tensor self);

@Namespace("at") public static native @ByRef Tensor complex_out(@ByRef Tensor out, @Const @ByRef Tensor real, @Const @ByRef Tensor imag);
@Namespace("at") public static native @ByRef Tensor complex_outf(@Const @ByRef Tensor real, @Const @ByRef Tensor imag, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor polar(@Const @ByRef Tensor abs, @Const @ByRef Tensor angle);
@Namespace("at") public static native @ByRef Tensor polar_out(@ByRef Tensor out, @Const @ByRef Tensor abs, @Const @ByRef Tensor angle);
@Namespace("at") public static native @ByRef Tensor polar_outf(@Const @ByRef Tensor abs, @Const @ByRef Tensor angle, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor constant_pad_nd(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef pad, @ByVal(nullValue = "c10::Scalar(0)") Scalar value);
@Namespace("at") public static native @ByVal Tensor constant_pad_nd(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef pad);
@Namespace("at") public static native @ByVal Tensor constant_pad_nd(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] pad, @ByVal(nullValue = "c10::Scalar(0)") Scalar value);
@Namespace("at") public static native @ByVal Tensor constant_pad_nd(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... pad);
@Namespace("at") public static native @ByVal Tensor convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor convolution_overrideable(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor convolution_overrideable(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor convolution_backward_overrideable(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor convolution_backward_overrideable(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal Tensor _convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor _convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor _convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled);
@Namespace("at") public static native @ByVal Tensor _convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled);
@Namespace("at") public static native @ByVal Tensor _convolution_nogroup(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding);
@Namespace("at") public static native @ByVal Tensor _convolution_nogroup(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... output_padding);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor _convolution_double_backward(@Const @ByRef TensorOptional ggI, @Const @ByRef TensorOptional ggW, @Const @ByRef TensorOptional ggb, @Const @ByRef Tensor gO, @Const @ByRef Tensor weight, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean transposed, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled, @Cast("bool") boolean allow_tf32, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor _convolution_double_backward(@Const @ByRef TensorOptional ggI, @Const @ByRef TensorOptional ggW, @Const @ByRef TensorOptional ggb, @Const @ByRef Tensor gO, @Const @ByRef Tensor weight, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean transposed, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean cudnn_enabled, @Cast("bool") boolean allow_tf32, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor conv_tbc(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor bias, @Cast("int64_t") long pad/*=0*/);
@Namespace("at") public static native @ByVal Tensor conv_tbc(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef Tensor bias);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor conv_tbc_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef Tensor bias, @Cast("int64_t") long pad);
@Namespace("at") public static native @ByVal Tensor conv_transpose1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor conv_transpose1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv_transpose1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByVal Tensor conv_transpose2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor conv_transpose2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv_transpose2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByVal Tensor conv_transpose3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor conv_transpose3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor conv_transpose3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @Cast("int64_t") long groups/*=1*/, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByVal Tensor _copy_from(@Const @ByRef Tensor self, @Const @ByRef Tensor dst, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByVal Tensor _copy_from(@Const @ByRef Tensor self, @Const @ByRef Tensor dst);
@Namespace("at") public static native @ByVal Tensor cos(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor cos_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor cos_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor cos_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor cosh(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor cosh_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor cosh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor cosh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor cosine_embedding_loss(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor target, double margin/*=0.0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor cosine_embedding_loss(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByVal Tensor count_nonzero(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor count_nonzero(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dim);
@Namespace("at") public static native @ByVal Tensor count_nonzero(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByVal Tensor count_nonzero(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor cudnn_affine_grid_generator(@Const @ByRef Tensor theta, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long H, @Cast("int64_t") long W);
@Namespace("at") public static native @ByVal Tensor cudnn_affine_grid_generator_backward(@Const @ByRef Tensor grad, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long H, @Cast("int64_t") long W);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor>*") Tensor cudnn_batch_norm(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double exponential_average_factor, double epsilon);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor cudnn_batch_norm_backward(@Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_var, double epsilon, @Const @ByRef Tensor reserveSpace);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_backward_input(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef self_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_backward_input(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] self_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor cudnn_convolution_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor cudnn_convolution_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_backward_weight(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_backward_weight(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor cudnn_convolution_transpose_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor cudnn_convolution_transpose_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose_backward_input(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose_backward_input(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose_backward_weight(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor cudnn_convolution_transpose_backward_weight(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @Cast("bool") boolean allow_tf32);
@Namespace("at") public static native @ByVal Tensor cudnn_grid_sampler(@Const @ByRef Tensor self, @Const @ByRef Tensor grid);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor cudnn_grid_sampler_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grid, @Const @ByRef Tensor grad_output);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor cummax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummax_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummax_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor values, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor cummax(@Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummax_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummax_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByRef Tensor values, @ByRef Tensor indices);
@Namespace("at") public static native void _cummax_helper(@Const @ByRef Tensor self, @ByRef Tensor values, @ByRef Tensor indices, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor cummin(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummin_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummin_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor values, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor cummin(@Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummin_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> cummin_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByRef Tensor values, @ByRef Tensor indices);
@Namespace("at") public static native void _cummin_helper(@Const @ByRef Tensor self, @ByRef Tensor values, @ByRef Tensor indices, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal Tensor cummaxmin_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor input, @Const @ByRef Tensor indices, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal Tensor cumprod(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor cumprod(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByRef Tensor cumprod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor cumprod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByRef Tensor cumprod_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor cumprod(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor cumprod(@Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByRef Tensor cumprod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor cumprod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByRef Tensor cumprod_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor cumprod_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor input, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal Tensor cumsum(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor cumsum(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByRef Tensor cumsum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor cumsum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByRef Tensor cumsum_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor cumsum(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor cumsum(@Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByRef Tensor cumsum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor cumsum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByRef Tensor cumsum_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Cast("int64_t") long blank/*=0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths);
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Cast("int64_t") long blank/*=0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... target_lengths);
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @Const @ByRef Tensor input_lengths, @Const @ByRef Tensor target_lengths, @Cast("int64_t") long blank/*=0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @Const @ByRef Tensor input_lengths, @Const @ByRef Tensor target_lengths);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Cast("int64_t") long blank/*=0*/, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Cast("int64_t") long blank/*=0*/, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _ctc_loss(@Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... target_lengths);
@Namespace("at") public static native @ByVal Tensor _ctc_loss_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Const @ByRef Tensor neg_log_likelihood, @Const @ByRef Tensor log_alpha, @Cast("int64_t") long blank, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal Tensor _ctc_loss_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_lengths, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef target_lengths, @Const @ByRef Tensor neg_log_likelihood, @Const @ByRef Tensor log_alpha, @Cast("int64_t") long blank);
@Namespace("at") public static native @ByVal Tensor _ctc_loss_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Const @ByRef Tensor neg_log_likelihood, @Const @ByRef Tensor log_alpha, @Cast("int64_t") long blank, @Cast("bool") boolean zero_infinity/*=false*/);
@Namespace("at") public static native @ByVal Tensor _ctc_loss_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor log_probs, @Const @ByRef Tensor targets, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_lengths, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] target_lengths, @Const @ByRef Tensor neg_log_likelihood, @Const @ByRef Tensor log_alpha, @Cast("int64_t") long blank);
@Namespace("at") public static native @ByVal Tensor diag_embed(@Const @ByRef Tensor self, @Cast("int64_t") long offset/*=0*/, @Cast("int64_t") long dim1/*=-2*/, @Cast("int64_t") long dim2/*=-1*/);
@Namespace("at") public static native @ByVal Tensor diag_embed(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor diagflat(@Const @ByRef Tensor self, @Cast("int64_t") long offset/*=0*/);
@Namespace("at") public static native @ByVal Tensor diagflat(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor diagonal(@Const @ByRef Tensor self, @Cast("int64_t") long offset/*=0*/, @Cast("int64_t") long dim1/*=0*/, @Cast("int64_t") long dim2/*=1*/);
@Namespace("at") public static native @ByVal Tensor diagonal(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor diagonal(@Const @ByRef Tensor self, @ByVal Dimname outdim, @ByVal Dimname dim1, @ByVal Dimname dim2, @Cast("int64_t") long offset/*=0*/);
@Namespace("at") public static native @ByVal Tensor diagonal(@Const @ByRef Tensor self, @ByVal Dimname outdim, @ByVal Dimname dim1, @ByVal Dimname dim2);
@Namespace("at") public static native @ByVal Tensor diagonal_backward(@Const @ByRef Tensor grad, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_sizes, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2);
@Namespace("at") public static native @ByVal Tensor diagonal_backward(@Const @ByRef Tensor grad, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_sizes, @Cast("int64_t") long offset, @Cast("int64_t") long dim1, @Cast("int64_t") long dim2);
@Namespace("at") public static native @ByVal Tensor diff(@Const @ByRef Tensor self, @Cast("int64_t") long n/*=1*/, @Cast("int64_t") long dim/*=-1*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional prepend, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional append);
@Namespace("at") public static native @ByVal Tensor diff(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor diff_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long n/*=1*/, @Cast("int64_t") long dim/*=-1*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional prepend, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional append);
@Namespace("at") public static native @ByRef Tensor diff_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor diff_outf(@Const @ByRef Tensor self, @Cast("int64_t") long n, @Cast("int64_t") long dim, @Const @ByRef TensorOptional prepend, @Const @ByRef TensorOptional append, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor div(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor div_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor div_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor div(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @StdString BytePointer rounding_mode);
@Namespace("at") public static native @ByVal Tensor div(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @StdString String rounding_mode);
@Namespace("at") public static native @ByRef Tensor div_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @StdString BytePointer rounding_mode);
@Namespace("at") public static native @ByRef Tensor div_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @StdString String rounding_mode);
@Namespace("at") public static native @ByRef Tensor div_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @StdString BytePointer rounding_mode, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor div_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @StdString String rounding_mode, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor div(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByVal Tensor div(@Const @ByRef Tensor self, @ByVal Scalar other, @StdString BytePointer rounding_mode);
@Namespace("at") public static native @ByVal Tensor div(@Const @ByRef Tensor self, @ByVal Scalar other, @StdString String rounding_mode);
@Namespace("at") public static native @ByVal Tensor divide(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor divide_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor divide_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor divide(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByVal Tensor divide(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @StdString BytePointer rounding_mode);
@Namespace("at") public static native @ByVal Tensor divide(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @StdString String rounding_mode);
@Namespace("at") public static native @ByRef Tensor divide_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @StdString BytePointer rounding_mode);
@Namespace("at") public static native @ByRef Tensor divide_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @StdString String rounding_mode);
@Namespace("at") public static native @ByRef Tensor divide_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @StdString BytePointer rounding_mode, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor divide_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @StdString String rounding_mode, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor divide(@Const @ByRef Tensor self, @ByVal Scalar other, @StdString BytePointer rounding_mode);
@Namespace("at") public static native @ByVal Tensor divide(@Const @ByRef Tensor self, @ByVal Scalar other, @StdString String rounding_mode);
@Namespace("at") public static native @ByVal Tensor true_divide(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor true_divide_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor true_divide_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor true_divide(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByVal Tensor dot(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor);
@Namespace("at") public static native @ByRef Tensor dot_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor tensor);
@Namespace("at") public static native @ByRef Tensor dot_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor vdot(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor vdot_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor vdot_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor einsum(@StdString BytePointer equation, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor einsum(@StdString String equation, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor embedding(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Cast("int64_t") long padding_idx/*=-1*/, @Cast("bool") boolean scale_grad_by_freq/*=false*/, @Cast("bool") boolean sparse/*=false*/);
@Namespace("at") public static native @ByVal Tensor embedding(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByVal Tensor embedding_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Cast("int64_t") long num_weights, @Cast("int64_t") long padding_idx, @Cast("bool") boolean scale_grad_by_freq, @Cast("bool") boolean sparse);
@Namespace("at") public static native @ByVal Tensor embedding_dense_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor indices, @Cast("int64_t") long num_weights, @Cast("int64_t") long padding_idx, @Cast("bool") boolean scale_grad_by_freq);
@Namespace("at") public static native @ByRef Tensor embedding_renorm_(@ByRef Tensor self, @Const @ByRef Tensor indices, double max_norm, double norm_type);
@Namespace("at") public static native @ByVal Tensor embedding_sparse_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Cast("int64_t") long num_weights, @Cast("int64_t") long padding_idx, @Cast("bool") boolean scale_grad_by_freq);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor>*") Tensor _embedding_bag_forward_only(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Cast("bool") boolean scale_grad_by_freq/*=false*/, @Cast("int64_t") long mode/*=0*/, @Cast("bool") boolean sparse/*=false*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional per_sample_weights, @Cast("bool") boolean include_last_offset/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor>*") Tensor _embedding_bag_forward_only(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _rowwise_prune(@Const @ByRef Tensor weight, @Const @ByRef Tensor mask, ScalarType compressed_indices_dtype);
@Namespace("at") public static native @ByVal Tensor row_stack(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor row_stack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor row_stack_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor>*") Tensor embedding_bag(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Cast("bool") boolean scale_grad_by_freq/*=false*/, @Cast("int64_t") long mode/*=0*/, @Cast("bool") boolean sparse/*=false*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional per_sample_weights, @Cast("bool") boolean include_last_offset/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor>*") Tensor embedding_bag(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor>*") Tensor _embedding_bag(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Cast("bool") boolean scale_grad_by_freq/*=false*/, @Cast("int64_t") long mode/*=0*/, @Cast("bool") boolean sparse/*=false*/, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional per_sample_weights, @Cast("bool") boolean include_last_offset/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor>*") Tensor _embedding_bag(@Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets);
@Namespace("at") public static native @ByVal Tensor _embedding_bag_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Const @ByRef Tensor maximum_indices, @Cast("int64_t") long num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Cast("bool") boolean sparse, @Const @ByRef TensorOptional per_sample_weights);
@Namespace("at") public static native @ByVal Tensor _embedding_bag_sparse_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Cast("int64_t") long num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Const @ByRef TensorOptional per_sample_weights);
@Namespace("at") public static native @ByVal Tensor _embedding_bag_dense_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Const @ByRef Tensor offset2bag, @Const @ByRef Tensor bag_size, @Const @ByRef Tensor maximum_indices, @Cast("int64_t") long num_weights, @Cast("bool") boolean scale_grad_by_freq, @Cast("int64_t") long mode, @Const @ByRef TensorOptional per_sample_weights);
@Namespace("at") public static native @ByVal Tensor _embedding_bag_per_sample_weights_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor weight, @Const @ByRef Tensor indices, @Const @ByRef Tensor offsets, @Const @ByRef Tensor offset2bag, @Cast("int64_t") long mode);
@Namespace("at") public static native @ByVal Tensor empty_meta(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty_meta(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor empty_meta(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty_meta(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at") public static native @ByVal Tensor empty_meta(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty_meta(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor _empty_affine_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options, double scale/*=1*/, @Cast("int64_t") long zero_point/*=0*/, @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor _empty_affine_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor _empty_affine_quantized(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options, double scale/*=1*/, @Cast("int64_t") long zero_point/*=0*/, @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor _empty_affine_quantized(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at") public static native @ByVal Tensor _empty_affine_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, double scale, @Cast("int64_t") long zero_point, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor _empty_affine_quantized(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, double scale, @Cast("int64_t") long zero_point, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor _empty_per_channel_affine_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor _empty_per_channel_affine_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis);
@Namespace("at") public static native @ByVal Tensor _empty_per_channel_affine_quantized(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::MemoryFormat::Contiguous)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor _empty_per_channel_affine_quantized(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis);
@Namespace("at") public static native @ByVal Tensor _empty_per_channel_affine_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor _empty_per_channel_affine_quantized(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty_quantized(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor qtensor);
@Namespace("at") public static native @ByVal Tensor empty_quantized(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor qtensor);
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor empty_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at") public static native @ByRef Tensor empty_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor empty_outf(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal MemoryFormatOptional memory_format, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor empty_like(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty_like(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor empty_like(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... stride);
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor empty_strided(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor erf(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor erf_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor erf_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor erf_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor erfc(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor erfc_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor erfc_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor erfc_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor exp(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor exp_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor exp_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor exp_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor exp2(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor exp2_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor exp2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor exp2_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor expm1(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor expm1_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor expm1_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor expm1_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n);
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n, @Cast("int64_t") long m, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n, @Cast("int64_t") long m);
@Namespace("at") public static native @ByVal Tensor eye(@Cast("int64_t") long n, @Cast("int64_t") long m, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByRef Tensor eye_out(@ByRef Tensor out, @Cast("int64_t") long n);
@Namespace("at") public static native @ByRef Tensor eye_outf(@Cast("int64_t") long n, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor eye_out(@ByRef Tensor out, @Cast("int64_t") long n, @Cast("int64_t") long m);
@Namespace("at") public static native @ByRef Tensor eye_outf(@Cast("int64_t") long n, @Cast("int64_t") long m, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor flatten(@Const @ByRef Tensor self, @Cast("int64_t") long start_dim/*=0*/, @Cast("int64_t") long end_dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor flatten(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor flatten(@Const @ByRef Tensor self, @Cast("int64_t") long start_dim, @Cast("int64_t") long end_dim, @ByVal Dimname out_dim);
@Namespace("at") public static native @ByVal Tensor flatten(@Const @ByRef Tensor self, @ByVal Dimname start_dim, @ByVal Dimname end_dim, @ByVal Dimname out_dim);
@Namespace("at") public static native @ByVal Tensor flatten(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dims, @ByVal Dimname out_dim);
@Namespace("at") public static native @ByRef Tensor fill_(@ByRef Tensor self, @ByVal Scalar value);
@Namespace("at") public static native @ByRef Tensor fill_(@ByRef Tensor self, @Const @ByRef Tensor value);
@Namespace("at") public static native @ByVal Tensor floor(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor floor_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor floor_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor floor_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor floor_divide(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor floor_divide_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor floor_divide_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor floor_divide(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByVal Tensor frac(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor frac_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor frac_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor frac_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal Scalar fill_value, @ByVal DimnameListOptional names, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal Scalar fill_value, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal Scalar fill_value, @ByVal DimnameListOptional names, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal Scalar fill_value, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal Scalar fill_value, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal Scalar fill_value, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal Scalar fill_value, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal Scalar fill_value);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal Scalar fill_value, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal Scalar fill_value);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal Scalar fill_value, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor full(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal Scalar fill_value, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByRef Tensor full_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal Scalar fill_value);
@Namespace("at") public static native @ByRef Tensor full_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal Scalar fill_value);
@Namespace("at") public static native @ByRef Tensor full_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal Scalar fill_value, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor full_outf(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal Scalar fill_value, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor full_like(@Const @ByRef Tensor self, @ByVal Scalar fill_value, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor full_like(@Const @ByRef Tensor self, @ByVal Scalar fill_value);
@Namespace("at") public static native @ByVal Tensor full_like(@Const @ByRef Tensor self, @ByVal Scalar fill_value, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor from_file(@StdString BytePointer filename, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional shared, @ByVal(nullValue = "c10::optional<int64_t>(0)") LongOptional size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_file(@StdString BytePointer filename);
@Namespace("at") public static native @ByVal Tensor from_file(@StdString String filename, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional shared, @ByVal(nullValue = "c10::optional<int64_t>(0)") LongOptional size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_file(@StdString String filename);
@Namespace("at") public static native @ByVal Tensor from_file(@StdString BytePointer filename, @ByVal BoolOptional shared, @ByVal LongOptional size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor from_file(@StdString String filename, @ByVal BoolOptional shared, @ByVal LongOptional size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByRef Tensor gcd_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor gcd_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor gcd(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor gcd_(@ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor lcm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor lcm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor lcm(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor lcm_(@ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByVal Tensor grid_sampler(@Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor grid_sampler_2d(@Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor grid_sampler_2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor _grid_sampler_2d_cpu_fallback(@Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _grid_sampler_2d_cpu_fallback_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor grid_sampler_3d(@Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor grid_sampler_3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor grid, @Cast("int64_t") long interpolation_mode, @Cast("int64_t") long padding_mode, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length);
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
@Namespace("at") public static native @ByVal Tensor hann_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, double beta, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, double beta);
@Namespace("at") public static native @ByVal Tensor hamming_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double alpha, double beta, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length);
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic);
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double beta, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double beta);
@Namespace("at") public static native @ByVal Tensor kaiser_window(@Cast("int64_t") long window_length, @Cast("bool") boolean periodic, double beta, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor hinge_embedding_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, double margin/*=1.0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor hinge_embedding_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByVal Tensor group_norm(@Const @ByRef Tensor input, @Cast("int64_t") long num_groups, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, double eps/*=1e-05*/, @Cast("bool") boolean cudnn_enabled/*=true*/);
@Namespace("at") public static native @ByVal Tensor group_norm(@Const @ByRef Tensor input, @Cast("int64_t") long num_groups);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor native_group_norm(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long HxW, @Cast("int64_t") long group, double eps);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor native_group_norm_backward(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Cast("int64_t") long N, @Cast("int64_t") long C, @Cast("int64_t") long HxW, @Cast("int64_t") long group, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal Tensor _fft_r2c(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("bool") boolean onesided);
@Namespace("at") public static native @ByVal Tensor _fft_r2c(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("bool") boolean onesided);
@Namespace("at") public static native @ByRef Tensor _fft_r2c_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("bool") boolean onesided);
@Namespace("at") public static native @ByRef Tensor _fft_r2c_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("bool") boolean onesided);
@Namespace("at") public static native @ByRef Tensor _fft_r2c_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("bool") boolean onesided, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _fft_r2c_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("bool") boolean onesided, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor _fft_c2r(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("int64_t") long last_dim_size);
@Namespace("at") public static native @ByVal Tensor _fft_c2r(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("int64_t") long last_dim_size);
@Namespace("at") public static native @ByRef Tensor _fft_c2r_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("int64_t") long last_dim_size);
@Namespace("at") public static native @ByRef Tensor _fft_c2r_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("int64_t") long last_dim_size);
@Namespace("at") public static native @ByRef Tensor _fft_c2r_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("int64_t") long last_dim_size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _fft_c2r_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("int64_t") long last_dim_size, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor _fft_c2c(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("bool") boolean forward);
@Namespace("at") public static native @ByVal Tensor _fft_c2c(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("bool") boolean forward);
@Namespace("at") public static native @ByRef Tensor _fft_c2c_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("bool") boolean forward);
@Namespace("at") public static native @ByRef Tensor _fft_c2c_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("bool") boolean forward);
@Namespace("at") public static native @ByRef Tensor _fft_c2c_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("int64_t") long normalization, @Cast("bool") boolean forward, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _fft_c2c_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("int64_t") long normalization, @Cast("bool") boolean forward, @ByRef Tensor out);
@Namespace("at") public static native @Cast("int64_t") long _cufft_get_plan_cache_size(@Cast("int64_t") long device_index);
@Namespace("at") public static native @Cast("int64_t") long _cufft_get_plan_cache_max_size(@Cast("int64_t") long device_index);
@Namespace("at") public static native void _cufft_set_plan_cache_max_size(@Cast("int64_t") long device_index, @Cast("int64_t") long max_size);
@Namespace("at") public static native void _cufft_clear_plan_cache(@Cast("int64_t") long device_index);
@Namespace("at") public static native @ByVal Tensor index_copy(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source);
@Namespace("at") public static native @ByVal Tensor index_copy(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source);
@Namespace("at") public static native @ByVal Tensor instance_norm(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean use_input_stats, double momentum, double eps, @Cast("bool") boolean cudnn_enabled);
@Namespace("at") public static native @ByVal Tensor inverse(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor inverse_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor inverse_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor _inverse_helper(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor isclose(@Const @ByRef Tensor self, @Const @ByRef Tensor other, double rtol/*=1e-05*/, double atol/*=1e-08*/, @Cast("bool") boolean equal_nan/*=false*/);
@Namespace("at") public static native @ByVal Tensor isclose(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByVal Tensor isnan(@Const @ByRef Tensor self);
@Namespace("at") public static native @Cast("bool") boolean is_distributed(@Const @ByRef Tensor self);
@Namespace("at") public static native @Cast("bool") boolean is_floating_point(@Const @ByRef Tensor self);
@Namespace("at") public static native @Cast("bool") boolean is_complex(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor isreal(@Const @ByRef Tensor self);
@Namespace("at") public static native @Cast("bool") boolean is_nonzero(@Const @ByRef Tensor self);
@Namespace("at") public static native @Cast("bool") boolean is_same_size(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @Cast("bool") boolean is_signed(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor kl_div(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("bool") boolean log_target/*=false*/);
@Namespace("at") public static native @ByVal Tensor kl_div(@Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByVal Tensor kl_div_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("bool") boolean log_target/*=false*/);
@Namespace("at") public static native @ByVal Tensor kl_div_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByVal Tensor kron(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor kron_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor kron_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor kthvalue(@Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor kthvalue(@Const @ByRef Tensor self, @Cast("int64_t") long k);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> kthvalue_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> kthvalue_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> kthvalue_outf(@Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor kthvalue(@Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor kthvalue(@Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> kthvalue_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> kthvalue_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> kthvalue_outf(@Const @ByRef Tensor self, @Cast("int64_t") long k, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal Tensor layer_norm(@Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef normalized_shape, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, double eps/*=1e-05*/, @Cast("bool") boolean cudnn_enable/*=true*/);
@Namespace("at") public static native @ByVal Tensor layer_norm(@Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef normalized_shape);
@Namespace("at") public static native @ByVal Tensor layer_norm(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] normalized_shape, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, double eps/*=1e-05*/, @Cast("bool") boolean cudnn_enable/*=true*/);
@Namespace("at") public static native @ByVal Tensor layer_norm(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... normalized_shape);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor native_layer_norm(@Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef normalized_shape, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, double eps);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor native_layer_norm(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] normalized_shape, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, double eps);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor native_layer_norm_backward(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef normalized_shape, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor native_layer_norm_backward(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] normalized_shape, @Const @ByRef Tensor mean, @Const @ByRef Tensor rstd, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal Tensor nan_to_num(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional nan, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional posinf, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional neginf);
@Namespace("at") public static native @ByVal Tensor nan_to_num(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor nan_to_num_(@ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional nan, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional posinf, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional neginf);
@Namespace("at") public static native @ByRef Tensor nan_to_num_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor nan_to_num_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional nan, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional posinf, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional neginf);
@Namespace("at") public static native @ByRef Tensor nan_to_num_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor nan_to_num_outf(@Const @ByRef Tensor self, @ByVal DoubleOptional nan, @ByVal DoubleOptional posinf, @ByVal DoubleOptional neginf, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor linear(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias);
@Namespace("at") public static native @ByVal Tensor linear(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor mkldnn_linear(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias);
@Namespace("at") public static native @ByVal Tensor mkldnn_linear(@Const @ByRef Tensor self, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor mkldnn_linear_backward_input(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor mkldnn_linear_backward_input(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor mkldnn_linear_backward_weights(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Cast("bool") boolean bias_defined);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor mkldnn_linear_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal Tensor fbgemm_linear_int8_weight_fp32_activation(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef Tensor packed, @Const @ByRef Tensor col_offsets, @ByVal Scalar weight_scale, @ByVal Scalar weight_zero_point, @Const @ByRef Tensor bias);
@Namespace("at") public static native @ByVal Tensor fbgemm_linear_int8_weight(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef Tensor packed, @Const @ByRef Tensor col_offsets, @ByVal Scalar weight_scale, @ByVal Scalar weight_zero_point, @Const @ByRef Tensor bias);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,double,int64_t>*") Tensor fbgemm_linear_quantize_weight(@Const @ByRef Tensor input);
@Namespace("at") public static native @ByVal Tensor fbgemm_pack_gemm_matrix_fp16(@Const @ByRef Tensor input);
@Namespace("at") public static native @ByVal Tensor fbgemm_linear_fp16_weight_fp32_activation(@Const @ByRef Tensor input, @Const @ByRef Tensor packed_weight, @Const @ByRef Tensor bias);
@Namespace("at") public static native @ByVal Tensor fbgemm_linear_fp16_weight(@Const @ByRef Tensor input, @Const @ByRef Tensor packed_weight, @Const @ByRef Tensor bias);
@Namespace("at") public static native @ByVal Tensor fbgemm_pack_quantized_matrix(@Const @ByRef Tensor input);
@Namespace("at") public static native @ByVal Tensor fbgemm_pack_quantized_matrix(@Const @ByRef Tensor input, @Cast("int64_t") long K, @Cast("int64_t") long N);
@Namespace("at") public static native @ByVal Tensor ldexp(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor ldexp_(@ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor ldexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor ldexp_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor linspace(@ByVal Scalar start, @ByVal Scalar end, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional steps, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor linspace(@ByVal Scalar start, @ByVal Scalar end);
@Namespace("at") public static native @ByVal Tensor linspace(@ByVal Scalar start, @ByVal Scalar end, @ByVal LongOptional steps, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByRef Tensor linspace_out(@ByRef Tensor out, @ByVal Scalar start, @ByVal Scalar end, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional steps);
@Namespace("at") public static native @ByRef Tensor linspace_out(@ByRef Tensor out, @ByVal Scalar start, @ByVal Scalar end);
@Namespace("at") public static native @ByRef Tensor linspace_outf(@ByVal Scalar start, @ByVal Scalar end, @ByVal LongOptional steps, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor log(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor log_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor log_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor log_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor log10(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor log10_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor log10_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor log10_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor log1p(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor log1p_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor log1p_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor log1p_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor log2(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor log2_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor log2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor log2_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor logaddexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor logaddexp_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor logaddexp(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor logaddexp2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor logaddexp2_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor logaddexp2(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByVal Tensor xlogy(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByVal Tensor xlogy(@ByVal Scalar self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByVal Tensor xlogy(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor xlogy_(@ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor xlogy_(@ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor xlogy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor xlogy_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor xlogy_out(@ByRef Tensor out, @ByVal Scalar self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor xlogy_outf(@ByVal Scalar self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor xlogy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor xlogy_outf(@Const @ByRef Tensor self, @ByVal Scalar other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor logdet(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor logspace(@ByVal Scalar start, @ByVal Scalar end, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional steps, double base/*=10.0*/, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor logspace(@ByVal Scalar start, @ByVal Scalar end);
@Namespace("at") public static native @ByVal Tensor logspace(@ByVal Scalar start, @ByVal Scalar end, @ByVal LongOptional steps, double base, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByRef Tensor logspace_out(@ByRef Tensor out, @ByVal Scalar start, @ByVal Scalar end, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional steps, double base/*=10.0*/);
@Namespace("at") public static native @ByRef Tensor logspace_out(@ByRef Tensor out, @ByVal Scalar start, @ByVal Scalar end);
@Namespace("at") public static native @ByRef Tensor logspace_outf(@ByVal Scalar start, @ByVal Scalar end, @ByVal LongOptional steps, double base, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal Tensor log_softmax(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor log_softmax(@Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal Tensor _log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float);
@Namespace("at") public static native @ByVal Tensor _log_softmax_backward_data(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor _logcumsumexp(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByRef Tensor _logcumsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByRef Tensor _logcumsumexp_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor logcumsumexp(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByRef Tensor logcumsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByRef Tensor logcumsumexp_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor logcumsumexp(@Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByRef Tensor logcumsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByRef Tensor logcumsumexp_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dim);
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dim);
@Namespace("at") public static native @ByRef Tensor logsumexp_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor logsumexp_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor logsumexp(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor logsumexp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByRef Tensor logsumexp_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor margin_ranking_loss(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor target, double margin/*=0.0*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor margin_ranking_loss(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByVal Tensor matmul(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor matmul_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor matmul_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor matrix_rank(@Const @ByRef Tensor self, double tol, @Cast("bool") boolean symmetric/*=false*/);
@Namespace("at") public static native @ByVal Tensor matrix_rank(@Const @ByRef Tensor self, double tol);
@Namespace("at") public static native @ByVal Tensor matrix_rank(@Const @ByRef Tensor self, @Cast("bool") boolean symmetric/*=false*/);
@Namespace("at") public static native @ByVal Tensor matrix_rank(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor matrix_power(@Const @ByRef Tensor self, @Cast("int64_t") long n);
@Namespace("at") public static native @ByVal Tensor matrix_exp(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor matrix_exp_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _aminmax(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _aminmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _aminmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal Tensor _compute_linear_combination(@Const @ByRef Tensor input, @Const @ByRef Tensor coefficients);
@Namespace("at") public static native @ByRef Tensor _compute_linear_combination_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor coefficients);
@Namespace("at") public static native @ByRef Tensor _compute_linear_combination_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor coefficients, @ByRef Tensor out);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor max(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor max(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_out(@ByRef Tensor max, @ByRef Tensor max_values, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_out(@ByRef Tensor max, @ByRef Tensor max_values, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor max, @ByRef Tensor max_values);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor max(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor max(@Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_out(@ByRef Tensor max, @ByRef Tensor max_values, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_out(@ByRef Tensor max, @ByRef Tensor max_values, @Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor max, @ByRef Tensor max_values);
@Namespace("at") public static native @ByVal Tensor value_selecting_reduction_backward(@Const @ByRef Tensor grad, @Cast("int64_t") long dim, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes, @Cast("bool") boolean keepdim);
@Namespace("at") public static native @ByVal Tensor value_selecting_reduction_backward(@Const @ByRef Tensor grad, @Cast("int64_t") long dim, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] sizes, @Cast("bool") boolean keepdim);
@Namespace("at") public static native @ByVal Tensor amax(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor amax(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor amax(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor amax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor amax_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor amax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor amax_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor amax_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor max_pool1d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor max_pool1d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor max_pool1d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor max_pool1d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByVal Tensor max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByVal Tensor max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor quantized_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByVal Tensor max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal Tensor max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dim);
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dim);
@Namespace("at") public static native @ByRef Tensor mean_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor mean_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor mean_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByRef Tensor mean_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor median(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor median(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor median(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> median_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> median_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> median_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor median(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor median(@Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> median_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> median_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> median_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal Tensor nanmedian(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor nanmedian(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor nanmedian(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nanmedian_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nanmedian_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nanmedian_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor nanmedian(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor nanmedian(@Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nanmedian_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nanmedian_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nanmedian_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor min(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor min(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> min_out(@ByRef Tensor min, @ByRef Tensor min_indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> min_out(@ByRef Tensor min, @ByRef Tensor min_indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> min_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor min, @ByRef Tensor min_indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor min(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor min(@Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> min_out(@ByRef Tensor min, @ByRef Tensor min_indices, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> min_out(@ByRef Tensor min, @ByRef Tensor min_indices, @Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> min_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor min, @ByRef Tensor min_indices);
@Namespace("at") public static native @ByVal Tensor amin(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor amin(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor amin(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor amin_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor amin_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor amin_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor amin_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor amin_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor mkldnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor mkldnn_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor mkldnn_convolution_backward_input(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef self_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean bias_defined);
@Namespace("at") public static native @ByVal Tensor mkldnn_convolution_backward_input(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] self_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean bias_defined);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor mkldnn_convolution_backward_weights(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean bias_defined);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor mkldnn_convolution_backward_weights(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean bias_defined);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor mkldnn_convolution_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor mkldnn_convolution_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor miopen_batch_norm(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double exponential_average_factor, double epsilon);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor miopen_batch_norm_backward(@Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_var, double epsilon);
@Namespace("at") public static native @ByVal Tensor miopen_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_backward_input(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef self_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_backward_input(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] self_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor miopen_convolution_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor miopen_convolution_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_backward_bias(@Const @ByRef Tensor grad_output);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_backward_weight(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_backward_weight(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_transpose(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor miopen_convolution_transpose_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor miopen_convolution_transpose_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_transpose_backward_input(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_transpose_backward_input(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_transpose_backward_weight(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_convolution_transpose_backward_weight(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_depthwise_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_depthwise_convolution(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_depthwise_convolution_backward_input(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef self_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_depthwise_convolution_backward_input(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] self_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor miopen_depthwise_convolution_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor miopen_depthwise_convolution_backward(@Const @ByRef Tensor self, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal Tensor miopen_depthwise_convolution_backward_weight(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal Tensor miopen_depthwise_convolution_backward_weight(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] weight_size, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups, @Cast("bool") boolean benchmark, @Cast("bool") boolean deterministic);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor>*") Tensor miopen_rnn(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor>*") Tensor miopen_rnn(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Const @ByRef TensorOptional dropout_state);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor,std::vector<at::Tensor> >*") Tensor miopen_rnn_backward(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor,std::vector<at::Tensor> >*") Tensor miopen_rnn_backward(@Const @ByRef Tensor input, @ByVal TensorArrayRef weight, @Cast("int64_t") long weight_stride0, @Const @ByRef Tensor weight_buf, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional cx, @Const @ByRef Tensor output, @Const @ByRef TensorOptional grad_output, @Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Cast("int64_t") long mode, @Cast("int64_t") long hidden_size, @Cast("int64_t") long num_layers, @Cast("bool") boolean batch_first, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] batch_sizes, @Const @ByRef TensorOptional dropout_state, @Const @ByRef Tensor reserve, @ByVal @Cast("std::array<bool,4>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal Tensor mm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat2);
@Namespace("at") public static native @ByRef Tensor mm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat2);
@Namespace("at") public static native @ByRef Tensor mm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat2, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor _sparse_mm(@Const @ByRef Tensor sparse, @Const @ByRef Tensor dense);
@Namespace("at") public static native @ByVal Tensor _sparse_sparse_matmul(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByVal Tensor _sparse_matrix_mask_helper(@Const @ByRef Tensor t, @Const @ByRef Tensor mask_indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor mode(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor mode(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mode_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mode_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mode_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor mode(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor mode(@Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mode_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mode_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> mode_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal Tensor mul(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor mul_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor mul_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor mul(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByVal Tensor multiply(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor multiply_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor multiply_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor multiply(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByVal Tensor mv(@Const @ByRef Tensor self, @Const @ByRef Tensor vec);
@Namespace("at") public static native @ByRef Tensor mv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor vec);
@Namespace("at") public static native @ByRef Tensor mv_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor vec, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor mvlgamma(@Const @ByRef Tensor self, @Cast("int64_t") long p);
@Namespace("at") public static native @ByVal Tensor narrow_copy(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long length);
@Namespace("at") public static native @ByRef Tensor narrow_copy_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long length);
@Namespace("at") public static native @ByRef Tensor narrow_copy_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long length, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor narrow(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long length);
@Namespace("at") public static native @ByVal Tensor narrow(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor start, @Cast("int64_t") long length);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor native_batch_norm(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double momentum, double eps);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_batch_norm_out(@ByRef Tensor out, @ByRef Tensor save_mean, @ByRef Tensor save_invstd, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double momentum, double eps);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> native_batch_norm_outf(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Cast("bool") boolean training, double momentum, double eps, @ByRef Tensor out, @ByRef Tensor save_mean, @ByRef Tensor save_invstd);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor batch_norm_stats(@Const @ByRef Tensor input, double eps);
@Namespace("at") public static native @ByVal Tensor batch_norm_elemt(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, double eps);
@Namespace("at") public static native @ByRef Tensor batch_norm_elemt_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, double eps);
@Namespace("at") public static native @ByRef Tensor batch_norm_elemt_outf(@Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional bias, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, double eps, @ByRef Tensor out);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor batch_norm_gather_stats(@Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum, double eps, @Cast("int64_t") long count);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor batch_norm_gather_stats_with_counts(@Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum, double eps, @Const @ByRef Tensor counts);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor native_batch_norm_backward(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef TensorOptional weight, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, @Const @ByRef TensorOptional save_mean, @Const @ByRef TensorOptional save_invstd, @Cast("bool") boolean train, double eps, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor>*") Tensor batch_norm_backward_reduce(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional weight, @Cast("bool") boolean input_g, @Cast("bool") boolean weight_g, @Cast("bool") boolean bias_g);
@Namespace("at") public static native @ByVal Tensor batch_norm_backward_elemt(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor input, @Const @ByRef Tensor mean, @Const @ByRef Tensor invstd, @Const @ByRef TensorOptional weight, @Const @ByRef Tensor mean_dy, @Const @ByRef Tensor mean_dy_xmu);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor batch_norm_update_stats(@Const @ByRef Tensor input, @Const @ByRef TensorOptional running_mean, @Const @ByRef TensorOptional running_var, double momentum);
@Namespace("at") public static native @Cast("bool") boolean is_vulkan_available();
@Namespace("at") public static native @Cast("bool") boolean _nnpack_available();
@Namespace("at") public static native @ByVal Tensor _nnpack_spatial_convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor _nnpack_spatial_convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor _nnpack_spatial_convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... stride);
@Namespace("at") public static native @ByVal Tensor _nnpack_spatial_convolution(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor _nnpack_spatial_convolution_backward(@Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor _nnpack_spatial_convolution_backward(@Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal Tensor _nnpack_spatial_convolution_backward_input(@Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor _nnpack_spatial_convolution_backward_input(@Const @ByRef Tensor input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByVal Tensor _nnpack_spatial_convolution_backward_weight(@Const @ByRef Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef weightsize, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor _nnpack_spatial_convolution_backward_weight(@Const @ByRef Tensor input, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] weightsize, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor ones(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByRef Tensor ones_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor ones_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at") public static native @ByRef Tensor ones_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor ones_outf(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor ones_like(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor ones_like(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor ones_like(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor pairwise_distance(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2, double p/*=2*/, double eps/*=1e-06*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor pairwise_distance(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2);
@Namespace("at") public static native @ByVal Tensor cdist(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2, double p/*=2*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional compute_mode);
@Namespace("at") public static native @ByVal Tensor cdist(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2);
@Namespace("at") public static native @ByVal Tensor _euclidean_dist(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2);
@Namespace("at") public static native @ByVal Tensor _cdist_forward(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2, double p, @ByVal LongOptional compute_mode);
@Namespace("at") public static native @ByVal Tensor _cdist_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor x1, @Const @ByRef Tensor x2, double p, @Const @ByRef Tensor cdist);
@Namespace("at") public static native @ByVal Tensor pdist(@Const @ByRef Tensor self, double p/*=2*/);
@Namespace("at") public static native @ByVal Tensor pdist(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor _pdist_forward(@Const @ByRef Tensor self, double p/*=2*/);
@Namespace("at") public static native @ByVal Tensor _pdist_forward(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor _pdist_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, double p, @Const @ByRef Tensor pdist);
@Namespace("at") public static native @ByVal Tensor cosine_similarity(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2, @Cast("int64_t") long dim/*=1*/, double eps/*=1e-08*/);
@Namespace("at") public static native @ByVal Tensor cosine_similarity(@Const @ByRef Tensor x1, @Const @ByRef Tensor x2);
@Namespace("at") public static native @ByVal Tensor movedim(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef source, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef destination);
@Namespace("at") public static native @ByVal Tensor movedim(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] source, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... destination);
@Namespace("at") public static native @ByVal Tensor movedim(@Const @ByRef Tensor self, @Cast("int64_t") long source, @Cast("int64_t") long destination);
@Namespace("at") public static native @ByVal Tensor moveaxis(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef source, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef destination);
@Namespace("at") public static native @ByVal Tensor moveaxis(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] source, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... destination);
@Namespace("at") public static native @ByVal Tensor moveaxis(@Const @ByRef Tensor self, @Cast("int64_t") long source, @Cast("int64_t") long destination);
@Namespace("at") public static native @ByVal Tensor pixel_shuffle(@Const @ByRef Tensor self, @Cast("int64_t") long upscale_factor);
@Namespace("at") public static native @ByVal Tensor pixel_unshuffle(@Const @ByRef Tensor self, @Cast("int64_t") long downscale_factor);
@Namespace("at") public static native @ByVal Tensor channel_shuffle(@Const @ByRef Tensor self, @Cast("int64_t") long groups);
@Namespace("at") public static native @ByVal Tensor pinverse(@Const @ByRef Tensor self, double rcond/*=1e-15*/);
@Namespace("at") public static native @ByVal Tensor pinverse(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor poisson_nll_loss(@Const @ByRef Tensor input, @Const @ByRef Tensor target, @Cast("bool") boolean log_input, @Cast("bool") boolean full, double eps, @Cast("int64_t") long reduction);
@Namespace("at") public static native @ByVal Tensor rad2deg(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor rad2deg_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor rad2deg_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor rad2deg_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor deg2rad(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor deg2rad_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor deg2rad_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor deg2rad_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor scalar_tensor(@ByVal Scalar s, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor scalar_tensor(@ByVal Scalar s);
@Namespace("at") public static native @ByVal Tensor scalar_tensor(@ByVal Scalar s, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor rand(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor rand_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor rand_outf(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor rand_like(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor rand_like(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor rand_like(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randint(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor randint_out(@ByRef Tensor out, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randint_outf(@Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long high, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long high);
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long high, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long low, @Cast("int64_t") long high);
@Namespace("at") public static native @ByVal Tensor randint_like(@Const @ByRef Tensor self, @Cast("int64_t") long low, @Cast("int64_t") long high, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randn(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor randn_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randn_outf(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor randn_like(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor randn_like(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor randn_like(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n);
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n, @ByVal GeneratorOptional generator, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor randperm(@Cast("int64_t") long n, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByRef Tensor randperm_out(@ByRef Tensor out, @Cast("int64_t") long n);
@Namespace("at") public static native @ByRef Tensor randperm_outf(@Cast("int64_t") long n, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor randperm_out(@ByRef Tensor out, @Cast("int64_t") long n, @ByVal GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor randperm_outf(@Cast("int64_t") long n, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor range(@ByVal Scalar start, @ByVal Scalar end, @ByVal(nullValue = "c10::Scalar(1)") Scalar step, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor range(@ByVal Scalar start, @ByVal Scalar end, @ByVal Scalar step, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor range(@ByVal Scalar start, @ByVal Scalar end, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor range(@ByVal Scalar start, @ByVal Scalar end, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByRef Tensor range_out(@ByRef Tensor out, @ByVal Scalar start, @ByVal Scalar end, @ByVal(nullValue = "c10::Scalar(1)") Scalar step);
@Namespace("at") public static native @ByRef Tensor range_out(@ByRef Tensor out, @ByVal Scalar start, @ByVal Scalar end);
@Namespace("at") public static native @ByRef Tensor range_outf(@ByVal Scalar start, @ByVal Scalar end, @ByVal Scalar step, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor ravel(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor reciprocal(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor reciprocal_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor reciprocal_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor reciprocal_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor neg(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor neg_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor neg_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor neg_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor negative(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor negative_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor negative_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor negative_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor repeat_interleave(@Const @ByRef Tensor repeats);
@Namespace("at") public static native @ByVal Tensor repeat_interleave(@Const @ByRef Tensor self, @Const @ByRef Tensor repeats, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByVal Tensor repeat_interleave(@Const @ByRef Tensor self, @Const @ByRef Tensor repeats);
@Namespace("at") public static native @ByVal Tensor repeat_interleave(@Const @ByRef Tensor self, @Cast("int64_t") long repeats, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByVal Tensor repeat_interleave(@Const @ByRef Tensor self, @Cast("int64_t") long repeats);
@Namespace("at") public static native @ByVal Tensor reshape(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape);
@Namespace("at") public static native @ByVal Tensor reshape(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... shape);
@Namespace("at") public static native @ByVal Tensor _mkldnn_reshape(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shape);
@Namespace("at") public static native @ByVal Tensor _mkldnn_reshape(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... shape);
@Namespace("at") public static native @ByVal Tensor round(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor round_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor round_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor round_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor rrelu(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::Scalar(0.125)") Scalar lower, @ByVal(nullValue = "c10::Scalar(0.3333333333333333)") Scalar upper, @Cast("bool") boolean training/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor rrelu(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor rrelu_(@ByRef Tensor self, @ByVal(nullValue = "c10::Scalar(0.125)") Scalar lower, @ByVal(nullValue = "c10::Scalar(0.3333333333333333)") Scalar upper, @Cast("bool") boolean training/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor rrelu_(@ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor relu(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor relu_(@ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor prelu(@Const @ByRef Tensor self, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor prelu_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal Tensor gelu(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor gelu_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor infinitely_differentiable_gelu_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor hardshrink(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::Scalar(0.5)") Scalar lambd);
@Namespace("at") public static native @ByVal Tensor hardshrink(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor hardshrink_backward(@Const @ByRef Tensor grad_out, @Const @ByRef Tensor self, @ByVal Scalar lambd);
@Namespace("at") public static native @ByVal Tensor rsqrt(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor rsqrt_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor rsqrt_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor rsqrt_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor select(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("int64_t") long index);
@Namespace("at") public static native @ByVal Tensor select(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("int64_t") long index);
@Namespace("at") public static native @ByVal Tensor select_backward(@Const @ByRef Tensor grad, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long index);
@Namespace("at") public static native @ByVal Tensor select_backward(@Const @ByRef Tensor grad, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long index);
@Namespace("at") public static native @ByVal Tensor selu(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor selu_(@ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor celu(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::Scalar(1.0)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor celu(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor celu_(@ByRef Tensor self, @ByVal(nullValue = "c10::Scalar(1.0)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor celu_(@ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor silu(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor silu_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor silu_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor silu_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor silu_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor sigmoid(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor sigmoid_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor sigmoid_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor sigmoid_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor logit(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByVal Tensor logit(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor logit_(@ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByRef Tensor logit_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor logit_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByRef Tensor logit_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor logit_outf(@Const @ByRef Tensor self, @ByVal DoubleOptional eps, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor sin(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor sin_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor sin_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor sin_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor sinc(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor sinc_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor sinc_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor sinc_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor sinh(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor sinh_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor sinh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor sinh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor detach(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor detach_(@ByRef Tensor self);
@Namespace("at") public static native @Cast("int64_t") long __dispatch_size(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @Cast("int64_t") long size(@Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal Tensor slice(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=0*/, @ByVal(nullValue = "c10::optional<int64_t>(0)") LongOptional start, @ByVal(nullValue = "c10::optional<int64_t>(9223372036854775807L)") LongOptional end, @Cast("int64_t") long step/*=1*/);
@Namespace("at") public static native @ByVal Tensor slice(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor slice_backward(@Const @ByRef Tensor grad, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long end, @Cast("int64_t") long step);
@Namespace("at") public static native @ByVal Tensor slice_backward(@Const @ByRef Tensor grad, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long end, @Cast("int64_t") long step);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor slogdet(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor smm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat2);
@Namespace("at") public static native @ByVal Tensor softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal Tensor softmax(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor softmax(@Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal Tensor _softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float);
@Namespace("at") public static native @ByVal Tensor _softmax_backward_data(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, @Const @ByRef Tensor self);
@Namespace("at") public static native @StdMove TensorVector unsafe_split(@Const @ByRef Tensor self, @Cast("int64_t") long split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @StdMove TensorVector unsafe_split(@Const @ByRef Tensor self, @Cast("int64_t") long split_size);
@Namespace("at") public static native @StdMove TensorVector split(@Const @ByRef Tensor self, @Cast("int64_t") long split_size, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @StdMove TensorVector split(@Const @ByRef Tensor self, @Cast("int64_t") long split_size);
@Namespace("at") public static native @StdMove TensorVector unsafe_split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @StdMove TensorVector unsafe_split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef split_sizes);
@Namespace("at") public static native @StdMove TensorVector unsafe_split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @StdMove TensorVector unsafe_split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... split_sizes);
@Namespace("at") public static native @StdMove TensorVector split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @StdMove TensorVector split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef split_sizes);
@Namespace("at") public static native @StdMove TensorVector split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] split_sizes, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @StdMove TensorVector split_with_sizes(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... split_sizes);
@Namespace("at") public static native @ByVal Tensor squeeze(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor squeeze(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal Tensor squeeze(@Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal Tensor sspaddmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @ByVal(nullValue = "c10::Scalar(1)") Scalar beta, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor sspaddmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);
@Namespace("at") public static native @ByRef Tensor sspaddmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @ByVal(nullValue = "c10::Scalar(1)") Scalar beta, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor sspaddmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);
@Namespace("at") public static native @ByRef Tensor sspaddmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @ByVal Scalar beta, @ByVal Scalar alpha, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor stack(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor stack(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor stack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByRef Tensor stack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor stack_outf(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor _stack(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor _stack(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor _stack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByRef Tensor _stack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor _stack_outf(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor hstack(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor hstack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor hstack_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor vstack(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor vstack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor vstack_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor dstack(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor dstack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor dstack_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor stft(@Const @ByRef Tensor self, @Cast("int64_t") long n_fft, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional hop_length, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional win_length, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional window, @Cast("bool") boolean normalized/*=false*/, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional onesided, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional return_complex);
@Namespace("at") public static native @ByVal Tensor stft(@Const @ByRef Tensor self, @Cast("int64_t") long n_fft);
@Namespace("at") public static native @ByVal Tensor istft(@Const @ByRef Tensor self, @Cast("int64_t") long n_fft, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional hop_length, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional win_length, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional window, @Cast("bool") boolean center/*=true*/, @Cast("bool") boolean normalized/*=false*/, @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional onesided, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional length, @Cast("bool") boolean return_complex/*=false*/);
@Namespace("at") public static native @ByVal Tensor istft(@Const @ByRef Tensor self, @Cast("int64_t") long n_fft);
@Namespace("at") public static native @Cast("int64_t") long __dispatch_stride(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @Cast("int64_t") long stride(@Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dim);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor sum(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dim);
@Namespace("at") public static native @ByRef Tensor sum_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor sum_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor sum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByRef Tensor sum_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor nansum(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor nansum(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor nansum(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor nansum(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor nansum(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor nansum(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dim);
@Namespace("at") public static native @ByRef Tensor nansum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor nansum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor nansum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor nansum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dim);
@Namespace("at") public static native @ByRef Tensor nansum_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor nansum_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor sqrt(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor sqrt_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor sqrt_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor sqrt_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor square(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor square_(@ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @Cast("bool") boolean unbiased/*=true*/);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor std_mean(@Const @ByRef Tensor self, @Cast("bool") boolean unbiased/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor std_mean(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor std_mean(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor std_mean(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor std_mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor std_mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor std_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor std_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dim);
@Namespace("at") public static native @ByRef Tensor std_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor std_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor std_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByRef Tensor std_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByRef Tensor prod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor prod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByRef Tensor prod_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor prod(@Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByRef Tensor prod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor prod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByRef Tensor prod_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor t(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor tan(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor tan_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor tan_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor tan_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor tanh(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor tanh_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor tanh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor tanh_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor tensordot(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims_self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims_other);
@Namespace("at") public static native @ByVal Tensor tensordot(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dims_self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dims_other);
@Namespace("at") public static native @ByRef Tensor tensordot_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims_self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims_other);
@Namespace("at") public static native @ByRef Tensor tensordot_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dims_self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dims_other);
@Namespace("at") public static native @ByRef Tensor tensordot_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims_self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims_other, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor tensordot_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dims_self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dims_other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor threshold(@Const @ByRef Tensor self, @ByVal Scalar threshold, @ByVal Scalar value);
@Namespace("at") public static native @ByRef Tensor threshold_(@ByRef Tensor self, @ByVal Scalar threshold, @ByVal Scalar value);
@Namespace("at") public static native @ByRef Tensor threshold_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Scalar threshold, @ByVal Scalar value);
@Namespace("at") public static native @ByRef Tensor threshold_outf(@Const @ByRef Tensor self, @ByVal Scalar threshold, @ByVal Scalar value, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor threshold_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal Scalar threshold);
@Namespace("at") public static native @ByVal Tensor tile(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("at") public static native @ByVal Tensor tile(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dims);
@Namespace("at") public static native @ByVal Tensor transpose(@Const @ByRef Tensor self, @Cast("int64_t") long dim0, @Cast("int64_t") long dim1);
@Namespace("at") public static native @ByVal Tensor transpose(@Const @ByRef Tensor self, @ByVal Dimname dim0, @ByVal Dimname dim1);
@Namespace("at") public static native @ByVal Tensor _mkldnn_transpose(@Const @ByRef Tensor self, @Cast("int64_t") long dim0, @Cast("int64_t") long dim1);
@Namespace("at") public static native @ByRef Tensor _mkldnn_transpose_(@ByRef Tensor self, @Cast("int64_t") long dim0, @Cast("int64_t") long dim1);
@Namespace("at") public static native @ByVal Tensor one_hot(@Const @ByRef Tensor self, @Cast("int64_t") long num_classes/*=-1*/);
@Namespace("at") public static native @ByVal Tensor one_hot(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor flip(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("at") public static native @ByVal Tensor flip(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dims);
@Namespace("at") public static native @ByVal Tensor fliplr(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor flipud(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor roll(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shifts, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("at") public static native @ByVal Tensor roll(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef shifts);
@Namespace("at") public static native @ByVal Tensor roll(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] shifts, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dims);
@Namespace("at") public static native @ByVal Tensor roll(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... shifts);
@Namespace("at") public static native @ByVal Tensor rot90(@Const @ByRef Tensor self, @Cast("int64_t") long k/*=1*/, @ByVal(nullValue = "c10::IntArrayRef({0,1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dims);
@Namespace("at") public static native @ByVal Tensor rot90(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor rot90(@Const @ByRef Tensor self, @Cast("int64_t") long k/*=1*/, @ByVal(nullValue = "c10::IntArrayRef({0,1})") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dims);
@Namespace("at") public static native @ByVal Tensor trapz(@Const @ByRef Tensor y, @Const @ByRef Tensor x, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor trapz(@Const @ByRef Tensor y, @Const @ByRef Tensor x);
@Namespace("at") public static native @ByVal Tensor trapz(@Const @ByRef Tensor y, double dx/*=1*/, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor trapz(@Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal Tensor _trilinear(@Const @ByRef Tensor i1, @Const @ByRef Tensor i2, @Const @ByRef Tensor i3, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand1, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand2, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand3, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sumdim, @Cast("int64_t") long unroll_dim/*=1*/);
@Namespace("at") public static native @ByVal Tensor _trilinear(@Const @ByRef Tensor i1, @Const @ByRef Tensor i2, @Const @ByRef Tensor i3, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand1, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand2, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef expand3, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sumdim);
@Namespace("at") public static native @ByVal Tensor _trilinear(@Const @ByRef Tensor i1, @Const @ByRef Tensor i2, @Const @ByRef Tensor i3, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] expand1, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] expand2, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] expand3, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] sumdim, @Cast("int64_t") long unroll_dim/*=1*/);
@Namespace("at") public static native @ByVal Tensor _trilinear(@Const @ByRef Tensor i1, @Const @ByRef Tensor i2, @Const @ByRef Tensor i3, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] expand1, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] expand2, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] expand3, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... sumdim);
@Namespace("at") public static native @ByVal Tensor triplet_margin_loss(@Const @ByRef Tensor anchor, @Const @ByRef Tensor positive, @Const @ByRef Tensor negative, double margin/*=1.0*/, double p/*=2*/, double eps/*=1e-06*/, @Cast("bool") boolean swap/*=false*/, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor triplet_margin_loss(@Const @ByRef Tensor anchor, @Const @ByRef Tensor positive, @Const @ByRef Tensor negative);
@Namespace("at") public static native @ByVal Tensor trunc(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor trunc_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor trunc_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor trunc_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor fix(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fix_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fix_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fix_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @Cast("bool") boolean _has_compatible_shallow_copy_type(@Const @ByRef Tensor self, @Const @ByRef Tensor from);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _unique(@Const @ByRef Tensor self, @Cast("bool") boolean sorted/*=true*/, @Cast("bool") boolean return_inverse/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _unique(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor unique_dim(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean sorted/*=true*/, @Cast("bool") boolean return_inverse/*=false*/, @Cast("bool") boolean return_counts/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor unique_dim(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor unique_consecutive(@Const @ByRef Tensor self, @Cast("bool") boolean return_inverse/*=false*/, @Cast("bool") boolean return_counts/*=false*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor unique_consecutive(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor unique_dim_consecutive(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean return_inverse/*=false*/, @Cast("bool") boolean return_counts/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor unique_dim_consecutive(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor _unique2(@Const @ByRef Tensor self, @Cast("bool") boolean sorted/*=true*/, @Cast("bool") boolean return_inverse/*=false*/, @Cast("bool") boolean return_counts/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor _unique2(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor _unsafe_view(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor _unsafe_view(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at") public static native @ByVal Tensor unsqueeze(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal Tensor vander(@Const @ByRef Tensor x, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional N, @Cast("bool") boolean increasing/*=false*/);
@Namespace("at") public static native @ByVal Tensor vander(@Const @ByRef Tensor x);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @Cast("bool") boolean unbiased/*=true*/);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dim);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dim);
@Namespace("at") public static native @ByRef Tensor var_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor var_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor var_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByRef Tensor var_outf(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor var_mean(@Const @ByRef Tensor self, @Cast("bool") boolean unbiased/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor var_mean(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor var_mean(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor var_mean(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor var_mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor var_mean(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor var_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim, @Cast("bool") boolean unbiased/*=true*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor var_mean(@Const @ByRef Tensor self, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByVal Tensor where(@Const @ByRef Tensor condition, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByVal Tensor where(@Const @ByRef Tensor condition, @ByVal Scalar self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByVal Tensor where(@Const @ByRef Tensor condition, @Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByVal Tensor where(@Const @ByRef Tensor condition, @ByVal Scalar self, @ByVal Scalar other);
@Namespace("at") public static native @StdMove TensorVector where(@Const @ByRef Tensor condition);
@Namespace("at") public static native @ByVal Tensor _s_where(@Const @ByRef Tensor condition, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByVal Tensor norm_except_dim(@Const @ByRef Tensor v, @Cast("int64_t") long pow/*=2*/, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor norm_except_dim(@Const @ByRef Tensor v);
@Namespace("at") public static native @ByVal Tensor _weight_norm(@Const @ByRef Tensor v, @Const @ByRef Tensor g, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor _weight_norm(@Const @ByRef Tensor v, @Const @ByRef Tensor g);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _weight_norm_cuda_interface(@Const @ByRef Tensor v, @Const @ByRef Tensor g, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _weight_norm_cuda_interface(@Const @ByRef Tensor v, @Const @ByRef Tensor g);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _weight_norm_cuda_interface_backward(@Const @ByRef Tensor grad_w, @Const @ByRef Tensor saved_v, @Const @ByRef Tensor saved_g, @Const @ByRef Tensor saved_norms, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _weight_norm_differentiable_backward(@Const @ByRef Tensor grad_w, @Const @ByRef Tensor saved_v, @Const @ByRef Tensor saved_g, @Const @ByRef Tensor saved_norms, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal DimnameListOptional names, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor zeros(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByRef Tensor zeros_out(@ByRef Tensor out, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor zeros_out(@ByRef Tensor out, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at") public static native @ByRef Tensor zeros_outf(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor zeros_outf(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor zeros_like(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options, @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor zeros_like(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor zeros_like(@Const @ByRef Tensor self, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory, @ByVal MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor _standard_gamma_grad(@Const @ByRef Tensor self, @Const @ByRef Tensor output);
@Namespace("at") public static native @ByVal Tensor _standard_gamma(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor _standard_gamma(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor _dirichlet_grad(@Const @ByRef Tensor x, @Const @ByRef Tensor alpha, @Const @ByRef Tensor total);
@Namespace("at") public static native @ByVal Tensor _sample_dirichlet(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor _sample_dirichlet(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor poisson(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor poisson(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor binomial(@Const @ByRef Tensor count, @Const @ByRef Tensor prob, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor binomial(@Const @ByRef Tensor count, @Const @ByRef Tensor prob);
@Namespace("at") public static native @ByVal Tensor native_norm(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::Scalar(2)") Scalar p);
@Namespace("at") public static native @ByVal Tensor native_norm(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor native_norm(@Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor native_norm(@Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor _sparse_sum(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor _sparse_sum(@Const @ByRef Tensor self, ScalarType dtype);
@Namespace("at") public static native @ByVal Tensor _sparse_sum(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor _sparse_sum(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dim);
@Namespace("at") public static native @ByVal Tensor _sparse_sum(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, ScalarType dtype);
@Namespace("at") public static native @ByVal Tensor _sparse_sum(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, ScalarType dtype);
@Namespace("at") public static native @ByVal Tensor _sparse_sum_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor _sparse_sum_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dim);
@Namespace("at") public static native @ByVal Tensor _sparse_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor _sparse_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal Tensor _sparse_softmax(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor _sparse_softmax(@Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal Tensor _sparse_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float);
@Namespace("at") public static native @ByVal Tensor _sparse_softmax_backward_data(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor _sparse_log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor _sparse_log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByVal Tensor _sparse_log_softmax(@Const @ByRef Tensor self, @ByVal Dimname dim, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor _sparse_log_softmax(@Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal Tensor _sparse_log_softmax(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean half_to_float);
@Namespace("at") public static native @ByVal Tensor _sparse_log_softmax_backward_data(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @Cast("int64_t") long dim, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @ByVal ScalarOptional p, ScalarType dtype);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::Scalar(2)") Scalar p);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, ScalarType dtype);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dim);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, ScalarType dtype);
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, ScalarType dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dim);
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor norm(@Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype);
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, ScalarType dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal DimnameArrayRef dim);
@Namespace("at") public static native @ByRef Tensor norm_outf(@Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByVal DimnameArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor frobenius_norm(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor frobenius_norm(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor frobenius_norm(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor frobenius_norm(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor frobenius_norm(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dim);
@Namespace("at") public static native @ByRef Tensor frobenius_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor frobenius_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor frobenius_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor frobenius_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dim);
@Namespace("at") public static native @ByRef Tensor frobenius_norm_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor frobenius_norm_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_outf(@Const @ByRef Tensor self, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor nuclear_norm(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dim);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dim);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor nuclear_norm_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor clone(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByVal Tensor clone(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor resize_as_(@ByRef Tensor self, @Const @ByRef Tensor the_template, @ByVal(nullValue = "c10::optional<c10::MemoryFormat>(c10::nullopt)") MemoryFormatOptional memory_format);
@Namespace("at") public static native @ByRef Tensor resize_as_(@ByRef Tensor self, @Const @ByRef Tensor the_template);
@Namespace("at") public static native @ByRef Tensor zero_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor sub_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor sub_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor sub_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal Scalar alpha, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor sub(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor sub(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByVal Tensor sub(@Const @ByRef Tensor self, @ByVal Scalar other, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor sub(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor subtract_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor subtract_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor subtract_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal Scalar alpha, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor subtract(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor subtract(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByVal Tensor subtract(@Const @ByRef Tensor self, @ByVal Scalar other, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor subtract(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByVal Tensor rsub(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor rsub(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor heaviside_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor values);
@Namespace("at") public static native @ByRef Tensor heaviside_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor values, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor heaviside(@Const @ByRef Tensor self, @Const @ByRef Tensor values);
@Namespace("at") public static native @ByVal Tensor rsub(@Const @ByRef Tensor self, @ByVal Scalar other, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor rsub(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByVal Tensor _sparse_addmm(@Const @ByRef Tensor self, @Const @ByRef Tensor sparse, @Const @ByRef Tensor dense, @ByVal(nullValue = "c10::Scalar(1)") Scalar beta, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor _sparse_addmm(@Const @ByRef Tensor self, @Const @ByRef Tensor sparse, @Const @ByRef Tensor dense);
@Namespace("at") public static native @ByRef Tensor addmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @ByVal(nullValue = "c10::Scalar(1)") Scalar beta, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor addmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);
@Namespace("at") public static native @ByRef Tensor addmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @ByVal Scalar beta, @ByVal Scalar alpha, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor addmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @ByVal(nullValue = "c10::Scalar(1)") Scalar beta, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor addmm(@Const @ByRef Tensor self, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor sparse_coo_tensor(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_unsafe(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native void _validate_sparse_coo_tensor_args(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native void _validate_sparse_coo_tensor_args(@Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims_and_tensors(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims_and_tensors(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal TensorOptions options);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims_and_tensors(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor _sparse_coo_tensor_with_dims_and_tensors(@Cast("int64_t") long sparse_dim, @Cast("int64_t") long dense_dim, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @Const @ByRef Tensor indices, @Const @ByRef Tensor values, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor to_dense_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor input);
@Namespace("at") public static native @ByRef Tensor hspmm_out(@ByRef Tensor out, @Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);
@Namespace("at") public static native @ByRef Tensor hspmm_outf(@Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor hspmm(@Const @ByRef Tensor mat1, @Const @ByRef Tensor mat2);
@Namespace("at") public static native @ByRef Tensor copy_sparse_to_sparse_(@ByRef Tensor self, @Const @ByRef Tensor src, @Cast("bool") boolean non_blocking/*=false*/);
@Namespace("at") public static native @ByRef Tensor copy_sparse_to_sparse_(@ByRef Tensor self, @Const @ByRef Tensor src);
@Namespace("at") public static native @StdMove TensorVector unbind(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @StdMove TensorVector unbind(@Const @ByRef Tensor self);
@Namespace("at") public static native @StdMove TensorVector unbind(@Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv2d_weight(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv2d_weight(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv2d_weight(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv3d_weight(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv3d_weight(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor mkldnn_reorder_conv3d_weight(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("int64_t") long groups/*=1*/);
@Namespace("at") public static native @ByVal Tensor to_mkldnn_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor input);
@Namespace("at") public static native @ByVal Tensor quantize_per_tensor(@Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point, ScalarType dtype);
@Namespace("at") public static native @StdMove TensorVector quantize_per_tensor(@ByVal TensorArrayRef tensors, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, ScalarType dtype);
@Namespace("at") public static native @ByVal Tensor quantize_per_channel(@Const @ByRef Tensor self, @Const @ByRef Tensor scales, @Const @ByRef Tensor zero_points, @Cast("int64_t") long axis, ScalarType dtype);
@Namespace("at") public static native @ByVal Tensor dequantize(@Const @ByRef Tensor self);
@Namespace("at") public static native @StdMove TensorVector dequantize(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native double q_scale(@Const @ByRef Tensor self);
@Namespace("at") public static native @Cast("int64_t") long q_zero_point(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor q_per_channel_scales(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor q_per_channel_zero_points(@Const @ByRef Tensor self);
@Namespace("at") public static native @Cast("int64_t") long q_per_channel_axis(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor int_repr(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor _make_per_tensor_quantized_tensor(@Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point);
@Namespace("at") public static native @ByVal Tensor _make_per_channel_quantized_tensor(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis);
@Namespace("at") public static native @ByVal Tensor fake_quantize_per_tensor_affine(@Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor fake_quantize_per_tensor_affine_cachemask(@Const @ByRef Tensor self, double scale, @Cast("int64_t") long zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);
@Namespace("at") public static native @ByVal Tensor fake_quantize_per_tensor_affine_cachemask_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor mask);
@Namespace("at") public static native @ByVal Tensor _fake_quantize_learnable_per_tensor_affine(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, double grad_factor/*=1.0*/);
@Namespace("at") public static native @ByVal Tensor _fake_quantize_learnable_per_tensor_affine(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor _fake_quantize_learnable_per_tensor_affine_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, double grad_factor/*=1.0*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor _fake_quantize_learnable_per_tensor_affine_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);
@Namespace("at") public static native @ByVal Tensor fake_quantize_per_channel_affine(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor fake_quantize_per_channel_affine_cachemask(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);
@Namespace("at") public static native @ByVal Tensor fake_quantize_per_channel_affine_cachemask_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor mask);
@Namespace("at") public static native @ByVal Tensor _fake_quantize_learnable_per_channel_affine(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, double grad_factor/*=1.0*/);
@Namespace("at") public static native @ByVal Tensor _fake_quantize_learnable_per_channel_affine(@Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor _fake_quantize_learnable_per_channel_affine_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max, double grad_factor/*=1.0*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor _fake_quantize_learnable_per_channel_affine_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @Const @ByRef Tensor scale, @Const @ByRef Tensor zero_point, @Cast("int64_t") long axis, @Cast("int64_t") long quant_min, @Cast("int64_t") long quant_max);
@Namespace("at") public static native @ByVal @Cast("std::tuple<double,int64_t>*") LongPointer _choose_qparams_per_tensor(@Const @ByRef Tensor self, @Cast("bool") boolean reduce_range/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<double,int64_t>*") LongPointer _choose_qparams_per_tensor(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor _saturate_weight_to_fp16(@Const @ByRef Tensor weight);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor choose_qparams_optimized(@Const @ByRef Tensor input, @Cast("int64_t") long numel, @Cast("int64_t") long n_bins, double ratio, @Cast("int64_t") long bit_width);
@Namespace("at") public static native @StdMove TensorVector meshgrid(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor cartesian_prod(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByVal Tensor combinations(@Const @ByRef Tensor self, @Cast("int64_t") long r/*=2*/, @Cast("bool") boolean with_replacement/*=false*/);
@Namespace("at") public static native @ByVal Tensor combinations(@Const @ByRef Tensor self);
@Namespace("at") public static native ScalarType result_type(@Const @ByRef Tensor tensor, @Const @ByRef Tensor other);
@Namespace("at") public static native ScalarType result_type(@Const @ByRef Tensor tensor, @ByVal Scalar other);
@Namespace("at") public static native ScalarType result_type(@ByVal Scalar scalar, @Const @ByRef Tensor tensor);
@Namespace("at") public static native ScalarType result_type(@ByVal Scalar scalar1, @ByVal Scalar scalar2);
@Namespace("at") public static native @Cast("bool") boolean can_cast(ScalarType from, ScalarType to);
@Namespace("at") public static native ScalarType promote_types(ScalarType type1, ScalarType type2);
@Namespace("at") public static native @ByVal Scalar _local_scalar_dense(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor _thnn_fused_lstm_cell(@Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef Tensor cx, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional input_bias, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional hidden_bias);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor _thnn_fused_lstm_cell(@Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef Tensor cx);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor>*") Tensor _thnn_fused_lstm_cell_backward(@Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Const @ByRef Tensor cx, @Const @ByRef Tensor cy, @Const @ByRef Tensor workspace, @Cast("bool") boolean has_bias);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor>*") Tensor _thnn_differentiable_lstm_cell_backward(@Const @ByRef TensorOptional grad_hy, @Const @ByRef TensorOptional grad_cy, @Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef TensorOptional input_bias, @Const @ByRef TensorOptional hidden_bias, @Const @ByRef Tensor cx, @Const @ByRef Tensor cy);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _thnn_fused_gru_cell(@Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef Tensor hx, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional input_bias, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional hidden_bias);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _thnn_fused_gru_cell(@Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef Tensor hx);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor>*") Tensor _thnn_fused_gru_cell_backward(@Const @ByRef Tensor grad_hy, @Const @ByRef Tensor workspace, @Cast("bool") boolean has_bias);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor>*") Tensor _thnn_differentiable_gru_cell_backward(@Const @ByRef Tensor grad_hy, @Const @ByRef Tensor input_gates, @Const @ByRef Tensor hidden_gates, @Const @ByRef Tensor hx, @Const @ByRef TensorOptional input_bias, @Const @ByRef TensorOptional hidden_bias);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor lstm(@Const @ByRef Tensor input, @ByVal TensorArrayRef hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor lstm(@Const @ByRef Tensor data, @Const @ByRef Tensor batch_sizes, @ByVal TensorArrayRef hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor gru(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor gru(@Const @ByRef Tensor data, @Const @ByRef Tensor batch_sizes, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor rnn_tanh(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor rnn_tanh(@Const @ByRef Tensor data, @Const @ByRef Tensor batch_sizes, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor rnn_relu(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional, @Cast("bool") boolean batch_first);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor rnn_relu(@Const @ByRef Tensor data, @Const @ByRef Tensor batch_sizes, @Const @ByRef Tensor hx, @ByVal TensorArrayRef params, @Cast("bool") boolean has_biases, @Cast("int64_t") long num_layers, double dropout, @Cast("bool") boolean train, @Cast("bool") boolean bidirectional);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor lstm_cell(@Const @ByRef Tensor input, @ByVal TensorArrayRef hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_ih, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_hh);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor lstm_cell(@Const @ByRef Tensor input, @ByVal TensorArrayRef hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh);
@Namespace("at") public static native @ByVal Tensor gru_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_ih, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_hh);
@Namespace("at") public static native @ByVal Tensor gru_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh);
@Namespace("at") public static native @ByVal Tensor rnn_tanh_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_ih, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_hh);
@Namespace("at") public static native @ByVal Tensor rnn_tanh_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh);
@Namespace("at") public static native @ByVal Tensor rnn_relu_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_ih, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional b_hh);
@Namespace("at") public static native @ByVal Tensor rnn_relu_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor quantized_lstm_cell(@Const @ByRef Tensor input, @ByVal TensorArrayRef hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef Tensor b_ih, @Const @ByRef Tensor b_hh, @Const @ByRef Tensor packed_ih, @Const @ByRef Tensor packed_hh, @Const @ByRef Tensor col_offsets_ih, @Const @ByRef Tensor col_offsets_hh, @ByVal Scalar scale_ih, @ByVal Scalar scale_hh, @ByVal Scalar zero_point_ih, @ByVal Scalar zero_point_hh);
@Namespace("at") public static native @ByVal Tensor quantized_gru_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef Tensor b_ih, @Const @ByRef Tensor b_hh, @Const @ByRef Tensor packed_ih, @Const @ByRef Tensor packed_hh, @Const @ByRef Tensor col_offsets_ih, @Const @ByRef Tensor col_offsets_hh, @ByVal Scalar scale_ih, @ByVal Scalar scale_hh, @ByVal Scalar zero_point_ih, @ByVal Scalar zero_point_hh);
@Namespace("at") public static native @ByVal Tensor quantized_rnn_relu_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef Tensor b_ih, @Const @ByRef Tensor b_hh, @Const @ByRef Tensor packed_ih, @Const @ByRef Tensor packed_hh, @Const @ByRef Tensor col_offsets_ih, @Const @ByRef Tensor col_offsets_hh, @ByVal Scalar scale_ih, @ByVal Scalar scale_hh, @ByVal Scalar zero_point_ih, @ByVal Scalar zero_point_hh);
@Namespace("at") public static native @ByVal Tensor quantized_rnn_tanh_cell(@Const @ByRef Tensor input, @Const @ByRef Tensor hx, @Const @ByRef Tensor w_ih, @Const @ByRef Tensor w_hh, @Const @ByRef Tensor b_ih, @Const @ByRef Tensor b_hh, @Const @ByRef Tensor packed_ih, @Const @ByRef Tensor packed_hh, @Const @ByRef Tensor col_offsets_ih, @Const @ByRef Tensor col_offsets_hh, @ByVal Scalar scale_ih, @ByVal Scalar scale_hh, @ByVal Scalar zero_point_ih, @ByVal Scalar zero_point_hh);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _pack_padded_sequence(@Const @ByRef Tensor input, @Const @ByRef Tensor lengths, @Cast("bool") boolean batch_first);
@Namespace("at") public static native @ByVal Tensor _pack_padded_sequence_backward(@Const @ByRef Tensor grad, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Const @ByRef Tensor batch_sizes, @Cast("bool") boolean batch_first);
@Namespace("at") public static native @ByVal Tensor _pack_padded_sequence_backward(@Const @ByRef Tensor grad, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Const @ByRef Tensor batch_sizes, @Cast("bool") boolean batch_first);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _pad_packed_sequence(@Const @ByRef Tensor data, @Const @ByRef Tensor batch_sizes, @Cast("bool") boolean batch_first, @ByVal Scalar padding_value, @Cast("int64_t") long total_length);
@Namespace("at") public static native @ByVal Tensor masked_fill(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @ByVal Scalar value);
@Namespace("at") public static native @ByVal Tensor masked_fill(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @Const @ByRef Tensor value);
@Namespace("at") public static native @ByVal Tensor masked_scatter(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @Const @ByRef Tensor source);
@Namespace("at") public static native @ByVal Tensor index_add(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source);
@Namespace("at") public static native @ByVal Tensor index_add(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source);
@Namespace("at") public static native @ByVal Tensor index_fill(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @ByVal Scalar value);
@Namespace("at") public static native @ByVal Tensor index_fill(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor value);
@Namespace("at") public static native @ByVal Tensor index_fill(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @ByVal Scalar value);
@Namespace("at") public static native @ByVal Tensor index_fill(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Tensor value);
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src);
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @ByVal Scalar value);
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src);
@Namespace("at") public static native @ByVal Tensor scatter(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @ByVal Scalar value);
@Namespace("at") public static native @ByVal Tensor scatter_add(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src);
@Namespace("at") public static native @ByVal Tensor scatter_add(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Const @ByRef Tensor src);
@Namespace("at") public static native @ByRef Tensor bitwise_and_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor bitwise_and_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor bitwise_and_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor bitwise_and_outf(@Const @ByRef Tensor self, @ByVal Scalar other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor bitwise_and(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByVal Tensor bitwise_and(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByVal Tensor __and__(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByVal Tensor __and__(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor bitwise_or_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor bitwise_or_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor bitwise_or_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor bitwise_or_outf(@Const @ByRef Tensor self, @ByVal Scalar other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor bitwise_or(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByVal Tensor bitwise_or(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByVal Tensor __or__(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByVal Tensor __or__(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor bitwise_xor_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor bitwise_xor_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor bitwise_xor_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor bitwise_xor_outf(@Const @ByRef Tensor self, @ByVal Scalar other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor bitwise_xor(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByVal Tensor bitwise_xor(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByVal Tensor __xor__(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByVal Tensor __xor__(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByVal Tensor __lshift__(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByVal Tensor __lshift__(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByVal Tensor __rshift__(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByVal Tensor __rshift__(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor addbmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @ByVal(nullValue = "c10::Scalar(1)") Scalar beta, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByRef Tensor addbmm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2);
@Namespace("at") public static native @ByRef Tensor addbmm_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @ByVal Scalar beta, @ByVal Scalar alpha, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor addbmm(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2, @ByVal(nullValue = "c10::Scalar(1)") Scalar beta, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor addbmm(@Const @ByRef Tensor self, @Const @ByRef Tensor batch1, @Const @ByRef Tensor batch2);
@Namespace("at") public static native @ByRef Tensor diag_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByRef Tensor diag_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor diag_outf(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor diag(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByVal Tensor diag(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor diag_backward(@Const @ByRef Tensor grad, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_sizes, @Cast("int64_t") long diagonal);
@Namespace("at") public static native @ByVal Tensor diag_backward(@Const @ByRef Tensor grad, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_sizes, @Cast("int64_t") long diagonal);
@Namespace("at") public static native @ByRef Tensor cross_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByRef Tensor cross_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor cross_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal LongOptional dim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor cross(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim);
@Namespace("at") public static native @ByVal Tensor cross(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor triu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByRef Tensor triu_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor triu_outf(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor triu(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByVal Tensor triu(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor tril_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByRef Tensor tril_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor tril_outf(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor tril(@Const @ByRef Tensor self, @Cast("int64_t") long diagonal/*=0*/);
@Namespace("at") public static native @ByVal Tensor tril(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor tril_indices(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset/*=0*/, @ByVal(nullValue = "c10::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor tril_indices(@Cast("int64_t") long row, @Cast("int64_t") long col);
@Namespace("at") public static native @ByVal Tensor tril_indices(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor triu_indices(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset/*=0*/, @ByVal(nullValue = "c10::TensorOptions(at::kLong)") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor triu_indices(@Cast("int64_t") long row, @Cast("int64_t") long col);
@Namespace("at") public static native @ByVal Tensor triu_indices(@Cast("int64_t") long row, @Cast("int64_t") long col, @Cast("int64_t") long offset, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor trace(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor trace_backward(@Const @ByRef Tensor grad, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("at") public static native @ByVal Tensor trace_backward(@Const @ByRef Tensor grad, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... sizes);
@Namespace("at") public static native @ByRef Tensor ne_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor ne_outf(@Const @ByRef Tensor self, @ByVal Scalar other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor ne(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor ne_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor ne_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor ne(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor not_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor not_equal_outf(@Const @ByRef Tensor self, @ByVal Scalar other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor not_equal(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor not_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor not_equal_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor not_equal(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor eq_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor eq_outf(@Const @ByRef Tensor self, @ByVal Scalar other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor eq(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor eq_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor eq_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor eq(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor ge_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor ge_outf(@Const @ByRef Tensor self, @ByVal Scalar other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor ge(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor ge_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor ge_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor ge(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor greater_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor greater_equal_outf(@Const @ByRef Tensor self, @ByVal Scalar other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor greater_equal(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor greater_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor greater_equal_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor greater_equal(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor le_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor le_outf(@Const @ByRef Tensor self, @ByVal Scalar other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor le(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor le_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor le_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor le(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor less_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor less_equal_outf(@Const @ByRef Tensor self, @ByVal Scalar other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor less_equal(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor less_equal_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor less_equal_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor less_equal(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor gt_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor gt_outf(@Const @ByRef Tensor self, @ByVal Scalar other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor gt(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor gt_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor gt_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor gt(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor greater_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor greater_outf(@Const @ByRef Tensor self, @ByVal Scalar other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor greater(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor greater_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor greater_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor greater(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor lt_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor lt_outf(@Const @ByRef Tensor self, @ByVal Scalar other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor lt(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor lt_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor lt_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor lt(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor less_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor less_outf(@Const @ByRef Tensor self, @ByVal Scalar other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor less(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor less_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor less_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor less(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor take_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor index);
@Namespace("at") public static native @ByRef Tensor take_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor index, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor take(@Const @ByRef Tensor self, @Const @ByRef Tensor index);
@Namespace("at") public static native @ByVal Tensor take_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor input, @Const @ByRef Tensor index);
@Namespace("at") public static native @ByRef Tensor index_select_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index);
@Namespace("at") public static native @ByRef Tensor index_select_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor index_select(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index);
@Namespace("at") public static native @ByRef Tensor index_select_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index);
@Namespace("at") public static native @ByRef Tensor index_select_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor index_select(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index);
@Namespace("at") public static native @ByVal Tensor index_select_backward(@Const @ByRef Tensor grad, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef self_sizes, @Cast("int64_t") long dim, @Const @ByRef Tensor index);
@Namespace("at") public static native @ByVal Tensor index_select_backward(@Const @ByRef Tensor grad, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] self_sizes, @Cast("int64_t") long dim, @Const @ByRef Tensor index);
@Namespace("at") public static native @ByRef Tensor masked_select_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor mask);
@Namespace("at") public static native @ByRef Tensor masked_select_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor mask, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor masked_select(@Const @ByRef Tensor self, @Const @ByRef Tensor mask);
@Namespace("at") public static native @ByVal Tensor masked_select_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor input, @Const @ByRef Tensor mask);
@Namespace("at") public static native @ByRef Tensor nonzero_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor nonzero_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor nonzero(@Const @ByRef Tensor self);
@Namespace("at") public static native @StdMove TensorVector nonzero_numpy(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor gather_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad/*=false*/);
@Namespace("at") public static native @ByRef Tensor gather_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index);
@Namespace("at") public static native @ByRef Tensor gather_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor gather(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad/*=false*/);
@Namespace("at") public static native @ByVal Tensor gather(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index);
@Namespace("at") public static native @ByVal Tensor gather_backward(@Const @ByRef Tensor grad, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad);
@Namespace("at") public static native @ByRef Tensor gather_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad/*=false*/);
@Namespace("at") public static native @ByRef Tensor gather_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index);
@Namespace("at") public static native @ByRef Tensor gather_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor gather(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index, @Cast("bool") boolean sparse_grad/*=false*/);
@Namespace("at") public static native @ByVal Tensor gather(@Const @ByRef Tensor self, @ByVal Dimname dim, @Const @ByRef Tensor index);
@Namespace("at") public static native @ByVal Tensor _gather_sparse_backward(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor grad);
@Namespace("at") public static native @ByRef Tensor addcmul_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @ByVal(nullValue = "c10::Scalar(1)") Scalar value);
@Namespace("at") public static native @ByRef Tensor addcmul_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2);
@Namespace("at") public static native @ByRef Tensor addcmul_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @ByVal Scalar value, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor addcmul(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @ByVal(nullValue = "c10::Scalar(1)") Scalar value);
@Namespace("at") public static native @ByVal Tensor addcmul(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2);
@Namespace("at") public static native @ByRef Tensor addcdiv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @ByVal(nullValue = "c10::Scalar(1)") Scalar value);
@Namespace("at") public static native @ByRef Tensor addcdiv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2);
@Namespace("at") public static native @ByRef Tensor addcdiv_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @ByVal Scalar value, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor addcdiv(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2, @ByVal(nullValue = "c10::Scalar(1)") Scalar value);
@Namespace("at") public static native @ByVal Tensor addcdiv(@Const @ByRef Tensor self, @Const @ByRef Tensor tensor1, @Const @ByRef Tensor tensor2);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> lstsq_out(@ByRef Tensor X, @ByRef Tensor qr, @Const @ByRef Tensor self, @Const @ByRef Tensor A);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> lstsq_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor A, @ByRef Tensor X, @ByRef Tensor qr);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor lstsq(@Const @ByRef Tensor self, @Const @ByRef Tensor A);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> triangular_solve_out(@ByRef Tensor X, @ByRef Tensor M, @Const @ByRef Tensor self, @Const @ByRef Tensor A, @Cast("bool") boolean upper/*=true*/, @Cast("bool") boolean transpose/*=false*/, @Cast("bool") boolean unitriangular/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> triangular_solve_out(@ByRef Tensor X, @ByRef Tensor M, @Const @ByRef Tensor self, @Const @ByRef Tensor A);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> triangular_solve_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor A, @Cast("bool") boolean upper, @Cast("bool") boolean transpose, @Cast("bool") boolean unitriangular, @ByRef Tensor X, @ByRef Tensor M);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor triangular_solve(@Const @ByRef Tensor self, @Const @ByRef Tensor A, @Cast("bool") boolean upper/*=true*/, @Cast("bool") boolean transpose/*=false*/, @Cast("bool") boolean unitriangular/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor triangular_solve(@Const @ByRef Tensor self, @Const @ByRef Tensor A);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _triangular_solve_helper(@Const @ByRef Tensor self, @Const @ByRef Tensor A, @Cast("bool") boolean upper, @Cast("bool") boolean transpose, @Cast("bool") boolean unitriangular);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> symeig_out(@ByRef Tensor e, @ByRef Tensor V, @Const @ByRef Tensor self, @Cast("bool") boolean eigenvectors/*=false*/, @Cast("bool") boolean upper/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> symeig_out(@ByRef Tensor e, @ByRef Tensor V, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> symeig_outf(@Const @ByRef Tensor self, @Cast("bool") boolean eigenvectors, @Cast("bool") boolean upper, @ByRef Tensor e, @ByRef Tensor V);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor symeig(@Const @ByRef Tensor self, @Cast("bool") boolean eigenvectors/*=false*/, @Cast("bool") boolean upper/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor symeig(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _symeig_helper(@Const @ByRef Tensor self, @Cast("bool") boolean eigenvectors, @Cast("bool") boolean upper);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> eig_out(@ByRef Tensor e, @ByRef Tensor v, @Const @ByRef Tensor self, @Cast("bool") boolean eigenvectors/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> eig_out(@ByRef Tensor e, @ByRef Tensor v, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> eig_outf(@Const @ByRef Tensor self, @Cast("bool") boolean eigenvectors, @ByRef Tensor e, @ByRef Tensor v);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor eig(@Const @ByRef Tensor self, @Cast("bool") boolean eigenvectors/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor eig(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> svd_out(@ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor V, @Const @ByRef Tensor self, @Cast("bool") boolean some/*=true*/, @Cast("bool") boolean compute_uv/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> svd_out(@ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor V, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> svd_outf(@Const @ByRef Tensor self, @Cast("bool") boolean some, @Cast("bool") boolean compute_uv, @ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor V);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor svd(@Const @ByRef Tensor self, @Cast("bool") boolean some/*=true*/, @Cast("bool") boolean compute_uv/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor svd(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor _svd_helper(@Const @ByRef Tensor self, @Cast("bool") boolean some, @Cast("bool") boolean compute_uv);
@Namespace("at") public static native @ByVal Tensor swapaxes(@Const @ByRef Tensor self, @Cast("int64_t") long axis0, @Cast("int64_t") long axis1);
@Namespace("at") public static native @ByVal Tensor swapdims(@Const @ByRef Tensor self, @Cast("int64_t") long dim0, @Cast("int64_t") long dim1);
@Namespace("at") public static native @ByRef Tensor cholesky_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByRef Tensor cholesky_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor cholesky_outf(@Const @ByRef Tensor self, @Cast("bool") boolean upper, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor cholesky(@Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByVal Tensor cholesky(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor _cholesky_helper(@Const @ByRef Tensor self, @Cast("bool") boolean upper);
@Namespace("at") public static native @ByRef Tensor cholesky_solve_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByRef Tensor cholesky_solve_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor input2);
@Namespace("at") public static native @ByRef Tensor cholesky_solve_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Cast("bool") boolean upper, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor cholesky_solve(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByVal Tensor cholesky_solve(@Const @ByRef Tensor self, @Const @ByRef Tensor input2);
@Namespace("at") public static native @ByVal Tensor _cholesky_solve_helper(@Const @ByRef Tensor self, @Const @ByRef Tensor A, @Cast("bool") boolean upper);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor solve(@Const @ByRef Tensor self, @Const @ByRef Tensor A);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> solve_out(@ByRef Tensor solution, @ByRef Tensor lu, @Const @ByRef Tensor self, @Const @ByRef Tensor A);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> solve_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor A, @ByRef Tensor solution, @ByRef Tensor lu);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _solve_helper(@Const @ByRef Tensor self, @Const @ByRef Tensor A);
@Namespace("at") public static native @ByVal Tensor cholesky_inverse(@Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByVal Tensor cholesky_inverse(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor cholesky_inverse_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("bool") boolean upper/*=false*/);
@Namespace("at") public static native @ByRef Tensor cholesky_inverse_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor cholesky_inverse_outf(@Const @ByRef Tensor self, @Cast("bool") boolean upper, @ByRef Tensor out);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> qr_out(@ByRef Tensor Q, @ByRef Tensor R, @Const @ByRef Tensor self, @Cast("bool") boolean some/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> qr_out(@ByRef Tensor Q, @ByRef Tensor R, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> qr_outf(@Const @ByRef Tensor self, @Cast("bool") boolean some, @ByRef Tensor Q, @ByRef Tensor R);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor qr(@Const @ByRef Tensor self, @Cast("bool") boolean some/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor qr(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> geqrf_out(@ByRef Tensor a, @ByRef Tensor tau, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> geqrf_outf(@Const @ByRef Tensor self, @ByRef Tensor a, @ByRef Tensor tau);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor geqrf(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor orgqr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor input2);
@Namespace("at") public static native @ByRef Tensor orgqr_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor orgqr(@Const @ByRef Tensor self, @Const @ByRef Tensor input2);
@Namespace("at") public static native @ByRef Tensor ormqr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Const @ByRef Tensor input3, @Cast("bool") boolean left/*=true*/, @Cast("bool") boolean transpose/*=false*/);
@Namespace("at") public static native @ByRef Tensor ormqr_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Const @ByRef Tensor input3);
@Namespace("at") public static native @ByRef Tensor ormqr_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Const @ByRef Tensor input3, @Cast("bool") boolean left, @Cast("bool") boolean transpose, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor ormqr(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Const @ByRef Tensor input3, @Cast("bool") boolean left/*=true*/, @Cast("bool") boolean transpose/*=false*/);
@Namespace("at") public static native @ByVal Tensor ormqr(@Const @ByRef Tensor self, @Const @ByRef Tensor input2, @Const @ByRef Tensor input3);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor _lu_with_info(@Const @ByRef Tensor self, @Cast("bool") boolean pivot/*=true*/, @Cast("bool") boolean check_errors/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor _lu_with_info(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor lu_solve_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots);
@Namespace("at") public static native @ByRef Tensor lu_solve_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor lu_solve(@Const @ByRef Tensor self, @Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots);
@Namespace("at") public static native @ByVal Tensor _lu_solve_helper(@Const @ByRef Tensor self, @Const @ByRef Tensor LU_data, @Const @ByRef Tensor LU_pivots);
@Namespace("at") public static native @ByRef Tensor multinomial_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long num_samples, @Cast("bool") boolean replacement/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor multinomial_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long num_samples);
@Namespace("at") public static native @ByRef Tensor multinomial_outf(@Const @ByRef Tensor self, @Cast("int64_t") long num_samples, @Cast("bool") boolean replacement, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor multinomial(@Const @ByRef Tensor self, @Cast("int64_t") long num_samples, @Cast("bool") boolean replacement/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor multinomial(@Const @ByRef Tensor self, @Cast("int64_t") long num_samples);
@Namespace("at") public static native @ByRef Tensor lgamma_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor lgamma_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor lgamma(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor digamma_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor digamma_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor digamma(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor polygamma_out(@ByRef Tensor out, @Cast("int64_t") long n, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor polygamma_outf(@Cast("int64_t") long n, @Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor polygamma(@Cast("int64_t") long n, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor erfinv(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor erfinv_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor erfinv_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor i0(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor i0_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor i0_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor i0_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor sign(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor sign_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor sign_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor signbit(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor signbit_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor signbit_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor dist(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::Scalar(2)") Scalar p);
@Namespace("at") public static native @ByVal Tensor dist(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor atan2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor atan2_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor atan2(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor lerp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor end, @ByVal Scalar weight);
@Namespace("at") public static native @ByRef Tensor lerp_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor end, @ByVal Scalar weight, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor lerp_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor end, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByRef Tensor lerp_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor end, @Const @ByRef Tensor weight, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor lerp(@Const @ByRef Tensor self, @Const @ByRef Tensor end, @ByVal Scalar weight);
@Namespace("at") public static native @ByVal Tensor lerp(@Const @ByRef Tensor self, @Const @ByRef Tensor end, @Const @ByRef Tensor weight);
@Namespace("at") public static native @ByRef Tensor histc_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long bins/*=100*/, @ByVal(nullValue = "c10::Scalar(0)") Scalar min, @ByVal(nullValue = "c10::Scalar(0)") Scalar max);
@Namespace("at") public static native @ByRef Tensor histc_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor histc_outf(@Const @ByRef Tensor self, @Cast("int64_t") long bins, @ByVal Scalar min, @ByVal Scalar max, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor histc(@Const @ByRef Tensor self, @Cast("int64_t") long bins/*=100*/, @ByVal(nullValue = "c10::Scalar(0)") Scalar min, @ByVal(nullValue = "c10::Scalar(0)") Scalar max);
@Namespace("at") public static native @ByVal Tensor histc(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fmod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor fmod_outf(@Const @ByRef Tensor self, @ByVal Scalar other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor fmod(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor fmod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor fmod_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor fmod(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor hypot_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor hypot_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor hypot(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor igamma_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor igamma_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor igamma(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor igammac_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor igammac_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor igammac(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor nextafter_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor nextafter_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor nextafter(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor remainder_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor remainder_outf(@Const @ByRef Tensor self, @ByVal Scalar other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor remainder(@Const @ByRef Tensor self, @ByVal Scalar other);
@Namespace("at") public static native @ByRef Tensor remainder_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor remainder_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor remainder(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByVal Tensor min(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fmin(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor fmin_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor fmin_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor max(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fmax(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor fmax_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor fmax_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor maximum(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor maximum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor maximum_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor max(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor max_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor max_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor minimum(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor minimum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor minimum_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor min_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor min_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor min(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor quantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, double q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor quantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, double q);
@Namespace("at") public static native @ByRef Tensor quantile_outf(@Const @ByRef Tensor self, double q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor quantile(@Const @ByRef Tensor self, double q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor quantile(@Const @ByRef Tensor self, double q);
@Namespace("at") public static native @ByRef Tensor quantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor quantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor q);
@Namespace("at") public static native @ByRef Tensor quantile_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor quantile(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor quantile(@Const @ByRef Tensor self, @Const @ByRef Tensor q);
@Namespace("at") public static native @ByRef Tensor nanquantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, double q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor nanquantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, double q);
@Namespace("at") public static native @ByRef Tensor nanquantile_outf(@Const @ByRef Tensor self, double q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor nanquantile(@Const @ByRef Tensor self, double q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor nanquantile(@Const @ByRef Tensor self, double q);
@Namespace("at") public static native @ByRef Tensor nanquantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByRef Tensor nanquantile_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor q);
@Namespace("at") public static native @ByRef Tensor nanquantile_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal LongOptional dim, @Cast("bool") boolean keepdim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor nanquantile(@Const @ByRef Tensor self, @Const @ByRef Tensor q, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional dim, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal Tensor nanquantile(@Const @ByRef Tensor self, @Const @ByRef Tensor q);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean descending, @ByRef Tensor values, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor sort(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor sort(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> sort_outf(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean descending, @ByRef Tensor values, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor sort(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor sort(@Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByRef Tensor msort_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor msort_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor msort(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor argsort(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal Tensor argsort(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor argsort(@Const @ByRef Tensor self, @ByVal Dimname dim, @Cast("bool") boolean descending/*=false*/);
@Namespace("at") public static native @ByVal Tensor argsort(@Const @ByRef Tensor self, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> topk_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean largest/*=true*/, @Cast("bool") boolean sorted/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> topk_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long k);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> topk_outf(@Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim, @Cast("bool") boolean largest, @Cast("bool") boolean sorted, @ByRef Tensor values, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor topk(@Const @ByRef Tensor self, @Cast("int64_t") long k, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean largest/*=true*/, @Cast("bool") boolean sorted/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor topk(@Const @ByRef Tensor self, @Cast("int64_t") long k);
@Namespace("at") public static native @ByVal Tensor all(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor any(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor renorm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Scalar p, @Cast("int64_t") long dim, @ByVal Scalar maxnorm);
@Namespace("at") public static native @ByRef Tensor renorm_outf(@Const @ByRef Tensor self, @ByVal Scalar p, @Cast("int64_t") long dim, @ByVal Scalar maxnorm, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor renorm(@Const @ByRef Tensor self, @ByVal Scalar p, @Cast("int64_t") long dim, @ByVal Scalar maxnorm);
@Namespace("at") public static native @ByVal Tensor unfold_backward(@Const @ByRef Tensor grad_in, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long size, @Cast("int64_t") long step);
@Namespace("at") public static native @ByVal Tensor unfold_backward(@Const @ByRef Tensor grad_in, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_sizes, @Cast("int64_t") long dim, @Cast("int64_t") long size, @Cast("int64_t") long step);
@Namespace("at") public static native @Cast("bool") boolean equal(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor pow_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor exponent);
@Namespace("at") public static native @ByRef Tensor pow_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor exponent, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor pow(@Const @ByRef Tensor self, @Const @ByRef Tensor exponent);
@Namespace("at") public static native @ByRef Tensor pow_out(@ByRef Tensor out, @ByVal Scalar self, @Const @ByRef Tensor exponent);
@Namespace("at") public static native @ByRef Tensor pow_outf(@ByVal Scalar self, @Const @ByRef Tensor exponent, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor pow(@ByVal Scalar self, @Const @ByRef Tensor exponent);
@Namespace("at") public static native @ByRef Tensor pow_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Scalar exponent);
@Namespace("at") public static native @ByRef Tensor pow_outf(@Const @ByRef Tensor self, @ByVal Scalar exponent, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor pow(@Const @ByRef Tensor self, @ByVal Scalar exponent);
@Namespace("at") public static native @ByRef Tensor float_power_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor exponent);
@Namespace("at") public static native @ByRef Tensor float_power_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor exponent, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor float_power(@Const @ByRef Tensor self, @Const @ByRef Tensor exponent);
@Namespace("at") public static native @ByRef Tensor float_power_out(@ByRef Tensor out, @ByVal Scalar self, @Const @ByRef Tensor exponent);
@Namespace("at") public static native @ByRef Tensor float_power_outf(@ByVal Scalar self, @Const @ByRef Tensor exponent, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor float_power(@ByVal Scalar self, @Const @ByRef Tensor exponent);
@Namespace("at") public static native @ByRef Tensor float_power_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal Scalar exponent);
@Namespace("at") public static native @ByRef Tensor float_power_outf(@Const @ByRef Tensor self, @ByVal Scalar exponent, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor float_power(@Const @ByRef Tensor self, @ByVal Scalar exponent);
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, @Const @ByRef Tensor mean, double std/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, @Const @ByRef Tensor mean);
@Namespace("at") public static native @ByRef Tensor normal_outf(@Const @ByRef Tensor mean, double std, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor normal(@Const @ByRef Tensor mean, double std/*=1*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor normal(@Const @ByRef Tensor mean);
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, double mean, @Const @ByRef Tensor std, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, double mean, @Const @ByRef Tensor std);
@Namespace("at") public static native @ByRef Tensor normal_outf(double mean, @Const @ByRef Tensor std, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor normal(double mean, @Const @ByRef Tensor std, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor normal(double mean, @Const @ByRef Tensor std);
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, @Const @ByRef Tensor mean, @Const @ByRef Tensor std, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, @Const @ByRef Tensor mean, @Const @ByRef Tensor std);
@Namespace("at") public static native @ByRef Tensor normal_outf(@Const @ByRef Tensor mean, @Const @ByRef Tensor std, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor normal(@Const @ByRef Tensor mean, @Const @ByRef Tensor std, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor normal(@Const @ByRef Tensor mean, @Const @ByRef Tensor std);
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByVal Tensor normal(double mean, double std, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, double mean, double std, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, double mean, double std, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, double mean, double std, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor normal_out(@ByRef Tensor out, double mean, double std, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);
@Namespace("at") public static native @ByRef Tensor normal_outf(double mean, double std, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor normal_outf(double mean, double std, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor alias(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor _index_copy_(@ByRef Tensor self, @Cast("int64_t") long dim, @Const @ByRef Tensor index, @Const @ByRef Tensor source);
@Namespace("at") public static native @ByVal Tensor _cumsum(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByRef Tensor _cumsum_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByRef Tensor _cumsum_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor _cumprod(@Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByRef Tensor _cumprod_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByRef Tensor _cumprod_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor _var(@Const @ByRef Tensor self, @Cast("bool") boolean unbiased/*=true*/);
@Namespace("at") public static native @ByVal Tensor _var(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor _std(@Const @ByRef Tensor self, @Cast("bool") boolean unbiased/*=true*/);
@Namespace("at") public static native @ByVal Tensor _std(@Const @ByRef Tensor self);
@Namespace("at") public static native void _amp_foreach_non_finite_check_and_unscale_(@ByVal TensorArrayRef self, @ByRef Tensor found_inf, @Const @ByRef Tensor inv_scale);
@Namespace("at") public static native @ByVal Tensor _amp_update_scale(@ByRef Tensor growth_tracker, @Const @ByRef Tensor current_scale, @Const @ByRef Tensor found_inf, double scale_growth_factor, double scale_backoff_factor, @Cast("int64_t") long growth_interval);
@Namespace("at") public static native @ByVal Tensor _cat(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByVal Tensor _cat(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor _cat_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors, @Cast("int64_t") long dim/*=0*/);
@Namespace("at") public static native @ByRef Tensor _cat_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor _cat_outf(@ByVal TensorArrayRef tensors, @Cast("int64_t") long dim, @ByRef Tensor out);
@Namespace("at") public static native @StdMove TensorVector _foreach_add(@ByVal TensorArrayRef tensors, @ByVal Scalar scalar);
@Namespace("at") public static native void _foreach_add_(@ByVal TensorArrayRef self, @ByVal Scalar scalar);
@Namespace("at") public static native @StdMove TensorVector _foreach_sub(@ByVal TensorArrayRef tensors, @ByVal Scalar scalar);
@Namespace("at") public static native void _foreach_sub_(@ByVal TensorArrayRef self, @ByVal Scalar scalar);
@Namespace("at") public static native @StdMove TensorVector _foreach_mul(@ByVal TensorArrayRef tensors, @ByVal Scalar scalar);
@Namespace("at") public static native void _foreach_mul_(@ByVal TensorArrayRef self, @ByVal Scalar scalar);
@Namespace("at") public static native @StdMove TensorVector _foreach_div(@ByVal TensorArrayRef tensors, @ByVal Scalar scalar);
@Namespace("at") public static native void _foreach_div_(@ByVal TensorArrayRef self, @ByVal Scalar scalar);
@Namespace("at") public static native @StdMove TensorVector _foreach_add(@ByVal TensorArrayRef tensors1, @ByVal TensorArrayRef tensors2, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @StdMove TensorVector _foreach_add(@ByVal TensorArrayRef tensors1, @ByVal TensorArrayRef tensors2);
@Namespace("at") public static native void _foreach_add_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native void _foreach_add_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);
@Namespace("at") public static native @StdMove TensorVector _foreach_sub(@ByVal TensorArrayRef tensors1, @ByVal TensorArrayRef tensors2, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @StdMove TensorVector _foreach_sub(@ByVal TensorArrayRef tensors1, @ByVal TensorArrayRef tensors2);
@Namespace("at") public static native void _foreach_sub_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native void _foreach_sub_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);
@Namespace("at") public static native @StdMove TensorVector _foreach_mul(@ByVal TensorArrayRef tensors1, @ByVal TensorArrayRef tensors2);
@Namespace("at") public static native void _foreach_mul_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);
@Namespace("at") public static native @StdMove TensorVector _foreach_div(@ByVal TensorArrayRef tensors1, @ByVal TensorArrayRef tensors2);
@Namespace("at") public static native void _foreach_div_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef other);
@Namespace("at") public static native @StdMove TensorVector _foreach_add(@ByVal TensorArrayRef tensors, @ByVal DoubleArrayRef scalars);
@Namespace("at") public static native void _foreach_add_(@ByVal TensorArrayRef self, @ByVal DoubleArrayRef scalars);
@Namespace("at") public static native @StdMove TensorVector _foreach_sub(@ByVal TensorArrayRef tensors, @ByVal DoubleArrayRef scalars);
@Namespace("at") public static native void _foreach_sub_(@ByVal TensorArrayRef self, @ByVal DoubleArrayRef scalars);
@Namespace("at") public static native @StdMove TensorVector _foreach_div(@ByVal TensorArrayRef tensors, @ByVal DoubleArrayRef scalars);
@Namespace("at") public static native void _foreach_div_(@ByVal TensorArrayRef self, @ByVal DoubleArrayRef scalars);
@Namespace("at") public static native @StdMove TensorVector _foreach_mul(@ByVal TensorArrayRef tensors, @ByVal DoubleArrayRef scalars);
@Namespace("at") public static native void _foreach_mul_(@ByVal TensorArrayRef self, @ByVal DoubleArrayRef scalars);
@Namespace("at") public static native @StdMove TensorVector _foreach_exp(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_zero_(@ByVal TensorArrayRef self);
@Namespace("at") public static native void _foreach_exp_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_sqrt(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_sqrt_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_abs(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_abs_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_acos(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_acos_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_asin(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_asin_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_atan(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_atan_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_ceil(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_ceil_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_cos(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_cos_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_cosh(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_cosh_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_erf(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_erf_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_erfc(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_erfc_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_expm1(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_expm1_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_floor(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_floor_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_log(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_log_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_log10(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_log10_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_log1p(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_log1p_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_log2(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_log2_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_neg(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_neg_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_tan(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_tan_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_tanh(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_tanh_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_sin(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_sin_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_sinh(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_sinh_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_round(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_round_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_lgamma(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_lgamma_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_frac(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_frac_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_reciprocal(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_reciprocal_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_sigmoid(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_sigmoid_(@ByVal TensorArrayRef self);
@Namespace("at") public static native @StdMove TensorVector _foreach_trunc(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native void _foreach_trunc_(@ByVal TensorArrayRef self);
@Namespace("at") public static native void _foreach_addcdiv_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @ByVal(nullValue = "c10::Scalar(1)") Scalar value);
@Namespace("at") public static native void _foreach_addcdiv_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2);
@Namespace("at") public static native void _foreach_addcmul_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @ByVal(nullValue = "c10::Scalar(1)") Scalar value);
@Namespace("at") public static native void _foreach_addcmul_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2);
@Namespace("at") public static native void _foreach_addcdiv_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @ByVal DoubleArrayRef scalars);
@Namespace("at") public static native void _foreach_addcmul_(@ByVal TensorArrayRef self, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @ByVal DoubleArrayRef scalars);
@Namespace("at") public static native @StdMove TensorVector _foreach_addcdiv(@ByVal TensorArrayRef input, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @ByVal(nullValue = "c10::Scalar(1)") Scalar value);
@Namespace("at") public static native @StdMove TensorVector _foreach_addcdiv(@ByVal TensorArrayRef input, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2);
@Namespace("at") public static native @StdMove TensorVector _foreach_addcmul(@ByVal TensorArrayRef input, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @ByVal(nullValue = "c10::Scalar(1)") Scalar value);
@Namespace("at") public static native @StdMove TensorVector _foreach_addcmul(@ByVal TensorArrayRef input, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2);
@Namespace("at") public static native @StdMove TensorVector _foreach_addcdiv(@ByVal TensorArrayRef input, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @ByVal DoubleArrayRef scalars);
@Namespace("at") public static native @StdMove TensorVector _foreach_addcmul(@ByVal TensorArrayRef input, @ByVal TensorArrayRef tensor1, @ByVal TensorArrayRef tensor2, @ByVal DoubleArrayRef scalars);
@Namespace("at") public static native @StdMove TensorVector _foreach_maximum(@ByVal TensorArrayRef tensors1, @ByVal TensorArrayRef tensors2);
@Namespace("at") public static native @StdMove TensorVector _foreach_minimum(@ByVal TensorArrayRef tensors1, @ByVal TensorArrayRef tensors2);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _mode(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _mode(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _mode_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/, @Cast("bool") boolean keepdim/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _mode_out(@ByRef Tensor values, @ByRef Tensor indices, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> _mode_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @Cast("bool") boolean keepdim, @ByRef Tensor values, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal Tensor bucketize(@Const @ByRef Tensor self, @Const @ByRef Tensor boundaries, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/);
@Namespace("at") public static native @ByVal Tensor bucketize(@Const @ByRef Tensor self, @Const @ByRef Tensor boundaries);
@Namespace("at") public static native @ByRef Tensor bucketize_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor boundaries, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/);
@Namespace("at") public static native @ByRef Tensor bucketize_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor boundaries);
@Namespace("at") public static native @ByRef Tensor bucketize_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor boundaries, @Cast("bool") boolean out_int32, @Cast("bool") boolean right, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor bucketize(@ByVal Scalar self, @Const @ByRef Tensor boundaries, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/);
@Namespace("at") public static native @ByVal Tensor bucketize(@ByVal Scalar self, @Const @ByRef Tensor boundaries);
@Namespace("at") public static native @ByVal Tensor searchsorted(@Const @ByRef Tensor sorted_sequence, @Const @ByRef Tensor self, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/);
@Namespace("at") public static native @ByVal Tensor searchsorted(@Const @ByRef Tensor sorted_sequence, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor searchsorted_out(@ByRef Tensor out, @Const @ByRef Tensor sorted_sequence, @Const @ByRef Tensor self, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/);
@Namespace("at") public static native @ByRef Tensor searchsorted_out(@ByRef Tensor out, @Const @ByRef Tensor sorted_sequence, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor searchsorted_outf(@Const @ByRef Tensor sorted_sequence, @Const @ByRef Tensor self, @Cast("bool") boolean out_int32, @Cast("bool") boolean right, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor searchsorted(@Const @ByRef Tensor sorted_sequence, @ByVal Scalar self, @Cast("bool") boolean out_int32/*=false*/, @Cast("bool") boolean right/*=false*/);
@Namespace("at") public static native @ByVal Tensor searchsorted(@Const @ByRef Tensor sorted_sequence, @ByVal Scalar self);
@Namespace("at") public static native @ByRef Tensor mse_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor mse_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByRef Tensor mse_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor mse_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor mse_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByRef Tensor mse_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);
@Namespace("at") public static native @ByRef Tensor mse_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor mse_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);
@Namespace("at") public static native @ByRef Tensor l1_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor l1_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByRef Tensor l1_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor l1_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor l1_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByRef Tensor l1_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);
@Namespace("at") public static native @ByRef Tensor l1_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor l1_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @ByVal(nullValue = "c10::Scalar(1)") Scalar p, @ByVal(nullValue = "c10::Scalar(1)") Scalar margin, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @ByVal Scalar p, @ByVal Scalar margin, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor multi_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @ByVal(nullValue = "c10::Scalar(1)") Scalar p, @ByVal(nullValue = "c10::Scalar(1)") Scalar margin, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor multi_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @ByVal Scalar p, @ByVal Scalar margin, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @ByVal Scalar p, @ByVal Scalar margin);
@Namespace("at") public static native @ByRef Tensor multi_margin_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @ByVal Scalar p, @ByVal Scalar margin, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor multi_margin_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @ByVal Scalar p, @ByVal Scalar margin, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor multi_margin_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @ByVal Scalar p, @ByVal Scalar margin);
@Namespace("at") public static native @ByRef Tensor multilabel_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor multilabel_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByRef Tensor multilabel_margin_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor multilabel_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor multilabel_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> multilabel_margin_loss_forward_out(@ByRef Tensor output, @ByRef Tensor is_target, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> multilabel_margin_loss_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor output, @ByRef Tensor is_target);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor multilabel_margin_loss_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);
@Namespace("at") public static native @ByRef Tensor multilabel_margin_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @Const @ByRef Tensor is_target);
@Namespace("at") public static native @ByRef Tensor multilabel_margin_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @Const @ByRef Tensor is_target, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor multilabel_margin_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @Const @ByRef Tensor is_target);
@Namespace("at") public static native @ByRef Tensor nll_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("int64_t") long ignore_index/*=-100*/);
@Namespace("at") public static native @ByRef Tensor nll_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByRef Tensor nll_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor nll_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("int64_t") long ignore_index/*=-100*/);
@Namespace("at") public static native @ByVal Tensor nll_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nll_loss_forward_out(@ByRef Tensor output, @ByRef Tensor total_weight, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nll_loss_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @ByRef Tensor output, @ByRef Tensor total_weight);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor nll_loss_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index);
@Namespace("at") public static native @ByRef Tensor nll_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight);
@Namespace("at") public static native @ByRef Tensor nll_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor nll_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight);
@Namespace("at") public static native @ByRef Tensor nll_loss2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("int64_t") long ignore_index/*=-100*/);
@Namespace("at") public static native @ByRef Tensor nll_loss2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByRef Tensor nll_loss2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor nll_loss2d(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional weight, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, @Cast("int64_t") long ignore_index/*=-100*/);
@Namespace("at") public static native @ByVal Tensor nll_loss2d(@Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nll_loss2d_forward_out(@ByRef Tensor output, @ByRef Tensor total_weight, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> nll_loss2d_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @ByRef Tensor output, @ByRef Tensor total_weight);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor nll_loss2d_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index);
@Namespace("at") public static native @ByRef Tensor nll_loss2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight);
@Namespace("at") public static native @ByRef Tensor nll_loss2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor nll_loss2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Const @ByRef TensorOptional weight, @Cast("int64_t") long reduction, @Cast("int64_t") long ignore_index, @Const @ByRef Tensor total_weight);
@Namespace("at") public static native @ByRef Tensor smooth_l1_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, double beta/*=1.0*/);
@Namespace("at") public static native @ByRef Tensor smooth_l1_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByRef Tensor smooth_l1_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double beta, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor smooth_l1_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/, double beta/*=1.0*/);
@Namespace("at") public static native @ByVal Tensor smooth_l1_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByRef Tensor smooth_l1_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double beta);
@Namespace("at") public static native @ByRef Tensor smooth_l1_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double beta, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor smooth_l1_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, double beta);
@Namespace("at") public static native @ByRef Tensor soft_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByRef Tensor soft_margin_loss_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByRef Tensor soft_margin_loss_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor soft_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction/*=at::Reduction::Mean*/);
@Namespace("at") public static native @ByVal Tensor soft_margin_loss(@Const @ByRef Tensor self, @Const @ByRef Tensor target);
@Namespace("at") public static native @ByRef Tensor soft_margin_loss_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);
@Namespace("at") public static native @ByRef Tensor soft_margin_loss_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor soft_margin_loss_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor target, @Cast("int64_t") long reduction);
@Namespace("at") public static native @ByRef Tensor elu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha, @ByVal(nullValue = "c10::Scalar(1)") Scalar scale, @ByVal(nullValue = "c10::Scalar(1)") Scalar input_scale);
@Namespace("at") public static native @ByRef Tensor elu_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor elu_outf(@Const @ByRef Tensor self, @ByVal Scalar alpha, @ByVal Scalar scale, @ByVal Scalar input_scale, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor elu(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha, @ByVal(nullValue = "c10::Scalar(1)") Scalar scale, @ByVal(nullValue = "c10::Scalar(1)") Scalar input_scale);
@Namespace("at") public static native @ByVal Tensor elu(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor elu_backward(@Const @ByRef Tensor grad_output, @ByVal Scalar alpha, @ByVal Scalar scale, @ByVal Scalar input_scale, @Cast("bool") boolean is_result, @Const @ByRef Tensor self_or_result);
@Namespace("at") public static native @ByRef Tensor elu_(@ByRef Tensor self, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha, @ByVal(nullValue = "c10::Scalar(1)") Scalar scale, @ByVal(nullValue = "c10::Scalar(1)") Scalar input_scale);
@Namespace("at") public static native @ByRef Tensor elu_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor glu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByRef Tensor glu_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor glu_outf(@Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor glu(@Const @ByRef Tensor self, @Cast("int64_t") long dim/*=-1*/);
@Namespace("at") public static native @ByVal Tensor glu(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor glu_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByRef Tensor glu_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Cast("int64_t") long dim, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor glu_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Cast("int64_t") long dim);
@Namespace("at") public static native @ByRef Tensor hardsigmoid_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor hardsigmoid_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor hardsigmoid(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor hardsigmoid_(@ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor hardsigmoid_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor hardtanh_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::Scalar(-1)") Scalar min_val, @ByVal(nullValue = "c10::Scalar(1)") Scalar max_val);
@Namespace("at") public static native @ByRef Tensor hardtanh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor hardtanh_outf(@Const @ByRef Tensor self, @ByVal Scalar min_val, @ByVal Scalar max_val, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor hardtanh(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::Scalar(-1)") Scalar min_val, @ByVal(nullValue = "c10::Scalar(1)") Scalar max_val);
@Namespace("at") public static native @ByVal Tensor hardtanh(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor hardtanh_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal Scalar min_val, @ByVal Scalar max_val);
@Namespace("at") public static native @ByRef Tensor hardtanh_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal Scalar min_val, @ByVal Scalar max_val, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor hardtanh_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal Scalar min_val, @ByVal Scalar max_val);
@Namespace("at") public static native @ByRef Tensor hardtanh_(@ByRef Tensor self, @ByVal(nullValue = "c10::Scalar(-1)") Scalar min_val, @ByVal(nullValue = "c10::Scalar(1)") Scalar max_val);
@Namespace("at") public static native @ByRef Tensor hardtanh_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor hardswish_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor hardswish_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor hardswish(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor hardswish_(@ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor hardswish_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor leaky_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::Scalar(0.01)") Scalar negative_slope);
@Namespace("at") public static native @ByRef Tensor leaky_relu_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor leaky_relu_outf(@Const @ByRef Tensor self, @ByVal Scalar negative_slope, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor leaky_relu(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::Scalar(0.01)") Scalar negative_slope);
@Namespace("at") public static native @ByVal Tensor leaky_relu(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor leaky_relu_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal Scalar negative_slope, @Cast("bool") boolean self_is_result);
@Namespace("at") public static native @ByRef Tensor leaky_relu_(@ByRef Tensor self, @ByVal(nullValue = "c10::Scalar(0.01)") Scalar negative_slope);
@Namespace("at") public static native @ByRef Tensor leaky_relu_(@ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor log_sigmoid_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor log_sigmoid_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor log_sigmoid(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> log_sigmoid_forward_out(@ByRef Tensor output, @ByRef Tensor buffer, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> log_sigmoid_forward_outf(@Const @ByRef Tensor self, @ByRef Tensor output, @ByRef Tensor buffer);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor log_sigmoid_forward(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor log_sigmoid_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor buffer);
@Namespace("at") public static native @ByRef Tensor log_sigmoid_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor buffer, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor log_sigmoid_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor buffer);
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor noise, @ByVal(nullValue = "c10::Scalar(0.125)") Scalar lower, @ByVal(nullValue = "c10::Scalar(0.3333333333333333)") Scalar upper, @Cast("bool") boolean training/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor noise);
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor noise, @ByVal Scalar lower, @ByVal Scalar upper, @Cast("bool") boolean training, @ByVal GeneratorOptional generator, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor rrelu_with_noise(@Const @ByRef Tensor self, @Const @ByRef Tensor noise, @ByVal(nullValue = "c10::Scalar(0.125)") Scalar lower, @ByVal(nullValue = "c10::Scalar(0.3333333333333333)") Scalar upper, @Cast("bool") boolean training/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByVal Tensor rrelu_with_noise(@Const @ByRef Tensor self, @Const @ByRef Tensor noise);
@Namespace("at") public static native @ByVal Tensor rrelu_with_noise_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor noise, @ByVal Scalar lower, @ByVal Scalar upper, @Cast("bool") boolean training, @Cast("bool") boolean self_is_result);
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_(@ByRef Tensor self, @Const @ByRef Tensor noise, @ByVal(nullValue = "c10::Scalar(0.125)") Scalar lower, @ByVal(nullValue = "c10::Scalar(0.3333333333333333)") Scalar upper, @Cast("bool") boolean training/*=false*/, @ByVal(nullValue = "c10::optional<at::Generator>(c10::nullopt)") GeneratorOptional generator);
@Namespace("at") public static native @ByRef Tensor rrelu_with_noise_(@ByRef Tensor self, @Const @ByRef Tensor noise);
@Namespace("at") public static native @ByRef Tensor softplus_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::Scalar(1)") Scalar beta, @ByVal(nullValue = "c10::Scalar(20)") Scalar threshold);
@Namespace("at") public static native @ByRef Tensor softplus_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor softplus_outf(@Const @ByRef Tensor self, @ByVal Scalar beta, @ByVal Scalar threshold, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor softplus(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::Scalar(1)") Scalar beta, @ByVal(nullValue = "c10::Scalar(20)") Scalar threshold);
@Namespace("at") public static native @ByVal Tensor softplus(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor softplus_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal Scalar beta, @ByVal Scalar threshold, @Const @ByRef Tensor output);
@Namespace("at") public static native @ByRef Tensor softplus_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal Scalar beta, @ByVal Scalar threshold, @Const @ByRef Tensor output, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor softplus_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal Scalar beta, @ByVal Scalar threshold, @Const @ByRef Tensor output);
@Namespace("at") public static native @ByRef Tensor softshrink_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::Scalar(0.5)") Scalar lambd);
@Namespace("at") public static native @ByRef Tensor softshrink_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor softshrink_outf(@Const @ByRef Tensor self, @ByVal Scalar lambd, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor softshrink(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::Scalar(0.5)") Scalar lambd);
@Namespace("at") public static native @ByVal Tensor softshrink(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor softshrink_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal Scalar lambd);
@Namespace("at") public static native @ByRef Tensor softshrink_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal Scalar lambd, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor softshrink_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal Scalar lambd);
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... output_size);
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... output_size);
@Namespace("at") public static native @ByVal Tensor mkldnn_adaptive_avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor mkldnn_adaptive_avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... output_size);
@Namespace("at") public static native @ByVal Tensor _adaptive_avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor _adaptive_avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... output_size);
@Namespace("at") public static native @ByVal Tensor _adaptive_avg_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... output_size);
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... output_size);
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor adaptive_avg_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor adaptive_avg_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool2d_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool2d_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... output_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByRef Tensor out, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByRef Tensor out, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor adaptive_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor adaptive_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... output_size);
@Namespace("at") public static native @ByRef Tensor adaptive_max_pool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor adaptive_max_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor adaptive_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool3d_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool3d_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... output_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByRef Tensor out, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> adaptive_max_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByRef Tensor out, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor adaptive_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor adaptive_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... output_size);
@Namespace("at") public static native @ByRef Tensor adaptive_max_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor adaptive_max_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor adaptive_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor avg_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor avg_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode/*=false*/, @Cast("bool") boolean count_include_pad/*=true*/, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor avg_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor avg_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
@Namespace("at") public static native @ByVal Tensor avg_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @Cast("bool") boolean ceil_mode, @Cast("bool") boolean count_include_pad, @ByVal LongOptional divisor_override);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool2d_out(@ByRef Tensor output, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor random_samples);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool2d_out(@ByRef Tensor output, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor random_samples);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor random_samples, @ByRef Tensor output, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor random_samples, @ByRef Tensor output, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor fractional_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor random_samples);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor fractional_max_pool2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor random_samples);
@Namespace("at") public static native @ByRef Tensor fractional_max_pool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor fractional_max_pool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor fractional_max_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor fractional_max_pool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor fractional_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByVal Tensor fractional_max_pool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool3d_out(@ByRef Tensor output, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor random_samples);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool3d_out(@ByRef Tensor output, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor random_samples);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor random_samples, @ByRef Tensor output, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> fractional_max_pool3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor random_samples, @ByRef Tensor output, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor fractional_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor random_samples);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor fractional_max_pool3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor random_samples);
@Namespace("at") public static native @ByRef Tensor fractional_max_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor fractional_max_pool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor fractional_max_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor fractional_max_pool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor fractional_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByVal Tensor fractional_max_pool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool2d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool2d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool2d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool2d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool2d_with_indices_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool2d_with_indices_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor max_pool2d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor max_pool2d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor max_pool2d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor max_pool2d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByRef Tensor max_pool2d_with_indices_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor max_pool2d_with_indices_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor max_pool2d_with_indices_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor max_pool2d_with_indices_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor max_pool2d_with_indices_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByVal Tensor max_pool2d_with_indices_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool3d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool3d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool3d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool3d_with_indices_out(@ByRef Tensor out, @ByRef Tensor indices, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool3d_with_indices_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> max_pool3d_with_indices_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @ByRef Tensor out, @ByRef Tensor indices);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor max_pool3d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor max_pool3d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor max_pool3d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal(nullValue = "c10::IntArrayRef{}") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode/*=false*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor max_pool3d_with_indices(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByRef Tensor max_pool3d_with_indices_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor max_pool3d_with_indices_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor max_pool3d_with_indices_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor max_pool3d_with_indices_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor max_pool3d_with_indices_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByVal Tensor max_pool3d_with_indices_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Cast("bool") boolean ceil_mode, @Const @ByRef Tensor indices);
@Namespace("at") public static native @ByRef Tensor max_unpool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor max_unpool2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... output_size);
@Namespace("at") public static native @ByRef Tensor max_unpool2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor max_unpool2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor max_unpool2d(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor max_unpool2d(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... output_size);
@Namespace("at") public static native @ByRef Tensor max_unpool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor max_unpool2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... output_size);
@Namespace("at") public static native @ByRef Tensor max_unpool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor max_unpool2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor max_unpool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor max_unpool2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... output_size);
@Namespace("at") public static native @ByRef Tensor max_unpool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor max_unpool3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor max_unpool3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor max_unpool3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor max_unpool3d(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor max_unpool3d(@Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor max_unpool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor max_unpool3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor max_unpool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor max_unpool3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor max_unpool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor max_unpool3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor indices, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor reflection_pad1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor reflection_pad1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor reflection_pad1d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor reflection_pad1d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor reflection_pad1d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor reflection_pad2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor reflection_pad2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor reflection_pad2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor reflection_pad2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor reflection_pad2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor replication_pad1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor replication_pad1d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor replication_pad1d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor replication_pad1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor replication_pad1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor replication_pad1d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor replication_pad1d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor replication_pad1d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad1d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor replication_pad2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor replication_pad2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor replication_pad2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor replication_pad2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor replication_pad2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor replication_pad2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor replication_pad2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor replication_pad2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor replication_pad3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor replication_pad3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor replication_pad3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor replication_pad3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor replication_pad3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor replication_pad3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor replication_pad3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor replication_pad3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor replication_pad3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor replication_pad3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d(@Const @ByRef Tensor input, @ByVal LongArrayRefOptional output_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward(@Const @ByRef Tensor grad_output, @ByVal LongArrayRefOptional output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal DoubleArrayRefOptional scale_factors);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_linear1d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_linear1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_bilinear2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bilinear2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_bicubic2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_bicubic2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_trilinear3d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_trilinear3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @Cast("bool") boolean align_corners);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... output_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal DoubleOptional scales, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal DoubleOptional scales, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... output_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... input_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal DoubleOptional scales, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_nearest1d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal DoubleOptional scales, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales);
@Namespace("at") public static native @ByVal Tensor upsample_nearest1d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... input_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... output_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... output_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... input_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_nearest2d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest2d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... input_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... output_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... output_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... input_size);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor upsample_nearest3d_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal DoubleOptional scales_d, @ByVal DoubleOptional scales_h, @ByVal DoubleOptional scales_w, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_d, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_h, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional scales_w);
@Namespace("at") public static native @ByVal Tensor upsample_nearest3d_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... input_size);
@Namespace("at") public static native @ByRef Tensor sigmoid_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output);
@Namespace("at") public static native @ByRef Tensor sigmoid_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor sigmoid_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output);
@Namespace("at") public static native @ByRef Tensor logit_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByRef Tensor logit_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor logit_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal DoubleOptional eps, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor logit_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional eps);
@Namespace("at") public static native @ByVal Tensor logit_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor tanh_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor output);
@Namespace("at") public static native @ByRef Tensor tanh_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor tanh_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor output);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv_transpose2d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Const @ByRef Tensor columns, @Const @ByRef Tensor ones);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv_transpose2d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Const @ByRef Tensor columns, @Const @ByRef Tensor ones);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv_transpose2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Const @ByRef Tensor columns, @Const @ByRef Tensor ones, @ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv_transpose2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Const @ByRef Tensor columns, @Const @ByRef Tensor ones, @ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor slow_conv_transpose2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Const @ByRef Tensor columns, @Const @ByRef Tensor ones, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor slow_conv_transpose2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Const @ByRef Tensor columns, @Const @ByRef Tensor ones, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv_transpose3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_transpose3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv_transpose3d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv_transpose3d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv_transpose3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input, @ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv_transpose3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input, @ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor slow_conv_transpose3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor slow_conv_transpose3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor thnn_conv2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor thnn_conv2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor thnn_conv2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor thnn_conv2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByVal Tensor thnn_conv2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> thnn_conv2d_forward_out(@ByRef Tensor output, @ByRef Tensor finput, @ByRef Tensor fgrad_input, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> thnn_conv2d_forward_out(@ByRef Tensor output, @ByRef Tensor finput, @ByRef Tensor fgrad_input, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> thnn_conv2d_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor output, @ByRef Tensor finput, @ByRef Tensor fgrad_input);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> thnn_conv2d_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor output, @ByRef Tensor finput, @ByRef Tensor fgrad_input);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor thnn_conv2d_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor thnn_conv2d_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> thnn_conv2d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> thnn_conv2d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> thnn_conv2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input, @ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> thnn_conv2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input, @ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor thnn_conv2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor thnn_conv2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByRef Tensor thnn_conv_depthwise2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor thnn_conv_depthwise2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor thnn_conv_depthwise2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByRef Tensor thnn_conv_depthwise2d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByRef Tensor thnn_conv_depthwise2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor thnn_conv_depthwise2d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor thnn_conv_depthwise2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor thnn_conv_depthwise2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor thnn_conv_depthwise2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByVal Tensor thnn_conv_depthwise2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByRef Tensor thnn_conv_depthwise2d_forward_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByRef Tensor thnn_conv_depthwise2d_forward_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByRef Tensor thnn_conv_depthwise2d_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor thnn_conv_depthwise2d_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor thnn_conv_depthwise2d_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor thnn_conv_depthwise2d_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> thnn_conv_depthwise2d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> thnn_conv_depthwise2d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> thnn_conv_depthwise2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByRef Tensor grad_input, @ByRef Tensor grad_weight);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> thnn_conv_depthwise2d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByRef Tensor grad_input, @ByRef Tensor grad_weight);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor thnn_conv_depthwise2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor thnn_conv_depthwise2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast("std::array<bool,2>*") BoolPointer output_mask);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor slow_conv3d_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor slow_conv3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal Tensor slow_conv3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByVal Tensor slow_conv3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv3d_forward_out(@ByRef Tensor output, @ByRef Tensor finput, @ByRef Tensor fgrad_input, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv3d_forward_out(@ByRef Tensor output, @ByRef Tensor finput, @ByRef Tensor fgrad_input, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv3d_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByRef Tensor output, @ByRef Tensor finput, @ByRef Tensor fgrad_input);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv3d_forward_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByRef Tensor output, @ByRef Tensor finput, @ByRef Tensor fgrad_input);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor slow_conv3d_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor slow_conv3d_forward(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef TensorOptional bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv3d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv3d_backward_out(@ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias, @Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input, @ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> slow_conv3d_backward_outf(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input, @ByRef Tensor grad_input, @ByRef Tensor grad_weight, @ByRef Tensor grad_bias);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor slow_conv3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor slow_conv3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @Const @ByRef Tensor finput, @Const @ByRef Tensor fgrad_input, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated2d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor slow_conv_dilated2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor slow_conv_dilated2d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @Const @ByRef(nullValue = "c10::optional<at::Tensor>{}") TensorOptional bias, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal(nullValue = "c10::IntArrayRef(0)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal(nullValue = "c10::IntArrayRef(1)") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dilation);
@Namespace("at") public static native @ByVal Tensor slow_conv_dilated3d(@Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... kernel_size);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor slow_conv_dilated3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor slow_conv_dilated3d_backward(@Const @ByRef Tensor grad_output, @Const @ByRef Tensor self, @Const @ByRef Tensor weight, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast("std::array<bool,3>*") BoolPointer output_mask);
@Namespace("at") public static native @ByRef Tensor col2im_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor col2im_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... stride);
@Namespace("at") public static native @ByRef Tensor col2im_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor col2im_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor col2im(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor col2im(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... stride);
@Namespace("at") public static native @ByRef Tensor col2im_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor col2im_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... stride);
@Namespace("at") public static native @ByRef Tensor col2im_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor col2im_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor col2im_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor col2im_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... stride);
@Namespace("at") public static native @ByVal Tensor column_stack(@ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor column_stack_out(@ByRef Tensor out, @ByVal TensorArrayRef tensors);
@Namespace("at") public static native @ByRef Tensor column_stack_outf(@ByVal TensorArrayRef tensors, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor im2col_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor im2col_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... stride);
@Namespace("at") public static native @ByRef Tensor im2col_outf(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor im2col_outf(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor im2col(@Const @ByRef Tensor self, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor im2col(@Const @ByRef Tensor self, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... stride);
@Namespace("at") public static native @ByRef Tensor im2col_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByRef Tensor im2col_backward_out(@ByRef Tensor grad_input, @Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... stride);
@Namespace("at") public static native @ByRef Tensor im2col_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByRef Tensor im2col_backward_outf(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride, @ByRef Tensor grad_input);
@Namespace("at") public static native @ByVal Tensor im2col_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef input_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride);
@Namespace("at") public static native @ByVal Tensor im2col_backward(@Const @ByRef Tensor grad_output, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] input_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dilation, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... stride);
@Namespace("at") public static native @ByVal Tensor isfinite(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor isinf(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor isposinf(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor isposinf_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor isposinf_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor isneginf(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor isneginf_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor isneginf_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor _add_batch_dim(@Const @ByRef Tensor self, @Cast("int64_t") long batch_dim, @Cast("int64_t") long level);
@Namespace("at") public static native @ByVal Tensor _remove_batch_dim(@Const @ByRef Tensor self, @Cast("int64_t") long level, @Cast("int64_t") long batch_size, @Cast("int64_t") long out_dim);
@Namespace("at") public static native @ByVal Tensor fft_fft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_fft(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_fft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_fft_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_fft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal StringOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor fft_ifft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_ifft(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_ifft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_ifft_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_ifft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal StringOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor fft_rfft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_rfft(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_rfft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_rfft_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_rfft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal StringOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor fft_irfft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_irfft(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_irfft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_irfft_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_irfft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal StringOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor fft_hfft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_hfft(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_hfft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_hfft_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_hfft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal StringOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor fft_ihfft(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_ihfft(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_ihfft_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<int64_t>(c10::nullopt)") LongOptional n, @Cast("int64_t") long dim/*=-1*/, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_ihfft_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_ihfft_outf(@Const @ByRef Tensor self, @ByVal LongOptional n, @Cast("int64_t") long dim, @ByVal StringOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor fft_fft2(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_fft2(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_fft2(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::IntArrayRef({-2,-1})") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_fft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_fft2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_fft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::IntArrayRef({-2,-1})") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_fft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal StringOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_fft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal StringOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor fft_ifft2(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_ifft2(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_ifft2(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::IntArrayRef({-2,-1})") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_ifft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_ifft2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_ifft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::IntArrayRef({-2,-1})") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_ifft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal StringOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_ifft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal StringOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor fft_rfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_rfft2(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_rfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::IntArrayRef({-2,-1})") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_rfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_rfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_rfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::IntArrayRef({-2,-1})") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_rfft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal StringOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_rfft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal StringOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor fft_irfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_irfft2(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_irfft2(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::IntArrayRef({-2,-1})") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_irfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::IntArrayRef({-2,-1})") @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_irfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_irfft2_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::IntArrayRef({-2,-1})") @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_irfft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dim, @ByVal StringOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor fft_irfft2_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] dim, @ByVal StringOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor fft_fftn(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_fftn(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_fftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_fftn_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_fftn_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal StringOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor fft_ifftn(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_ifftn(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_ifftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_ifftn_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_ifftn_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal StringOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor fft_rfftn(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_rfftn(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_rfftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_rfftn_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_rfftn_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal StringOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor fft_irfftn(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByVal Tensor fft_irfftn(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_irfftn_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional s, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @ByVal(nullValue = "c10::optional<std::string>(c10::nullopt)") StringOptional norm);
@Namespace("at") public static native @ByRef Tensor fft_irfftn_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor fft_irfftn_outf(@Const @ByRef Tensor self, @ByVal LongArrayRefOptional s, @ByVal LongArrayRefOptional dim, @ByVal StringOptional norm, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor fft_fftfreq(@Cast("int64_t") long n, double d/*=1.0*/, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor fft_fftfreq(@Cast("int64_t") long n);
@Namespace("at") public static native @ByVal Tensor fft_fftfreq(@Cast("int64_t") long n, double d, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByRef Tensor fft_fftfreq_out(@ByRef Tensor out, @Cast("int64_t") long n, double d/*=1.0*/);
@Namespace("at") public static native @ByRef Tensor fft_fftfreq_out(@ByRef Tensor out, @Cast("int64_t") long n);
@Namespace("at") public static native @ByRef Tensor fft_fftfreq_outf(@Cast("int64_t") long n, double d, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor fft_rfftfreq(@Cast("int64_t") long n, double d/*=1.0*/, @ByVal(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor fft_rfftfreq(@Cast("int64_t") long n);
@Namespace("at") public static native @ByVal Tensor fft_rfftfreq(@Cast("int64_t") long n, double d, @ByVal ScalarTypeOptional dtype, @ByVal LayoutOptional layout, @ByVal DeviceOptional device, @ByVal BoolOptional pin_memory);
@Namespace("at") public static native @ByRef Tensor fft_rfftfreq_out(@ByRef Tensor out, @Cast("int64_t") long n, double d/*=1.0*/);
@Namespace("at") public static native @ByRef Tensor fft_rfftfreq_out(@ByRef Tensor out, @Cast("int64_t") long n);
@Namespace("at") public static native @ByRef Tensor fft_rfftfreq_outf(@Cast("int64_t") long n, double d, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor fft_fftshift(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim);
@Namespace("at") public static native @ByVal Tensor fft_fftshift(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor fft_ifftshift(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim);
@Namespace("at") public static native @ByVal Tensor fft_ifftshift(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor linalg_cholesky(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor linalg_cholesky_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor linalg_cholesky_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor linalg_det(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor det(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor linalg_slogdet(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_slogdet_out(@ByRef Tensor sign, @ByRef Tensor logabsdet, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_slogdet_outf(@Const @ByRef Tensor self, @ByRef Tensor sign, @ByRef Tensor logabsdet);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _syevd_helper(@Const @ByRef Tensor self, @Cast("bool") boolean compute_eigenvectors, @StdString BytePointer uplo);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _syevd_helper(@Const @ByRef Tensor self, @Cast("bool") boolean compute_eigenvectors, @StdString String uplo);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor linalg_eigh(@Const @ByRef Tensor self, @StdString BytePointer UPLO/*="L"*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor linalg_eigh(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor linalg_eigh(@Const @ByRef Tensor self, @StdString String UPLO/*="L"*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_eigh_out(@ByRef Tensor eigvals, @ByRef Tensor eigvecs, @Const @ByRef Tensor self, @StdString BytePointer UPLO/*="L"*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_eigh_out(@ByRef Tensor eigvals, @ByRef Tensor eigvecs, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_eigh_out(@ByRef Tensor eigvals, @ByRef Tensor eigvecs, @Const @ByRef Tensor self, @StdString String UPLO/*="L"*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_eigh_outf(@Const @ByRef Tensor self, @StdString BytePointer UPLO, @ByRef Tensor eigvals, @ByRef Tensor eigvecs);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_eigh_outf(@Const @ByRef Tensor self, @StdString String UPLO, @ByRef Tensor eigvals, @ByRef Tensor eigvecs);
@Namespace("at") public static native @ByVal Tensor linalg_eigvalsh(@Const @ByRef Tensor self, @StdString BytePointer UPLO/*="L"*/);
@Namespace("at") public static native @ByVal Tensor linalg_eigvalsh(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor linalg_eigvalsh(@Const @ByRef Tensor self, @StdString String UPLO/*="L"*/);
@Namespace("at") public static native @ByRef Tensor linalg_eigvalsh_out(@ByRef Tensor out, @Const @ByRef Tensor self, @StdString BytePointer UPLO/*="L"*/);
@Namespace("at") public static native @ByRef Tensor linalg_eigvalsh_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor linalg_eigvalsh_out(@ByRef Tensor out, @Const @ByRef Tensor self, @StdString String UPLO/*="L"*/);
@Namespace("at") public static native @ByRef Tensor linalg_eigvalsh_outf(@Const @ByRef Tensor self, @StdString BytePointer UPLO, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor linalg_eigvalsh_outf(@Const @ByRef Tensor self, @StdString String UPLO, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _linalg_inv_out_helper_(@ByRef Tensor self, @ByRef Tensor infos_lu, @ByRef Tensor infos_getri);
@Namespace("at") public static native @ByVal Tensor linalg_inv(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor linalg_inv_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor linalg_inv_outf(@Const @ByRef Tensor self, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor inner(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor inner_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor inner_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor outer(@Const @ByRef Tensor self, @Const @ByRef Tensor vec2);
@Namespace("at") public static native @ByRef Tensor outer_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor vec2);
@Namespace("at") public static native @ByRef Tensor outer_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor vec2, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor ger(@Const @ByRef Tensor self, @Const @ByRef Tensor vec2);
@Namespace("at") public static native @ByRef Tensor ger_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor vec2);
@Namespace("at") public static native @ByRef Tensor ger_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor vec2, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor linalg_norm(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::Scalar>(c10::nullopt)") ScalarOptional ord, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor linalg_norm(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor linalg_norm(@Const @ByRef Tensor self, @StdString BytePointer ord, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor linalg_norm(@Const @ByRef Tensor self, @StdString BytePointer ord);
@Namespace("at") public static native @ByVal Tensor linalg_norm(@Const @ByRef Tensor self, @StdString String ord, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByVal Tensor linalg_norm(@Const @ByRef Tensor self, @StdString String ord);
@Namespace("at") public static native @ByRef Tensor linalg_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::Scalar>(c10::nullopt)") ScalarOptional ord, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor linalg_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor linalg_norm_outf(@Const @ByRef Tensor self, @ByVal ScalarOptional ord, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor linalg_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @StdString BytePointer ord, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor linalg_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @StdString BytePointer ord);
@Namespace("at") public static native @ByRef Tensor linalg_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @StdString String ord, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dim, @Cast("bool") boolean keepdim/*=false*/, @ByVal(nullValue = "c10::optional<at::ScalarType>(c10::nullopt)") ScalarTypeOptional dtype);
@Namespace("at") public static native @ByRef Tensor linalg_norm_out(@ByRef Tensor out, @Const @ByRef Tensor self, @StdString String ord);
@Namespace("at") public static native @ByRef Tensor linalg_norm_outf(@Const @ByRef Tensor self, @StdString BytePointer ord, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor linalg_norm_outf(@Const @ByRef Tensor self, @StdString String ord, @ByVal LongArrayRefOptional dim, @Cast("bool") boolean keepdim, @ByVal ScalarTypeOptional dtype, @ByRef Tensor out);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_svd_out(@ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor V, @Const @ByRef Tensor self, @Cast("bool") boolean full_matrices/*=true*/, @Cast("bool") boolean compute_uv/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_svd_out(@ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor V, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_svd_outf(@Const @ByRef Tensor self, @Cast("bool") boolean full_matrices, @Cast("bool") boolean compute_uv, @ByRef Tensor U, @ByRef Tensor S, @ByRef Tensor V);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor linalg_svd(@Const @ByRef Tensor self, @Cast("bool") boolean full_matrices/*=true*/, @Cast("bool") boolean compute_uv/*=true*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor,at::Tensor>*") Tensor linalg_svd(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor linalg_cond(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::Scalar>(c10::nullopt)") ScalarOptional p);
@Namespace("at") public static native @ByVal Tensor linalg_cond(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor linalg_cond_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<c10::Scalar>(c10::nullopt)") ScalarOptional p);
@Namespace("at") public static native @ByRef Tensor linalg_cond_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor linalg_cond_outf(@Const @ByRef Tensor self, @ByVal ScalarOptional p, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor linalg_cond(@Const @ByRef Tensor self, @StdString BytePointer p);
@Namespace("at") public static native @ByVal Tensor linalg_cond(@Const @ByRef Tensor self, @StdString String p);
@Namespace("at") public static native @ByRef Tensor linalg_cond_out(@ByRef Tensor out, @Const @ByRef Tensor self, @StdString BytePointer p);
@Namespace("at") public static native @ByRef Tensor linalg_cond_out(@ByRef Tensor out, @Const @ByRef Tensor self, @StdString String p);
@Namespace("at") public static native @ByRef Tensor linalg_cond_outf(@Const @ByRef Tensor self, @StdString BytePointer p, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor linalg_cond_outf(@Const @ByRef Tensor self, @StdString String p, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self, double rcond/*=1e-15*/, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self, @Const @ByRef Tensor rcond, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_pinv(@Const @ByRef Tensor self, @Const @ByRef Tensor rcond);
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, double rcond/*=1e-15*/, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor linalg_pinv_outf(@Const @ByRef Tensor self, double rcond, @Cast("bool") boolean hermitian, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor rcond, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_pinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor rcond);
@Namespace("at") public static native @ByRef Tensor linalg_pinv_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor rcond, @Cast("bool") boolean hermitian, @ByRef Tensor out);
@Namespace("at") public static native @ByRef Tensor _linalg_solve_out_helper_(@ByRef Tensor self, @ByRef Tensor other, @ByRef Tensor infos);
@Namespace("at") public static native @ByVal Tensor linalg_solve(@Const @ByRef Tensor input, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor linalg_solve_out(@ByRef Tensor out, @Const @ByRef Tensor input, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor linalg_solve_outf(@Const @ByRef Tensor input, @Const @ByRef Tensor other, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor linalg_tensorinv(@Const @ByRef Tensor self, @Cast("int64_t") long ind/*=2*/);
@Namespace("at") public static native @ByVal Tensor linalg_tensorinv(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor linalg_tensorinv_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Cast("int64_t") long ind/*=2*/);
@Namespace("at") public static native @ByRef Tensor linalg_tensorinv_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor linalg_tensorinv_outf(@Const @ByRef Tensor self, @Cast("int64_t") long ind, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor linalg_tensorsolve(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dims);
@Namespace("at") public static native @ByVal Tensor linalg_tensorsolve(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor linalg_tensorsolve_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::optional<c10::IntArrayRef>(c10::nullopt)") LongArrayRefOptional dims);
@Namespace("at") public static native @ByRef Tensor linalg_tensorsolve_out(@ByRef Tensor out, @Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByRef Tensor linalg_tensorsolve_outf(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal LongArrayRefOptional dims, @ByRef Tensor out);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor linalg_qr(@Const @ByRef Tensor self, @StdString BytePointer mode/*="reduced"*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor linalg_qr(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor linalg_qr(@Const @ByRef Tensor self, @StdString String mode/*="reduced"*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_qr_out(@ByRef Tensor Q, @ByRef Tensor R, @Const @ByRef Tensor self, @StdString BytePointer mode/*="reduced"*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_qr_out(@ByRef Tensor Q, @ByRef Tensor R, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_qr_out(@ByRef Tensor Q, @ByRef Tensor R, @Const @ByRef Tensor self, @StdString String mode/*="reduced"*/);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_qr_outf(@Const @ByRef Tensor self, @StdString BytePointer mode, @ByRef Tensor Q, @ByRef Tensor R);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor&,at::Tensor&>*") PointerPointer<Tensor> linalg_qr_outf(@Const @ByRef Tensor self, @StdString String mode, @ByRef Tensor Q, @ByRef Tensor R);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _linalg_qr_helper(@Const @ByRef Tensor self, @StdString BytePointer mode);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor _linalg_qr_helper(@Const @ByRef Tensor self, @StdString String mode);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_rank(@Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional tol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByVal Tensor linalg_matrix_rank(@Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_out(@ByRef Tensor out, @Const @ByRef Tensor self, @ByVal(nullValue = "c10::optional<double>(c10::nullopt)") DoubleOptional tol, @Cast("bool") boolean hermitian/*=false*/);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_out(@ByRef Tensor out, @Const @ByRef Tensor self);
@Namespace("at") public static native @ByRef Tensor linalg_matrix_rank_outf(@Const @ByRef Tensor self, @ByVal DoubleOptional tol, @Cast("bool") boolean hermitian, @ByRef Tensor out);
@Namespace("at") public static native @ByVal Tensor _test_serialization_subcmul(@Const @ByRef Tensor self, @Const @ByRef Tensor other, @ByVal(nullValue = "c10::Scalar(1)") Scalar alpha);
@Namespace("at") public static native @ByVal Tensor _test_serialization_subcmul(@Const @ByRef Tensor self, @Const @ByRef Tensor other);
@Namespace("at") public static native @ByVal Tensor _test_optional_intlist(@Const @ByRef Tensor values, @ByVal LongArrayRefOptional addends);
@Namespace("at") public static native @ByVal Tensor _test_optional_filled_intlist(@Const @ByRef Tensor values, @ByVal LongArrayRefOptional addends);
@Namespace("at") public static native @ByVal Tensor _test_optional_floatlist(@Const @ByRef Tensor values, @ByVal DoubleArrayRefOptional addends);
@Namespace("at") public static native @ByVal Tensor _test_string_default(@Const @ByRef Tensor dummy, @StdString BytePointer a/*="\"'\\"*/, @StdString BytePointer b/*="\"'\\"*/);
@Namespace("at") public static native @ByVal Tensor _test_string_default(@Const @ByRef Tensor dummy);
@Namespace("at") public static native @ByVal Tensor _test_string_default(@Const @ByRef Tensor dummy, @StdString String a/*="\"'\\"*/, @StdString String b/*="\"'\\"*/);
@Namespace("at") public static native @ByVal Tensor _test_ambiguous_defaults(@Const @ByRef Tensor dummy, @Cast("int64_t") long a/*=1*/, @Cast("int64_t") long b/*=1*/);
@Namespace("at") public static native @ByVal Tensor _test_ambiguous_defaults(@Const @ByRef Tensor dummy);
@Namespace("at") public static native @ByVal Tensor _test_ambiguous_defaults(@Const @ByRef Tensor dummy, @Cast("int64_t") long a, @StdString BytePointer b);
@Namespace("at") public static native @ByVal Tensor _test_ambiguous_defaults(@Const @ByRef Tensor dummy, @Cast("int64_t") long a, @StdString String b);

// Special C++ only overloads for std()-like functions (See gh-40287)
// These are needed because int -> bool conversion takes precedence over int -> IntArrayRef
// So, for example std(0) would select the std(unbiased=False) overload
@Namespace("at") public static native @ByVal Tensor var(@Const @ByRef Tensor self, int dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor var_mean(@Const @ByRef Tensor self, int dim);
@Namespace("at") public static native @ByVal Tensor std(@Const @ByRef Tensor self, int dim);
@Namespace("at") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor std_mean(@Const @ByRef Tensor self, int dim);
  @Namespace("at") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector zero_sizes(@Const @ByRef TensorOptions options);


@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @Const @ByRef Deleter deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options,
    @Const @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional target_device);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @Const @ByRef Deleter deleter);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @Const @ByRef Deleter deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options,
    @Const @ByVal(nullValue = "c10::optional<c10::Device>(c10::nullopt)") DeviceOptional target_device);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @Const @ByRef Deleter deleter);

@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @Const @ByRef Deleter deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @Const @ByRef Deleter deleter);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @Const @ByRef Deleter deleter,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @Const @ByRef Deleter deleter);

@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef strides);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] strides,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... strides);

@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] sizes,
    @Const @ByRef(nullValue = "c10::TensorOptions{}") TensorOptions options);
@Namespace("at") public static native @ByVal Tensor from_blob(
    Pointer data,
    @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... sizes);

@Namespace("at") public static native @Cast("int64_t") long numel(@Const @ByRef Tensor tensor);

@Namespace("at") public static native @Cast("int64_t") long size(@Const @ByRef Tensor tensor, @Cast("int64_t") long dim);

@Namespace("at") public static native @Cast("int64_t") long stride(@Const @ByRef Tensor tensor, @Cast("int64_t") long dim);




// Parsed from ATen/NamedTensor.h

// #include <ATen/core/NamedTensor.h>


// Parsed from ATen/NamedTensorUtils.h

// #pragma once
// #include <ATen/NamedTensor.h>
// #include <ATen/TensorNames.h>

// #include <ATen/core/Tensor.h>
// #include <ATen/core/DimVector.h>
// #include <functional>

@Namespace("at") public static native @Cast("bool") boolean has_names(@ByVal TensorArrayRef tensors);

// Converts dim to an positional index. Errors if `dim` cannot be used to
// refer to any dimension of tensor.
@Namespace("at") public static native @Cast("int64_t") long dimname_to_position(@Const @ByRef Tensor tensor, @ByVal Dimname dim);
@Namespace("at") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector dimnames_to_positions(@Const @ByRef Tensor tensor, @ByVal DimnameArrayRef dims);

// Unifies two DimnameList to produce a third. This is useful for implementing
// the named inference rule for binary broadcasting operations like add.
//
// There are three main constraints:
// 1) Check matching: Names must match positionally from the right.
// 2) Check misaligned: If a name `n` is in `names`, then it must appear at
//    the same index from the right in other.
// 3) The output names are obtained by unifying the names individually from the right.
@Namespace("at") public static native @StdMove DimnameVector unify_from_right(@ByVal DimnameArrayRef names, @ByVal DimnameArrayRef other, @Cast("const char*") BytePointer action/*="broadcast"*/);
@Namespace("at") public static native @StdMove DimnameVector unify_from_right(@ByVal DimnameArrayRef names, @ByVal DimnameArrayRef other);
@Namespace("at") public static native @StdMove DimnameVector unify_from_right(@ByVal DimnameArrayRef names, @ByVal DimnameArrayRef other, String action/*="broadcast"*/);

@Namespace("at") public static native void reportNYIDimnameOverload(@Cast("const char*") BytePointer op_name);
@Namespace("at") public static native void reportNYIDimnameOverload(String op_name);

// [NOTE] Writing name inference rules
//
// Operators that support named tensors are either composed of operations that
// support named tensors or implement some name inference rule. An op that
// implements its own name inference rule generally looks like the following:
//
// Tensor op(...) {
//   perform_shape_checks(...);
//   # (1)
//   auto maybe_outnames = compute_outnames(...);
//   auto result = [&]() {
//     NoNamesGuard guard;
//     return op_impl(...);
//   }();
//   # (2)
//   propagate_names_if_nonempty(result, maybe_outnames);
//
// Each op has (1) a compute outnames step and (2) a propagate names step.
//
// compute_outnames is responsible for checking that input names match and
// determining what the output names should be. It returns either:
// - {} (if the inputs tensors are all unnamed)
// - non-empty outnames.
//
// propagate_names_if_nonempty propagates the outnames if they exist to the result
// tensors.
//
// The {} case is an optimization; if the user does not use named tensors they
// pay no perf cost for it.

// Propagates `names` to `result` if `names` is not empty.
// `names` can be empty; see [NOTE] Writing name inference rules
// If `names` is not empty, `names.size()` should equal `result.dim()`.
// When in doubt, use this overload instead of the others.
@Namespace("at::namedinference") public static native @ByRef Tensor propagate_names_if_nonempty(
    @ByRef Tensor result,
    @ByVal DimnameArrayRef maybe_names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native @ByRef Tensor propagate_names_if_nonempty(
    @ByRef Tensor result,
    @ByVal DimnameArrayRef maybe_names);

// Propagates `names` to `result`. Only use this if we are certain that there are
// names to propagate (that names is not empty).
@Namespace("at::namedinference") public static native @ByRef Tensor propagate_names(
    @ByRef Tensor result,
    @ByVal DimnameArrayRef names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native @ByRef Tensor propagate_names(
    @ByRef Tensor result,
    @ByVal DimnameArrayRef names);

// Propagates all names from src to result.
@Namespace("at::namedinference") public static native void propagate_names(@ByRef Tensor result, @Const @ByRef Tensor src);

// Propagates all names except for those at the excluded_idxs.
@Namespace("at::namedinference") public static native void propagate_names_except(@ByRef Tensor result, @Const @ByRef Tensor src, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef excluded_idxs);
@Namespace("at::namedinference") public static native void propagate_names_except(@ByRef Tensor result, @Const @ByRef Tensor src, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... excluded_idxs);

// Used for reduction ops that have a `keepdim` arg.
@Namespace("at::namedinference") public static native void propagate_names_for_reduction(@ByRef Tensor result, @Const @ByRef Tensor src, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef excluded_idxs, @Cast("bool") boolean keepdim);
@Namespace("at::namedinference") public static native void propagate_names_for_reduction(@ByRef Tensor result, @Const @ByRef Tensor src, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] excluded_idxs, @Cast("bool") boolean keepdim);

@Namespace("at::namedinference") public static native void propagate_names_for_expand(@ByRef Tensor result, @Const @ByRef Tensor self);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_cat_outnames(@ByVal TensorArrayRef tensors);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_broadcast_outnames(
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other);

@Namespace("at::namedinference") public static native @StdMove DimnameVector broadcast_to_outnames(
    @Const @ByRef Tensor tensor,
    @Const @ByRef Tensor reference_tensor,
    @Cast("const char*") BytePointer op_name);
@Namespace("at::namedinference") public static native @StdMove DimnameVector broadcast_to_outnames(
    @Const @ByRef Tensor tensor,
    @Const @ByRef Tensor reference_tensor,
    String op_name);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_matmul_outnames(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_cdist_outnames(@Const @ByRef Tensor self, @Const @ByRef Tensor other);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_bmm_outnames(
    @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_squeeze_outnames(@Const @ByRef Tensor tensor);



// TensorImpl* overloads for Legacy TH/THC code. Use these sparingly.

@Namespace("at::namedinference") public static native TensorImpl propagate_names_if_nonempty(
    TensorImpl result,
    @ByVal DimnameArrayRef maybe_names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native TensorImpl propagate_names_if_nonempty(
    TensorImpl result,
    @ByVal DimnameArrayRef maybe_names);

@Namespace("at::namedinference") public static native TensorImpl propagate_names(
    TensorImpl result,
    @ByVal DimnameArrayRef names,
    @Cast("bool") boolean validate_names/*=false*/);
@Namespace("at::namedinference") public static native TensorImpl propagate_names(
    TensorImpl result,
    @ByVal DimnameArrayRef names);

@Namespace("at::namedinference") public static native void propagate_names(TensorImpl result,TensorImpl src);

// result = m1 @ m2 + bias
@Namespace("at::namedinference") public static native void propagate_names_for_addmm(
    @ByRef Tensor result,
    @Const @ByRef Tensor m1,
    @Const @ByRef Tensor m2,
    @Const @ByRef Tensor bias);

@Namespace("at::namedinference") public static native void propagate_names_for_addmv(
    @ByRef Tensor result,
    @Const @ByRef Tensor mat,
    @Const @ByRef Tensor vec,
    @Const @ByRef Tensor bias);

@Namespace("at::namedinference") public static native void check_names_for_dot(TensorImpl vec1, TensorImpl vec2);

@Namespace("at::namedinference") public static native @StdMove DimnameVector compute_baddbmm_outnames(
    @ByRef Tensor result,
    @Const @ByRef Tensor self,
    @Const @ByRef Tensor other,
    @Const @ByRef Tensor bias);

@Namespace("at::namedinference") public static native @Cast("bool") boolean are_names_equal(TensorImpl self, TensorImpl other);

 // namespace namedinference

 // namespace at


// Parsed from ATen/ScalarOps.h

// #pragma once

// #include <c10/core/Scalar.h>
// #include <ATen/Tensor.h>
// #include <ATen/Functions.h>
// When filling a number to 1-element CPU tensor, we want to skip
// everything but manipulate data ptr directly.
// Ideally this fast pass should be implemented in TensorIterator,
// but we also want to skip compute_types which in not avoidable
// in TensorIterator for now.

@Namespace("at::detail") public static native @ByVal Tensor scalar_tensor_static(@ByVal Scalar s, @ByVal ScalarTypeOptional dtype_opt, @ByVal DeviceOptional device_opt);
 // namespace detail
 // namespace at

// This is in the c10 namespace because we use ADL to find the functions in it.

// FIXME: this should be (and was) Scalar::toTensor, but there is currently no way
// to implement this without going through Derived Types (which are not part of core).
@Namespace("c10") public static native @ByVal Tensor scalar_to_tensor(@ByVal Scalar s, @Const @ByVal(nullValue = "c10::Device(at::kCPU)") Device device);
@Namespace("c10") public static native @ByVal Tensor scalar_to_tensor(@ByVal Scalar s);




// Parsed from ATen/TensorIndexing.h

// #pragma once

// #include <c10/util/Optional.h>
// #include <ATen/core/TensorBody.h>
// #include <ATen/ExpandUtils.h>
// #include <ATen/Functions.h>
// #include <ATen/ScalarOps.h>

// TODO: try to remove this
// There is some back story, see https://github.com/pytorch/pytorch/issues/48684
// #include <ATen/NativeFunctions.h>

// #include <ATen/core/List.h>

@Namespace("at::indexing") @MemberGetter public static native @Cast("const int64_t") long INDEX_MAX();
@Namespace("at::indexing") @MemberGetter public static native @Cast("const int64_t") long INDEX_MIN();

@Namespace("at::indexing") public enum TensorIndexType { None(0), Ellipsis(1), Integer(2), Boolean(3), Slice(4), Tensor(5);

    public final int value;
    private TensorIndexType(int v) { this.value = v; }
    private TensorIndexType(TensorIndexType e) { this.value = e.value; }
    public TensorIndexType intern() { for (TensorIndexType e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

@Namespace("at::indexing") @MemberGetter public static native @ByRef @Cast("const c10::nullopt_t*") Pointer None();
// Targeting ../EllipsisIndexType.java


@Namespace("at::indexing") @MemberGetter public static native @Const @ByRef EllipsisIndexType Ellipsis();
// Targeting ../Slice.java



@Namespace("at::indexing") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer stream, @Const @ByRef Slice slice);
// Targeting ../TensorIndex.java



@Namespace("at::indexing") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer stream, @Const @ByRef TensorIndex tensor_index);
@Namespace("at::indexing") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer stream, @Const @ByRef TensorIndexVector tensor_indices);
@Namespace("at::indexing::impl") public static native @ByVal Tensor applySlice(
    @Const @ByRef Tensor self,
    @Cast("int64_t") long dim,
    @Cast("int64_t") long start,
    @Cast("int64_t") long stop,
    @Cast("int64_t") long step,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device self_device,
    @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef self_sizes);
@Namespace("at::indexing::impl") public static native @ByVal Tensor applySlice(
    @Const @ByRef Tensor self,
    @Cast("int64_t") long dim,
    @Cast("int64_t") long start,
    @Cast("int64_t") long stop,
    @Cast("int64_t") long step,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device self_device,
    @ByRef @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... self_sizes);

@Namespace("at::indexing::impl") public static native @ByVal Tensor applySelect(
    @Const @ByRef Tensor self,
    @Cast("int64_t") long dim,
    @Cast("int64_t") long index,
    @Cast("int64_t") long real_dim,
    @Const @ByRef Device self_device,
    @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef self_sizes);
@Namespace("at::indexing::impl") public static native @ByVal Tensor applySelect(
    @Const @ByRef Tensor self,
    @Cast("int64_t") long dim,
    @Cast("int64_t") long index,
    @Cast("int64_t") long real_dim,
    @Const @ByRef Device self_device,
    @ByRef @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... self_sizes);

@Namespace("at::indexing::impl") public static native @ByVal Tensor boolToIndexingTensorCPUOrCUDA(@Const @ByRef Tensor self, @Cast("bool") boolean value);

@Namespace("at::indexing::impl") public static native @ByVal Tensor boolToIndexingTensorNonNativeDeviceType(@Const @ByRef Tensor self, @Cast("bool") boolean value);

@Namespace("at::indexing::impl") public static native @ByVal Tensor boolToIndexingTensor(@Const @ByRef Tensor self, @Cast("bool") boolean value, @Const @ByRef Device self_device);

@Namespace("at::indexing::impl") public static native @ByVal Tensor scalarToTensorNonNativeDeviceType(@ByVal Scalar v, @Const @ByRef TensorOptions options);

@Namespace("at::indexing::impl") public static native void recordTensorIndex(@Const @ByRef Tensor tensor, @ByRef TensorVector outIndices, @Cast("int64_t*") LongPointer dim_ptr);
@Namespace("at::indexing::impl") public static native void recordTensorIndex(@Const @ByRef Tensor tensor, @ByRef TensorVector outIndices, @Cast("int64_t*") LongBuffer dim_ptr);
@Namespace("at::indexing::impl") public static native void recordTensorIndex(@Const @ByRef Tensor tensor, @ByRef TensorVector outIndices, @Cast("int64_t*") long[] dim_ptr);

// NOTE: Why do we mirror instead of replace the `count_specified_dimensions` function
// in torch/csrc/autograd/python_variable_indexing.cpp? It's because
// `count_specified_dimensions` is on the hot path of Python tensor multi-dim indexing
// (i.e. it's called by `applySlicing` which is called by `THPVariable_getitem` /
// `THPVariable_setitem` when handling indexing of more than one dimension). If we were
// to merge the Python/C++ `count_specified_dimensions` function, on the Python side
// we would have to construct a `std::vector` container to be consumed by the C++
// `count_specified_dimensions` function, which adds 100s of nanoseconds overhead and
// is undesirable.
@Namespace("at::indexing::impl") public static native @Cast("int64_t") long count_specified_dimensions(@Const @ByRef TensorIndexArrayRef indices);
 // namespace impl

// NOTE: Many functions below are only for consumption from Python indexing
// implementation, they include:
//
// - `Tensor scalarToTensor(...)`
// - `IntArrayRef slicePrefix1sSize(...)`
// - `void copy_to(...)`
// - `Tensor handleDimInMultiDimIndexing(...)`
// - `Tensor dispatch_index(...)`
// - `Tensor dispatch_index_put_(...)`
// - `Tensor get_item(...)`
// - `void set_item(...)`
//
// The rest of the functions are in `at::indexing::impl` namespace, signifying
// that they shouldn't be used from Python indexing implementation.
@Namespace("at::indexing") public static native @ByVal Tensor scalarToTensor(@ByVal Scalar v, @Const @ByRef TensorOptions options, @Const @ByRef Device self_device);

// To match numpy semantics:
// As a special case for backwards compatibility,
// strip away unit dimensions from the left of 'src'
@Namespace("at::indexing") public static native @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef slicePrefix1sSize(@ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef sizes);

@Namespace("at::indexing") public static native void copy_to(@Const @ByRef Tensor dst, @Const @ByRef Tensor src);

// See NOTE [ Setting `disable_slice_optimization` when calling C++ tensor indexing functions from Python ]
@Namespace("at::indexing") public static native @ByVal Tensor handleDimInMultiDimIndexing(
    @Const @ByRef Tensor prev_dim_result,
    @Const @ByRef Tensor original_tensor,
    @Const @ByRef TensorIndex index,
    @Cast("int64_t*") LongPointer dim_ptr,
    @Cast("int64_t*") LongPointer specified_dims_ptr,
    @Cast("int64_t") long real_dim,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device original_tensor_device,
    @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef prev_dim_result_sizes);
@Namespace("at::indexing") public static native @ByVal Tensor handleDimInMultiDimIndexing(
    @Const @ByRef Tensor prev_dim_result,
    @Const @ByRef Tensor original_tensor,
    @Const @ByRef TensorIndex index,
    @Cast("int64_t*") LongBuffer dim_ptr,
    @Cast("int64_t*") LongBuffer specified_dims_ptr,
    @Cast("int64_t") long real_dim,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device original_tensor_device,
    @ByRef @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... prev_dim_result_sizes);
@Namespace("at::indexing") public static native @ByVal Tensor handleDimInMultiDimIndexing(
    @Const @ByRef Tensor prev_dim_result,
    @Const @ByRef Tensor original_tensor,
    @Const @ByRef TensorIndex index,
    @Cast("int64_t*") long[] dim_ptr,
    @Cast("int64_t*") long[] specified_dims_ptr,
    @Cast("int64_t") long real_dim,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device original_tensor_device,
    @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef prev_dim_result_sizes);
@Namespace("at::indexing") public static native @ByVal Tensor handleDimInMultiDimIndexing(
    @Const @ByRef Tensor prev_dim_result,
    @Const @ByRef Tensor original_tensor,
    @Const @ByRef TensorIndex index,
    @Cast("int64_t*") LongPointer dim_ptr,
    @Cast("int64_t*") LongPointer specified_dims_ptr,
    @Cast("int64_t") long real_dim,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device original_tensor_device,
    @ByRef @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... prev_dim_result_sizes);
@Namespace("at::indexing") public static native @ByVal Tensor handleDimInMultiDimIndexing(
    @Const @ByRef Tensor prev_dim_result,
    @Const @ByRef Tensor original_tensor,
    @Const @ByRef TensorIndex index,
    @Cast("int64_t*") LongBuffer dim_ptr,
    @Cast("int64_t*") LongBuffer specified_dims_ptr,
    @Cast("int64_t") long real_dim,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device original_tensor_device,
    @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef prev_dim_result_sizes);
@Namespace("at::indexing") public static native @ByVal Tensor handleDimInMultiDimIndexing(
    @Const @ByRef Tensor prev_dim_result,
    @Const @ByRef Tensor original_tensor,
    @Const @ByRef TensorIndex index,
    @Cast("int64_t*") long[] dim_ptr,
    @Cast("int64_t*") long[] specified_dims_ptr,
    @Cast("int64_t") long real_dim,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device original_tensor_device,
    @ByRef @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... prev_dim_result_sizes);
// This mirrors `applySlicing` in torch/csrc/autograd/python_variable_indexing.cpp
@Namespace("at::indexing::impl") public static native @ByVal Tensor applySlicing(
    @Const @ByRef Tensor self,
    @Const @ByRef TensorIndexArrayRef indices,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device self_device,
    @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef self_sizes);
@Namespace("at::indexing::impl") public static native @ByVal Tensor applySlicing(
    @Const @ByRef Tensor self,
    @Const @ByRef TensorIndexArrayRef indices,
    @ByRef TensorVector outIndices,
    @Cast("bool") boolean disable_slice_optimization,
    @Const @ByRef Device self_device,
    @ByRef @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... self_sizes);
 // namespace impl

@Namespace("at::indexing") public static native @ByVal Tensor dispatch_index(@Const @ByRef Tensor self, @StdMove TensorVector indices);

@Namespace("at::indexing") public static native @ByVal Tensor dispatch_index_put_(@ByRef Tensor self, @StdMove TensorVector indices, @Const @ByRef Tensor value);

// NOTE [ Setting `disable_slice_optimization` when calling C++ tensor indexing functions from Python ]
//
// Question: When should we set `disable_slice_optimization` to `true` when calling C++ tensor indexing
// functions from Python indexing code?
//
// Answer: What "slice optimization" means: when we have a slicing expression like `x[0:5, 0]`, where the sliced tensor
// was of size 5 in dimension 0, we would skip dispatching the actual slice call as an optimization. However, here are
// the cases where we DON'T want this optimization:
//
// 1. When we are doing 1-D slicing (e.g. `tensor[:]`).
//    Reason: we always return a shallow copy for expressions such as `tensor[:]` / `tensor[...]` / `tensor[:, :]`.
//    (Note that for `tensor[:, :]`, we return an alias of `tensor` by doing the following:
//    ```
//    Tensor sliced = impl::applySlicing(self, indices, tensorIndices, disable_slice_optimization, self_device, self_sizes);
//    if (tensorIndices.empty()) {
//      if (sliced.is_same(self)) {
//        // ensure we return a shallow copy for things like x[...]
//        sliced = at::alias(sliced);
//      }
//      return sliced;
//    }
//    ```)
// 2. When we are doing JIT tracing.
//    Reason: JIT tracing needs the `self.slice(...)` call to properly trace the slice operation.

// This mirrors `THPVariable_getitem` in torch/csrc/autograd/python_variable_indexing.cpp
// See NOTE [ Setting `disable_slice_optimization` when calling C++ tensor indexing functions from Python ]
@Namespace("at::indexing") public static native @ByVal Tensor get_item(@Const @ByRef Tensor self, @Const @ByRef TensorIndexArrayRef indices, @Cast("bool") boolean disable_slice_optimization/*=false*/);
@Namespace("at::indexing") public static native @ByVal Tensor get_item(@Const @ByRef Tensor self, @Const @ByRef TensorIndexArrayRef indices);

// This mirrors `THPVariable_setitem` in torch/csrc/autograd/python_variable_indexing.cpp
// for "the assigned value is a Tensor" case
// See NOTE [ Setting `disable_slice_optimization` when calling C++ tensor indexing functions from Python ]
@Namespace("at::indexing") public static native void set_item(@ByRef Tensor self, @Const @ByRef TensorIndexArrayRef indices, @Const @ByRef Tensor value, @Cast("bool") boolean disable_slice_optimization/*=false*/);
@Namespace("at::indexing") public static native void set_item(@ByRef Tensor self, @Const @ByRef TensorIndexArrayRef indices, @Const @ByRef Tensor value);

 // namespace indexing
 // namespace at


// Parsed from ATen/TensorOperators.h

// #pragma once

// #include <c10/core/Scalar.h>
// #include <ATen/Tensor.h>

// #include <string>
// #include <stdexcept>





















// #define AT_FORALL_BINARY_OPS(_)
// _(+,x.add(y), y.add(x))
// _(*,x.mul(y), y.mul(x))
// _(-,x.sub(y), ::at::empty_like(y, at::MemoryFormat::Preserve).fi_(x).sub_(y))
// _(/,x.div(y), ::at::empty_like(y, at::MemoryFormat::Preserve).fi_(x).div_(y))
// _(%,x.remainder(y), ::at::empty_like(y, at::MemoryFormat::Preserve).fi_(x).remainder_(y))
// _(&,x.bitwise_and(y), y.bitwise_and(x))
// _(|,x.bitwise_or(y), y.bitwise_or(x))
// _(^,x.bitwise_xor(y), y.bitwise_xor(x))
// _(<,x.t(y), y.gt(x))
// _(<=,x.e(y), y.ge(x))
// _(>,x.gt(y),y.t(x))
// _(>=,x.ge(y), y.e(x))
// _(==,x.eq(y), y.eq(x))
// _(!=,x.ne(y), y.ne(x))

// #define DEFINE_OPERATOR(op,body,reverse_scalar_body)
// static inline Tensor operator op(const Tensor & x, const Tensor & y) {
//   return body;
// }
// static inline Tensor operator op(const Tensor & x, Scalar y) {
//   return body;
// }
// static inline Tensor operator op(Scalar x, const Tensor & y) {
//   return reverse_scalar_body;
// }
@Namespace("at") public static native @ByVal @Name("operator +") Tensor add(@ByVal Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator *") Tensor multiply(@ByVal Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator -") Tensor subtract(@ByVal Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator /") Tensor divide(@ByVal Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator %") Tensor mod(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator %") Tensor mod(@Const @ByRef Tensor x, @ByVal Scalar y);
@Namespace("at") public static native @ByVal @Name("operator %") Tensor mod(@ByVal Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator &") Tensor and(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator &") Tensor and(@Const @ByRef Tensor x, @ByVal Scalar y);
@Namespace("at") public static native @ByVal @Name("operator &") Tensor and(@ByVal Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator |") Tensor or(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator |") Tensor or(@Const @ByRef Tensor x, @ByVal Scalar y);
@Namespace("at") public static native @ByVal @Name("operator |") Tensor or(@ByVal Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator ^") Tensor xor(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator ^") Tensor xor(@Const @ByRef Tensor x, @ByVal Scalar y);
@Namespace("at") public static native @ByVal @Name("operator ^") Tensor xor(@ByVal Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator <") Tensor lessThan(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator <") Tensor lessThan(@Const @ByRef Tensor x, @ByVal Scalar y);
@Namespace("at") public static native @ByVal @Name("operator <") Tensor lessThan(@ByVal Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator <=") Tensor lessThanEquals(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator <=") Tensor lessThanEquals(@Const @ByRef Tensor x, @ByVal Scalar y);
@Namespace("at") public static native @ByVal @Name("operator <=") Tensor lessThanEquals(@ByVal Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator >") Tensor greaterThan(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator >") Tensor greaterThan(@Const @ByRef Tensor x, @ByVal Scalar y);
@Namespace("at") public static native @ByVal @Name("operator >") Tensor greaterThan(@ByVal Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator >=") Tensor greaterThanEquals(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator >=") Tensor greaterThanEquals(@Const @ByRef Tensor x, @ByVal Scalar y);
@Namespace("at") public static native @ByVal @Name("operator >=") Tensor greaterThanEquals(@ByVal Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator ==") Tensor equals(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator ==") Tensor equals(@Const @ByRef Tensor x, @ByVal Scalar y);
@Namespace("at") public static native @ByVal @Name("operator ==") Tensor equals(@ByVal Scalar x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator !=") Tensor notEquals(@Const @ByRef Tensor x, @Const @ByRef Tensor y);
@Namespace("at") public static native @ByVal @Name("operator !=") Tensor notEquals(@Const @ByRef Tensor x, @ByVal Scalar y);
@Namespace("at") public static native @ByVal @Name("operator !=") Tensor notEquals(@ByVal Scalar x, @Const @ByRef Tensor y);
// #undef DEFINE_OPERATOR
// #undef AT_FORALL_BINARY_OPS




// Parsed from ATen/Version.h

// #include <ATen/Context.h>

/** Returns a detailed string describing the configuration PyTorch. */
@Namespace("at") public static native @StdString BytePointer show_config();

@Namespace("at") public static native @StdString BytePointer get_mkl_version();

@Namespace("at") public static native @StdString BytePointer get_mkldnn_version();

@Namespace("at") public static native @StdString BytePointer get_openmp_version();

@Namespace("at") public static native @StdString BytePointer get_cxx_flags();

  // namespace at


// Parsed from torch/autograd.h

// #pragma once

// #include <torch/csrc/autograd/autograd.h>
// #include <torch/csrc/autograd/custom_function.h>


// Parsed from torch/csrc/WindowsTorchApiMacro.h

// #pragma once

// #include <c10/macros/Export.h>

// #ifdef _WIN32
// #else
// #define TORCH_PYTHON_API TORCH_API
// #endif


// Parsed from torch/csrc/autograd/edge.h

// #pragma once

// #include <cstdint>
// #include <functional>
// #include <memory>

// #include <c10/util/hash.h>
// Targeting ../Edge.java


 // namespace torch::autograd

// The idiomatic way of enabling use of a custom type as the key of hash
// containers in C++11. This method removes the requirement of having to pass
// a custom hasher to std::unordered_{map, set}.
// See http://en.cppreference.com/w/cpp/utility/hash for more information.
 // namespace std


// Parsed from torch/csrc/autograd/function_hook.h

// #pragma once

// #include <vector>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <ATen/Tensor.h>

// A hook that's called on gradients
// Targeting ../FunctionPreHook.java


// Targeting ../FunctionPostHook.java



 // namespace torch::autograd


// Parsed from torch/csrc/autograd/cpp_hook.h

// #pragma once
// #include <torch/csrc/autograd/function_hook.h>
// #include <functional>
// #include <memory>
// Targeting ../CppFunctionPreHook.java


 // namespace torch::autograd


// Parsed from torch/csrc/autograd/forward_grad.h

// #pragma once

// #include <ATen/ATen.h>

// [ Using ForwardGrad ]
// ForwardGrad needs to be a shared_ptr to satisfy constraints of its inner design. But
// this shared_ptr must be uniquely associated with the object that stores it (as of
// writing, either AutogradMeta or SavedVariable). This object is called the "owning object"
// in the discussions below. This owning object must call `ForwardGrad::clear()` when it
// is destroyed to ensure that the ForwardGrad is properly de-allocated.

// This file contains two classes that are used to store forward AD gradients and
// ensure that they are scoped properly.
// Because forward AD runs concurrently with the evaluation of the function, we need
// a mechanism to separate different forward AD invocations and be able to compute the
// right gradients. We model such invocations as levels here.
// The particular scoping issue mentioned above has two main drivers:
//   - Ensure that we can conveniently use forward AD within a high level API without
//     leaking the forward AD states outside.
//   - Ensure that we can keep the level that we expose to the user API simple (an integer
//     that represents the nesting depth) while avoiding confusions when the level index
//     is re-used.

// The important external APIs from this file are:
//   - ForwardADLevel::get_next_idx() that can be used to enter a new level and get its index
//   - ForwardADLevel::release_idx() that can be used to exit a given level.
//   - ForwardGrad() can be used to store a given forward gradient that will handle the level
//     tracking automatically.

// The basic implementation strategy is as follows:
// Every tensor has a ForwardGrad, maintaining a map from levels to tangents.
// ForwardGrad is responsible for registering itself to the appropriate ForwardADLevel when a new
// tangent is added to it via ForwardGrad::set_value and to un-register itself from this same level
// if that tangent is removed via ForwardGrad::reset.
// The ForwardADLevel is created when a new level is entered via ForwardADLevel::get_next_idx.
// A reference to the new ForwardADLevel is stored into a global (for the whole process) vector that
// ensure it can be accessed via ForwardADLevel::get_by_idx. This reference is deleted when the index is
// released by the user when calling ForwardADLevel::release_idx.
// When it is destructed, the ForwardADLevel is responsible for clearing all the tangents for its
// level stored in all the ForwardGrad that registered with it.
//
// This process-wide level design, compared to a thread local one, allows us to use very simple user facing
// handle for the level (an int) while enabling cross-thread forward AD.
// The only required synchronization for the user is when entering and exiting the levels.
// Some discussion on alternative design is in https://github.com/pytorch/pytorch/pull/49097#discussion_r543716453
// and can be refined in the future.

// Correctness of concurrency:
// Each class uses its own lock when reading or modifying internal storages. This allows in particular
// to safely remove tangents from ForwardGrad when the ForwardADLevel is being exited.
// We ensure no deadlock by ensuring that a methods never calls into another class's method while
// the local class's lock is held except in one single case: calling from ForwardADLevel's destructor
// into ForwardGrad::reset with update_level=false.

// The lifetime of these objects is as follows:
// The ForwardADLevel can be in three states:
//      - Initialized: where one of its reference is held by the global vector and there may be more
//        references held by temporary variables in ForwardGrad's methods.
//      - About to be destructed: where "release_idx" has been called and the only reason for the
//        ForwardADLevel not to be destructed right away is that some methods in ForwardGrad have
//        owning reference to it. This is done so that a ForwardADLevel can never be destructed when
//        a ForwardGrad is registered with it and in the process of adding something to its internal state.
//      - Being destructed: Here the ForwardADLevel is not referenced anymore and can be safely reset
//        all of the ForwardGrad. Note that we can have more than one reset being called here (which is ok)
//        but we are guaranteed that there is at least one.
// The ForwardGrad is simpler as there is no intermediary state and no special destructor for. The logic to
// unregister it from the different ForwardADLevel is done when the owning object (AutogradMeta or
// SavedVariable) is being destroyed.

// Other considered design:
// To avoid having the ForwardGrad::clear, we considered storing weak_ptr inside the ForwardADLevel. While this
// would work, it would mean that the set inside the ForwardADLevel would only grow unless we do an
// expensive linear scan to remove all the dangling weak pointers. Hence this approach was not used.

// Data structures in this file are optimized for this maximum number of levels.
// The number of levels corresponds to the degree of the gradient being
// computed using forward AD and we don't expect more than second order gradients
// to be common.
public static final int EXPECTED_MAX_LEVEL = 2;
// Targeting ../ForwardADLevel.java


// Targeting ../ForwardGrad.java



// Temporary functions to disable forward AD
// TODO(alband) remove these when perf issues are solved
@Namespace("torch::autograd") public static native @Cast("bool") boolean isForwardADEnabled();
@Namespace("torch::autograd") public static native void setForwardADEnabled(@Cast("bool") boolean value);

 // namespace torch::autograd


// Parsed from torch/csrc/autograd/variable.h

// #pragma once

// #include <torch/csrc/utils/python_stub.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/csrc/autograd/edge.h>
// #include <torch/csrc/autograd/function_hook.h>
// #include <torch/csrc/autograd/cpp_hook.h>
// #include <torch/csrc/autograd/forward_grad.h>

// #include <ATen/ATen.h>
// #include <ATen/NamedTensorUtils.h>
// #include <c10/util/Exception.h>

// #include <memory>
// #include <mutex>
// #include <stdexcept>
// #include <string>
// #include <utility>
// #include <vector>
// #include <cstdint>

/** {@code Variable} is exactly the same as {@code Tensor} (i.e. we have {@code using Variable = at::Tensor}).
 *  This means you can perform all the usual mathematical and other
 *  operations you can perform on {@code Tensor}s also on {@code Variable}s.
 * 
 *  The only reason we are keeping the {@code Variable} class is backward compatibility
 *  with external user's legacy C++ frontend code. Our intention is to eliminate
 *  the {@code Variable} class in the near future. */

 // namespace autograd
 // namespace torch

// The following are all internal APIs and should not be shown in libtorch docs.
// Therefore, we wrap the following code with `#ifndef DOXYGEN_SHOULD_SKIP_THIS ... #endif`

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

/** Check if this type is supported by the autograd engine.
 *  If you change this, update the doc at the top of the torch/autograd/__init__.py file
 *  and "test_set_requires_grad_only_for_continuous_types" in test/test_autograd.py */
@Namespace("torch::autograd") public static native @Cast("bool") boolean isDifferentiableType(ScalarType t);

/**~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *                                 Variable
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  A {@code Variable} augments a {@code Tensor} with the ability to interact in our
 *  autograd machinery. Conceptually, {@code Variable}s travel along {@code Edge}s between
 *  {@code Node}s in the autograd graph. A {@code Variable} can either be a leaf, like a
 *  weight in a neural network, or an interior variable, when it is the result
 *  of an operation between variables. Every {@code Variable} also stores another
 *  {@code Variable} called its {@code grad} (gradient). If the variable is a leaf, its
 *  gradient will be accumulated into this variable.
 * 
 *  Every Tensor is a Variable, but sometimes we colloquially refer to Variables
 *  that don't require gradients as Tensors (since none of the autograd
 *  machinery for Variables applies).  Historically, Variables and Tensors
 *  were separate concepts, but now they are exactly the same (i.e. we have
 *  {@code using Variable = at::Tensor}).
 * 
 *                               Gradient Edges
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  Furthermore, {@code Variable}s have the notion of a {@code gradient_edge}, which is the
 *  edge in the autograd graph that connects the variable to a particular input
 *  of the gradient function that will be invoked with the variable during the
 *  backward pass. More precisely, this gradient function can be one of two
 *  things:
 *  1. A {@code grad_fn}, if the variable is in the interior of the graph. This is the
 *     gradient of the function that produced the variable.
 *  2. A {@code grad_accumulator}, if the variable is a leaf, which accumulates a
 *     scalar gradient value into its {@code grad} variable.
 * 
 *                                Versioning
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  Another major feature of {@code Variable}s are *versions*. Versions are
 *  incremented when an in-place mutation of a variable occurs. Versions are
 *  useful when constructing {@code SavedVariable}s, which take a snapshot of a
 *  {@code Variable} at a certain version. You can retrieve a {@code Variable}'s version
 *  through its {@code current_version()} method.
 * 
 *                                  Views
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  It is possible for a  {@code Variable} to be a *view* of another {@code Variable}, in
 *  which case it tracks that {@code Variable}'s data and autograd history. Beyond
 *  construction, the interface of a view is identical to that of a regular
 *  {@code Variable}. You can determine whether {@code Variable} is in fact a view by
 *  probing its {@code is_view()} method. Note that the *view* semantics are only
 *  meaningful for {@code Variable} relations that are relevant to autograd.
 *  See NOTE [ Autograd View Variables ] for more details.
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ */

// Private-ish functions for manipulating variables; we don't want to put them
// on Tensor proper

  // WARNING: This may return a nullptr.  If you require AutogradMeta to return
  // a materialized structure, use materialize_autograd_meta instead.
  @Namespace("torch::autograd::impl") public static native AutogradMeta get_autograd_meta(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

  // Returns the current autograd meta, materializing it if it was previously
  // none.  This counts as a *mutating* operation, so do not call it on
  // "read-only" operators; in particular, this is NOT thread safe
  @Namespace("torch::autograd::impl") public static native AutogradMeta materialize_autograd_meta(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

  /** Set the gradient accumulator of the {@code Variable}. This is only applicable to
   *  leaf variables. Interior variables should call {@code set_gradient_edge()}. */

  /** Attempts to get a pointer to the gradient accumulator of the {@code Variable},
   *  if it still exists. If the gradient accumulator function has been
   *  destroyed, returns a {@code nullptr}. */
  @Namespace("torch::autograd::impl") public static native @SharedPtr Node try_get_grad_accumulator(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

  /** Gets the gradient accumulator of the {@code Variable} if it has one, or else
   *  create one on the fly and return it. */
  @Namespace("torch::autograd::impl") public static native @SharedPtr Node grad_accumulator(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

  /** Returns the "canonical" gradient edge of this {@code Variable}, i.e. either the
   *  gradient function if this is an interior {@code Variable}, or the gradient
   *  accumulator otherwise. If the {@code Variable} is interior, the returned {@code Edge}
   *  will store the input index of the {@code Node} to which this variable is
   *  connected in its {@code input_nr} field. For leaves, the {@code input_nr} is always
   *  zero. Note that {@code set_gradient_edge} and {@code gradient_edge} are not
   *  symmetric. You must use {@code set_gradient_edge} to set the {@code grad_fn} and
   *  {@code set_grad_accumulator} to set the accumulator. */
  @Namespace("torch::autograd::impl") public static native @ByVal Edge gradient_edge(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

  /** Set the gradient edge -- i.e. {@code grad_fn} and {@code input_nr} -- of the
   *  {@code Variable}.
   *  NOTE: This will always set the {@code grad_fn}, even if this is a leaf variable,
   *  and never the {@code grad_accumulator}. For the latter, use
   *  {@code set_grad_accumulator}. This allows late construction of an interior
   *  {@code Variable}. */
  
  ///
  @Namespace("torch::autograd::impl") public static native void set_gradient_edge(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0, @ByVal Edge edge);

  // Autograd Graph Interaction
  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  /** Update the {@code grad_fn} of an existing Variable. Called after in-place
   *  modifications.
   * 
   *  For View Variables:
   *  Called after in-place modifications. Modifies the grad_fn of the base
   *  Variable. */
  @Namespace("torch::autograd::impl") public static native void rebase_history(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0, @ByVal Edge gradient_edge);

  /** Gets the raw gradient function pointer, whatever it currently is. */
  @Namespace("torch::autograd::impl") public static native Node grad_fn_unsafe(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

  /** Increments the version count of this {@code Variable}. */
  @Namespace("torch::autograd::impl") public static native void bump_version(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);
  @Namespace("torch::autograd::impl") public static native void set_version_counter(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0, @Const @ByRef VariableVersion version_counter);

  /** Retrieves this {@code Variable}s version counter. */
  @Namespace("torch::autograd::impl") public static native @Const @ByRef VariableVersion version_counter(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

  @Namespace("torch::autograd::impl") public static native @Cast("PyObject*") Pointer pyobj(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);
  @Namespace("torch::autograd::impl") public static native void set_pyobj(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0, @Cast("PyObject*") Pointer pyobj);

  @Namespace("torch::autograd::impl") public static native void set_name(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0, @StdString BytePointer name);
  @Namespace("torch::autograd::impl") public static native void set_name(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0, @StdString String name);

  @Namespace("torch::autograd::impl") public static native void add_hook(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0, @SharedPtr FunctionPreHook hook);
  @Namespace("torch::autograd::impl") public static native @Const @ByRef FunctionPreVector hooks(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);
  @Namespace("torch::autograd::impl") public static native void clear_hooks(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

  @Namespace("torch::autograd::impl") public static native void create_cpp_hook(@Cast("const torch::autograd::Variable*") @ByRef Tensor arg0);

// Targeting ../AutogradMeta.java



//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
//                     DifferentiableViewMeta
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

/** NOTE [ Autograd View Variables ]
 * 
 *  Many operations return Variable that shares storage with an input Variable.
 *  The returned Variable is called a **view** Variable on the input **base**
 *  Variable.
 * 
 *  In PyTorch, we have two types of views: differentiable views, and
 *  non-differentiable views. In either type, to support proper version
 *  checking, the base and view Variables must always share the same
 *  version_counter.
 * 
 * 
 *  Differentiable Views
 *  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  This class allows to track both forward and backward AD differentiable views.
 *  These views can have different base as non-differentiable view for forward
 *  and backward mode AD are not the same.
 * 
 *  Most function are either both forward and backward differentiable views (for
 *  example: view, select, narrow, transpose, etc) or both not forward and not
 *  backward differentiable views (for example: indices, values, eq, lt, etc).
 *  But there are also functions that are forward but not backward differentiable
 *  views (only detach for now) or functions that are backward but not forward
 *  differentiable view (only make_dual and unpack dual for now).
 * 
 *  A concrete example of two views with different bases is as follow:
 * 
 *      # Have:
 *      #   dual is a dual Tensor that is neither a forward or backward view
 *      detached_dual = dual.detach()
 *      view = detached_dual.view_as(dual)
 *      # The forward base of view is dual
 *      # The backward base of view is detached_dual
 * 
 *  - Backward Mode View
 *  Differentiable views are the view variables where you want gradients to flow
 *  back to the base variables. Out-of-place operations on views are quite
 *  straightforward, but in-place ones are very tricky. Even if the base
 *  variable may not require grad when we create the view, we still need to
 *  track the view relation because future in-place ops may require back-proping
 *  through it. For example, we need to support
 * 
 *    (1) in-place operation on view, e.g.,
 * 
 *      # Have:
 *      #   base.requires_grad = False
 *      #   var.requires_grad = True
 *      base[1] = var  # i.e., base[1].copy_(var)
 *      torch.autograd.grad(base.sum(), var)  <- should return an all ones tensor
 * 
 *    (2) in-place operation on base after view is created, e.g.,
 * 
 *      # Have:
 *      #   base.requires_grad = False
 *      #   var.requires_grad = True
 *      view = base[1]
 *      base.copy_(var)
 *      torch.autograd.grad(view.sum(), var)  <- should return a tensor with
 *                                               var[1] filled with all ones and
 *                                               zeros everywhere else
 * 
 *  - Forward Mode View
 *  Forward differentiable views follow the same semantic as backward ones but
 *  show up differently as they are computed along with the forward evaluation.
 *  The hard examples above are thus very similar
 * 
 *    (1) in-place operation on view, e.g.,
 * 
 *      # Have:
 *      #   base is a regular Tensor
 *      #   var is a dual Tensor whose tangent is all ones
 *      base[1] = var  # i.e., base[1].copy_(var)
 *      # Now, base is a dual Tensor
 *      _, fw_grad = fwAD.unpack_dual(base) <- fw_grad should be a tensor with
 *                                               fw_grad[1] filled with all ones and
 *                                               zeros everywhere else
 * 
 *    (2) in-place operation on base after view is created, e.g.,
 * 
 *      # Have:
 *      #   base is a regular Tensor
 *      #   var is a dual Tensor whose tangent is all ones
 *      view = base[1]
 *      base.copy_(var)
 *      _, fw_grad = fwAD.unpack_dual(view) <- fw_grad should be an all ones tensor
 * 
 *  See Note [Forward Grad View/inplace] for more details on how we handle these hard cases.
 * 
 * 
 *  DifferentiableViewMeta is created to support gradient tracking of
 *  such **in-place** operations. In particular,
 *    + if an in-place op is done on base, the grad_fn field of the view may
 *      become stale. So accesses should always go through grad_fn(), which
 *      reconstructs an updated grad_fn if the version_counter has incremented.
 *      All other fields are always valid.
 *    + if an in-place op is done on view, in rebase_history() of view, which is
 *      called after every in-place op in VariableType.cpp, the grad_fn of base
 *      is updated.
 *    + if a single autograd Node returns multiple differentiable views, if any
 *      output is modified by an inplace operation, the autograd engine will make
 *      an equivalent graph (corresponding to the view operations) without using
 *      equivalent graph, where each output is treated as if it were produced by a
 *      distinct view operation. This discards the original (e.g., user provided)
 *      grad_fn. If the provided grad_fn does more than the backward of the view,
 *      then the DifferentiableViewMeta must be created with creation_meta=
 *      CreationMeta::MULTI_OUTPUT_NODE to prevent the engine from ignoring the
 *      provided grad_fn.
 * 
 *  Interaction with GradMode:
 *  The particular case that we consider here is:
 * 
 *      # Have:
 *      #   base.requires_grad = True or False
 *      with torch.no_grad():
 *          view = base[1]
 *      base.requires_grad_()
 *      view.copy_(var)
 *      torch.autograd.grad(base.sum(), var)  <- what should it return?
 * 
 *  Given that this particular code example is ambiguous and can easily be replace by
 *  either moving both inside the no_grad block or both outside, we explicitly forbid
 *  it. For now, it is deprecated by a warning. This is achieved by setting
 *  creation_meta=CreationMeta::NO_GRAD_MODE for all differentiable views created
 *  in no_grad mode.
 * 
 *  See Note [View + Inplace update for base tensor]
 *  and Note [View + Inplace update for view tensor] for the details how autograd
 *  handles inplace update with view ops.
 * 
 *  Non-Differentiable Views
 *  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *  In certain cases, although function outputs share storage with inputs, they
 *  will **never** require gradient history tracking. Instead of registering the
 *  view relation via DifferentiableViewMeta in autograd, the views will be
 *  using usual AutogradMeta and just share the version counters with the base
 *  Variables.
 *  Such views include:
 *    1. Views created from .detach()
 *    2. Views that are non-differentiable by its nature.
 *       E.g., {@code sparse_tensor.indices()} is a integral view on a (possibly)
 *       floating point tensor.
 *       See top of {@code derivatives.yaml} on how to specify that outputs of a
 *       function are non-differentiable.
 *  These are called non-differentiable views as the gradients do not flow
 *  through the view relation.
 * 
 *  Relevant logic for both differentiable and non-differentiable views is implemented in
 *  make_variable_(non_)differentiable_view below, and wrap_output of gen_variable_type.py.
 <p>
 <p>
 *  NOTE [ View + Inplace detection ]
 * 
 *  We want to detect views followed by inplace as they are often forbidden to ensure
 *  correctness of the computed gradients. But since we want to only notify the user
 *  when both happen, we tag the DifferentiableViewMeta when the view is created
 *  via the {@code make_variable_*_view()} functions. This tag is then checked by the
 *  {@code check_inplace()} function from {@code VariableTypeUtils.h} that should be called before
 *  every inplace operation and to detect cases where other views are modified and this
 *  one is rebased by side effect, we also check in the {@code VariableHooks::grad_fn()}.
 <p>
 *  Flag that gives more information about when this view was created:
 *  - IN_CUSTOM_FUNCTION should be set when the view is created inside a custom
 *    autograd Function is returned.
 *  - NO_GRAD_MODE should be set when a view in created when GradMode is disabled
 *  - MULTI_OUTPUT_NODE should be set when a Node created by codegen code returns
 *    multiple differentiable views
 *  - MULTI_OUTPUT_SAFE should be set when a view was returned by a function
 *    that returns multiple views, and unsafe_* version of that function
 *    exists. These are note considered as views for now for the view+inplace
 *    logic! The graph won't be rewritten when an inplace is done, only a
 *    warning will be thrown.
 *  - DEFAULT is for all other cases */
@Namespace("torch::autograd") public enum CreationMeta { DEFAULT((byte)(0)), IN_CUSTOM_FUNCTION((byte)(1)), MULTI_OUTPUT_NODE((byte)(2)),
                                   NO_GRAD_MODE((byte)(3)), MULTI_OUTPUT_SAFE((byte)(4));

    public final byte value;
    private CreationMeta(byte v) { this.value = v; }
    private CreationMeta(CreationMeta e) { this.value = e.value; }
    public CreationMeta intern() { for (CreationMeta e : values()) if (e.value == value) return e; return this; }
    @Override public String toString() { return intern().name(); }
}

/** Handles correctly propagating CreationMeta when a new view is created from a previous view.
 *  In general, we don't want the new view to be _less_ restrictive than the previous view
 *  (it's okay to be _more_ restrictive). A CreationMeta value of DEFAULT is currently the least
 *  restrictive, as the behavior for all other CreationMeta values is to error out for in-place ops.
 *  If this changes, the logic here will need to be updated to properly handle the new semantics. */
@Namespace("torch::autograd") public static native CreationMeta propagate_creation_meta(CreationMeta prev_view_creation_meta, CreationMeta new_view_creation_meta);
@Namespace("torch::autograd") public static native @Cast("torch::autograd::CreationMeta") byte propagate_creation_meta(@Cast("torch::autograd::CreationMeta") byte prev_view_creation_meta, @Cast("torch::autograd::CreationMeta") byte new_view_creation_meta);

/** Unified function to handle error checking when rebase happens
 *  indirect=true means that the caller is not doing the inplace, but the inplace happened
 *  somewhere else. */
@Namespace("torch::autograd") public static native void handle_view_on_rebase(DifferentiableViewMeta diff_view_meta, @Cast("bool") boolean indirect/*=false*/);
@Namespace("torch::autograd") public static native void handle_view_on_rebase(DifferentiableViewMeta diff_view_meta);
// Targeting ../DifferentiableViewMeta.java



//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
//                        Variable Implementation
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

// Factory Functions
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

/** Creates a {@code Variable} that is a *view* of another (*base*) variable.
 *  The {@code gradient_edge} is an optional (gradient_function, input_number) pair.
 *  {@code is_differentiable} is a bool that specifies whether this view is
 *  differentiable, i.e., whether the relation should be tracked by autograd.
 *  See NOTE [ Autograd View Variables ] for details.
 <p>
 *  NOTE: {@code allow_tensor_metadata_change} is set to true by default, because there
 *  are a lot of call sites to these factory functions that need to change the
 *  variable's size or storage afterwards, and they don't expect the original
 *  tensor (where the variable is created from) to be updated. Setting
 *  {@code allow_tensor_metadata_change_} to false by default would unnecessarily
 *  prevent those changes from happening and is undesirable. */

// See NOTE [ Autograd View Variables ] for details.
// Differentiable view. Track history with DifferentiableViewMeta.
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_differentiable_view(
    @Const @ByRef Tensor data,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer backward_info,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer forward_info,
    CreationMeta creation_meta,
    @Cast("bool") boolean allow_tensor_metadata_change/*=true*/);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_differentiable_view(
    @Const @ByRef Tensor data,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer backward_info,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer forward_info,
    CreationMeta creation_meta);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_differentiable_view(
    @Const @ByRef Tensor data,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer backward_info,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer forward_info,
    @Cast("torch::autograd::CreationMeta") byte creation_meta,
    @Cast("bool") boolean allow_tensor_metadata_change/*=true*/);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_differentiable_view(
    @Const @ByRef Tensor data,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer backward_info,
    @ByVal @Cast("c10::optional<torch::autograd::ViewInfo>*") Pointer forward_info,
    @Cast("torch::autograd::CreationMeta") byte creation_meta);

// See NOTE [ Autograd View Variables ] for details.
// Non-differentiable view. Just share version counter.

///
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_non_differentiable_view(
    @ByVal @Cast("torch::autograd::Variable*") Tensor base,
    @Const @ByRef Tensor data,
    @Cast("bool") boolean allow_tensor_metadata_change/*=true*/);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable_non_differentiable_view(
    @ByVal @Cast("torch::autograd::Variable*") Tensor base,
    @Const @ByRef Tensor data);

/** Creates a {@code Variable} from the given {@code Tensor}, copying its underlying {@code TensorImpl}.
 *  {@code requires_grad} should be
 *  set only for leaves, and determines whether the {@code Variable} will accumulate
 *  gradients. NOTE: {@code data} must *not* be a {@code Variable} already. Its dynamic
 *  type *must* be {@code Tensor}.
 * 
 *  TODO: Eliminate this function as much as possible, as it can be expressed
 *  more clearly as detach() or a no-op in most call sites (especially when
 *  there is only one use of the variable). */
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable(
    @ByVal Tensor data,
    @Cast("bool") boolean requires_grad/*=false*/,
    @Cast("bool") boolean allow_tensor_metadata_change/*=true*/);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable(
    @ByVal Tensor data);

/** Creates a {@code Variable} from the given {@code Tensor}, copying its underlying {@code TensorImpl}.
 *  {@code gradient_edge} should be a (function, input_nr) pair specifying the function
 *  in the autograd graph, and what particular input of that function, this
 *  variable is connected to. */
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable(
    @ByVal Tensor data,
    @ByVal Edge gradient_edge,
    @Cast("bool") boolean allow_tensor_metadata_change/*=true*/);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::Variable*") Tensor make_variable(
    @ByVal Tensor data,
    @ByVal Edge gradient_edge);


 // namespace torch::autograd

// #endif /* DOXYGEN_SHOULD_SKIP_THIS */


// Parsed from torch/csrc/autograd/autograd.h

// #pragma once

// #include <torch/csrc/autograd/variable.h>

// #include <ATen/ATen.h>

/** Computes the sum of gradients of given tensors with respect to graph leaves.
 * 
 *  The graph is differentiated using the chain rule. If any of {@code }tensors{@code }
 *  are non-scalar (i.e. their data has more than one element) and require gradient,
 *  then the Jacobian-vector product would be computed, in this case the function
 *  additionally requires specifying {@code grad_tensors}. It should be a sequence of
 *  matching length, that contains the "vector" in the Jacobian-vector product,
 *  usually the gradient of the differentiated function w.r.t. corresponding tensors
 *  ({@code torch::Tensor()} is an acceptable value for all tensors that don't need
 *  gradient tensors).
 * 
 *  This function accumulates gradients in the leaves - you might need to zero them
 *  before calling it.
 * 
 *  @param tensors Tensors of which the derivative will be computed.
 *  @param grad_tensors The "vector" in the Jacobian-vector product, usually gradients
 *      w.r.t. each element of corresponding tensors. {@code torch::Tensor()} values can be
 *      specified for scalar Tensors or ones that don't require grad. If a {@code torch::Tensor()} value
 *      would be acceptable for all grad_tensors, then this argument is optional.
 *  @param retain_graph If {@code false}, the graph used to compute the grad will be freed.
 *      Note that in nearly all cases setting this option to {@code true} is not needed
 *      and often can be worked around in a much more efficient way. Defaults to the
 *      value of {@code create_graph}.
 *  @param create_graph If {@code true}, graph of the derivative will be constructed, allowing
 *      to compute higher order derivative products. Defaults to {@code false}.
 *  @param inputs Inputs w.r.t. which the gradient will be accumulated into
 *      {@code at::Tensor::grad}. All other Tensors will be ignored. If not provided, the gradient
 *      is accumulated into all the leaf Tensors that were used to compute param {@code tensors}.
 *      All the provided inputs must be leaf Tensors. */

///
///
@Namespace("torch::autograd") public static native void backward(
    @Cast("const torch::autograd::variable_list*") @ByRef TensorVector tensors,
    @Cast("const torch::autograd::variable_list*") @ByRef(nullValue = "torch::autograd::variable_list{}") TensorVector grad_tensors,
    @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional retain_graph,
    @Cast("bool") boolean create_graph/*=false*/,
    @Cast("const torch::autograd::variable_list*") @ByRef(nullValue = "torch::autograd::variable_list{}") TensorVector inputs);
@Namespace("torch::autograd") public static native void backward(
    @Cast("const torch::autograd::variable_list*") @ByRef TensorVector tensors);

/** Computes and returns the sum of gradients of outputs with respect to the inputs.
 * 
 *  {@code }grad_outputs{@code } should be a sequence of length matching {@code }output{@code }
 *  containing the "vector" in Jacobian-vector product, usually the pre-computed
 *  gradients w.r.t. each of the outputs. If an output doesn't require_grad,
 *  then the gradient can be {@code }torch::Tensor(){@code }).
 * 
 *  @param outputs outputs of the differentiated function.
 *  @param inputs Inputs w.r.t. which the gradient will be
 *      returned (and not accumulated into {@code }at::Tensor::grad{@code }).
 *  @param grad_outputs The "vector" in the Jacobian-vector product.
 *      Usually gradients w.r.t. each output. {@code torch::Tensor()} values can be specified for scalar
 *      Tensors or ones that don't require grad. If a {@code torch::Tensor()} value would be acceptable
 *      for all grad_tensors, then this argument is optional. Default: {@code {}}.
 *  @param retain_graph If {@code }false{@code }, the graph used to compute the grad
 *      will be freed. Note that in nearly all cases setting this option to {@code }true{@code }
 *      is not needed and often can be worked around in a much more efficient
 *      way. Defaults to the value of {@code }create_graph{@code }.
 *  @param create_graph If {@code }true{@code }, graph of the derivative will
 *      be constructed, allowing to compute higher order derivative products.
 *      Default: {@code }false{@code }.
 *  @param allow_unused If {@code }false{@code }, specifying inputs that were not
 *      used when computing outputs (and therefore their grad is always zero)
 *      is an error. Defaults to {@code }false{@code }. */
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::variable_list*") TensorVector grad(
    @Cast("const torch::autograd::variable_list*") @ByRef TensorVector outputs,
    @Cast("const torch::autograd::variable_list*") @ByRef TensorVector inputs,
    @Cast("const torch::autograd::variable_list*") @ByRef(nullValue = "torch::autograd::variable_list{}") TensorVector grad_outputs,
    @ByVal(nullValue = "c10::optional<bool>(c10::nullopt)") BoolOptional retain_graph,
    @Cast("bool") boolean create_graph/*=false*/,
    @Cast("bool") boolean allow_unused/*=false*/);
@Namespace("torch::autograd") public static native @ByVal @Cast("torch::autograd::variable_list*") TensorVector grad(
    @Cast("const torch::autograd::variable_list*") @ByRef TensorVector outputs,
    @Cast("const torch::autograd::variable_list*") @ByRef TensorVector inputs);

/** Creates a new dual level and returns its index. This level index should then be used to call
 *  into the other functions below.
 *  This API supports entering a new level before the previous one is exited. We call them nested
 *  forward AD levels. These can be used to compute higher order derivatives. */
@Namespace("torch::autograd::forward_ad") public static native @Cast("uint64_t") long enter_dual_level();

/** Exits the given level. This will clear up all the gradients from this level and all dual Tensors
 *  that had gradients for this level will become regular Tensors again.
 *  This function can only be used to exit the innermost nesting level and so exiting must happen in
 *  reverse order compared to the entering that was done with the function above. */
@Namespace("torch::autograd::forward_ad") public static native void exit_dual_level(@Cast("uint64_t") long level);

 // namespace forward_ad
 // namespace autograd
 // namespace torch


// Parsed from torch/arg.h

// #pragma once

// #include <utility>

// #define TORCH_ARG(T, name)
//  public:
//   inline auto name(const T& new_##name)->decltype(*this) { /* NOLINT */
//     this->name##_ = new_##name;
//     return *this;
//   }
//   inline auto name(T&& new_##name)->decltype(*this) { /* NOLINT */
//     this->name##_ = std::move(new_##name);
//     return *this;
//   }
//   inline const T& name() const noexcept { /* NOLINT */
//     return this->name##_;
//   }
//   inline T& name() noexcept { /* NOLINT */
//     return this->name##_;
//   }
//  private:
//   T name##_ /* NOLINT */


// Parsed from torch/enum.h

// #pragma once

// #include <string>

// #include <ATen/core/Reduction.h>
// #include <c10/util/Exception.h>
// #include <c10/util/variant.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>

// #define TORCH_ENUM_DECLARE(name)
// namespace torch {
// namespace enumtype {
//   /*
//    NOTE: We need to provide the default constructor for each struct,
//    otherwise Clang 3.8 would complain:
//    ```
//    error: default initialization of an object of const type 'const enumtype::Enum1'
//    without a user-provided default constructor
//    ```
//  */
//   struct k##name { k##name() {} };
// }
// TORCH_API extern const enumtype::k##name k##name;
// }

// #define TORCH_ENUM_DEFINE(name)
// namespace torch {
// const enumtype::k##name k##name;
// }

// #define TORCH_ENUM_PRETTY_PRINT(name)
// std::string operator()(const enumtype::k##name& v) const {
//   std::string k("k");
//   return k + #name;
// }

// NOTE: Backstory on why we need the following two macros:
//
// Consider the following options class:
//
// ```
// struct TORCH_API SomeOptions {
//   typedef c10::variant<enumtype::kNone, enumtype::kMean, enumtype::kSum> reduction_t;
//   SomeOptions(reduction_t reduction = torch::kMean) : reduction_(reduction) {}
//
//   TORCH_ARG(reduction_t, reduction);
// };
// ```
//
// and the functional that uses it:
//
// ```
// Tensor some_functional(
//     const Tensor& input,
//     SomeOptions options = {}) {
//   ...
// }
// ```
//
// Normally, we would expect this to work:
//
// `F::some_functional(input, torch::kNone)`
//
// However, it throws the following error instead:
//
// ```
// error: could not convert `torch::kNone` from `const torch::enumtype::kNone` to `torch::nn::SomeOptions`
// ```
//
// To get around this problem, we explicitly provide the following constructors for `SomeOptions`:
//
// ```
// SomeOptions(torch::enumtype::kNone reduction) : reduction_(torch::kNone) {}
// SomeOptions(torch::enumtype::kMean reduction) : reduction_(torch::kMean) {}
// SomeOptions(torch::enumtype::kSum reduction) : reduction_(torch::kSum) {}
// ```
//
// so that the conversion from `torch::kNone` to `SomeOptions` would work.
//
// Note that we also provide the default constructor `SomeOptions() {}`, so that
// `SomeOptions options = {}` can work.
// #define TORCH_OPTIONS_CTOR_VARIANT_ARG3(OPTIONS_NAME, ARG_NAME, TYPE1, TYPE2, TYPE3)
// OPTIONS_NAME() {}
// OPTIONS_NAME(torch::enumtype::TYPE1 ARG_NAME) : ARG_NAME##_(torch::TYPE1) {}
// OPTIONS_NAME(torch::enumtype::TYPE2 ARG_NAME) : ARG_NAME##_(torch::TYPE2) {}
// OPTIONS_NAME(torch::enumtype::TYPE3 ARG_NAME) : ARG_NAME##_(torch::TYPE3) {}

// #define TORCH_OPTIONS_CTOR_VARIANT_ARG4(OPTIONS_NAME, ARG_NAME, TYPE1, TYPE2, TYPE3, TYPE4)
// OPTIONS_NAME() {}
// OPTIONS_NAME(torch::enumtype::TYPE1 ARG_NAME) : ARG_NAME##_(torch::TYPE1) {}
// OPTIONS_NAME(torch::enumtype::TYPE2 ARG_NAME) : ARG_NAME##_(torch::TYPE2) {}
// OPTIONS_NAME(torch::enumtype::TYPE3 ARG_NAME) : ARG_NAME##_(torch::TYPE3) {}
// OPTIONS_NAME(torch::enumtype::TYPE4 ARG_NAME) : ARG_NAME##_(torch::TYPE4) {}
// Targeting ../kLinear.java

  
// Targeting ../kConv1D.java

  
// Targeting ../kConv2D.java

  
// Targeting ../kConv3D.java

  
// Targeting ../kConvTranspose1D.java

  
// Targeting ../kConvTranspose2D.java

  
// Targeting ../kConvTranspose3D.java

  
// Targeting ../kSigmoid.java

  
// Targeting ../kTanh.java

  
// Targeting ../kReLU.java

  
// Targeting ../kGELU.java

  
// Targeting ../kSiLU.java

  
// Targeting ../kLeakyReLU.java

  
// Targeting ../kFanIn.java

  
// Targeting ../kFanOut.java

  
// Targeting ../kConstant.java

  
// Targeting ../kReflect.java

  
// Targeting ../kReplicate.java

  
// Targeting ../kCircular.java

  
// Targeting ../kNearest.java

  
// Targeting ../kBilinear.java

  
// Targeting ../kBicubic.java

  
// Targeting ../kTrilinear.java

  
// Targeting ../kArea.java

  
// Targeting ../kSum.java

  
// Targeting ../kMean.java

  
// Targeting ../kMax.java

  
// Targeting ../kNone.java

  
// Targeting ../kBatchMean.java

  
// Targeting ../kZeros.java

  
// Targeting ../kBorder.java

  
// Targeting ../kReflection.java

  
// Targeting ../kRNN_TANH.java

  
// Targeting ../kRNN_RELU.java

  
// Targeting ../kLSTM.java

  
// Targeting ../kGRU.java

  
// Targeting ../_compute_enum_name.java



 // namespace enumtype
 // namespace torch


// Parsed from torch/types.h

// #pragma once

// #include <ATen/ATen.h>

// #include <c10/util/Optional.h>

// #include <torch/csrc/autograd/generated/variable_factories.h>
// #include <torch/csrc/autograd/variable.h>

// NOTE [ Exposing declarations in `at::` to `torch::` ]
//
// The following line `using namespace at;` is responsible for exposing all declarations in
// `at::` namespace to `torch::` namespace.
//
// According to the rules laid out in
// https://en.cppreference.com/w/cpp/language/qualified_lookup, section "Namespace members":
// ```
// Qualified lookup within the scope of a namespace N first considers all declarations that are
// located in N and all declarations that are located in the inline namespace members of N
// (and, transitively, in their inline namespace members). If there are no declarations in that set
// then it considers declarations in all namespaces named by using-directives found in N and
// in all transitive inline namespace members of N.
// ```
//
// This means that if both `at::` and `torch::` namespaces have a function with the same signature
// (e.g. both `at::func()` and `torch::func()` exist), after `namespace torch { using namespace at; }`,
// when we call `torch::func()`, the `func()` function defined in `torch::` namespace will always
// be called, and the `func()` function defined in `at::` namespace is always hidden. // NOLINT

/** Fixed width dtypes. */

/** Rust-style short dtypes. */
 // namespace torch


// Parsed from torch/utils.h

// #pragma once

// #include <ATen/Parallel.h>
// #include <ATen/record_function.h>
// #include <torch/csrc/autograd/grad_mode.h>
// #include <torch/csrc/api/include/torch/types.h>
// #include <cstdint>

/** A RAII, thread-local guard that disabled gradient calculation.
 * 
 *  Disabling gradient calculation is useful for inference, when you are sure
 *  that you will not call {@code at::Tensor::backward}. It will reduce memory
 *  consumption for computations that would otherwise have {@code requires_grad() == true}.
 * 
 *  In this mode, the result of every computation will have
 *  {@code requires_grad() == false}, even when the inputs have {@code requires_grad() == true}.
 * 
 *  This context manager is thread-local; it will not affect computation
 *  in other threads.
 * 
 *  Example:
 *  <pre>{@code
 *  auto x = torch::tensor({1.}, torch::requires_grad());
 *  {
 *    torch::NoGradGuard no_grad;
 *    auto y = x * 2;
 *    std::cout << y.requires_grad() << std::endl; // prints `false`
 *  }
 *  {
 *    auto doubler = [](torch::Tensor x) {
 *      torch::NoGradGuard no_grad;
 *      return x * 2;
 *    };
 *    auto z = doubler(x);
 *    std::cout << z.requires_grad() << std::endl; // prints `false`
 *  }
 *  }</pre> */

///
///
///
///

/** A RAII, thread-local guard that sets gradient calculation to on or off.
 * 
 *  {@code }AutoGradMode{@code } will enable or disable grads based on its argument {@code enabled}.
 * 
 *  This context manager is thread-local; it will not affect computation
 *  in other threads.
 * 
 *  @param enabled: Flag whether to enable grad ({@code }true{@code }), or disable
 *               ({@code }false{@code }). This can be used to conditionally enable
 *               gradients.
 * 
 *  Example:
 *  <pre>{@code
 *  auto x = torch::tensor({1.}, torch::requires_grad());
 *  {
 *    torch::AutoGradMode enable_grad(true);
 *    auto y = x * 2;
 *    std::cout << y.requires_grad() << std::endl; // prints `true`
 *  }
 *  {
 *    torch::AutoGradMode enable_grad(false);
 *    auto y = x * 2;
 *    std::cout << y.requires_grad() << std::endl; // prints `false`
 *  }
 *  }</pre> */

/** Sets the global random seed for all newly created CPU and CUDA tensors. */

// Called during new thread initialization

// Returns the number of threads used in parallel region.

// Sets the number of threads to be used in parallel region.

// Returns the number of threads used for inter-op parallelism.

// Sets the number of threads to be used for inter-op parallelism.

// Returns true if both t1, t2 are undefined or both are defined and equal
@Namespace("torch") public static native @Cast("bool") boolean equal_if_defined(@ByVal Tensor t1, @ByVal Tensor t2);

// RecordFunction API

 // namespace torch


// Parsed from torch/data.h

// #pragma once

// #include <torch/data/dataloader.h>
// #include <torch/data/datasets.h>
// #include <torch/data/samplers.h>
// #include <torch/data/transforms.h>

// Some "exports".
 // namespace data
 // namespace torch


// Parsed from torch/data/example.h

// #pragma once

// #include <torch/types.h>
// Targeting ../Example.java


// Targeting ../NoTarget.java


 //  namespace example

/** A specialization for {@code Example} that does not have have a target.
 * 
 *  This class exists so that code can be written for a templated {@code Example}
 *  type, and work both for labeled and unlabeled datasets. */
 // namespace data
 // namespace torch


// Parsed from torch/data/iterator.h

// #pragma once

// #include <torch/csrc/utils/variadic.h>
// #include <torch/types.h>

// #include <c10/util/Exception.h>

// #include <functional>
// #include <iterator>
// #include <memory>
// #include <type_traits>
// #include <utility>
// For increased safety and more separated logic, this implementation of
// `Iterator` consists of a `ValidIterator` and a `SentinelIterator`. A
// `ValidIterator` yields new batches until the `DataLoader` is exhausted. While
// the `DataLoader` is not exhausted, `ValidIterator`s compare equal if they are
// the same object. When the `ValidIterator` becomes exhausted, it compares equal
// to the `SentinelIterator`, but not before. Half the code here is to implement
// double dispatch for the comparison. Got damnit, C++.

/** Base class for the {@code ValidIterator} and {@code SentinelIterator} */

// Targeting ../ExampleIterator.java


// Targeting ../ExampleVectorIterator.java


 // namespace data
 // namespace torch


// Parsed from torch/data/worker_exception.h

// #pragma once

// #include <exception>
// #include <string>
// #include <utility>
// Targeting ../WorkerException.java



 // namespace data
 // namespace torch


// Parsed from torch/data/dataloader.h

// #pragma once

// #include <torch/data/dataloader/stateful.h>
// #include <torch/data/dataloader/stateless.h>

// #include <torch/csrc/utils/memory.h>
// #include <torch/csrc/utils/variadic.h>

// #include <c10/util/Exception.h>

// #include <cstddef>
// #include <memory>
// #include <type_traits>
// #include <utility>

/** Creates a {@code DataLoader} instance for a stateless {@code dataset}, a {@code sampler} and
 *  some {@code options}. */

/** Creates a {@code DataLoader} instance for a stateless {@code dataset} and some
 *  {@code options}. A sampler (by default a {@code RandomSampler}) will be constructed from
 *  the size of the dataset. */

/** Creates a {@code DataLoader} for a stateful {@code dataset} and some {@code options}. */
 // namespace data
 // namespace torch


// Parsed from torch/data/dataloader/base.h

// #pragma once

// #include <torch/data/dataloader_options.h>
// #include <torch/data/detail/data_shuttle.h>
// #include <torch/data/detail/sequencers.h>
// #include <torch/data/iterator.h>
// #include <torch/data/samplers/random.h>
// #include <torch/data/worker_exception.h>
// #include <torch/types.h>

// #include <torch/csrc/utils/memory.h>
// #include <torch/csrc/utils/variadic.h>

// #include <c10/util/Exception.h>

// #include <cstddef>
// #include <exception>
// #include <memory>
// #include <thread>
// #include <type_traits>
// #include <utility>
// #include <vector>
// Targeting ../MNISTRandomDataLoaderBase.java


 // namespace data
 // namespace torch


// Parsed from torch/data/dataloader_options.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/types.h>

// #include <chrono>
// #include <cstddef>
// Targeting ../DataLoaderOptions.java


// Targeting ../FullDataLoaderOptions.java


 // namespace data
 // namespace torch


// Parsed from torch/data/dataloader/stateful.h

// #pragma once

// #include <torch/data/dataloader/base.h>

// #include <cstddef>
// #include <thread>
// #include <utility>

/** A dataloader for stateful datasets.
 * 
 *  A dataloader for stateful datatasets differs from one for stateless
 *  datasets one in that the dataset is shared among worker threads, and that
 *  this dataset is itself responsible for producing batches rather than
 *  depending on a sampler. The statefulness here actually refers to the
 *  dataset. The StatefulDataLoader simply alters the data loading algorithm to
 *  accommodate the stateful, shared nature of the dataset. Note that the dataset
 *  must be thread safe if more than one worker thread is used.
 * 
 *  A stateful dataloader is created by calling {@code make_data_loader} with a
 *  stateful dataset. */
 // namespace data
 // namespace torch


// Parsed from torch/data/dataloader/stateless.h

// #pragma once

// #include <torch/data/dataloader/base.h>
// #include <torch/data/worker_exception.h>

// #include <torch/csrc/utils/memory.h>

// #include <c10/util/Exception.h>

// #include <cstddef>
// #include <thread>
// #include <utility>
// Targeting ../MNISTRandomDataLoader.java


 // namespace data
 // namespace torch


// Parsed from torch/data/datasets.h

// #pragma once

// #include <torch/data/datasets/base.h>
// #include <torch/data/datasets/chunk.h>
// #include <torch/data/datasets/map.h>
// #include <torch/data/datasets/mnist.h>
// #include <torch/data/datasets/shared.h>
// #include <torch/data/datasets/stateful.h>
// #include <torch/data/datasets/tensor.h>


// Parsed from torch/data/datasets/base.h

// #pragma once

// #include <torch/data/example.h>
// #include <torch/types.h>

// #include <c10/util/ArrayRef.h>

// #include <cstddef>
// #include <cstdint>
// #include <type_traits>
// #include <utility>
// #include <vector> // NOLINT
 // namespace datasets
 // namespace data
 // namespace torch

// Targeting ../MNISTBatchDataset.java


// Targeting ../MNISTMapBatchDataset.java


// Targeting ../MNISTDataSet.java



/** A {@code StreamDataset} represents a dataset that is a potentially infinite stream.
 *  It takes as batch index only a number, which is the batch size, and yields
 *  that many elements from the stream. */
 // namespace datasets
 // namespace data
 // namespace torch


// Parsed from torch/data/datasets/map.h

// #pragma once

// #include <torch/data/datasets/base.h>
// #include <torch/types.h>

// #include <c10/util/ArrayRef.h>

// #include <cstddef>
// #include <type_traits>
// #include <utility>

// Targeting ../MNISTMapDataset.java



/** Creates a {@code MapDataset} with the given dataset and transform. */

 // namespace datasets
 // namespace data
 // namespace torch


// Parsed from torch/data/datasets/mnist.h

// #pragma once

// #include <torch/data/datasets/base.h>
// #include <torch/data/example.h>
// #include <torch/types.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>

// #include <cstddef>
// #include <string>
// Targeting ../MNIST.java


 // namespace datasets
 // namespace data
 // namespace torch


// Parsed from torch/data/samplers.h

// #pragma once

// #include <torch/data/samplers/base.h>
// #include <torch/data/samplers/custom_batch_request.h>
// #include <torch/data/samplers/distributed.h>
// #include <torch/data/samplers/random.h>
// #include <torch/data/samplers/sequential.h>
// #include <torch/data/samplers/serialize.h>
// #include <torch/data/samplers/stream.h>


// Parsed from torch/data/samplers/base.h

// #pragma once

// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <vector>
// #include <mutex>
 // namespace serialize
 // namespace torch
// Targeting ../Sampler.java



 // namespace samplers
 // namespace data
 // namespace torch


// Parsed from torch/data/samplers/random.h

// #pragma once

// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/data/samplers/base.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <vector>
 // namespace serialize
 // namespace torch
// Targeting ../RandomSampler.java


 // namespace samplers
 // namespace data
 // namespace torch


// Parsed from torch/data/transforms.h

// #pragma once

// #include <torch/data/transforms/base.h>
// #include <torch/data/transforms/collate.h>
// #include <torch/data/transforms/lambda.h>
// #include <torch/data/transforms/stack.h>
// #include <torch/data/transforms/tensor.h>


// Parsed from torch/data/transforms/base.h

// #pragma once

// #include <torch/types.h>

// #include <utility>
// #include <vector>
// Targeting ../ExampleCollation.java



/** A transformation of individual input examples to individual output examples.
 * 
 *  Just like a {@code Dataset} is a {@code BatchDataset}, a {@code Transform} is a
 *  {@code BatchTransform} that can operate on the level of individual examples rather
 *  than entire batches. The batch-level transform is implemented (by default)
 *  in terms of the example-level transform, though this can be customized. */
 // namespace transforms
 // namespace data
 // namespace torch


// Parsed from torch/data/transforms/collate.h

// #pragma once

// #include <torch/data/example.h>
// #include <torch/data/transforms/lambda.h>

// #include <vector>

/** A {@code Collation} is a transform that reduces a batch into a single value.
 *  The result is a {@code BatchDataset} that has the type of the single value as its
 *  {@code BatchType}. */

///
///

/** A {@code Collate} allows passing a custom function to reduce/collate a batch
 *  into a single value. It's effectively the lambda version of {@code Collation},
 *  which you could subclass and override {@code operator()} to achieve the same.
 * 
 *  \rst
 *  .. code-block:: cpp
 *    using namespace torch::data;
 * 
 *    auto dataset = datasets::MNIST("path/to/mnist")
 *      .map(transforms::Collate<Example<>>([](std::vector<Example<>> e) {
 *        return std::move(e.front());
 *      }));
 *  \endrst */
 // namespace transforms
 // namespace data
 // namespace torch


// Parsed from torch/data/transforms/lambda.h

// #pragma once

// #include <torch/data/transforms/base.h>

// #include <functional>
// #include <utility>
// #include <vector>

/** A {@code BatchTransform} that applies a user-provided functor to a batch. */

// A `Transform` that applies a user-provided functor to individual examples.

 // namespace transforms
 // namespace data
 // namespace torch


// Parsed from torch/data/transforms/stack.h

// #pragma once

// #include <torch/data/example.h>
// #include <torch/data/transforms/collate.h>
// #include <torch/types.h>

// #include <utility>
// #include <vector>
// Targeting ../ExampleStack.java



/** A {@code Collation} for {@code Example<Tensor, NoTarget>} types that stacks all data
 *  tensors into one tensor. */
 // namespace transforms
 // namespace data
 // namespace torch


// Parsed from torch/data/transforms/tensor.h

// #pragma once

// #include <torch/data/example.h>
// #include <torch/data/transforms/base.h>
// #include <torch/types.h>

// #include <functional>
// #include <utility>

/** A {@code Transform} that is specialized for the typical {@code Example<Tensor, Tensor>}
 *  combination. It exposes a single {@code operator()} interface hook (for
 *  subclasses), and calls this function on input {@code Example} objects. */

/** A {@code Lambda} specialized for the typical {@code Example<Tensor, Tensor>} input type. */

/** Normalizes input tensors by subtracting the supplied mean and dividing by
 *  the given standard deviation. */
 // namespace transforms
 // namespace data
 // namespace torch


// Parsed from torch/serialize.h

// #pragma once

// #include <torch/serialize/archive.h>
// #include <torch/serialize/tensor.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>

// #include <utility>

/** Serializes the given {@code value}.
 *  There must be an overload of {@code operator<<} between {@code serialize::OutputArchive}
 *  and {@code Value} for this method to be well-formed. Currently, such an overload
 *  is provided for (subclasses of):
 * 
 *  - {@code torch::nn::Module},
 *  - {@code torch::optim::Optimizer}
 *  - {@code torch::Tensor}
 * 
 *  To perform the serialization, a {@code serialize::OutputArchive} is constructed,
 *  and all arguments after the {@code value} are forwarded to its {@code save_to} method.
 *  For example, you can pass a filename, or an {@code ostream}.
 * 
 *  \rst
 *  .. code-block:: cpp
 * 
 *    torch::nn::Linear model(3, 4);
 *    torch::save(model, "model.pt");
 * 
 *    torch::optim::SGD sgd(/*lr=* /0.9);
 *    std::ostringstream stream;
 *    // Note that the same stream cannot be used in multiple torch::save(...)
 *    // invocations, otherwise the header will be corrupted.
 *    torch::save(sgd, stream);
 * 
 *    auto tensor = torch::ones({3, 4});
 *    torch::save(tensor, "my_tensor.pt");
 *  \endrst */

/** Serializes the given {@code tensor_vec} of type {@code std::vector<torch::Tensor>}.
 * 
 *  To perform the serialization, a {@code serialize::OutputArchive} is constructed,
 *  and all arguments after the {@code tensor_vec} are forwarded to its {@code save_to}
 *  method. For example, you can pass a filename, or an {@code ostream}.
 * 
 *  \rst
 *  .. code-block:: cpp
 * 
 *    std::vector<torch::Tensor> tensor_vec = { torch::randn({1, 2}), torch::randn({3, 4}) };
 *    torch::save(tensor_vec, "my_tensor_vec.pt");
 * 
 *    std::vector<torch::Tensor> tensor_vec = { torch::randn({5, 6}), torch::randn({7, 8}) };
 *    std::ostringstream stream;
 *    // Note that the same stream cannot be used in multiple torch::save(...)
 *    // invocations, otherwise the header will be corrupted.
 *    torch::save(tensor_vec, stream);
 *  \endrst */

@Namespace("torch") public static native @Cast("char*") @StdVector BytePointer pickle_save(@Const @ByRef IValue ivalue);

///
///
///
///
///
///
@Namespace("torch") public static native @ByVal IValue pickle_load(@Cast("char*") @StdVector BytePointer data);
@Namespace("torch") public static native @ByVal IValue pickle_load(@Cast("char*") @StdVector ByteBuffer data);
@Namespace("torch") public static native @ByVal IValue pickle_load(@Cast("char*") @StdVector byte[] data);

/** Deserializes the given {@code value}.
 *  There must be an overload of {@code operator>>} between {@code serialize::InputArchive}
 *  and {@code Value} for this method to be well-formed. Currently, such an overload
 *  is provided for (subclasses of):
 * 
 *  - {@code torch::nn::Module},
 *  - {@code torch::optim::Optimizer}
 *  - {@code torch::Tensor}
 * 
 *  To perform the serialization, a {@code serialize::InputArchive} is constructed,
 *  and all arguments after the {@code value} are forwarded to its {@code load_from} method.
 *  For example, you can pass a filename, or an {@code istream}.
 * 
 *  \rst
 *  .. code-block:: cpp
 * 
 *    torch::nn::Linear model(3, 4);
 *    torch::load(model, "model.pt");
 * 
 *    torch::optim::SGD sgd(/*lr=* /0.9);
 *    std::istringstream stream("...");
 *    torch::load(sgd, stream);
 * 
 *    auto tensor = torch::ones({3, 4});
 *    torch::load(tensor, "my_tensor.pt");
 *  \endrst */

/** Deserializes the given {@code tensor_vec} of type {@code std::vector<torch::Tensor>}.
 * 
 *  To perform the serialization, a {@code serialize::InputArchive} is constructed,
 *  and all arguments after the {@code value} are forwarded to its {@code load_from} method.
 *  For example, you can pass a filename, or an {@code istream}.
 * 
 *  \rst
 *  .. code-block:: cpp
 * 
 *    std::vector<torch::Tensor> tensor_vec;
 *    torch::load(tensor_vec, "my_tensor_vec.pt");
 * 
 *    std::vector<torch::Tensor> tensor_vec;
 *    std::istringstream stream("...");
 *    torch::load(tensor_vec, stream);
 *  \endrst */
 // namespace torch


// Parsed from torch/serialize/archive.h

// #pragma once

// #include <torch/serialize/input-archive.h>
// #include <torch/serialize/output-archive.h>


// Parsed from torch/serialize/input-archive.h

// #pragma once

// #include <c10/util/Optional.h>
// #include <c10/core/Device.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// #include <torch/csrc/jit/api/module.h>

// #include <iosfwd>
// #include <memory>
// #include <string>
// #include <utility>
 // namespace at
 // namespace jit
 // namespace torch
// Targeting ../InputArchive.java


 // namespace serialize
 // namespace torch


// Parsed from torch/serialize/output-archive.h

// #pragma once

// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/csrc/jit/api/module.h>

// #include <iosfwd>
// #include <memory>
// #include <string>
// #include <utility>
 // namespace at
 // namespace jit

// Targeting ../OutputArchive.java


 // namespace serialize
 // namespace torch


// Parsed from torch/serialize/tensor.h

// #pragma once

// #include <torch/serialize/archive.h>
// #include <torch/types.h>
@Namespace("torch") public static native @ByRef @Name("operator <<") OutputArchive shiftLeft(
    @ByRef OutputArchive archive,
    @Const @ByRef Tensor tensor);

@Namespace("torch") public static native @ByRef @Name("operator >>") InputArchive shiftRight(
    @ByRef InputArchive archive,
    @ByRef Tensor tensor);
 // namespace torch


// Parsed from torch/nn.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional.h>
// #include <torch/nn/init.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules.h>
// #include <torch/nn/options.h>
// #include <torch/nn/pimpl.h>
// #include <torch/nn/utils.h>


// Parsed from torch/nn/cloneable.h

// #pragma once

// #include <torch/nn/module.h>
// #include <torch/types.h>
// #include <torch/utils.h>

// #include <c10/core/TensorOptions.h>
// #include <c10/util/Exception.h>

// #include <memory>
// #include <utility>
// Targeting ../ModuleDictImplCloneable.java


// Targeting ../ModuleListImplCloneable.java


// Targeting ../SequentialImplCloneable.java


// Targeting ../ParameterDictImplCloneable.java


// Targeting ../ParameterListImplCloneable.java


// Targeting ../AdaptiveLogSoftmaxWithLossImplCloneable.java


// Targeting ../BatchNorm1dImplCloneable.java


// Targeting ../InstanceNorm1dImplCloneable.java


// Targeting ../Conv1dImplCloneable.java


// Targeting ../ConvTranspose1dImplCloneable.java


// Targeting ../DropoutImplCloneable.java


// Targeting ../BatchNorm2dImplCloneable.java


// Targeting ../InstanceNorm2dImplCloneable.java


// Targeting ../Conv2dImplCloneable.java


// Targeting ../ConvTranspose2dImplCloneable.java


// Targeting ../Dropout2dImplCloneable.java


// Targeting ../BatchNorm3dImplCloneable.java


// Targeting ../InstanceNorm3dImplCloneable.java


// Targeting ../Conv3dImplCloneable.java


// Targeting ../ConvTranspose3dImplCloneable.java


// Targeting ../Dropout3dImplCloneable.java


// Targeting ../AlphaDropoutImplCloneable.java


// Targeting ../FeatureAlphaDropoutImplCloneable.java


// Targeting ../CosineSimilarityImplCloneable.java


// Targeting ../PairwiseDistanceImplCloneable.java


// Targeting ../EmbeddingImplCloneable.java


// Targeting ../EmbeddingBagImplCloneable.java


// Targeting ../FoldImplCloneable.java


// Targeting ../UnfoldImplCloneable.java


// Targeting ../IdentityImplCloneable.java


// Targeting ../LinearImplCloneable.java


// Targeting ../BilinearImplCloneable.java


// Targeting ../FlattenImplCloneable.java


// Targeting ../UnflattenImplCloneable.java


// Targeting ../L1LossImplCloneable.java


// Targeting ../KLDivLossImplCloneable.java


// Targeting ../MSELossImplCloneable.java


// Targeting ../BCELossImplCloneable.java


// Targeting ../HingeEmbeddingLossImplCloneable.java


// Targeting ../MultiMarginLossImplCloneable.java


// Targeting ../CosineEmbeddingLossImplCloneable.java


// Targeting ../SmoothL1LossImplCloneable.java


// Targeting ../MultiLabelMarginLossImplCloneable.java


// Targeting ../SoftMarginLossImplCloneable.java


// Targeting ../MultiLabelSoftMarginLossImplCloneable.java


// Targeting ../TripletMarginLossImplCloneable.java


// Targeting ../TripletMarginWithDistanceLossImplCloneable.java


// Targeting ../CTCLossImplCloneable.java


// Targeting ../PoissonNLLLossImplCloneable.java


// Targeting ../MarginRankingLossImplCloneable.java


// Targeting ../NLLLossImplCloneable.java


// Targeting ../CrossEntropyLossImplCloneable.java


// Targeting ../BCEWithLogitsLossImplCloneable.java


// Targeting ../ReflectionPad1dImplCloneable.java


// Targeting ../ReplicationPad1dImplCloneable.java


// Targeting ../ConstantPad1dImplCloneable.java


// Targeting ../AvgPool1dImplCloneable.java


// Targeting ../MaxPool1dImplCloneable.java


// Targeting ../AdaptiveAvgPool1dImplCloneable.java


// Targeting ../AdaptiveMaxPool1dImplCloneable.java


// Targeting ../MaxUnpool1dImplCloneable.java


// Targeting ../LPPool1dImplCloneable.java


// Targeting ../ReflectionPad2dImplCloneable.java


// Targeting ../ReplicationPad2dImplCloneable.java


// Targeting ../ConstantPad2dImplCloneable.java


// Targeting ../ZeroPad2dImplCloneable.java


// Targeting ../AvgPool2dImplCloneable.java


// Targeting ../MaxPool2dImplCloneable.java


// Targeting ../AdaptiveAvgPool2dImplCloneable.java


// Targeting ../AdaptiveMaxPool2dImplCloneable.java


// Targeting ../MaxUnpool2dImplCloneable.java


// Targeting ../FractionalMaxPool2dImplCloneable.java


// Targeting ../LPPool2dImplCloneable.java


// Targeting ../ReplicationPad3dImplCloneable.java


// Targeting ../ConstantPad3dImplCloneable.java


// Targeting ../AvgPool3dImplCloneable.java


// Targeting ../MaxPool3dImplCloneable.java


// Targeting ../AdaptiveAvgPool3dImplCloneable.java


// Targeting ../AdaptiveMaxPool3dImplCloneable.java


// Targeting ../MaxUnpool3dImplCloneable.java


// Targeting ../FractionalMaxPool3dImplCloneable.java


// Targeting ../RNNImplCloneable.java


// Targeting ../LSTMImplCloneable.java


// Targeting ../GRUImplCloneable.java


// Targeting ../RNNCellImplCloneable.java


// Targeting ../LSTMCellImplCloneable.java


// Targeting ../GRUCellImplCloneable.java


// Targeting ../PixelShuffleImplCloneable.java


// Targeting ../PixelUnshuffleImplCloneable.java


// Targeting ../UpsampleImplCloneable.java


// Targeting ../ELUImplCloneable.java


// Targeting ../SELUImplCloneable.java


// Targeting ../HardshrinkImplCloneable.java


// Targeting ../HardtanhImplCloneable.java


// Targeting ../LeakyReLUImplCloneable.java


// Targeting ../LogSigmoidImplCloneable.java


// Targeting ../SoftmaxImplCloneable.java


// Targeting ../SoftminImplCloneable.java


// Targeting ../LogSoftmaxImplCloneable.java


// Targeting ../Softmax2dImplCloneable.java


// Targeting ../PReLUImplCloneable.java


// Targeting ../ReLUImplCloneable.java


// Targeting ../ReLU6ImplCloneable.java


// Targeting ../RReLUImplCloneable.java


// Targeting ../CELUImplCloneable.java


// Targeting ../GLUImplCloneable.java


// Targeting ../GELUImplCloneable.java


// Targeting ../SiLUImplCloneable.java


// Targeting ../SigmoidImplCloneable.java


// Targeting ../SoftplusImplCloneable.java


// Targeting ../SoftshrinkImplCloneable.java


// Targeting ../SoftsignImplCloneable.java


// Targeting ../TanhImplCloneable.java


// Targeting ../TanhshrinkImplCloneable.java


// Targeting ../ThresholdImplCloneable.java


// Targeting ../MultiheadAttentionImplCloneable.java


// Targeting ../LayerNormImplCloneable.java


// Targeting ../LocalResponseNormImplCloneable.java


// Targeting ../CrossMapLRN2dImplCloneable.java


// Targeting ../GroupNormImplCloneable.java


// Targeting ../TransformerEncoderLayerImplCloneable.java


// Targeting ../TransformerDecoderLayerImplCloneable.java


// Targeting ../TransformerEncoderImplCloneable.java


// Targeting ../TransformerDecoderImplCloneable.java


// Targeting ../TransformerImplCloneable.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/init.h

// #pragma once

// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/enum.h>
// #include <torch/types.h>

 // namespace init
 // nn

/** Return the recommended gain value for the given nonlinearity function. */
@Namespace("torch::nn::init") public static native double calculate_gain(@ByVal @Cast("torch::nn::init::NonlinearityType*") Pointer nonlinearity, double param/*=0.01*/);
@Namespace("torch::nn::init") public static native double calculate_gain(@ByVal @Cast("torch::nn::init::NonlinearityType*") Pointer nonlinearity);

/** Fills the given {@code tensor} with the provided {@code value} in-place, and returns it.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor constant_(@ByVal Tensor tensor, @ByVal Scalar value);

/** Fills the given {@code tensor} with the Dirac delta function in-place, and returns
 *  it. No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor dirac_(@ByVal Tensor tensor);

/** Fills the given 2-dimensional {@code matrix} with an identity matrix.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor eye_(@ByVal Tensor matrix);

/** Fills the given 2-dimensional {@code matrix} with values drawn from a normal
 *  distribution parameterized by {@code mean} and {@code std}.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor normal_(@ByVal Tensor tensor, double mean/*=0*/, double std/*=1*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor normal_(@ByVal Tensor tensor);

/** Fills the given {@code tensor} with ones.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor ones_(@ByVal Tensor tensor);

/** Fills the input {@code Tensor} with a (semi) orthogonal matrix, as described in
 *  "Exact solutions to the nonlinear dynamics of learning in deep linear neural
 *  networks" - Saxe, A. et al. (2013). The input tensor must have at least 2
 *  dimensions, and for tensors with more than 2 dimensions the trailing
 *  dimensions are flattened.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor orthogonal_(@ByVal Tensor tensor, double gain/*=1.0*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor orthogonal_(@ByVal Tensor tensor);

/** Fills the 2D input {@code Tensor} as a sparse matrix, where the
 *  non-zero elements will be drawn from a centered normal distribution
 *  with the given standard deviation {@code std}, as described in "Deep learning via
 *  Hessian-free optimization" - Martens, J. (2010). The {@code sparsity} is a real
 *  value between 0 and 1 that controls the fraction of elements in each column
 *  to be set to zero.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor sparse_(@ByVal Tensor tensor, double sparsity, double std/*=0.01*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor sparse_(@ByVal Tensor tensor, double sparsity);

/** Fills the given 2-dimensional {@code matrix} with values drawn from a uniform
 *  distribution parameterized by {@code low} and {@code high}.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor uniform_(@ByVal Tensor tensor, double low/*=0*/, double high/*=1*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor uniform_(@ByVal Tensor tensor);

/** Fills the input {@code Tensor} with values according to the method
 *  described in "Delving deep into rectifiers: Surpassing human-level
 *  performance on ImageNet classification" - He, K. et al. (2015), using a
 *  normal distribution. Also known as He initialization.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor kaiming_normal_(
    @ByVal Tensor tensor,
    double a/*=0*/,
    @ByVal(nullValue = "torch::nn::init::FanModeType(torch::kFanIn)") @Cast("torch::nn::init::FanModeType*") Pointer mode,
    @ByVal(nullValue = "torch::nn::init::NonlinearityType(torch::kLeakyReLU)") @Cast("torch::nn::init::NonlinearityType*") Pointer nonlinearity);
@Namespace("torch::nn::init") public static native @ByVal Tensor kaiming_normal_(
    @ByVal Tensor tensor);

/** Fills the input {@code Tensor} with values according to the method
 *  described in "Delving deep into rectifiers: Surpassing human-level
 *  performance on ImageNet classification" - He, K. et al. (2015), using a
 *  uniform distribution. Also known as He initialization.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor kaiming_uniform_(
    @ByVal Tensor tensor,
    double a/*=0*/,
    @ByVal(nullValue = "torch::nn::init::FanModeType(torch::kFanIn)") @Cast("torch::nn::init::FanModeType*") Pointer mode,
    @ByVal(nullValue = "torch::nn::init::NonlinearityType(torch::kLeakyReLU)") @Cast("torch::nn::init::NonlinearityType*") Pointer nonlinearity);
@Namespace("torch::nn::init") public static native @ByVal Tensor kaiming_uniform_(
    @ByVal Tensor tensor);

/** Fills the input {@code Tensor} with values according to the method
 *  described in "Understanding the difficulty of training deep feedforward
 *  neural networks" - Glorot, X. & Bengio, Y. (2010). Values are scaled by the
 *  {@code gain} parameter. No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor xavier_normal_(@ByVal Tensor tensor, double gain/*=1.0*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor xavier_normal_(@ByVal Tensor tensor);

/** Fills the input {@code Tensor} with values according to the method
 *  described in "Understanding the difficulty of training deep feedforward
 *  neural networks" - Glorot, X. & Bengio, Y. (2010), using a uniform
 *  distribution. Values are scaled by the {@code gain} parameter
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor xavier_uniform_(@ByVal Tensor tensor, double gain/*=1.0*/);
@Namespace("torch::nn::init") public static native @ByVal Tensor xavier_uniform_(@ByVal Tensor tensor);

/** Fills the given {@code tensor} with zeros.
 *  No gradient will be recorded for this operation. */
@Namespace("torch::nn::init") public static native @ByVal Tensor zeros_(@ByVal Tensor tensor);

@Namespace("torch::nn::init") public static native @ByVal @Cast("std::tuple<int64_t,int64_t>*") LongPointer _calculate_fan_in_and_fan_out(@Const @ByRef Tensor tensor);

 // namespace init
 // namespace nn
 // namespace torch


// Parsed from torch/nn/pimpl.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/detail/static.h>
// #include <torch/serialize/archive.h>
// #include <torch/types.h>

// #include <torch/csrc/utils/variadic.h>

// #include <memory>
// #include <type_traits>
// #include <utility>
// Dump all the template metaprogramming in this file.
// #include <torch/csrc/api/include/torch/nn/pimpl-inl.h>
 // namespace detail
// Targeting ../ModuleHolder.java


// Targeting ../ModuleDictImplModuleHolder.java


// Targeting ../ModuleListImplModuleHolder.java


// Targeting ../SequentialImplModuleHolder.java


// Targeting ../ParameterDictImplModuleHolder.java


// Targeting ../ParameterListImplModuleHolder.java


// Targeting ../AdaptiveLogSoftmaxWithLossImplModuleHolder.java


// Targeting ../BatchNorm1dImplModuleHolder.java


// Targeting ../InstanceNorm1dImplModuleHolder.java


// Targeting ../Conv1dImplModuleHolder.java


// Targeting ../ConvTranspose1dImplModuleHolder.java


// Targeting ../DropoutImplModuleHolder.java


// Targeting ../BatchNorm2dImplModuleHolder.java


// Targeting ../InstanceNorm2dImplModuleHolder.java


// Targeting ../Conv2dImplModuleHolder.java


// Targeting ../ConvTranspose2dImplModuleHolder.java


// Targeting ../Dropout2dImplModuleHolder.java


// Targeting ../BatchNorm3dImplModuleHolder.java


// Targeting ../InstanceNorm3dImplModuleHolder.java


// Targeting ../Conv3dImplModuleHolder.java


// Targeting ../ConvTranspose3dImplModuleHolder.java


// Targeting ../Dropout3dImplModuleHolder.java


// Targeting ../AlphaDropoutImplModuleHolder.java


// Targeting ../FeatureAlphaDropoutImplModuleHolder.java


// Targeting ../CosineSimilarityImplModuleHolder.java


// Targeting ../PairwiseDistanceImplModuleHolder.java


// Targeting ../EmbeddingImplModuleHolder.java


// Targeting ../EmbeddingBagImplModuleHolder.java


// Targeting ../FoldImplModuleHolder.java


// Targeting ../UnfoldImplModuleHolder.java


// Targeting ../IdentityImplModuleHolder.java


// Targeting ../LinearImplModuleHolder.java


// Targeting ../BilinearImplModuleHolder.java


// Targeting ../FlattenImplModuleHolder.java


// Targeting ../UnflattenImplModuleHolder.java


// Targeting ../L1LossImplModuleHolder.java


// Targeting ../KLDivLossImplModuleHolder.java


// Targeting ../MSELossImplModuleHolder.java


// Targeting ../BCELossImplModuleHolder.java


// Targeting ../HingeEmbeddingLossImplModuleHolder.java


// Targeting ../MultiMarginLossImplModuleHolder.java


// Targeting ../CosineEmbeddingLossImplModuleHolder.java


// Targeting ../SmoothL1LossImplModuleHolder.java


// Targeting ../MultiLabelMarginLossImplModuleHolder.java


// Targeting ../SoftMarginLossImplModuleHolder.java


// Targeting ../MultiLabelSoftMarginLossImplModuleHolder.java


// Targeting ../TripletMarginLossImplModuleHolder.java


// Targeting ../TripletMarginWithDistanceLossImplModuleHolder.java


// Targeting ../CTCLossImplModuleHolder.java


// Targeting ../PoissonNLLLossImplModuleHolder.java


// Targeting ../MarginRankingLossImplModuleHolder.java


// Targeting ../NLLLossImplModuleHolder.java


// Targeting ../CrossEntropyLossImplModuleHolder.java


// Targeting ../BCEWithLogitsLossImplModuleHolder.java


// Targeting ../ReflectionPad1dImplModuleHolder.java


// Targeting ../ReplicationPad1dImplModuleHolder.java


// Targeting ../ConstantPad1dImplModuleHolder.java


// Targeting ../AvgPool1dImplModuleHolder.java


// Targeting ../MaxPool1dImplModuleHolder.java


// Targeting ../AdaptiveAvgPool1dImplModuleHolder.java


// Targeting ../AdaptiveMaxPool1dImplModuleHolder.java


// Targeting ../MaxUnpool1dImplModuleHolder.java


// Targeting ../LPPool1dImplModuleHolder.java


// Targeting ../ReflectionPad2dImplModuleHolder.java


// Targeting ../ReplicationPad2dImplModuleHolder.java


// Targeting ../ConstantPad2dImplModuleHolder.java


// Targeting ../ZeroPad2dImplModuleHolder.java


// Targeting ../AvgPool2dImplModuleHolder.java


// Targeting ../MaxPool2dImplModuleHolder.java


// Targeting ../AdaptiveAvgPool2dImplModuleHolder.java


// Targeting ../AdaptiveMaxPool2dImplModuleHolder.java


// Targeting ../MaxUnpool2dImplModuleHolder.java


// Targeting ../FractionalMaxPool2dImplModuleHolder.java


// Targeting ../LPPool2dImplModuleHolder.java


// Targeting ../ReplicationPad3dImplModuleHolder.java


// Targeting ../ConstantPad3dImplModuleHolder.java


// Targeting ../AvgPool3dImplModuleHolder.java


// Targeting ../MaxPool3dImplModuleHolder.java


// Targeting ../AdaptiveAvgPool3dImplModuleHolder.java


// Targeting ../AdaptiveMaxPool3dImplModuleHolder.java


// Targeting ../MaxUnpool3dImplModuleHolder.java


// Targeting ../FractionalMaxPool3dImplModuleHolder.java


// Targeting ../RNNImplModuleHolder.java


// Targeting ../LSTMImplModuleHolder.java


// Targeting ../GRUImplModuleHolder.java


// Targeting ../RNNCellImplModuleHolder.java


// Targeting ../LSTMCellImplModuleHolder.java


// Targeting ../GRUCellImplModuleHolder.java


// Targeting ../PixelShuffleImplModuleHolder.java


// Targeting ../PixelUnshuffleImplModuleHolder.java


// Targeting ../UpsampleImplModuleHolder.java


// Targeting ../ELUImplModuleHolder.java


// Targeting ../SELUImplModuleHolder.java


// Targeting ../HardshrinkImplModuleHolder.java


// Targeting ../HardtanhImplModuleHolder.java


// Targeting ../LeakyReLUImplModuleHolder.java


// Targeting ../LogSigmoidImplModuleHolder.java


// Targeting ../SoftmaxImplModuleHolder.java


// Targeting ../SoftminImplModuleHolder.java


// Targeting ../LogSoftmaxImplModuleHolder.java


// Targeting ../Softmax2dImplModuleHolder.java


// Targeting ../PReLUImplModuleHolder.java


// Targeting ../ReLUImplModuleHolder.java


// Targeting ../ReLU6ImplModuleHolder.java


// Targeting ../RReLUImplModuleHolder.java


// Targeting ../CELUImplModuleHolder.java


// Targeting ../GLUImplModuleHolder.java


// Targeting ../GELUImplModuleHolder.java


// Targeting ../SiLUImplModuleHolder.java


// Targeting ../SigmoidImplModuleHolder.java


// Targeting ../SoftplusImplModuleHolder.java


// Targeting ../SoftshrinkImplModuleHolder.java


// Targeting ../SoftsignImplModuleHolder.java


// Targeting ../TanhImplModuleHolder.java


// Targeting ../TanhshrinkImplModuleHolder.java


// Targeting ../ThresholdImplModuleHolder.java


// Targeting ../MultiheadAttentionImplModuleHolder.java


// Targeting ../LayerNormImplModuleHolder.java


// Targeting ../LocalResponseNormImplModuleHolder.java


// Targeting ../CrossMapLRN2dImplModuleHolder.java


// Targeting ../GroupNormImplModuleHolder.java


// Targeting ../TransformerEncoderLayerImplModuleHolder.java


// Targeting ../TransformerDecoderLayerImplModuleHolder.java


// Targeting ../TransformerEncoderImplModuleHolder.java


// Targeting ../TransformerDecoderImplModuleHolder.java


// Targeting ../TransformerImplModuleHolder.java



/** Pretty prints the given {@code Module} into the {@code ostream}. */

/** Serializes a {@code ModuleHolder} into an {@code OutputArchive}. */

/** Deserializes a {@code ModuleHolder} from an {@code InputArchive}. */

 // namespace nn
 // namespace torch

/** Defines a class {@code Name} which inherits from {@code nn::ModuleHolder} to provide a
 *  wrapper over a {@code std::shared_ptr<ImplType>}.
 *  {@code Impl} is a type alias for {@code ImplType} which provides a way to call static
 *  method of {@code ImplType}. */
// #define TORCH_MODULE_IMPL(Name, ImplType)
//   class Name : public torch::nn::ModuleHolder<ImplType> { /* NOLINT */
//    public:
//     using torch::nn::ModuleHolder<ImplType>::ModuleHolder;
//     using Impl = ImplType;
//   }

/** Like {@code TORCH_MODULE_IMPL}, but defaults the {@code ImplType} name to {@code <Name>Impl}. */
// #define TORCH_MODULE(Name) TORCH_MODULE_IMPL(Name, Name##Impl)


// Parsed from torch/nn/utils.h

// #pragma once

// #include <torch/nn/utils/clip_grad.h>
// #include <torch/nn/utils/convert_parameters.h>
// #include <torch/nn/utils/rnn.h>


// Parsed from torch/nn/utils/clip_grad.h

// #pragma once

// #include <torch/csrc/WindowsTorchApiMacro.h>

// Clips gradient norm of a vector of Tensors.
// See
// https://pytorch.org/docs/stable/nn.html?highlight=clip_grad_norm#torch.nn.utils.clip_grad_norm_
// for more details about this module.
@Namespace("torch::nn::utils") public static native double clip_grad_norm_(
    @StdMove TensorVector parameters,
    double max_norm,
    double norm_type/*=2.0*/);
@Namespace("torch::nn::utils") public static native double clip_grad_norm_(
    @StdMove TensorVector parameters,
    double max_norm);

// A wrapper around clip_grad_norm_ that allows us to call the function with a
// braced-init-list of Tensors.

// A wrapper around clip_grad_norm_ that allows us to call the function with a
// single Tensor.
@Namespace("torch::nn::utils") public static native double clip_grad_norm_(
    @ByVal Tensor parameter,
    double max_norm,
    double norm_type/*=2.0*/);
@Namespace("torch::nn::utils") public static native double clip_grad_norm_(
    @ByVal Tensor parameter,
    double max_norm);

// Clips gradient of an iterable of parameters at specified value.
// Gradients are modified in-place.
// See https://pytorch.org/docs/stable/nn.html#clip-grad-value
// for more details about this module.
@Namespace("torch::nn::utils") public static native void clip_grad_value_(
    @StdMove TensorVector parameters,
    double clip_value);

// A wrapper around clip_grad_value_ that allows us to call the function with a
// braced-init-list of Tensors.

// A wrapper around clip_grad_value_ that allows us to call the function with a
// single Tensor.
@Namespace("torch::nn::utils") public static native void clip_grad_value_(@ByVal Tensor parameter, double clip_value);

 // namespace utils
 // namespace nn
 // namespace torch


// Parsed from torch/nn/utils/convert_parameters.h

// #pragma once

// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>

// This helper function is to check if the parameters are located
// in the same device. Currently, the conversion between model parameters
// and single vector form is not supported for multiple allocations,
// e.g. parameters in different GPUs, or mixture of CPU/GPU.
@Namespace("torch::nn::utils") public static native @ByVal LongOptional _check_param_device(@Const @ByRef Tensor param, @ByVal LongOptional old_param_device);

// Convert parameters to one vector
@Namespace("torch::nn::utils") public static native @ByVal Tensor parameters_to_vector(@StdVector Tensor parameters);

// Convert one vector to the parameters
@Namespace("torch::nn::utils") public static native void vector_to_parameters(@Const @ByRef Tensor vec, @StdVector Tensor parameters);

 // namespace utils
 // namespace nn
 // namespace torch


// Parsed from torch/nn/utils/rnn.h

// #pragma once

// #include <torch/types.h>

@Namespace("torch::nn::utils::rnn") public static native @ByVal Tensor invert_permutation(@Const @ByRef Tensor permutation);
// Targeting ../PackedSequence.java



/** Packs a Tensor containing padded sequences of variable length.
 *  
 *  {@code input} can be of size {@code }T x B x *{@code } where {@code T} is the length of the
 *  longest sequence (equal to {@code }lengths[0]{@code }), {@code }B{@code } is the batch size, and
 *  {@code }*{@code } is any number of dimensions (including 0). If {@code }batch_first{@code } is
 *  {@code }true{@code }, {@code }B x T x *{@code } {@code input} is expected.
 *  
 *  For unsorted sequences, use {@code enforce_sorted = false}. If {@code enforce_sorted} is
 *  {@code }true{@code }, the sequences should be sorted by length in a decreasing order, i.e.
 *  {@code }input[:,0]{@code } should be the longest sequence, and {@code }input[:,B-1]{@code } the shortest
 *  one.
 *  
 *  Note:
 *      This function accepts any input that has at least two dimensions. You
 *      can apply it to pack the labels, and use the output of the RNN with
 *      them to compute the loss directly. A Tensor can be retrieved from
 *      a {@code PackedSequence} object by calling its {@code }.data(){@code } function.
 *  
 *  Arguments:
 *      input (Tensor): padded batch of variable length sequences.
 *      lengths (Tensor): list of sequences lengths of each batch element.
 *      batch_first (bool, optional): if {@code }true{@code }, the input is expected in {@code }B x T x *{@code }
 *          format. Default: {@code }false{@code }.
 *      enforce_sorted (bool, optional): if {@code }true{@code }, the input is expected to
 *          contain sequences sorted by length in a decreasing order. If
 *          {@code }false{@code }, this condition is not checked. Default: {@code }true{@code }.
 *  
 *  Returns:
 *      a {@code PackedSequence} object */
@Namespace("torch::nn::utils::rnn") public static native @ByVal PackedSequence pack_padded_sequence(
    @ByVal Tensor input,
    @ByVal Tensor lengths,
    @Cast("bool") boolean batch_first/*=false*/,
    @Cast("bool") boolean enforce_sorted/*=true*/);
@Namespace("torch::nn::utils::rnn") public static native @ByVal PackedSequence pack_padded_sequence(
    @ByVal Tensor input,
    @ByVal Tensor lengths);

/** Pads a packed batch of variable length sequences.
 *  
 *  It is an inverse operation to {@code pack_padded_sequence}.
 *  
 *  The returned Tensor's data will be of size {@code }T x B x *{@code }, where {@code T} is the length
 *  of the longest sequence and {@code B} is the batch size. If {@code }batch_first{@code } is true,
 *  the data will be transposed into {@code }B x T x *{@code } format.
 *  
 *  Batch elements will be ordered decreasingly by their length.
 *  
 *  Arguments:
 *      sequence (PackedSequence): batch to pad
 *      batch_first (bool, optional): if {@code }true{@code }, the output will be in {@code }B x T x *{@code }
 *          format.
 *      padding_value (double, optional): values for padded elements.
 *      total_length (int64_t, optional): if specified, the output will be padded to
 *          have length {@code total_length}. This method will throw error
 *          if {@code total_length} is less than the max sequence length in
 *          {@code sequence}.
 *  
 *  Returns:
 *      Tuple of Tensor containing the padded sequence, and a Tensor
 *      containing the list of lengths of each sequence in the batch. */

///
///
///
///
///
@Namespace("torch::nn::utils::rnn") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor pad_packed_sequence(
    @ByVal PackedSequence sequence,
    @Cast("bool") boolean batch_first/*=false*/,
    double padding_value/*=0.0*/,
    @ByVal(nullValue = "c10::optional<int64_t>(torch::nullopt)") LongOptional total_length);
@Namespace("torch::nn::utils::rnn") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor pad_packed_sequence(
    @ByVal PackedSequence sequence);

/** Pad a list of variable length Tensors with {@code }padding_value{@code }
 * 
 *  {@code }pad_sequence{@code } stacks a list of Tensors along a new dimension,
 *  and pads them to equal length. For example, if the input is list of
 *  sequences with size {@code }L x *{@code } and if batch_first is false, and {@code }T x B x *{@code }
 *  otherwise.
 * 
 *  {@code B} is batch size. It is equal to the number of elements in {@code }sequences{@code }.
 *  {@code T} is length of the longest sequence.
 *  {@code L} is length of the sequence.
 *  {@code *} is any number of trailing dimensions, including none.
 * 
 *  Note:
 *      This function returns a Tensor of size {@code }T x B x *{@code } or {@code }B x T x *{@code }
 *      where {@code T} is the length of the longest sequence. This function assumes
 *      trailing dimensions and type of all the Tensors in sequences are same.
 * 
 *  Arguments:
 *      sequences (torch::ArrayRef<Tensor>): list of variable length sequences.
 *      batch_first (bool, optional): output will be in {@code }B x T x *{@code } if true, or in
 *          {@code }T x B x *{@code } otherwise
 *      padding_value (double, optional): value for padded elements. Default: 0.
 * 
 *  Returns:
 *      Tensor of size {@code }T x B x *{@code } if {@code batch_first} is {@code }false{@code }.
 *      Tensor of size {@code }B x T x *{@code } otherwise */
@Namespace("torch::nn::utils::rnn") public static native @ByVal Tensor pad_sequence(
    @ByVal TensorArrayRef sequences,
    @Cast("bool") boolean batch_first/*=false*/,
    double padding_value/*=0*/);
@Namespace("torch::nn::utils::rnn") public static native @ByVal Tensor pad_sequence(
    @ByVal TensorArrayRef sequences);

/** Packs a list of variable length Tensors
 *  
 *  {@code }sequences{@code } should be a list of Tensors of size {@code }L x *{@code }, where {@code L} is
 *  the length of a sequence and {@code *} is any number of trailing dimensions,
 *  including zero.
 *  
 *  For unsorted sequences, use {@code enforce_sorted = false}. If {@code }enforce_sorted{@code }
 *  is {@code }true{@code }, the sequences should be sorted in the order of decreasing length.
 *  
 *  
 *  Arguments:
 *      sequences (torch::ArrayRef<Tensor>): A list of sequences of decreasing length.
 *      enforce_sorted (bool, optional): if {@code }true{@code }, checks that the input
 *          contains sequences sorted by length in a decreasing order. If
 *          {@code }false{@code }, this condition is not checked. Default: {@code }true{@code }.
 *  
 *  Returns:
 *      a {@code PackedSequence} object */
@Namespace("torch::nn::utils::rnn") public static native @ByVal PackedSequence pack_sequence(@ByVal TensorArrayRef sequences, @Cast("bool") boolean enforce_sorted/*=true*/);
@Namespace("torch::nn::utils::rnn") public static native @ByVal PackedSequence pack_sequence(@ByVal TensorArrayRef sequences);

 // namespace rnn
 // namespace utils
 // namespace nn
 // namespace torch


// Parsed from torch/nn/options.h

// #pragma once

// #include <torch/nn/options/batchnorm.h>
// #include <torch/nn/options/conv.h>
// #include <torch/nn/options/dropout.h>
// #include <torch/nn/options/fold.h>
// #include <torch/nn/options/linear.h>
// #include <torch/nn/options/loss.h>
// #include <torch/nn/options/normalization.h>
// #include <torch/nn/options/padding.h>
// #include <torch/nn/options/pooling.h>
// #include <torch/nn/options/vision.h>
// #include <torch/nn/options/rnn.h>
// #include <torch/nn/options/pixelshuffle.h>
// #include <torch/nn/options/upsampling.h>
// #include <torch/nn/options/transformerlayer.h>
// #include <torch/nn/options/transformercoder.h>
// #include <torch/nn/options/transformer.h>


// Parsed from torch/nn/options/activation.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// Targeting ../ELUOptions.java


/** Options for {@code torch::nn::functional::elu}.
 * 
 *  See the documentation for {@code torch::nn::ELUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::elu(x, F::ELUFuncOptions().alpha(0.42).inplace(true));
 *  }</pre> */

// Targeting ../SELUOptions.java


/** Options for {@code torch::nn::functional::selu}.
 * 
 *  See the documentation for {@code torch::nn::SELUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::selu(input, F::SELUFuncOptions(false));
 *  }</pre> */

// Targeting ../GLUOptions.java


/** Options for {@code torch::nn::functional::glu}.
 * 
 *  See the documentation for {@code torch::nn::GLUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::glu(input, GLUFuncOptions(1));
 *  }</pre> */

// Targeting ../HardshrinkOptions.java


/** Options for {@code torch::nn::functional::hardshrink}.
 * 
 *  See the documentation for {@code torch::nn::HardshrinkOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::hardshrink(x, F::HardshrinkFuncOptions().lambda(0.42));
 *  }</pre> */

// Targeting ../HardtanhOptions.java


/** Options for {@code torch::nn::functional::hardtanh}.
 * 
 *  See the documentation for {@code torch::nn::HardtanhOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::hardtanh(x, F::HardtanhFuncOptions().min_val(-1.0).max_val(1.0).inplace(true));
 *  }</pre> */

// Targeting ../LeakyReLUOptions.java


/** Options for {@code torch::nn::functional::leaky_relu}.
 * 
 *  See the documentation for {@code torch::nn::LeakyReLUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::leaky_relu(x, F::LeakyReLUFuncOptions().negative_slope(0.42).inplace(true));
 *  }</pre> */

// Targeting ../SoftmaxOptions.java



// ============================================================================
// Targeting ../SoftmaxFuncOptions.java




// Targeting ../SoftminOptions.java



// ============================================================================
// Targeting ../SoftminFuncOptions.java




// Targeting ../LogSoftmaxOptions.java



// ============================================================================
// Targeting ../LogSoftmaxFuncOptions.java




// Targeting ../PReLUOptions.java


// Targeting ../ReLUOptions.java


/** Options for {@code torch::nn::functional::relu}.
 * 
 *  See the documentation for {@code torch::nn::ReLUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::relu(x, F::ReLUFuncOptions().inplace(true));
 *  }</pre> */

// Targeting ../ReLU6Options.java


/** Options for {@code torch::nn::functional::relu6}.
 * 
 *  See the documentation for {@code torch::nn::ReLU6Options} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::relu6(x, F::ReLU6FuncOptions().inplace(true));
 *  }</pre> */

// Targeting ../RReLUOptions.java



// ============================================================================
// Targeting ../RReLUFuncOptions.java




// Targeting ../CELUOptions.java


/** Options for {@code torch::nn::functional::celu}.
 * 
 *  See the documentation for {@code torch::nn::CELUOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::celu(x, F::CELUFuncOptions().alpha(0.42).inplace(true));
 *  }</pre> */

// Targeting ../SoftplusOptions.java


/** Options for {@code torch::nn::functional::softplus}.
 * 
 *  See the documentation for {@code torch::nn::SoftplusOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::softplus(x, F::SoftplusFuncOptions().beta(0.5).threshold(3.0));
 *  }</pre> */

// Targeting ../SoftshrinkOptions.java


/** Options for {@code torch::nn::functional::softshrink}.
 * 
 *  See the documentation for {@code torch::nn::SoftshrinkOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::softshrink(x, F::SoftshrinkFuncOptions(0.42));
 *  }</pre> */

// Targeting ../ThresholdOptions.java


/** Options for {@code torch::nn::functional::threshold}.
 * 
 *  See the documentation for {@code torch::nn::ThresholdOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::threshold(x, F::ThresholdFuncOptions(0.5, 0.5).inplace(true));
 *  }</pre> */
 // namespace functional

// ============================================================================
// Targeting ../GumbelSoftmaxFuncOptions.java




// Targeting ../MultiheadAttentionOptions.java



// ============================================================================
// Targeting ../MultiheadAttentionForwardFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/adaptive.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// Targeting ../AdaptiveLogSoftmaxWithLossOptions.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/batchnorm.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// Targeting ../BatchNormOptions.java



/** Options for the {@code BatchNorm1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  BatchNorm1d model(BatchNorm1dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */

///

/** Options for the {@code BatchNorm2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  BatchNorm2d model(BatchNorm2dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */

///

/** Options for the {@code BatchNorm3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  BatchNorm3d model(BatchNorm3dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */

// ============================================================================
// Targeting ../BatchNormFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/conv.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/enum.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/expanding_array.h>
// #include <torch/types.h>
// Targeting ../DetailConv1dOptions.java


// Targeting ../DetailConv2dOptions.java


// Targeting ../DetailConv3dOptions.java




// Targeting ../Conv1dOptions.java


// Targeting ../Conv2dOptions.java


// Targeting ../Conv3dOptions.java



/** {@code ConvOptions} specialized for the {@code Conv1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  Conv1d model(Conv1dOptions(3, 2, 3).stride(1).bias(false));
 *  }</pre> */

///

/** {@code ConvOptions} specialized for the {@code Conv2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  Conv2d model(Conv2dOptions(3, 2, 3).stride(1).bias(false));
 *  }</pre> */

///

/** {@code ConvOptions} specialized for the {@code Conv3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  Conv3d model(Conv3dOptions(3, 2, 3).stride(1).bias(false));
 *  }</pre> */

// ============================================================================
// Targeting ../Conv1dFuncOptions.java


// Targeting ../Conv2dFuncOptions.java


// Targeting ../Conv3dFuncOptions.java



/** {@code ConvFuncOptions} specialized for {@code torch::nn::functional::conv1d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv1d(x, weight, F::Conv1dFuncOptions().stride(1));
 *  }</pre> */

///

/** {@code ConvFuncOptions} specialized for {@code torch::nn::functional::conv2d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv2d(x, weight, F::Conv2dFuncOptions().stride(1));
 *  }</pre> */

///

/** {@code ConvFuncOptions} specialized for {@code torch::nn::functional::conv3d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv3d(x, weight, F::Conv3dFuncOptions().stride(1));
 *  }</pre> */


// Targeting ../ConvTranspose1dOptions.java


// Targeting ../ConvTranspose2dOptions.java


// Targeting ../ConvTranspose3dOptions.java



/** {@code ConvTransposeOptions} specialized for the {@code ConvTranspose1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConvTranspose1d model(ConvTranspose1dOptions(3, 2, 3).stride(1).bias(false));
 *  }</pre> */

///

/** {@code ConvTransposeOptions} specialized for the {@code ConvTranspose2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConvTranspose2d model(ConvTranspose2dOptions(3, 2, 3).stride(1).bias(false));
 *  }</pre> */

///

/** {@code ConvTransposeOptions} specialized for the {@code ConvTranspose3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConvTranspose3d model(ConvTranspose3dOptions(2, 2, 2).stride(1).bias(false));
 *  }</pre> */

// ============================================================================
// Targeting ../ConvTranspose1dFuncOptions.java


// Targeting ../ConvTranspose2dFuncOptions.java


// Targeting ../ConvTranspose3dFuncOptions.java



/** {@code ConvTransposeFuncOptions} specialized for {@code torch::nn::functional::conv_transpose1d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv_transpose1d(x, weight, F::ConvTranspose1dFuncOptions().stride(1));
 *  }</pre> */

///

/** {@code ConvTransposeFuncOptions} specialized for {@code torch::nn::functional::conv_transpose2d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv_transpose2d(x, weight, F::ConvTranspose2dFuncOptions().stride(1));
 *  }</pre> */

///

/** {@code ConvTransposeFuncOptions} specialized for {@code torch::nn::functional::conv_transpose3d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::conv_transpose3d(x, weight, F::ConvTranspose3dFuncOptions().stride(1));
 *  }</pre> */

 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/distance.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// Targeting ../CosineSimilarityOptions.java


/** Options for {@code torch::nn::functional::cosine_similarity}.
 * 
 *  See the documentation for {@code torch::nn::CosineSimilarityOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::cosine_similarity(input1, input2, F::CosineSimilarityFuncOptions().dim(1));
 *  }</pre> */

// Targeting ../PairwiseDistanceOptions.java


/** Options for {@code torch::nn::functional::pairwise_distance}.
 * 
 *  See the documentation for {@code torch::nn::PairwiseDistanceOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::pairwise_distance(input1, input2, F::PairwiseDistanceFuncOptions().p(1));
 *  }</pre> */
 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/dropout.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// Targeting ../DropoutOptions.java



/** Options for the {@code Dropout2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  Dropout2d model(Dropout2dOptions().p(0.42).inplace(true));
 *  }</pre> */

///

/** Options for the {@code Dropout3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  Dropout3d model(Dropout3dOptions().p(0.42).inplace(true));
 *  }</pre> */

///

/** Options for the {@code AlphaDropout} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AlphaDropout model(AlphaDropoutOptions(0.2).inplace(true));
 *  }</pre> */

///

/** Options for the {@code FeatureAlphaDropout} module.
 * 
 *  Example:
 *  <pre>{@code
 *  FeatureAlphaDropout model(FeatureAlphaDropoutOptions(0.2).inplace(true));
 *  }</pre> */
// Targeting ../DropoutFuncOptions.java



/** Options for {@code torch::nn::functional::dropout2d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::dropout2d(input, F::Dropout2dFuncOptions().p(0.5));
 *  }</pre> */

///

/** Options for {@code torch::nn::functional::dropout3d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::dropout3d(input, F::Dropout3dFuncOptions().p(0.5));
 *  }</pre> */

///
// Targeting ../AlphaDropoutFuncOptions.java


// Targeting ../FeatureAlphaDropoutFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/embedding.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// #include <torch/enum.h>
// Targeting ../EmbeddingOptions.java


// Targeting ../EmbeddingFromPretrainedOptions.java



// ============================================================================
// Targeting ../EmbeddingFuncOptions.java



 // namespace functional

// ============================================================================


///
// Targeting ../EmbeddingBagOptions.java


// Targeting ../EmbeddingBagFromPretrainedOptions.java



// ============================================================================
// Targeting ../EmbeddingBagFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/fold.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/expanding_array.h>
// #include <torch/types.h>
// Targeting ../FoldOptions.java


/** Options for {@code torch::nn::functional::fold}.
 * 
 *  See the documentation for {@code torch::nn::FoldOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::fold(input, F::FoldFuncOptions({3, 2}, {2, 2}));
 *  }</pre> */

// Targeting ../UnfoldOptions.java


/** Options for {@code torch::nn::functional::unfold}.
 * 
 *  See the documentation for {@code torch::nn::UnfoldOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::unfold(input, F::UnfoldFuncOptions({2, 2}).padding(1).stride(2));
 *  }</pre> */
 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/linear.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// #include <c10/util/variant.h>
// Targeting ../LinearOptions.java


// Targeting ../FlattenOptions.java


// Targeting ../UnflattenOptions.java


// Targeting ../BilinearOptions.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/loss.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/enum.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// Targeting ../L1LossOptions.java


/** Options for {@code torch::nn::functional::l1_loss}.
 * 
 *  See the documentation for {@code torch::nn::L1LossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::l1_loss(input, target, F::L1LossFuncOptions(torch::kNone));
 *  }</pre> */

// Targeting ../KLDivLossOptions.java


/** Options for {@code torch::nn::functional::kl_div}.
 * 
 *  See the documentation for {@code torch::nn::KLDivLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::kl_div(input, target, F::KLDivFuncOptions().reduction(torch::kNone).log_target(false));
 *  }</pre> */

// Targeting ../MSELossOptions.java


/** Options for {@code torch::nn::functional::mse_loss}.
 * 
 *  See the documentation for {@code torch::nn::MSELossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::mse_loss(input, target, F::MSELossFuncOptions(torch::kNone));
 *  }</pre> */

// Targeting ../BCELossOptions.java


/** Options for {@code torch::nn::functional::binary_cross_entropy}.
 * 
 *  See the documentation for {@code torch::nn::BCELossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::binary_cross_entropy(input, target, F::BinaryCrossEntropyFuncOptions().weight(weight));
 *  }</pre> */

// Targeting ../HingeEmbeddingLossOptions.java


/** Options for {@code torch::nn::functional::hinge_embedding_loss}.
 * 
 *  See the documentation for {@code torch::nn::HingeEmbeddingLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::hinge_embedding_loss(input, target, F::HingeEmbeddingLossFuncOptions().margin(2));
 *  }</pre> */

// Targeting ../MultiMarginLossOptions.java


/** Options for {@code torch::nn::functional::multi_margin_loss}.
 * 
 *  See the documentation for {@code torch::nn::MultiMarginLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::multi_margin_loss(input, target, F::MultiMarginLossFuncOptions().margin(2).weight(weight));
 *  }</pre> */

// Targeting ../CosineEmbeddingLossOptions.java


/** Options for {@code torch::nn::functional::cosine_embedding_loss}.
 * 
 *  See the documentation for {@code torch::nn::CosineEmbeddingLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::cosine_embedding_loss(input1, input2, target, F::CosineEmbeddingLossFuncOptions().margin(0.5));
 *  }</pre> */

// Targeting ../MultiLabelMarginLossOptions.java


/** Options for {@code torch::nn::functional::multilabel_margin_loss}.
 * 
 *  See the documentation for {@code torch::nn::MultiLabelMarginLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::multilabel_margin_loss(input, target, F::MultilabelMarginLossFuncOptions(torch::kNone));
 *  }</pre> */

// Targeting ../SoftMarginLossOptions.java


/** Options for {@code torch::nn::functional::soft_margin_loss}.
 * 
 *  See the documentation for {@code torch::nn::SoftMarginLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::soft_margin_loss(input, target, F::SoftMarginLossFuncOptions(torch::kNone));
 *  }</pre> */

// Targeting ../MultiLabelSoftMarginLossOptions.java


/** Options for {@code torch::nn::functional::multilabel_soft_margin_loss}.
 * 
 *  See the documentation for {@code torch::nn::MultiLabelSoftMarginLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::multilabel_soft_margin_loss(input, target, F::MultilabelSoftMarginLossFuncOptions().reduction(torch::kNone).weight(weight));
 *  }</pre> */

// Targeting ../TripletMarginLossOptions.java


/** Options for {@code torch::nn::functional::triplet_margin_loss}.
 * 
 *  See the documentation for {@code torch::nn::TripletMarginLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::triplet_margin_loss(anchor, positive, negative, F::TripletMarginLossFuncOptions().margin(1.0));
 *  }</pre> */

// Targeting ../TripletMarginWithDistanceLossOptions.java


/** Options for {@code torch::nn::functional::triplet_margin_with_distance_loss}.
 * 
 *  See the documentation for {@code torch::nn::TripletMarginWithDistanceLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::triplet_margin_with_distance_loss(anchor, positive, negative, F::TripletMarginWithDistanceLossFuncOptions().margin(1.0));
 *  }</pre> */

// Targeting ../CTCLossOptions.java


/** Options for {@code torch::nn::functional::ctc_loss}.
 * 
 *  See the documentation for {@code torch::nn::CTCLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::ctc_loss(log_probs, targets, input_lengths, target_lengths, F::CTCLossFuncOptions().reduction(torch::kNone));
 *  }</pre> */

// Targeting ../SmoothL1LossOptions.java


/** Options for {@code torch::nn::functional::smooth_l1_loss}.
 * 
 *  See the documentation for {@code torch::nn::SmoothL1LossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::smooth_l1_loss(input, target, F::SmoothL1LossFuncOptions(torch::kNone));
 *  }</pre> */

// Targeting ../PoissonNLLLossOptions.java


/** Options for {@code torch::nn::functional::poisson_nll_loss}.
 * 
 *  See the documentation for {@code torch::nn::PoissonNLLLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::poisson_nll_loss(input, target, F::PoissonNLLLossFuncOptions().reduction(torch::kNone));
 *  }</pre> */

// Targeting ../MarginRankingLossOptions.java


/** Options for {@code torch::nn::functional::margin_ranking_loss}.
 * 
 *  See the documentation for {@code torch::nn::MarginRankingLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::margin_ranking_loss(input1, input2, target, F::MarginRankingLossFuncOptions().margin(0.5).reduction(torch::kSum));
 *  }</pre> */

// Targeting ../NLLLossOptions.java


/** Options for {@code torch::nn::functional::nll_loss}.
 * 
 *  See the documentation for {@code torch::nn::NLLLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::nll_loss(input, target, F::NLLLossFuncOptions().ignore_index(-100).reduction(torch::kMean));
 *  }</pre> */

// Targeting ../CrossEntropyLossOptions.java


/** Options for {@code torch::nn::functional::cross_entropy}.
 * 
 *  See the documentation for {@code torch::nn::CrossEntropyLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::cross_entropy(input, target, F::CrossEntropyFuncOptions().ignore_index(-100).reduction(torch::kMean));
 *  }</pre> */

// Targeting ../BCEWithLogitsLossOptions.java


/** Options for {@code torch::nn::functional::binary_cross_entropy_with_logits}.
 * 
 *  See the documentation for {@code torch::nn::BCEWithLogitsLossOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::binary_cross_entropy_with_logits(input, target, F::BinaryCrossEntropyWithLogitsFuncOptions().pos_weight(pos_weight).reduction(torch::kSum));
 *  }</pre> */
 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/normalization.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// #include <vector>
// Targeting ../LayerNormOptions.java



// ============================================================================
// Targeting ../LayerNormFuncOptions.java




// Targeting ../LocalResponseNormOptions.java


/** Options for {@code torch::nn::functional::local_response_norm}.
 * 
 *  See the documentation for {@code torch::nn::LocalResponseNormOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::local_response_norm(x, F::LocalResponseNormFuncOptions(2));
 *  }</pre> */

// Targeting ../CrossMapLRN2dOptions.java



// ============================================================================
// Targeting ../NormalizeFuncOptions.java




// Targeting ../GroupNormOptions.java



// ============================================================================
// Targeting ../GroupNormFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/padding.h

// #pragma once

// #include <c10/util/variant.h>
// #include <torch/arg.h>
// #include <torch/enum.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/expanding_array.h>
// #include <torch/types.h>
// Targeting ../ReflectionPad1dOptions.java


// Targeting ../ReflectionPad2dOptions.java



/** {@code ReflectionPadOptions} specialized for the {@code ReflectionPad1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ReflectionPad1d model(ReflectionPad1dOptions({3, 1}));
 *  }</pre> */

///

/** {@code ReflectionPadOptions} specialized for the {@code ReflectionPad2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ReflectionPad2d model(ReflectionPad2dOptions({1, 1, 2, 0}));
 *  }</pre> */
// Targeting ../ReplicationPad1dOptions.java


// Targeting ../ReplicationPad2dOptions.java


// Targeting ../ReplicationPad3dOptions.java



/** {@code ReplicationPadOptions} specialized for the {@code ReplicationPad1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ReplicationPad1d model(ReplicationPad1dOptions({3, 1}));
 *  }</pre> */

///

/** {@code ReplicationPadOptions} specialized for the {@code ReplicationPad2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ReplicationPad2d model(ReplicationPad2dOptions({1, 1, 2, 0}));
 *  }</pre> */

///

/** {@code ReplicationPadOptions} specialized for the {@code ReplicationPad3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ReplicationPad3d model(ReplicationPad3dOptions({1, 2, 1, 2, 1, 2}));
 *  }</pre> */

///
// Targeting ../ZeroPad2dOptions.java


// Targeting ../ConstantPad1dOptions.java


// Targeting ../ConstantPad2dOptions.java


// Targeting ../ConstantPad3dOptions.java



/** {@code ConstantPadOptions} specialized for the {@code ConstantPad1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConstantPad1d model(ConstantPad1dOptions({3, 1}, 3.5));
 *  }</pre> */

///

/** {@code ConstantPadOptions} specialized for the {@code ConstantPad2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConstantPad2d model(ConstantPad2dOptions({3, 0, 2, 1}, 3.5));
 *  }</pre> */

///

/** {@code ConstantPadOptions} specialized for the {@code ConstantPad3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  ConstantPad3d model(ConstantPad3dOptions({1, 2, 1, 2, 1, 2}, 3.5));
 *  }</pre> */

// ============================================================================
// Targeting ../PadFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/pixelshuffle.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// Targeting ../PixelShuffleOptions.java


// Targeting ../PixelUnshuffleOptions.java


/** Options for {@code torch::nn::functional::pixel_shuffle}.
 * 
 *  See the documentation for {@code torch::nn::PixelShuffleOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::pixel_shuffle(x, F::PixelShuffleFuncOptions(2));
 *  }</pre> */

///
///

/** Options for {@code torch::nn::functional::pixel_unshuffle}.
 * 
 *  See the documentation for {@code torch::nn::PixelUnshuffleOptions} class to learn
 *  what arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::pixel_unshuffle(x, F::PixelUnshuffleFuncOptions(2));
 *  }</pre> */
 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/pooling.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/expanding_array.h>
// #include <torch/types.h>
// Targeting ../AvgPool1dOptions.java


// Targeting ../AvgPool2dOptions.java


// Targeting ../AvgPool3dOptions.java



/** {@code AvgPoolOptions} specialized for the {@code AvgPool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AvgPool1d model(AvgPool1dOptions(3).stride(2));
 *  }</pre> */

///

/** {@code AvgPoolOptions} specialized for the {@code AvgPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AvgPool2d model(AvgPool2dOptions({3, 2}).stride({2, 2}));
 *  }</pre> */

///

/** {@code AvgPoolOptions} specialized for the {@code AvgPool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AvgPool3d model(AvgPool3dOptions(5).stride(2));
 *  }</pre> */
/** Options for {@code torch::nn::functional::avg_pool1d}.
 * 
 *  See the documentation for {@code torch::nn::AvgPool1dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::avg_pool1d(x, F::AvgPool1dFuncOptions(3).stride(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::avg_pool2d}.
 * 
 *  See the documentation for {@code torch::nn::AvgPool2dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::avg_pool2d(x, F::AvgPool2dFuncOptions(3).stride(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::avg_pool3d}.
 * 
 *  See the documentation for {@code torch::nn::AvgPool3dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::avg_pool3d(x, F::AvgPool3dFuncOptions(3).stride(2));
 *  }</pre> */

// Targeting ../MaxPool1dOptions.java


// Targeting ../MaxPool2dOptions.java


// Targeting ../MaxPool3dOptions.java



/** {@code MaxPoolOptions} specialized for the {@code MaxPool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxPool1d model(MaxPool1dOptions(3).stride(2));
 *  }</pre> */

///

/** {@code MaxPoolOptions} specialized for the {@code MaxPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxPool2d model(MaxPool2dOptions({3, 2}).stride({2, 2}));
 *  }</pre> */

///

/** {@code MaxPoolOptions} specialized for the {@code MaxPool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxPool3d model(MaxPool3dOptions(3).stride(2));
 *  }</pre> */
/** Options for {@code torch::nn::functional::max_pool1d} and {@code torch::nn::functional::max_pool1d_with_indices}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_pool1d(x, F::MaxPool1dFuncOptions(3).stride(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::max_pool2d} and {@code torch::nn::functional::max_pool2d_with_indices}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_pool2d(x, F::MaxPool2dFuncOptions(3).stride(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::max_pool3d} and {@code torch::nn::functional::max_pool3d_with_indices}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_pool3d(x, F::MaxPool3dFuncOptions(3).stride(2));
 *  }</pre> */

// Targeting ../AdaptiveMaxPool1dOptions.java


// Targeting ../AdaptiveMaxPool2dOptions.java


// Targeting ../AdaptiveMaxPool3dOptions.java



/** {@code AdaptiveMaxPoolOptions} specialized for the {@code AdaptiveMaxPool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveMaxPool1d model(AdaptiveMaxPool1dOptions(3));
 *  }</pre> */

///

/** {@code AdaptiveMaxPoolOptions} specialized for the {@code AdaptiveMaxPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveMaxPool2d model(AdaptiveMaxPool2dOptions({3, 2}));
 *  }</pre> */

///

/** {@code AdaptiveMaxPoolOptions} specialized for the {@code AdaptiveMaxPool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveMaxPool3d model(AdaptiveMaxPool3dOptions(3));
 *  }</pre> */
/** Options for {@code torch::nn::functional::adaptive_max_pool1d} and {@code torch::nn::functional::adaptive_max_pool1d_with_indices}
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_max_pool1d(x, F::AdaptiveMaxPool1dFuncOptions(3));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::adaptive_max_pool2d} and {@code torch::nn::functional::adaptive_max_pool2d_with_indices}
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_max_pool2d(x, F::AdaptiveMaxPool2dFuncOptions(3));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::adaptive_max_pool3d} and {@code torch::nn::functional::adaptive_max_pool3d_with_indices}
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_max_pool3d(x, F::AdaptiveMaxPool3dFuncOptions(3));
 *  }</pre> */

// Targeting ../AdaptiveAvgPool1dOptions.java


// Targeting ../AdaptiveAvgPool2dOptions.java


// Targeting ../AdaptiveAvgPool3dOptions.java



/** {@code AdaptiveAvgPoolOptions} specialized for the {@code AdaptiveAvgPool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveAvgPool1d model(AdaptiveAvgPool1dOptions(5));
 *  }</pre> */

///

/** {@code AdaptiveAvgPoolOptions} specialized for the {@code AdaptiveAvgPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveAvgPool2d model(AdaptiveAvgPool2dOptions({3, 2}));
 *  }</pre> */

///

/** {@code AdaptiveAvgPoolOptions} specialized for the {@code AdaptiveAvgPool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  AdaptiveAvgPool3d model(AdaptiveAvgPool3dOptions(3));
 *  }</pre> */
/** Options for {@code torch::nn::functional::adaptive_avg_pool1d}.
 * 
 *  See the documentation for {@code torch::nn::AdaptiveAvgPool1dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_avg_pool1d(x, F::AdaptiveAvgPool1dFuncOptions(3));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::adaptive_avg_pool2d}.
 * 
 *  See the documentation for {@code torch::nn::AdaptiveAvgPool2dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_avg_pool2d(x, F::AdaptiveAvgPool2dFuncOptions(3));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::adaptive_avg_pool3d}.
 * 
 *  See the documentation for {@code torch::nn::AdaptiveAvgPool3dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_avg_pool3d(x, F::AdaptiveAvgPool3dFuncOptions(3));
 *  }</pre> */

// Targeting ../MaxUnpool1dOptions.java


// Targeting ../MaxUnpool2dOptions.java


// Targeting ../MaxUnpool3dOptions.java



/** {@code MaxUnpoolOptions} specialized for the {@code MaxUnpool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxUnpool1d model(MaxUnpool1dOptions(3).stride(2).padding(1));
 *  }</pre> */

///

/** {@code MaxUnpoolOptions} specialized for the {@code MaxUnpool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxUnpool2d model(MaxUnpool2dOptions(3).stride(2).padding(1));
 *  }</pre> */

///

/** {@code MaxUnpoolOptions} specialized for the {@code MaxUnpool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  MaxUnpool3d model(MaxUnpool3dOptions(3).stride(2).padding(1));
 *  }</pre> */

// ============================================================================
// Targeting ../MaxUnpool1dFuncOptions.java


// Targeting ../MaxUnpool2dFuncOptions.java


// Targeting ../MaxUnpool3dFuncOptions.java



/** {@code MaxUnpoolFuncOptions} specialized for {@code torch::nn::functional::max_unpool1d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_unpool1d(x, indices, F::MaxUnpool1dFuncOptions(3).stride(2).padding(1));
 *  }</pre> */

///

/** {@code MaxUnpoolFuncOptions} specialized for {@code torch::nn::functional::max_unpool2d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_unpool2d(x, indices, F::MaxUnpool2dFuncOptions(3).stride(2).padding(1));
 *  }</pre> */

///

/** {@code MaxUnpoolFuncOptions} specialized for {@code torch::nn::functional::max_unpool3d}.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::max_unpool3d(x, indices, F::MaxUnpool3dFuncOptions(3));
 *  }</pre> */


// Targeting ../FractionalMaxPool1dOptions.java


// Targeting ../FractionalMaxPool2dOptions.java


// Targeting ../FractionalMaxPool3dOptions.java



/** {@code FractionalMaxPoolOptions} specialized for the {@code FractionalMaxPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  FractionalMaxPool2d model(FractionalMaxPool2dOptions(5).output_size(1));
 *  }</pre> */

///

/** {@code FractionalMaxPoolOptions} specialized for the {@code FractionalMaxPool3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  FractionalMaxPool3d model(FractionalMaxPool3dOptions(5).output_size(1));
 *  }</pre> */
/** Options for {@code torch::nn::functional::fractional_max_pool2d} and {@code torch::nn::functional::fractional_max_pool2d_with_indices}
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::fractional_max_pool2d(x, F::FractionalMaxPool2dFuncOptions(3).output_size(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::fractional_max_pool3d} and {@code torch::nn::functional::fractional_max_pool3d_with_indices}
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::fractional_max_pool3d(x, F::FractionalMaxPool3dFuncOptions(3).output_size(2));
 *  }</pre> */

// Targeting ../LPPool1dOptions.java


// Targeting ../LPPool2dOptions.java


// Targeting ../LPPool3dOptions.java



/** {@code LPPoolOptions} specialized for the {@code LPPool1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  LPPool1d model(LPPool1dOptions(1, 2).stride(5).ceil_mode(true));
 *  }</pre> */

///

/** {@code LPPoolOptions} specialized for the {@code LPPool2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  LPPool2d model(LPPool2dOptions(1, std::vector<int64_t>({3, 4})).stride({5, 6}).ceil_mode(true));
 *  }</pre> */
/** Options for {@code torch::nn::functional::lp_pool1d}.
 * 
 *  See the documentation for {@code torch::nn::LPPool1dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::lp_pool1d(x, F::LPPool1dFuncOptions(2, 3).stride(2));
 *  }</pre> */
 // namespace functional
/** Options for {@code torch::nn::functional::lp_pool2d}.
 * 
 *  See the documentation for {@code torch::nn::LPPool2dOptions} class to learn what
 *  arguments are supported.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::lp_pool2d(x, F::LPPool2dFuncOptions(2, {2, 3}).stride(2));
 *  }</pre> */
 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/rnn.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/enum.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// Targeting ../RNNOptionsBase.java




// Targeting ../RNNOptions.java


// Targeting ../LSTMOptions.java


// Targeting ../GRUOptions.java


// Targeting ../RNNCellOptionsBase.java




// Targeting ../RNNCellOptions.java


// Targeting ../LSTMCellOptions.java


// Targeting ../GRUCellOptions.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/upsampling.h

// #pragma once

// #include <c10/util/variant.h>
// #include <torch/arg.h>
// #include <torch/enum.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/expanding_array.h>
// #include <torch/types.h>

// #include <vector>
// Targeting ../UpsampleOptions.java


// Targeting ../InterpolateFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/vision.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/enum.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// Targeting ../GridSampleFuncOptions.java



 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/instancenorm.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/nn/options/batchnorm.h>
// #include <torch/types.h>
// Targeting ../InstanceNormOptions.java



/** Options for the {@code InstanceNorm1d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  InstanceNorm1d model(InstanceNorm1dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */

///

/** Options for the {@code InstanceNorm2d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  InstanceNorm2d model(InstanceNorm2dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */

///

/** Options for the {@code InstanceNorm3d} module.
 * 
 *  Example:
 *  <pre>{@code
 *  InstanceNorm3d model(InstanceNorm3dOptions(4).eps(0.5).momentum(0.1).affine(false).track_running_stats(true));
 *  }</pre> */
// Targeting ../InstanceNormFuncOptions.java



 // namespace functional

 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/transformerlayer.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// #include <torch/enum.h>
// Targeting ../TransformerEncoderLayerOptions.java


// Targeting ../TransformerDecoderLayerOptions.java




 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/transformercoder.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// #include <torch/enum.h>

// #include <torch/nn/modules/transformerlayer.h>
// #include <torch/nn/modules/container/any.h>
// Targeting ../TransformerEncoderOptions.java


// Targeting ../TransformerDecoderOptions.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/options/transformer.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/csrc/WindowsTorchApiMacro.h>
// #include <torch/types.h>
// #include <torch/enum.h>

// #include <torch/nn/modules/container/any.h>
// Targeting ../TransformerOptions.java




 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional.h

// #pragma once

// #include <torch/nn/functional/batchnorm.h>
// #include <torch/nn/functional/conv.h>
// #include <torch/nn/functional/distance.h>
// #include <torch/nn/functional/dropout.h>
// #include <torch/nn/functional/embedding.h>
// #include <torch/nn/functional/fold.h>
// #include <torch/nn/functional/linear.h>
// #include <torch/nn/functional/loss.h>
// #include <torch/nn/functional/normalization.h>
// #include <torch/nn/functional/padding.h>
// #include <torch/nn/functional/pixelshuffle.h>
// #include <torch/nn/functional/pooling.h>
// #include <torch/nn/functional/upsampling.h>
// #include <torch/nn/functional/vision.h>
// #include <torch/nn/functional/instancenorm.h>


// Parsed from torch/nn/functional/activation.h

// #pragma once

// #include <torch/nn/options/activation.h>
// #include <torch/nn/options/linear.h>
// #include <torch/nn/options/dropout.h>
// #include <torch/types.h>
// #include <torch/nn/functional/linear.h>
// #include <torch/nn/functional/dropout.h>
// #include <limits>
// #include <ATen/Dispatch.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor elu(@ByVal Tensor input, double alpha, @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.elu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::ELUFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::elu(x, F::ELUFuncOptions().alpha(0.42).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor elu(@ByVal Tensor input, @Cast("const torch::nn::functional::ELUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::ELUFuncOptions{}") ELUOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor selu(@ByVal Tensor input, @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.selu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SELUFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::selu(input, F::SELUFuncOptions(false));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor selu(@ByVal Tensor input, @Cast("const torch::nn::functional::SELUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::SELUFuncOptions{}") SELUOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor hardshrink(@Const @ByRef Tensor input,
                         double lambda);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.hardshrink
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::HardshrinkFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::hardshrink(x, F::HardshrinkFuncOptions().lambda(0.42));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor hardshrink(@Const @ByRef Tensor input,
                         @Cast("const torch::nn::functional::HardshrinkFuncOptions*") @ByRef(nullValue = "torch::nn::functional::HardshrinkFuncOptions{}") HardshrinkOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor hardtanh(@ByVal Tensor input,
                       double min_val,
                       double max_val,
                       @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.hardtanh
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::HardtanhFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::hardtanh(x, F::HardtanhFuncOptions().min_val(-1.0).max_val(1.0).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor hardtanh(@ByVal Tensor input, @Cast("const torch::nn::functional::HardtanhFuncOptions*") @ByRef(nullValue = "torch::nn::functional::HardtanhFuncOptions{}") HardtanhOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor leaky_relu(@ByVal Tensor input,
                         double negative_slope,
                         @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.leaky_relu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::LeakyReLUFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::leaky_relu(x, F::LeakyReLUFuncOptions().negative_slope(0.42).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor leaky_relu(@ByVal Tensor input, @Cast("const torch::nn::functional::LeakyReLUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::LeakyReLUFuncOptions{}") LeakyReLUOptions options);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal Tensor logsigmoid(@Const @ByRef Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor gumbel_softmax(@Const @ByRef Tensor logits,
                             double tau,
                             @Cast("bool") boolean hard,
                             int dim);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.gumbel_softmax
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::GumbelSoftmaxFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::gumbel_softmax(logits, F::GumbelSoftmaxFuncOptions().hard(true).dim(-1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor gumbel_softmax(@Const @ByRef Tensor logits, @Const @ByRef(nullValue = "torch::nn::functional::GumbelSoftmaxFuncOptions{}") GumbelSoftmaxFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor gumbel_softmax(@Const @ByRef Tensor logits);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.softmax
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SoftmaxFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::softmax(input, F::SoftmaxFuncOptions(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor softmax(@Const @ByRef Tensor input, @Const @ByRef SoftmaxFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor softmin(@Const @ByRef Tensor input, @Cast("int64_t") long dim,
                      @ByVal ScalarTypeOptional dtype);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.softmin
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SoftminFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::softmin(input, F::SoftminFuncOptions(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor softmin(@Const @ByRef Tensor input, @Const @ByRef SoftminFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.log_softmax
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::LogSoftmaxFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::log_softmax(input, LogSoftmaxFuncOptions(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor log_softmax(@Const @ByRef Tensor input, @Const @ByRef LogSoftmaxFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.glu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::GLUFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::glu(input, GLUFuncOptions(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor glu(@Const @ByRef Tensor input, @Cast("const torch::nn::functional::GLUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::GLUFuncOptions{}") GLUOptions options);

// ============================================================================

// ============================================================================

// ============================================================================

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor relu(@ByVal Tensor input, @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.relu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::ReLUFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::relu(x, F::ReLUFuncOptions().inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor relu(@ByVal Tensor input, @Cast("const torch::nn::functional::ReLUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::ReLUFuncOptions{}") ReLUOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor relu6(@ByVal Tensor input, @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.relu6
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::ReLU6FuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::relu6(x, F::ReLU6FuncOptions().inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor relu6(@ByVal Tensor input, @Cast("const torch::nn::functional::ReLU6FuncOptions*") @ByRef(nullValue = "torch::nn::functional::ReLU6FuncOptions{}") ReLU6Options options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor relu6(@ByVal Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor rrelu(@ByVal Tensor input,
                    double lower,
                    double upper,
                    @Cast("bool") boolean training,
                    @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.rrelu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::RReLUFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::rrelu(x, F::RReLUFuncOptions().lower(0.1).upper(0.4).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor rrelu(@ByVal Tensor input, @Const @ByRef(nullValue = "torch::nn::functional::RReLUFuncOptions{}") RReLUFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor celu(@ByVal Tensor input,
                   double alpha,
                   @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.celu
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::CELUFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::celu(x, F::CELUFuncOptions().alpha(0.42).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor celu(@ByVal Tensor input, @Cast("const torch::nn::functional::CELUFuncOptions*") @ByRef(nullValue = "torch::nn::functional::CELUFuncOptions{}") CELUOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor softplus(@Const @ByRef Tensor input,
                       double beta,
                       double threshold);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.softplus
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SoftplusFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::softplus(x, F::SoftplusFuncOptions().beta(0.5).threshold(3.0));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor softplus(@Const @ByRef Tensor input,
                       @Cast("const torch::nn::functional::SoftplusFuncOptions*") @ByRef(nullValue = "torch::nn::functional::SoftplusFuncOptions{}") SoftplusOptions options);


// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor softshrink(@Const @ByRef Tensor input,
                         double lambda);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.softshrink
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SoftshrinkFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::softshrink(x, F::SoftshrinkFuncOptions(0.42));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor softshrink(@Const @ByRef Tensor input,
                         @Cast("const torch::nn::functional::SoftshrinkFuncOptions*") @ByRef(nullValue = "torch::nn::functional::SoftshrinkFuncOptions{}") SoftshrinkOptions options);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal Tensor softsign(@Const @ByRef Tensor input);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal Tensor tanhshrink(@Const @ByRef Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor threshold(@ByVal Tensor input,
                        double threshold,
                        double value,
                        @Cast("bool") boolean inplace);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.threshold
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::ThresholdFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::threshold(x, F::ThresholdFuncOptions(0.5, 0.5).inplace(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor threshold(@ByVal Tensor input, @Cast("const torch::nn::functional::ThresholdFuncOptions*") @ByRef ThresholdOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor multi_head_attention_forward(
  @Const @ByRef Tensor query,
  @Const @ByRef Tensor key,
  @Const @ByRef Tensor value,
  @Cast("int64_t") long embed_dim_to_check,
  @Cast("int64_t") long num_heads,
  @Const @ByRef Tensor in_proj_weight,
  @Const @ByRef Tensor in_proj_bias,
  @Const @ByRef Tensor bias_k,
  @Const @ByRef Tensor bias_v,
  @Cast("bool") boolean add_zero_attn,
  double dropout_p,
  @Const @ByRef Tensor out_proj_weight,
  @Const @ByRef Tensor out_proj_bias,
  @Cast("bool") boolean training/*=true*/,
  @Const @ByRef(nullValue = "at::Tensor{}") Tensor key_padding_mask,
  @Cast("bool") boolean need_weights/*=true*/,
  @Const @ByRef(nullValue = "at::Tensor{}") Tensor attn_mask,
  @Cast("bool") boolean use_separate_proj_weight/*=false*/,
  @Const @ByRef(nullValue = "at::Tensor{}") Tensor q_proj_weight,
  @Const @ByRef(nullValue = "at::Tensor{}") Tensor k_proj_weight,
  @Const @ByRef(nullValue = "at::Tensor{}") Tensor v_proj_weight,
  @Const @ByRef(nullValue = "at::Tensor{}") Tensor static_k,
  @Const @ByRef(nullValue = "at::Tensor{}") Tensor static_v);
@Namespace("torch::nn::functional::detail") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor multi_head_attention_forward(
  @Const @ByRef Tensor query,
  @Const @ByRef Tensor key,
  @Const @ByRef Tensor value,
  @Cast("int64_t") long embed_dim_to_check,
  @Cast("int64_t") long num_heads,
  @Const @ByRef Tensor in_proj_weight,
  @Const @ByRef Tensor in_proj_bias,
  @Const @ByRef Tensor bias_k,
  @Const @ByRef Tensor bias_v,
  @Cast("bool") boolean add_zero_attn,
  double dropout_p,
  @Const @ByRef Tensor out_proj_weight,
  @Const @ByRef Tensor out_proj_bias);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

@Namespace("torch::nn::functional") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor multi_head_attention_forward(
  @Const @ByRef Tensor query, @Const @ByRef Tensor key, @Const @ByRef Tensor value,
  @Const @ByRef MultiheadAttentionForwardFuncOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/batchnorm.h

// #pragma once

// #include <torch/nn/options/batchnorm.h>
// #include <torch/types.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor batch_norm(@Const @ByRef Tensor input,
                         @Const @ByRef Tensor running_mean,
                         @Const @ByRef Tensor running_var,
                         @ByVal Tensor weight,
                         @ByVal Tensor bias,
                         @Cast("bool") boolean training,
                         @ByVal DoubleOptional momentum,
                         double eps);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.batch_norm
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::BatchNormFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::batch_norm(input, mean, variance, F::BatchNormFuncOptions().weight(weight).bias(bias).momentum(0.1).eps(1e-05).training(false));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor batch_norm(@Const @ByRef Tensor input, @Const @ByRef Tensor running_mean,
                         @Const @ByRef Tensor running_var, @Const @ByRef(nullValue = "torch::nn::functional::BatchNormFuncOptions{}") BatchNormFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor batch_norm(@Const @ByRef Tensor input, @Const @ByRef Tensor running_mean,
                         @Const @ByRef Tensor running_var);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/conv.h

// #pragma once

// #include <torch/nn/options/conv.h>
// #include <torch/types.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv1d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer padding,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer dilation,
    @Cast("int64_t") long groups);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::Conv1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv1d(x, weight, F::Conv1dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv1d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::nn::functional::Conv1dFuncOptions{}") Conv1dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv2d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer dilation,
    @Cast("int64_t") long groups);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::Conv2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv2d(x, weight, F::Conv2dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv2d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::nn::functional::Conv2dFuncOptions{}") Conv2dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv3d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer padding,
    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer dilation,
    @Cast("int64_t") long groups);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::Conv3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv3d(x, weight, F::Conv3dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv3d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef(nullValue = "torch::nn::functional::Conv3dFuncOptions{}") Conv3dFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight,
                               @Const @ByRef Tensor bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride,
                               @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding,
                               @Cast("int64_t") long groups, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight,
                               @Const @ByRef Tensor bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride,
                               @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding,
                               @Cast("int64_t") long groups, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dilation);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv_transpose1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::ConvTranspose1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv_transpose1d(x, weight, F::ConvTranspose1dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv_transpose1d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight,
                               @Const @ByRef(nullValue = "torch::nn::functional::ConvTranspose1dFuncOptions{}") ConvTranspose1dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight,
                               @Const @ByRef Tensor bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride,
                               @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding,
                               @Cast("int64_t") long groups, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight,
                               @Const @ByRef Tensor bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride,
                               @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding,
                               @Cast("int64_t") long groups, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dilation);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv_transpose2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::ConvTranspose2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv_transpose2d(x, weight, F::ConvTranspose2dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv_transpose2d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight,
                               @Const @ByRef(nullValue = "torch::nn::functional::ConvTranspose2dFuncOptions{}") ConvTranspose2dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight,
                               @Const @ByRef Tensor bias, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride,
                               @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef output_padding,
                               @Cast("int64_t") long groups, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef dilation);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor conv_transpose3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight,
                               @Const @ByRef Tensor bias, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride,
                               @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] output_padding,
                               @Cast("int64_t") long groups, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... dilation);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.conv_transpose3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::ConvTranspose3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::conv_transpose3d(x, weight, F::ConvTranspose3dFuncOptions().stride(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor conv_transpose3d(@Const @ByRef Tensor input, @Const @ByRef Tensor weight,
                               @Const @ByRef(nullValue = "torch::nn::functional::ConvTranspose3dFuncOptions{}") ConvTranspose3dFuncOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/distance.h

// #pragma once

// #include <torch/nn/options/distance.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.cosine_similarity
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::CosineSimilarityFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::cosine_similarity(input1, input2, F::CosineSimilarityFuncOptions().dim(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor cosine_similarity(
    @Const @ByRef Tensor x1,
    @Const @ByRef Tensor x2,
    @Cast("const torch::nn::functional::CosineSimilarityFuncOptions*") @ByRef(nullValue = "torch::nn::functional::CosineSimilarityFuncOptions{}") CosineSimilarityOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.pairwise_distance
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::PairwiseDistanceFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::pairwise_distance(input1, input2, F::PairwiseDistanceFuncOptions().p(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor pairwise_distance(
    @Const @ByRef Tensor x1,
    @Const @ByRef Tensor x2,
    @Cast("const torch::nn::functional::PairwiseDistanceFuncOptions*") @ByRef(nullValue = "torch::nn::functional::PairwiseDistanceFuncOptions{}") PairwiseDistanceOptions options);

// ============================================================================

/** Computes the p-norm distance between every pair of row vectors in the input.
 *  This function will be faster if the rows are contiguous. */

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/dropout.h

// #pragma once

// #include <torch/nn/options/dropout.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor dropout(@ByVal Tensor input, double p, @Cast("bool") boolean training, @Cast("bool") boolean inplace);

 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.dropout
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::DropoutFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::dropout(input, F::DropoutFuncOptions().p(0.5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout(@ByVal Tensor input,
    @Const @ByRef(nullValue = "torch::nn::functional::DropoutFuncOptions{}") DropoutFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout(@ByVal Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor dropout2d(@ByVal Tensor input, double p, @Cast("bool") boolean training, @Cast("bool") boolean inplace);

 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.dropout2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::Dropout2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::dropout2d(input, F::Dropout2dFuncOptions().p(0.5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout2d(@ByVal Tensor input,
    @Cast("const torch::nn::functional::Dropout2dFuncOptions*") @ByRef(nullValue = "torch::nn::functional::Dropout2dFuncOptions{}") DropoutFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout2d(@ByVal Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor dropout3d(@ByVal Tensor input, double p, @Cast("bool") boolean training, @Cast("bool") boolean inplace);

 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.dropout3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::Dropout3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::dropout3d(input, F::Dropout3dFuncOptions().p(0.5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout3d(@ByVal Tensor input,
    @Cast("const torch::nn::functional::Dropout3dFuncOptions*") @ByRef(nullValue = "torch::nn::functional::Dropout3dFuncOptions{}") DropoutFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor dropout3d(@ByVal Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor alpha_dropout(@ByVal Tensor input, double p, @Cast("bool") boolean training, @Cast("bool") boolean inplace);

 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.alpha_dropout
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AlphaDropoutFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::alpha_dropout(input, F::AlphaDropoutFuncOptions().p(0.5).training(false));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor alpha_dropout(@ByVal Tensor input, @Const @ByRef(nullValue = "torch::nn::functional::AlphaDropoutFuncOptions{}") AlphaDropoutFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor alpha_dropout(@ByVal Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor feature_alpha_dropout(@ByVal Tensor input, double p, @Cast("bool") boolean training, @Cast("bool") boolean inplace);

 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.feature_alpha_dropout
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::FeatureAlphaDropoutFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::feature_alpha_dropout(input, F::FeatureAlphaDropoutFuncOptions().p(0.5).training(false));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor feature_alpha_dropout(@ByVal Tensor input, @Const @ByRef(nullValue = "torch::nn::functional::FeatureAlphaDropoutFuncOptions{}") FeatureAlphaDropoutFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor feature_alpha_dropout(@ByVal Tensor input);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/embedding.h

// #pragma once

// #include <torch/nn/options/embedding.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native void _no_grad_embedding_renorm_(@ByVal Tensor weight, @Const @ByRef Tensor input, float max_norm, float norm_type);

@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor embedding(@Const @ByRef Tensor input,
                        @Const @ByRef Tensor weight,
                        @ByVal LongOptional padding_idx,
                        @ByVal DoubleOptional max_norm,
                        double norm_type,
                        @Cast("bool") boolean scale_grad_by_freq,
                        @Cast("bool") boolean sparse);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.embedding
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::EmbeddingFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::embedding(input, weight, F::EmbeddingFuncOptions().norm_type(2.5).scale_grad_by_freq(true).sparse(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor embedding(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "torch::nn::functional::EmbeddingFuncOptions{}") EmbeddingFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor embedding_bag(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor offsets,
    @ByVal DoubleOptional max_norm,
    double norm_type,
    @Cast("bool") boolean scale_grad_by_freq,
    @ByVal @Cast("torch::nn::EmbeddingBagMode*") Pointer mode,
    @Cast("bool") boolean sparse,
    @Const @ByRef Tensor per_sample_weights,
    @Cast("bool") boolean include_last_offset);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.embedding_bag
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::EmbeddingBagFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::embedding_bag(input, weight, F::EmbeddingBagFuncOptions().mode(torch::kSum).offsets(offsets));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor embedding_bag(@Const @ByRef Tensor input, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "torch::nn::functional::EmbeddingBagFuncOptions{}") EmbeddingBagFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor embedding_bag(@Const @ByRef Tensor input, @Const @ByRef Tensor weight);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/fold.h

// #pragma once

// #include <torch/nn/options/fold.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor fold(@Const @ByRef Tensor input,
                   @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer output_size,
                   @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
                   @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer dilation,
                   @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
                   @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.fold
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::FoldFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::fold(input, F::FoldFuncOptions({3, 2}, {2, 2}));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor fold(@Const @ByRef Tensor input, @Cast("const torch::nn::functional::FoldFuncOptions*") @ByRef FoldOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor unfold(@Const @ByRef Tensor input,
                     @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
                     @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer dilation,
                     @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
                     @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.unfold
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::UnfoldFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::unfold(input, F::UnfoldFuncOptions({2, 2}).padding(1).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor unfold(@Const @ByRef Tensor input, @Cast("const torch::nn::functional::UnfoldFuncOptions*") @ByRef UnfoldOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/linear.h

// #pragma once

// #include <torch/types.h>

@Namespace("torch::nn::functional") public static native @ByVal Tensor bilinear(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor weight, @Const @ByRef(nullValue = "at::Tensor()") Tensor bias);
@Namespace("torch::nn::functional") public static native @ByVal Tensor bilinear(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2, @Const @ByRef Tensor weight);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal Tensor linear(@Const @ByRef Tensor input, @Const @ByRef Tensor weight,
                     @Const @ByRef(nullValue = "at::Tensor{}") Tensor bias);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/loss.h

// #pragma once

// #include <ATen/ExpandUtils.h>
// #include <torch/nn/functional/activation.h>
// #include <torch/nn/options/loss.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal @Cast("torch::nn::functional::L1LossFuncOptions::reduction_t*") Pointer reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.l1_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::L1LossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::l1_loss(input, target, F::L1LossFuncOptions(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::L1LossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::L1LossFuncOptions{}") L1LossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor kl_div(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal @Cast("torch::nn::functional::KLDivFuncOptions::reduction_t*") Pointer reduction,
    @Cast("bool") boolean log_target/*=false*/);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor kl_div(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal @Cast("torch::nn::functional::KLDivFuncOptions::reduction_t*") Pointer reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.kl_div
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::KLDivFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::kl_div(input, target, F::KLDivFuncOptions.reduction(torch::kNone).log_target(false));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor kl_div(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::KLDivFuncOptions*") @ByRef(nullValue = "torch::nn::functional::KLDivFuncOptions{}") KLDivLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor mse_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal @Cast("torch::nn::functional::MSELossFuncOptions::reduction_t*") Pointer reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.mse_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MSELossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::mse_loss(input, target, F::MSELossFuncOptions(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor mse_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::MSELossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::MSELossFuncOptions{}") MSELossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor binary_cross_entropy(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Const @ByRef Tensor weight,
    @ByVal @Cast("torch::nn::functional::BinaryCrossEntropyFuncOptions::reduction_t*") Pointer reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.binary_cross_entropy
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::BinaryCrossEntropyFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::binary_cross_entropy(input, target, F::BinaryCrossEntropyFuncOptions().weight(weight));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor binary_cross_entropy(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::BinaryCrossEntropyFuncOptions*") @ByRef(nullValue = "torch::nn::functional::BinaryCrossEntropyFuncOptions{}") BCELossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor hinge_embedding_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    double margin,
    @ByVal @Cast("torch::nn::functional::HingeEmbeddingLossFuncOptions::reduction_t*") Pointer reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.hinge_embedding_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::HingeEmbeddingLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::hinge_embedding_loss(input, target, F::HingeEmbeddingLossFuncOptions().margin(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor hinge_embedding_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::HingeEmbeddingLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::HingeEmbeddingLossFuncOptions{}") HingeEmbeddingLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor multi_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("int64_t") long p,
    double margin,
    @Const @ByRef Tensor weight,
    @ByVal @Cast("torch::nn::functional::MultiMarginLossFuncOptions::reduction_t*") Pointer reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.multi_margin_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MultiMarginLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::multi_margin_loss(input, target, F::MultiMarginLossFuncOptions().margin(2).weight(weight));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor multi_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::MultiMarginLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::MultiMarginLossFuncOptions{}") MultiMarginLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor cosine_embedding_loss(
    @Const @ByRef Tensor input1,
    @Const @ByRef Tensor input2,
    @Const @ByRef Tensor target,
    double margin,
    @ByVal @Cast("torch::nn::functional::CosineEmbeddingLossFuncOptions::reduction_t*") Pointer reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.cosine_embedding_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::CosineEmbeddingLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::cosine_embedding_loss(input1, input2, target, F::CosineEmbeddingLossFuncOptions().margin(0.5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor cosine_embedding_loss(
    @Const @ByRef Tensor input1,
    @Const @ByRef Tensor input2,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::CosineEmbeddingLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::CosineEmbeddingLossFuncOptions{}") CosineEmbeddingLossOptions options);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal Tensor _smooth_l1_loss(@Const @ByRef Tensor input, @Const @ByRef Tensor target, double beta/*=1.*/);
@Namespace("torch::nn::functional") public static native @ByVal Tensor _smooth_l1_loss(@Const @ByRef Tensor input, @Const @ByRef Tensor target);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor smooth_l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal @Cast("torch::nn::functional::SmoothL1LossFuncOptions::reduction_t*") Pointer reduction,
    double beta/*=1.*/);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor smooth_l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal @Cast("torch::nn::functional::SmoothL1LossFuncOptions::reduction_t*") Pointer reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.smooth_l1_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SmoothL1LossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::smooth_l1_loss(input, target, F::SmoothL1LossFuncOptions(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor smooth_l1_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::SmoothL1LossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::SmoothL1LossFuncOptions{}") SmoothL1LossOptions options,
    double beta/*=1.*/);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor multilabel_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal @Cast("torch::nn::functional::MultilabelMarginLossFuncOptions::reduction_t*") Pointer reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.multilabel_margin_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MultilabelMarginLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::multilabel_margin_loss(input, target, F::MultilabelMarginLossFuncOptions(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor multilabel_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::MultilabelMarginLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::MultilabelMarginLossFuncOptions{}") MultiLabelMarginLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor soft_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @ByVal @Cast("torch::nn::functional::SoftMarginLossFuncOptions::reduction_t*") Pointer reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.soft_margin_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::SoftMarginLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::soft_margin_loss(input, target, F::SoftMarginLossFuncOptions(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor soft_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::SoftMarginLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::SoftMarginLossFuncOptions{}") SoftMarginLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor multilabel_soft_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Const @ByRef Tensor weight,
    @ByVal @Cast("torch::nn::functional::MultilabelSoftMarginLossFuncOptions::reduction_t*") Pointer reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.multilabel_soft_margin_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MultilabelSoftMarginLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::multilabel_soft_margin_loss(input, target, F::MultilabelSoftMarginLossFuncOptions().reduction(torch::kNone).weight(weight));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor multilabel_soft_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::MultilabelSoftMarginLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::MultilabelSoftMarginLossFuncOptions{}") MultiLabelSoftMarginLossOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor multilabel_soft_margin_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor triplet_margin_loss(
    @Const @ByRef Tensor anchor,
    @Const @ByRef Tensor positive,
    @Const @ByRef Tensor negative,
    double margin,
    double p,
    double eps,
    @Cast("bool") boolean swap,
    @ByVal @Cast("torch::nn::functional::TripletMarginLossFuncOptions::reduction_t*") Pointer reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.triplet_margin_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::TripletMarginLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::triplet_margin_loss(anchor, positive, negative, F::TripletMarginLossFuncOptions().margin(1.0));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor triplet_margin_loss(
    @Const @ByRef Tensor anchor,
    @Const @ByRef Tensor positive,
    @Const @ByRef Tensor negative,
    @Cast("const torch::nn::functional::TripletMarginLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::TripletMarginLossFuncOptions{}") TripletMarginLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor triplet_margin_with_distance_loss(
    @Const @ByRef Tensor anchor,
    @Const @ByRef Tensor positive,
    @Const @ByRef Tensor negative,
    @ByVal @Cast("c10::optional<torch::nn::functional::TripletMarginWithDistanceLossFuncOptions::distance_function_t>*") Pointer distance_function,
    double margin,
    @Cast("bool") boolean swap,
    @ByVal @Cast("torch::nn::functional::TripletMarginWithDistanceLossFuncOptions::reduction_t*") Pointer reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.triplet_margin_with_distance_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::TripletMarginWithDistanceLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::triplet_margin_with_distance_loss(anchor, positive, negative, F::TripletMarginWithDistanceLossFuncOptions().margin(1.0));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor triplet_margin_with_distance_loss(
    @Const @ByRef Tensor anchor,
    @Const @ByRef Tensor positive,
    @Const @ByRef Tensor negative,
    @Cast("const torch::nn::functional::TripletMarginWithDistanceLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::TripletMarginWithDistanceLossFuncOptions{}") TripletMarginWithDistanceLossOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor triplet_margin_with_distance_loss(
    @Const @ByRef Tensor anchor,
    @Const @ByRef Tensor positive,
    @Const @ByRef Tensor negative);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs,
                       @Const @ByRef Tensor targets,
                       @Const @ByRef Tensor input_lengths,
                       @Const @ByRef Tensor target_lengths,
                       @Cast("int64_t") long blank,
                       @ByVal @Cast("torch::nn::functional::CTCLossFuncOptions::reduction_t*") Pointer reduction,
                       @Cast("bool") boolean zero_infinity);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.ctc_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::CTCLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::ctc_loss(log_probs, targets, input_lengths, target_lengths, F::CTCLossFuncOptions().reduction(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor ctc_loss(@Const @ByRef Tensor log_probs,
                       @Const @ByRef Tensor targets,
                       @Const @ByRef Tensor input_lengths,
                       @Const @ByRef Tensor target_lengths,
                       @Cast("const torch::nn::functional::CTCLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::CTCLossFuncOptions{}") CTCLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor poisson_nll_loss(@Const @ByRef Tensor input,
                               @Const @ByRef Tensor target,
                               @Cast("bool") boolean log_input,
                               @Cast("bool") boolean full,
                               double eps,
                               @ByVal @Cast("torch::nn::functional::PoissonNLLLossFuncOptions::reduction_t*") Pointer reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.poisson_nll_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::PoissonNLLLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::poisson_nll_loss(input, target, F::PoissonNLLLossFuncOptions().reduction(torch::kNone));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor poisson_nll_loss(@Const @ByRef Tensor input, @Const @ByRef Tensor target,
                               @Cast("const torch::nn::functional::PoissonNLLLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::PoissonNLLLossFuncOptions{}") PoissonNLLLossOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor poisson_nll_loss(@Const @ByRef Tensor input, @Const @ByRef Tensor target);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor margin_ranking_loss(@Const @ByRef Tensor input1,
                                  @Const @ByRef Tensor input2,
                                  @Const @ByRef Tensor target,
                                  double margin,
                                  @ByVal @Cast("torch::nn::functional::MarginRankingLossFuncOptions::reduction_t*") Pointer reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.margin_ranking_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MarginRankingLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::margin_ranking_loss(input1, input2, target, F::MarginRankingLossFuncOptions().margin(0.5).reduction(torch::kSum));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor margin_ranking_loss(@Const @ByRef Tensor input1, @Const @ByRef Tensor input2,
  @Const @ByRef Tensor target, @Cast("const torch::nn::functional::MarginRankingLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::MarginRankingLossFuncOptions{}") MarginRankingLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor nll_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Const @ByRef Tensor weight,
    @Cast("int64_t") long ignore_index,
    @ByVal @Cast("const torch::nn::functional::NLLLossFuncOptions::reduction_t*") Pointer reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.nll_loss
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::NLLLossFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::nll_loss(input, target, F::NLLLossFuncOptions().ignore_index(-100).reduction(torch::kMean));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor nll_loss(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::NLLLossFuncOptions*") @ByRef(nullValue = "torch::nn::functional::NLLLossFuncOptions{}") NLLLossOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor cross_entropy(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Const @ByRef Tensor weight,
    @Cast("int64_t") long ignore_index,
    @ByVal @Cast("torch::nn::functional::CrossEntropyFuncOptions::reduction_t*") Pointer reduction);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.cross_entropy
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::CrossEntropyFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::cross_entropy(input, target, F::CrossEntropyFuncOptions().ignore_index(-100).reduction(torch::kMean));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor cross_entropy(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target,
    @Cast("const torch::nn::functional::CrossEntropyFuncOptions*") @ByRef(nullValue = "torch::nn::functional::CrossEntropyFuncOptions{}") CrossEntropyLossOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor cross_entropy(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor target);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor binary_cross_entropy_with_logits(
  @Const @ByRef Tensor input, @Const @ByRef Tensor target, @Const @ByRef Tensor weight,
  @ByVal @Cast("torch::nn::functional::BinaryCrossEntropyWithLogitsFuncOptions::reduction_t*") Pointer reduction, @Const @ByRef Tensor pos_weight);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.binary_cross_entropy_with_logits
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::BinaryCrossEntropyWithLogitsFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::binary_cross_entropy_with_logits(input, target, F::BinaryCrossEntropyWithLogitsFuncOptions().pos_weight(pos_weight).reduction(torch::kSum));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor binary_cross_entropy_with_logits(
  @Const @ByRef Tensor input, @Const @ByRef Tensor target,
  @Cast("const torch::nn::functional::BinaryCrossEntropyWithLogitsFuncOptions*") @ByRef(nullValue = "torch::nn::functional::BinaryCrossEntropyWithLogitsFuncOptions{}") BCEWithLogitsLossOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/normalization.h

// #pragma once

// #include <torch/nn/options/normalization.h>
// #include <torch/nn/functional/padding.h>
// #include <torch/nn/functional/pooling.h>
// #include <torch/types.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor normalize(
    @Const @ByRef Tensor input,
    double p,
    @Cast("int64_t") long dim,
    double eps,
    @ByVal TensorOptional out);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.normalize
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::NormalizeFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::normalize(input, F::NormalizeFuncOptions().p(1).dim(-1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor normalize(
    @Const @ByRef Tensor input,
    @ByVal(nullValue = "torch::nn::functional::NormalizeFuncOptions{}") NormalizeFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor normalize(
    @Const @ByRef Tensor input);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor layer_norm(@Const @ByRef Tensor input,
                         @Cast("const std::vector<int64_t>*") @ByRef LongVector normalized_shape,
                         @Const @ByRef Tensor weight,
                         @Const @ByRef Tensor bias,
                         double eps);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.layer_norm
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::LayerNormFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::layer_norm(input, F::LayerNormFuncOptions({2, 2}).eps(2e-5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor layer_norm(@Const @ByRef Tensor input,
    @Const @ByRef LayerNormFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor local_response_norm(
    @Const @ByRef Tensor input,
    @Cast("int64_t") long size,
    double alpha,
    double beta,
    double k);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.local_response_norm
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::LocalResponseNormFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::local_response_norm(x, F::LocalResponseNormFuncOptions(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor local_response_norm(
    @Const @ByRef Tensor input,
    @Cast("const torch::nn::functional::LocalResponseNormFuncOptions*") @ByRef LocalResponseNormOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor group_norm(
    @Const @ByRef Tensor input,
    @Cast("int64_t") long num_groups,
    @Const @ByRef Tensor weight,
    @Const @ByRef Tensor bias,
    double eps);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.group_norm
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::GroupNormFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::group_norm(input, F::GroupNormFuncOptions(2).eps(2e-5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor group_norm(
    @Const @ByRef Tensor input,
    @Const @ByRef GroupNormFuncOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/padding.h

// #pragma once

// #include <torch/nn/options/padding.h>

@Namespace("torch::nn::functional") public static native @ByVal Tensor _narrow_with_range(@Const @ByRef Tensor input, @Cast("int64_t") long dim, @Cast("int64_t") long start, @Cast("int64_t") long end);

@Namespace("torch::nn::functional") public static native @ByVal Tensor _pad_circular(@ByVal Tensor input, @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding);
@Namespace("torch::nn::functional") public static native @ByVal Tensor _pad_circular(@ByVal Tensor input, @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... padding);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor pad(@Const @ByRef Tensor input,
                  @ByVal @Cast("c10::ArrayRef<int64_t>*") LongArrayRef pad,
                  @ByVal @Cast("torch::nn::functional::PadFuncOptions::mode_t*") Pointer mode,
                  double value);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor pad(@Const @ByRef Tensor input,
                  @ByVal @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] pad,
                  @ByVal @Cast("torch::nn::functional::PadFuncOptions::mode_t*") Pointer mode,
                  double value);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.pad
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::PadFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::pad(input, F::PadFuncOptions({1, 2, 2, 1, 1, 2}).mode(torch::kReplicate));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor pad(@Const @ByRef Tensor input, @Const @ByRef PadFuncOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/pixelshuffle.h

// #pragma once

// #include <torch/nn/options/pixelshuffle.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.pixel_shuffle
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::PixelShuffleFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::pixel_shuffle(x, F::PixelShuffleFuncOptions(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor pixel_shuffle(
    @Const @ByRef Tensor input,
    @Cast("const torch::nn::functional::PixelShuffleFuncOptions*") @ByRef PixelShuffleOptions options);

@Namespace("torch::nn::functional") public static native @ByVal Tensor pixel_unshuffle(
    @Const @ByRef Tensor input,
    @Cast("const torch::nn::functional::PixelUnshuffleFuncOptions*") @ByRef PixelUnshuffleOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/pooling.h

// #pragma once

// #include <torch/nn/functional/activation.h>
// #include <torch/nn/options/pooling.h>
// #include <torch/nn/modules/utils.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor avg_pool1d(@Const @ByRef Tensor input,
                         @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer kernel_size,
                         @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
                         @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer padding,
                         @Cast("bool") boolean ceil_mode,
                         @Cast("bool") boolean count_include_pad);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.avg_pool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AvgPool1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::avg_pool1d(x, F::AvgPool1dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor avg_pool1d(@Const @ByRef Tensor input, @Const @ByRef AvgPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor avg_pool2d(@Const @ByRef Tensor input,
                         @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
                         @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
                         @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
                         @Cast("bool") boolean ceil_mode,
                         @Cast("bool") boolean count_include_pad,
                         @ByVal LongOptional divisor_override);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.avg_pool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AvgPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::avg_pool2d(x, F::AvgPool2dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor avg_pool2d(@Const @ByRef Tensor input, @Const @ByRef AvgPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor avg_pool3d(@Const @ByRef Tensor input,
                         @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer kernel_size,
                         @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer stride,
                         @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer padding,
                         @Cast("bool") boolean ceil_mode,
                         @Cast("bool") boolean count_include_pad,
                         @ByVal LongOptional divisor_override);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.avg_pool3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AvgPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::avg_pool3d(x, F::AvgPool3dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor avg_pool3d(@Const @ByRef Tensor input, @Const @ByRef AvgPool3dOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_pool1d(@Const @ByRef Tensor input,
                         @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer kernel_size,
                         @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
                         @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer padding,
                         @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer dilation,
                         @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_pool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxPool1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool1d(x, F::MaxPool1dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_pool1d(@Const @ByRef Tensor input, @Const @ByRef MaxPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor max_pool1d_with_indices(
  @Const @ByRef Tensor input,
  @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer kernel_size,
  @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
  @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer padding,
  @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer dilation,
  @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::MaxPool1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool1d_with_indices(x, F::MaxPool1dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor max_pool1d_with_indices(@Const @ByRef Tensor input, @Const @ByRef MaxPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_pool2d(@Const @ByRef Tensor input,
                         @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
                         @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
                         @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
                         @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer dilation,
                         @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_pool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool2d(x, F::MaxPool2dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_pool2d(@Const @ByRef Tensor input, @Const @ByRef MaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor max_pool2d_with_indices(
  @Const @ByRef Tensor input,
  @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
  @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
  @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
  @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer dilation,
  @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::MaxPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool2d_with_indices(x, F::MaxPool2dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor max_pool2d_with_indices(@Const @ByRef Tensor input, @Const @ByRef MaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_pool3d(@Const @ByRef Tensor input,
                         @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer kernel_size,
                         @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer stride,
                         @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer padding,
                         @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer dilation,
                         @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_pool3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool3d(x, F::MaxPool3dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_pool3d(@Const @ByRef Tensor input, @Const @ByRef MaxPool3dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor max_pool3d_with_indices(
  @Const @ByRef Tensor input,
  @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer kernel_size,
  @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer stride,
  @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer padding,
  @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer dilation,
  @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::MaxPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_pool3d_with_indices(x, F::MaxPool3dFuncOptions(3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor max_pool3d_with_indices(@Const @ByRef Tensor input, @Const @ByRef MaxPool3dOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor adaptive_max_pool1d_with_indices(
    @Const @ByRef Tensor input, @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer output_size);
 // namespace detail

/** See the documentation for {@code torch::nn::functional::AdaptiveMaxPool1dFuncOptions} class to learn what
 *  optional arguments are supported for this functional.
 * 
 *  Example:
 *  <pre>{@code
 *  namespace F = torch::nn::functional;
 *  F::adaptive_max_pool1d_with_indices(x, F::AdaptiveMaxPool1dFuncOptions(3));
 *  }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor adaptive_max_pool1d_with_indices(
    @Const @ByRef Tensor input, @Const @ByRef AdaptiveMaxPool1dOptions options);
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_max_pool1d(@Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_max_pool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AdaptiveMaxPool1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_max_pool1d(x, F::AdaptiveMaxPool1dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_max_pool1d(@Const @ByRef Tensor input,
    @Const @ByRef AdaptiveMaxPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor adaptive_max_pool2d_with_indices(
    @Const @ByRef Tensor input, @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<2>*") LongPointer output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::AdaptiveMaxPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_max_pool2d_with_indices(x, F::AdaptiveMaxPool2dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor adaptive_max_pool2d_with_indices(
    @Const @ByRef Tensor input, @Const @ByRef AdaptiveMaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_max_pool2d(@Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<2>*") LongPointer output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_max_pool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AdaptiveMaxPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_max_pool2d(x, F::AdaptiveMaxPool2dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_max_pool2d(@Const @ByRef Tensor input,
    @Const @ByRef AdaptiveMaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor adaptive_max_pool3d_with_indices(
    @Const @ByRef Tensor input, @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<3>*") LongPointer output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::AdaptiveMaxPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_max_pool3d_with_indices(x, F::AdaptiveMaxPool3dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor adaptive_max_pool3d_with_indices(
    @Const @ByRef Tensor input, @Const @ByRef AdaptiveMaxPool3dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_max_pool3d(@Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<3>*") LongPointer output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_max_pool3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AdaptiveMaxPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_max_pool3d(x, F::AdaptiveMaxPool3dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_max_pool3d(@Const @ByRef Tensor input,
  @Const @ByRef AdaptiveMaxPool3dOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_avg_pool1d(@Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_avg_pool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AdaptiveAvgPool1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_avg_pool1d(x, F::AdaptiveAvgPool1dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_avg_pool1d(@Const @ByRef Tensor input,
    @Const @ByRef AdaptiveAvgPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_avg_pool2d(@Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<2>*") LongPointer output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_avg_pool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AdaptiveAvgPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_avg_pool2d(x, F::AdaptiveAvgPool2dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_avg_pool2d(@Const @ByRef Tensor input,
    @Const @ByRef AdaptiveAvgPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor adaptive_avg_pool3d(@Const @ByRef Tensor input,
    @ByVal @Cast("torch::ExpandingArrayWithOptionalElem<3>*") LongPointer output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.adaptive_avg_pool3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::AdaptiveAvgPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::adaptive_avg_pool3d(x, F::AdaptiveAvgPool3dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor adaptive_avg_pool3d(@Const @ByRef Tensor input,
    @Const @ByRef AdaptiveAvgPool3dOptions options);

// ============================================================================

@Namespace("torch::nn::functional") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector _unpool_output_size(@Const @ByRef Tensor input,
  @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef kernel_size, @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef stride,
  @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef padding, @Const @ByRef LongVectorOptional output_size);
@Namespace("torch::nn::functional") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector _unpool_output_size(@Const @ByRef Tensor input,
  @ByRef @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] kernel_size, @ByRef @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] stride,
  @ByRef @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] padding, @Const @ByRef LongVectorOptional output_size);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_unpool1d(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor indices,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer kernel_size,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
    @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer padding,
    @Const @ByRef LongVectorOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_unpool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxUnpool1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_unpool1d(x, indices, F::MaxUnpool1dFuncOptions(3).stride(2).padding(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_unpool1d(@Const @ByRef Tensor input, @Const @ByRef Tensor indices,
    @Const @ByRef MaxUnpool1dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_unpool2d(
  @Const @ByRef Tensor input,
  @Const @ByRef Tensor indices,
  @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
  @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
  @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer padding,
  @Const @ByRef LongVectorOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_unpool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxUnpool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_unpool2d(x, indices, F::MaxUnpool2dFuncOptions(3).stride(2).padding(1));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_unpool2d(@Const @ByRef Tensor input, @Const @ByRef Tensor indices,
  @Const @ByRef MaxUnpool2dFuncOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor max_unpool3d(
  @Const @ByRef Tensor input,
  @Const @ByRef Tensor indices,
  @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer kernel_size,
  @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer stride,
  @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer padding,
  @Const @ByRef LongVectorOptional output_size);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.max_unpool3d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::MaxUnpool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::max_unpool3d(x, indices, F::MaxUnpool3dFuncOptions(3));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor max_unpool3d(@Const @ByRef Tensor input, @Const @ByRef Tensor indices,
  @Const @ByRef MaxUnpool3dFuncOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor fractional_max_pool2d_with_indices(
    @Const @ByRef Tensor input,
    @Cast("const torch::ExpandingArray<2>*") @ByRef LongPointer kernel_size,
    @Cast("const c10::optional<torch::ExpandingArray<2> >*") @ByRef LongExpandingArrayOptional output_size,
    @Cast("const c10::optional<torch::ExpandingArray<2,double> >*") @ByRef DoubleExpandingArrayOptional output_ratio,
    @Const @ByRef Tensor _random_samples);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::FractionalMaxPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::fractional_max_pool2d_with_indices(x, F::FractionalMaxPool2dFuncOptions(3).output_size(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor fractional_max_pool2d_with_indices(@Const @ByRef Tensor input, @Const @ByRef FractionalMaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor fractional_max_pool2d(@Const @ByRef Tensor input,
                                    @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
                                    @ByVal @Cast("c10::optional<torch::ExpandingArray<2> >*") LongExpandingArrayOptional output_size,
                                    @ByVal @Cast("c10::optional<torch::ExpandingArray<2,double> >*") DoubleExpandingArrayOptional output_ratio,
                                    @Const @ByRef Tensor _random_samples);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::FractionalMaxPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::fractional_max_pool2d(x, F::FractionalMaxPool2dFuncOptions(3).output_size(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor fractional_max_pool2d(@Const @ByRef Tensor input, @Const @ByRef FractionalMaxPool2dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor fractional_max_pool3d_with_indices(
    @Const @ByRef Tensor input,
    @Cast("const torch::ExpandingArray<3>*") @ByRef LongPointer kernel_size,
    @Cast("const c10::optional<torch::ExpandingArray<3> >*") @ByRef LongExpandingArrayOptional output_size,
    @Cast("const c10::optional<torch::ExpandingArray<3,double> >*") @ByRef DoubleExpandingArrayOptional output_ratio,
    @Const @ByRef Tensor _random_samples);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::FractionalMaxPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::fractional_max_pool3d_with_indices(x, F::FractionalMaxPool3dFuncOptions(3).output_size(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal @Cast("std::tuple<at::Tensor,at::Tensor>*") Tensor fractional_max_pool3d_with_indices(@Const @ByRef Tensor input, @Const @ByRef FractionalMaxPool3dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor fractional_max_pool3d(@Const @ByRef Tensor input,
                                    @ByVal @Cast("torch::ExpandingArray<3>*") LongPointer kernel_size,
                                    @ByVal @Cast("c10::optional<torch::ExpandingArray<3> >*") LongExpandingArrayOptional output_size,
                                    @ByVal @Cast("c10::optional<torch::ExpandingArray<3,double> >*") DoubleExpandingArrayOptional output_ratio,
                                    @Const @ByRef Tensor _random_samples);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See the documentation for {@code torch::nn::functional::FractionalMaxPool3dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::fractional_max_pool3d(x, F::FractionalMaxPool3dFuncOptions(3).output_size(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor fractional_max_pool3d(@Const @ByRef Tensor input, @Const @ByRef FractionalMaxPool3dOptions options);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor lp_pool1d(
  @Const @ByRef Tensor input,
  double norm_type,
  @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer kernel_size,
  @ByVal @Cast("torch::ExpandingArray<1>*") LongPointer stride,
  @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.lp_pool1d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::LPPool1dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::lp_pool1d(x, F::LPPool1dFuncOptions(2, 3).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor lp_pool1d(@Const @ByRef Tensor input, @Const @ByRef LPPool1dOptions options);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor lp_pool2d(
  @Const @ByRef Tensor input,
  double norm_type,
  @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer kernel_size,
  @ByVal @Cast("torch::ExpandingArray<2>*") LongPointer stride,
  @Cast("bool") boolean ceil_mode);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.lp_pool2d
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::LPPool2dFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::lp_pool2d(x, F::LPPool2dFuncOptions(2, {2, 3}).stride(2));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor lp_pool2d(@Const @ByRef Tensor input, @Const @ByRef LPPool2dOptions options);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/upsampling.h

// #pragma once

// #include <torch/nn/functional/pooling.h>
// #include <torch/nn/options/upsampling.h>

// #include <cmath>

@Namespace("torch::nn::functional") public static native @ByVal @Cast("std::vector<int64_t>*") LongVector _interp_output_size(
  @Cast("int64_t") long dim,
  @ByVal @Cast("std::tuple<at::Tensor,c10::optional<std::vector<int64_t> >,c10::optional<std::vector<double> >,c10::optional<bool> >*") Pointer closed_over_args);

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor interpolate(
  @Const @ByRef Tensor input,
  @Const @ByRef LongVectorOptional size,
  @Const @ByRef DoubleVectorOptional scale_factor,
  @ByVal @Cast("torch::nn::functional::InterpolateFuncOptions::mode_t*") Pointer mode,
  @ByVal BoolOptional align_corners,
  @ByVal BoolOptional recompute_scale_factor);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.interpolate
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::InterpolateFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::interpolate(input, F::InterpolateFuncOptions().size({4}).mode(torch::kNearest));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor interpolate(@Const @ByRef Tensor input, @Const @ByRef(nullValue = "torch::nn::functional::InterpolateFuncOptions{}") InterpolateFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor interpolate(@Const @ByRef Tensor input);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/vision.h

// #pragma once

// #include <torch/nn/options/vision.h>
// #include <torch/types.h>

@Namespace("torch::nn::functional") public static native @ByVal Tensor affine_grid(
    @Const @ByRef Tensor theta,
    @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size,
    @Cast("bool") boolean align_corners/*=false*/);
@Namespace("torch::nn::functional") public static native @ByVal Tensor affine_grid(
    @Const @ByRef Tensor theta,
    @ByRef @Cast("c10::ArrayRef<int64_t>*") LongArrayRef size);
@Namespace("torch::nn::functional") public static native @ByVal Tensor affine_grid(
    @Const @ByRef Tensor theta,
    @ByRef @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long[] size,
    @Cast("bool") boolean align_corners/*=false*/);
@Namespace("torch::nn::functional") public static native @ByVal Tensor affine_grid(
    @Const @ByRef Tensor theta,
    @ByRef @Cast({"int64_t*", "std::vector<int64_t>&"}) @StdVector long... size);

// ============================================================================

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor grid_sample(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor grid,
    @ByVal @Cast("torch::nn::functional::GridSampleFuncOptions::mode_t*") Pointer mode,
    @ByVal @Cast("torch::nn::functional::GridSampleFuncOptions::padding_mode_t*") Pointer padding_mode,
    @ByVal BoolOptional align_corners);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.grid_sample
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::GridSampleFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::grid_sample(input, grid, F::GridSampleFuncOptions().mode(torch::kBilinear).padding_mode(torch::kZeros).align_corners(true));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor grid_sample(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor grid,
    @Const @ByRef(nullValue = "torch::nn::functional::GridSampleFuncOptions{}") GridSampleFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor grid_sample(
    @Const @ByRef Tensor input,
    @Const @ByRef Tensor grid);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/functional/instancenorm.h

// #pragma once

// #include <torch/nn/options/instancenorm.h>

// #ifndef DOXYGEN_SHOULD_SKIP_THIS
@Namespace("torch::nn::functional::detail") public static native @ByVal Tensor instance_norm(@Const @ByRef Tensor input, @Const @ByRef Tensor running_mean,
    @Const @ByRef Tensor running_var, @Const @ByRef Tensor weight, @Const @ByRef Tensor bias,
    @Cast("bool") boolean use_input_stats, double momentum, double eps);
 // namespace detail
// #endif /* DOXYGEN_SHOULD_SKIP_THIS */

/** See https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.instance_norm
/** about the exact behavior of this functional.
/**
/** See the documentation for {@code torch::nn::functional::InstanceNormFuncOptions} class to learn what
/** optional arguments are supported for this functional.
/**
/** Example:
/** <pre>{@code
/** namespace F = torch::nn::functional;
/** F::instance_norm(input, F::InstanceNormFuncOptions().running_mean(mean).running_var(variance).weight(weight).bias(bias).momentum(0.1).eps(1e-5));
/** }</pre> */
@Namespace("torch::nn::functional") public static native @ByVal Tensor instance_norm(@Const @ByRef Tensor input, @Const @ByRef(nullValue = "torch::nn::functional::InstanceNormFuncOptions{}") InstanceNormFuncOptions options);
@Namespace("torch::nn::functional") public static native @ByVal Tensor instance_norm(@Const @ByRef Tensor input);

 // namespace functional
 // namespace nn
 // namespace torch


// Parsed from torch/nn/module.h

// #pragma once

// #include <torch/nn/modules/container/any_module_holder.h>
// #include <torch/nn/modules/container/any_value.h>
// #include <torch/nn/pimpl.h>
// #include <torch/ordered_dict.h>
// #include <torch/serialize/archive.h>
// #include <torch/types.h>

// #include <ATen/ATen.h>

// #include <functional>
// #include <iosfwd>
// #include <map>
// #include <memory>
// #include <string>
// #include <type_traits>
// Targeting ../Module.java


@Namespace("torch::nn") public static native @Cast("std::ostream*") @ByRef @Name("operator <<") Pointer shiftLeft(@Cast("std::ostream*") @ByRef Pointer stream, @Const @ByRef Module module);


/** Serialize a {@code Module} pointer into an {@code OutputArchive}. */
@Namespace("torch::nn") public static native @ByRef @Name("operator <<") OutputArchive shiftLeft(
    @ByRef OutputArchive archive,
    @SharedPtr @Cast({"", "std::shared_ptr<torch::nn::Module>"}) Module module);

/** Deserializes a {@code Module} from an {@code InputArchive}. */
@Namespace("torch::nn") public static native @ByRef @Name("operator >>") InputArchive shiftRight(
    @ByRef InputArchive archive,
    @SharedPtr @Cast({"", "std::shared_ptr<torch::nn::Module>"}) Module module);

// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ nn::Module ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



















 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules.h

// #pragma once

// Common
// #include <torch/nn/modules/common.h>

// Containers
// #include <torch/nn/modules/container/any.h>
// #include <torch/nn/modules/container/functional.h>
// #include <torch/nn/modules/container/moduledict.h>
// #include <torch/nn/modules/container/modulelist.h>
// #include <torch/nn/modules/container/named_any.h>
// #include <torch/nn/modules/container/sequential.h>
// #include <torch/nn/modules/container/parameterdict.h>
// #include <torch/nn/modules/container/parameterlist.h>

// Layers
// #include <torch/nn/modules/adaptive.h>
// #include <torch/nn/modules/batchnorm.h>
// #include <torch/nn/modules/instancenorm.h>
// #include <torch/nn/modules/conv.h>
// #include <torch/nn/modules/dropout.h>
// #include <torch/nn/modules/distance.h>
// #include <torch/nn/modules/embedding.h>
// #include <torch/nn/modules/fold.h>
// #include <torch/nn/modules/linear.h>
// #include <torch/nn/modules/loss.h>
// #include <torch/nn/modules/padding.h>
// #include <torch/nn/modules/pooling.h>
// #include <torch/nn/modules/rnn.h>
// #include <torch/nn/modules/pixelshuffle.h>
// #include <torch/nn/modules/upsampling.h>
// #include <torch/nn/modules/activation.h>
// #include <torch/nn/modules/normalization.h>
// #include <torch/nn/modules/transformerlayer.h>
// #include <torch/nn/modules/transformercoder.h>
// #include <torch/nn/modules/transformer.h>


// Parsed from torch/nn/modules/common.h


///
///
///
///
///
// #pragma once

/** This macro enables a module with default arguments in its forward method
 *  to be used in a Sequential module.
 * 
 *  Example usage:
 * 
 *  Let's say we have a module declared like this:
 *  <pre>{@code
 *  struct MImpl : torch::nn::Module {
 *   public:
 *    explicit MImpl(int value_) : value(value_) {}
 *    torch::Tensor forward(int a, int b = 2, double c = 3.0) {
 *      return torch::tensor(a + b + c);
 *    }
 *   private:
 *    int value;
 *  };
 *  TORCH_MODULE(M);
 *  }</pre>
 * 
 *  If we try to use it in a Sequential module and run forward:
 *  <pre>{@code
 *  torch::nn::Sequential seq(M(1));
 *  seq->forward(1);
 *  }</pre>
 * 
 *  We will receive the following error message:
 *  <pre>{@code
 *  MImpl's forward() method expects 3 argument(s), but received 1.
 *  If MImpl's forward() method has default arguments, please make sure
 *  the forward() method is declared with a corresponding
 *  `FORWARD_HAS_DEFAULT_ARGS` macro.
 *  }</pre>
 * 
 *  The right way to fix this error is to use the {@code FORWARD_HAS_DEFAULT_ARGS} macro when
 *  declaring the module:
 *  <pre>{@code
 *  struct MImpl : torch::nn::Module {
 *   public:
 *    explicit MImpl(int value_) : value(value_) {}
 *    torch::Tensor forward(int a, int b = 2, double c = 3.0) {
 *      return torch::tensor(a + b + c);
 *    }
 *   protected:
 *    /*
 *    NOTE: looking at the argument list of `forward`:
 *    `forward(int a, int b = 2, double c = 3.0)`
 *    we saw the following default arguments:
 *    ----------------------------------------------------------------
 *    0-based index of default |         Default value of arg
 *    arg in forward arg list  |  (wrapped by `torch::nn::AnyValue()`)
 *    ----------------------------------------------------------------
 *                1            |       torch::nn::AnyValue(2)
 *                2            |       torch::nn::AnyValue(3.0)
 *    ----------------------------------------------------------------
 *    Thus we pass the following arguments to the `FORWARD_HAS_DEFAULT_ARGS` macro:
 *    */
/**   FORWARD_HAS_DEFAULT_ARGS({1, torch::nn::AnyValue(2)}, {2, torch::nn::AnyValue(3.0)})
/**  private:
/**   int value;
/** };
/** TORCH_MODULE(M);
/** }</pre>
/** Now, running the following would work:
/** <pre>{@code
/** torch::nn::Sequential seq(M(1));
/** seq->forward(1);  // This correctly populates the default arguments for `MImpl::forward`
/** }</pre> */
// #define FORWARD_HAS_DEFAULT_ARGS(...)
//   template <typename ModuleType, typename... ArgumentTypes>
//   friend struct torch::nn::AnyModuleHolder;
//   bool _forward_has_default_args() override {
//     return true;
//   }
//   unsigned int _forward_num_required_args() override {
//     std::vector<std::pair<unsigned int, torch::nn::AnyValue>> args_info = {__VA_ARGS__};
//     return args_info[0].first;
//   }
//   std::vector<torch::nn::AnyValue> _forward_populate_default_args(std::vector<torch::nn::AnyValue>&& arguments) override {
//     std::vector<std::pair<unsigned int, torch::nn::AnyValue>> args_info = {__VA_ARGS__};
//     unsigned int num_all_args = args_info[args_info.size() - 1].first + 1;
//     TORCH_INTERNAL_ASSERT(arguments.size() >= _forward_num_required_args() && arguments.size() <= num_all_args);
//     std::vector<torch::nn::AnyValue> ret;
//     ret.reserve(num_all_args);
//     for (size_t i = 0; i < arguments.size(); i++) {
//       ret.emplace_back(std::move(arguments[i]));
//     }
//     for (auto& arg_info : args_info) {
//       if (arg_info.first > ret.size() - 1) ret.emplace_back(std::move(arg_info.second));
//     }
//     return ret;
//   }


// Parsed from torch/nn/modules/container/any.h

// #pragma once

// #include <torch/detail/static.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/container/any_module_holder.h>
// #include <torch/nn/modules/container/any_value.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/autograd/variable.h>
// #include <torch/csrc/utils/memory.h>
// #include <torch/csrc/utils/variadic.h>

// #include <ATen/Device.h>

// #include <memory>
// #include <type_traits>
// #include <typeinfo>
// #include <utility>
// #include <vector>
// Targeting ../AnyModule.java



// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ AnyModule ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

































// Private Methods







 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/container/moduledict.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/ordered_dict.h>
// #include <vector>
// Targeting ../ModuleDictImpl.java


// Targeting ../ModuleDict.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/container/modulelist.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>

// #include <vector>
// Targeting ../ModuleListImpl.java


// Targeting ../ModuleList.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/container/named_any.h

// #pragma once

// #include <torch/detail/static.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/container/any.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/autograd/variable.h>
// #include <torch/csrc/utils/memory.h>
// #include <torch/csrc/utils/variadic.h>

// #include <ATen/Device.h>

// #include <initializer_list>
// #include <memory>
// #include <type_traits>
// #include <typeinfo>
// #include <utility>
// #include <vector>
// Targeting ../NamedAnyModule.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/container/sequential.h

// #pragma once

// #include <torch/detail/static.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/container/any.h>
// #include <torch/nn/modules/container/named_any.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <c10/util/Exception.h>

// #include <cstdint>
// #include <memory>
// #include <ostream>
// #include <string>
// #include <type_traits>
// #include <utility>
// #include <vector>
// Targeting ../SequentialImpl.java


// Targeting ../Sequential.java


 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/container/parameterdict.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/pimpl.h>
// #include <torch/ordered_dict.h>
// #include <vector>
// Targeting ../ParameterDictImpl.java


// Targeting ../ParameterDict.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/container/parameterlist.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>

// #include <vector>
// Targeting ../ParameterListImpl.java


// Targeting ../ParameterList.java


 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/adaptive.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/linear.h>
// #include <torch/nn/modules/container/modulelist.h>
// #include <torch/nn/modules/container/sequential.h>
// #include <torch/nn/functional/activation.h> 
// #include <torch/nn/options/adaptive.h>
// Targeting ../ASMoutput.java


// Targeting ../AdaptiveLogSoftmaxWithLossImpl.java


// Targeting ../AdaptiveLogSoftmaxWithLoss.java



 // namespace nn
 // namespace torc


// Parsed from torch/nn/modules/batchnorm.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/batchnorm.h>
// #include <torch/nn/options/batchnorm.h>
// #include <torch/nn/init.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <cstdint>
// Targeting ../BatchNorm1dImplBaseBase.java


// Targeting ../InstanceNorm1dImplBaseBase.java


// Targeting ../BatchNorm2dImplBaseBase.java


// Targeting ../InstanceNorm2dImplBaseBase.java


// Targeting ../BatchNorm3dImplBaseBase.java


// Targeting ../InstanceNorm3dImplBaseBase.java


// Targeting ../BatchNorm1dImplBase.java


// Targeting ../BatchNorm2dImplBase.java


// Targeting ../BatchNorm3dImplBase.java


// Targeting ../BatchNorm1dImpl.java


// Targeting ../BatchNorm1d.java


// Targeting ../BatchNorm2dImpl.java


// Targeting ../BatchNorm2d.java


// Targeting ../BatchNorm3dImpl.java


// Targeting ../BatchNorm3d.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/instancenorm.h

// #pragma once

// #include <torch/nn/modules/batchnorm.h>
// #include <torch/nn/options/instancenorm.h>
// Targeting ../InstanceNorm1dImplBase.java


// Targeting ../InstanceNorm2dImplBase.java


// Targeting ../InstanceNorm3dImplBase.java


// Targeting ../InstanceNorm1dImpl.java


// Targeting ../InstanceNorm1d.java


// Targeting ../InstanceNorm2dImpl.java


// Targeting ../InstanceNorm2d.java


// Targeting ../InstanceNorm3dImpl.java


// Targeting ../InstanceNorm3d.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/conv.h

// #pragma once

// #include <torch/expanding_array.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/init.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/modules/utils.h>
// #include <torch/nn/options/conv.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>

// #include <cstddef>
// #include <vector>
// Targeting ../Conv1dImplBase.java


// Targeting ../ConvTranspose1dImplBaseBase.java


// Targeting ../Conv2dImplBase.java


// Targeting ../ConvTranspose2dImplBaseBase.java


// Targeting ../Conv3dImplBase.java


// Targeting ../ConvTranspose3dImplBaseBase.java


// Targeting ../Conv1dImpl.java


// Targeting ../Conv1d.java


// Targeting ../Conv2dImpl.java


// Targeting ../Conv2d.java


// Targeting ../Conv3dImpl.java


// Targeting ../Conv3d.java


// Targeting ../ConvTranspose1dImplBase.java


// Targeting ../ConvTranspose2dImplBase.java


// Targeting ../ConvTranspose3dImplBase.java


// Targeting ../ConvTranspose1dImpl.java


// Targeting ../ConvTranspose1d.java


// Targeting ../ConvTranspose2dImpl.java


// Targeting ../ConvTranspose2d.java


// Targeting ../ConvTranspose3dImpl.java


// Targeting ../ConvTranspose3d.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/dropout.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/options/dropout.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>

// #include <cstddef>
// #include <vector>
// Targeting ../DropoutImplBase.java


// Targeting ../Dropout2dImplBase.java


// Targeting ../Dropout3dImplBase.java


// Targeting ../AlphaDropoutImplBase.java


// Targeting ../FeatureAlphaDropoutImplBase.java




// Targeting ../DropoutImpl.java


// Targeting ../Dropout.java


// Targeting ../Dropout2dImpl.java


// Targeting ../Dropout2d.java


// Targeting ../Dropout3dImpl.java


// Targeting ../Dropout3d.java


// Targeting ../AlphaDropoutImpl.java


// Targeting ../AlphaDropout.java


// Targeting ../FeatureAlphaDropoutImpl.java


// Targeting ../FeatureAlphaDropout.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/distance.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/distance.h>
// #include <torch/nn/options/distance.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>
// Targeting ../CosineSimilarityImpl.java


// Targeting ../CosineSimilarity.java


// Targeting ../PairwiseDistanceImpl.java


// Targeting ../PairwiseDistance.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/embedding.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/options/embedding.h>
// #include <torch/nn/functional/embedding.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <cstddef>
// Targeting ../EmbeddingImpl.java


// Targeting ../Embedding.java


// Targeting ../EmbeddingBagImpl.java


// Targeting ../EmbeddingBag.java


 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/fold.h

// #pragma once

// #include <torch/expanding_array.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/fold.h>
// #include <torch/nn/options/fold.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>
// Targeting ../FoldImpl.java


// Targeting ../Fold.java


// Targeting ../UnfoldImpl.java


// Targeting ../Unfold.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/linear.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/nn/options/linear.h>
// #include <torch/nn/pimpl.h>
// #include <torch/nn/functional/linear.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <vector>
// Targeting ../IdentityImpl.java


// Targeting ../Identity.java


// Targeting ../LinearImpl.java


// Targeting ../Linear.java


// Targeting ../FlattenImpl.java


// Targeting ../Flatten.java


// Targeting ../UnflattenImpl.java


// Targeting ../Unflatten.java


// Targeting ../BilinearImpl.java


// Targeting ../Bilinear.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/loss.h

// #pragma once

// #include <torch/expanding_array.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/loss.h>
// #include <torch/nn/options/loss.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>

// #include <cstddef>
// #include <vector>
// Targeting ../L1LossImpl.java


// Targeting ../L1Loss.java


// Targeting ../KLDivLossImpl.java


// Targeting ../KLDivLoss.java


// Targeting ../MSELossImpl.java


// Targeting ../MSELoss.java


// Targeting ../BCELossImpl.java


// Targeting ../BCELoss.java


// Targeting ../HingeEmbeddingLossImpl.java


// Targeting ../HingeEmbeddingLoss.java


// Targeting ../MultiMarginLossImpl.java


// Targeting ../MultiMarginLoss.java


// Targeting ../CosineEmbeddingLossImpl.java


// Targeting ../CosineEmbeddingLoss.java


// Targeting ../SmoothL1LossImpl.java


// Targeting ../SmoothL1Loss.java


// Targeting ../MultiLabelMarginLossImpl.java


// Targeting ../MultiLabelMarginLoss.java


// Targeting ../SoftMarginLossImpl.java


// Targeting ../SoftMarginLoss.java


// Targeting ../MultiLabelSoftMarginLossImpl.java


// Targeting ../MultiLabelSoftMarginLoss.java


// Targeting ../TripletMarginLossImpl.java


// Targeting ../TripletMarginLoss.java


// Targeting ../TripletMarginWithDistanceLossImpl.java


// Targeting ../TripletMarginWithDistanceLoss.java


// Targeting ../CTCLossImpl.java


// Targeting ../CTCLoss.java


// Targeting ../PoissonNLLLossImpl.java


// Targeting ../PoissonNLLLoss.java


// Targeting ../MarginRankingLossImpl.java


// Targeting ../MarginRankingLoss.java


// Targeting ../NLLLossImpl.java


// Targeting ../NLLLoss.java


// Targeting ../CrossEntropyLossImpl.java


// Targeting ../CrossEntropyLoss.java


// Targeting ../BCEWithLogitsLossImpl.java


// Targeting ../BCEWithLogitsLoss.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/padding.h

// #pragma once

// #include <torch/expanding_array.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/padding.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>
// Targeting ../ReflectionPad1dImplBase.java


// Targeting ../ReflectionPad2dImplBase.java


// Targeting ../ReflectionPad1dImpl.java


// Targeting ../ReflectionPad1d.java


// Targeting ../ReflectionPad2dImpl.java


// Targeting ../ReflectionPad2d.java


// Targeting ../ReplicationPad1dImplBase.java


// Targeting ../ReplicationPad2dImplBase.java


// Targeting ../ReplicationPad3dImplBase.java


// Targeting ../ReplicationPad1dImpl.java


// Targeting ../ReplicationPad1d.java


// Targeting ../ReplicationPad2dImpl.java


// Targeting ../ReplicationPad2d.java


// Targeting ../ReplicationPad3dImpl.java


// Targeting ../ReplicationPad3d.java


// Targeting ../ZeroPad2dImpl.java


// Targeting ../ZeroPad2d.java


// Targeting ../ConstantPad1dImplBase.java


// Targeting ../ConstantPad2dImplBase.java


// Targeting ../ConstantPad3dImplBase.java


// Targeting ../ConstantPad1dImpl.java


// Targeting ../ConstantPad1d.java


// Targeting ../ConstantPad2dImpl.java


// Targeting ../ConstantPad2d.java


// Targeting ../ConstantPad3dImpl.java


// Targeting ../ConstantPad3d.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/pooling.h

// #pragma once

// #include <torch/expanding_array.h>
// #include <torch/nn/cloneable.h>
// #include <torch/nn/options/pooling.h>
// #include <torch/nn/functional/pooling.h>
// #include <torch/nn/modules/common.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>
// Targeting ../AvgPool1dImplBase.java


// Targeting ../AvgPool2dImplBase.java


// Targeting ../AvgPool3dImplBase.java


// Targeting ../AvgPool1dImpl.java


// Targeting ../AvgPool1d.java


// Targeting ../AvgPool2dImpl.java


// Targeting ../AvgPool2d.java


// Targeting ../AvgPool3dImpl.java


// Targeting ../AvgPool3d.java


// Targeting ../MaxPool1dImplBase.java


// Targeting ../MaxPool2dImplBase.java


// Targeting ../MaxPool3dImplBase.java


// Targeting ../MaxPool1dImpl.java


// Targeting ../MaxPool1d.java


// Targeting ../MaxPool2dImpl.java


// Targeting ../MaxPool2d.java


// Targeting ../MaxPool3dImpl.java


// Targeting ../MaxPool3d.java


// Targeting ../AdaptiveMaxPool1dImplBase.java


// Targeting ../AdaptiveMaxPool2dImplBase.java


// Targeting ../AdaptiveMaxPool3dImplBase.java


// Targeting ../AdaptiveMaxPool1dImpl.java


// Targeting ../AdaptiveMaxPool1d.java


// Targeting ../AdaptiveMaxPool2dImpl.java


// Targeting ../AdaptiveMaxPool2d.java


// Targeting ../AdaptiveMaxPool3dImpl.java


// Targeting ../AdaptiveMaxPool3d.java


// Targeting ../AdaptiveAvgPool1dImplBase.java


// Targeting ../AdaptiveAvgPool2dImplBase.java


// Targeting ../AdaptiveAvgPool3dImplBase.java


// Targeting ../AdaptiveAvgPool1dImpl.java


// Targeting ../AdaptiveAvgPool1d.java


// Targeting ../AdaptiveAvgPool2dImpl.java


// Targeting ../AdaptiveAvgPool2d.java


// Targeting ../AdaptiveAvgPool3dImpl.java


// Targeting ../AdaptiveAvgPool3d.java


// Targeting ../MaxUnpool1dImplBase.java


// Targeting ../MaxUnpool2dImplBase.java


// Targeting ../MaxUnpool3dImplBase.java


// Targeting ../MaxUnpool1dImpl.java


// Targeting ../MaxUnpool1d.java


// Targeting ../MaxUnpool2dImpl.java


// Targeting ../MaxUnpool2d.java


// Targeting ../MaxUnpool3dImpl.java


// Targeting ../MaxUnpool3d.java


// Targeting ../FractionalMaxPool2dImpl.java


// Targeting ../FractionalMaxPool2d.java


// Targeting ../FractionalMaxPool3dImpl.java


// Targeting ../FractionalMaxPool3d.java


// Targeting ../LPPool1dImplBase.java


// Targeting ../LPPool2dImplBase.java


// Targeting ../LPPool1dImpl.java


// Targeting ../LPPool1d.java


// Targeting ../LPPool2dImpl.java


// Targeting ../LPPool2d.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/rnn.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/options/rnn.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/modules/dropout.h>
// #include <torch/nn/pimpl.h>
// #include <torch/nn/utils/rnn.h>
// #include <torch/types.h>

// #include <ATen/ATen.h>
// #include <c10/util/Exception.h>

// #include <cstddef>
// #include <functional>
// #include <memory>
// #include <vector>
// Targeting ../RNNImplBase.java


// Targeting ../LSTMImplBase.java


// Targeting ../GRUImplBase.java



// Targeting ../RNNImpl.java


// Targeting ../RNN.java


// Targeting ../LSTMImpl.java


// Targeting ../LSTM.java


// Targeting ../GRUImpl.java


// Targeting ../GRU.java



// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ RNNCellImplBase ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Targeting ../RNNCellImplBase.java


// Targeting ../LSTMCellImplBase.java


// Targeting ../GRUCellImplBase.java



// Targeting ../RNNCellImpl.java


// Targeting ../RNNCell.java


// Targeting ../LSTMCellImpl.java


// Targeting ../LSTMCell.java


// Targeting ../GRUCellImpl.java


// Targeting ../GRUCell.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/pixelshuffle.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/pixelshuffle.h>
// #include <torch/nn/options/pixelshuffle.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>
// Targeting ../PixelShuffleImpl.java


// Targeting ../PixelShuffle.java


// Targeting ../PixelUnshuffleImpl.java


// Targeting ../PixelUnshuffle.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/upsampling.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/functional/upsampling.h>
// #include <torch/nn/options/upsampling.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>

// #include <cstddef>
// #include <ostream>
// Targeting ../UpsampleImpl.java


// Targeting ../Upsample.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/activation.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/options/activation.h>
// #include <torch/nn/functional/activation.h>
// #include <torch/nn/modules/common.h>
// #include <torch/nn/modules/linear.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>
// Targeting ../ELUImpl.java


// Targeting ../ELU.java


// Targeting ../SELUImpl.java


// Targeting ../SELU.java


// Targeting ../HardshrinkImpl.java


// Targeting ../Hardshrink.java


// Targeting ../HardtanhImpl.java


// Targeting ../Hardtanh.java


// Targeting ../LeakyReLUImpl.java


// Targeting ../LeakyReLU.java


// Targeting ../LogSigmoidImpl.java


// Targeting ../LogSigmoid.java


// Targeting ../SoftmaxImpl.java


// Targeting ../Softmax.java


// Targeting ../SoftminImpl.java


// Targeting ../Softmin.java


// Targeting ../LogSoftmaxImpl.java


// Targeting ../LogSoftmax.java


// Targeting ../Softmax2dImpl.java


// Targeting ../Softmax2d.java


// Targeting ../PReLUImpl.java


// Targeting ../PReLU.java


// Targeting ../ReLUImpl.java


// Targeting ../ReLU.java


// Targeting ../ReLU6Impl.java


// Targeting ../ReLU6.java


// Targeting ../RReLUImpl.java


// Targeting ../RReLU.java


// Targeting ../CELUImpl.java


// Targeting ../CELU.java


// Targeting ../GLUImpl.java


// Targeting ../GLU.java


// Targeting ../GELUImpl.java


// Targeting ../GELU.java


// Targeting ../SiLUImpl.java


// Targeting ../SiLU.java


// Targeting ../SigmoidImpl.java


// Targeting ../Sigmoid.java


// Targeting ../SoftplusImpl.java


// Targeting ../Softplus.java


// Targeting ../SoftshrinkImpl.java


// Targeting ../Softshrink.java


// Targeting ../SoftsignImpl.java


// Targeting ../Softsign.java


// Targeting ../TanhImpl.java


// Targeting ../Tanh.java


// Targeting ../TanhshrinkImpl.java


// Targeting ../Tanhshrink.java


// Targeting ../ThresholdImpl.java


// Targeting ../Threshold.java


// Targeting ../MultiheadAttentionImpl.java


// Targeting ../MultiheadAttention.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/normalization.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/modules/_functions.h>
// #include <torch/nn/functional/normalization.h>
// #include <torch/nn/options/normalization.h>
// #include <torch/nn/pimpl.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <vector>
// Targeting ../LayerNormImpl.java


// Targeting ../LayerNorm.java


// Targeting ../LocalResponseNormImpl.java


// Targeting ../LocalResponseNorm.java


// Targeting ../CrossMapLRN2dImpl.java


// Targeting ../CrossMapLRN2d.java


// Targeting ../GroupNormImpl.java


// Targeting ../GroupNorm.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/transformerlayer.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/nn/options/transformerlayer.h>
// #include <torch/nn/pimpl.h>
// #include <torch/nn/modules/linear.h>
// #include <torch/nn/modules/dropout.h>
// #include <torch/nn/modules/normalization.h>
// #include <torch/nn/modules/activation.h>
// #include <torch/nn/modules/common.h>

// #include <torch/types.h>

// #include <ostream>
// Targeting ../TransformerEncoderLayerImpl.java


// Targeting ../TransformerEncoderLayer.java


// Targeting ../TransformerDecoderLayerImpl.java


// Targeting ../TransformerDecoderLayer.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/transformercoder.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/nn/modules/container/any.h>
// #include <torch/nn/modules/container/modulelist.h>
// #include <torch/nn/options/transformercoder.h>
// #include <torch/nn/pimpl.h>
// #include <torch/nn/modules/common.h>

// #include <torch/types.h>

// #include <ostream>
// Targeting ../TransformerEncoderImpl.java


// Targeting ../TransformerEncoder.java


// Targeting ../TransformerDecoderImpl.java


// Targeting ../TransformerDecoder.java



 // namespace nn
 // namespace torch


// Parsed from torch/nn/modules/transformer.h

// #pragma once

// #include <torch/nn/cloneable.h>
// #include <torch/nn/module.h>
// #include <torch/nn/options/transformer.h>
// #include <torch/nn/pimpl.h>
// #include <torch/nn/modules/common.h>

// #include <torch/types.h>

// #include <ostream>
// Targeting ../TransformerImpl.java


// Targeting ../Transformer.java



 // namespace nn
 // namespace torch


// Parsed from torch/optim.h

// #pragma once

// #include <torch/optim/adagrad.h>
// #include <torch/optim/adam.h>
// #include <torch/optim/adamw.h>
// #include <torch/optim/lbfgs.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/rmsprop.h>
// #include <torch/optim/sgd.h>


// Parsed from torch/optim/optimizer.h

// #pragma once

// #include <ATen/Tensor.h>
// #include <c10/util/flat_hash_map.h>
// #include <c10/util/Exception.h>

// #include <torch/csrc/WindowsTorchApiMacro.h>

// #include <algorithm>
// #include <functional>
// #include <iterator>
// #include <memory>
// #include <string>
// #include <vector>

// Forward declarations confuse Doxygen
// #ifndef DOXYGEN_SHOULD_SKIP_THIS
 // namespace at
 // namespace serialize

// Targeting ../OptimizerParamState.java


// Targeting ../OptimizerCloneableAdagradParamState.java


// Targeting ../OptimizerCloneableAdamParamState.java


// Targeting ../OptimizerCloneableAdamWParamState.java


// Targeting ../OptimizerCloneableLBFGSParamState.java


// Targeting ../OptimizerCloneableRMSpropParamState.java


// Targeting ../OptimizerCloneableSGDParamState.java


// Targeting ../OptimizerOptions.java


// Targeting ../OptimizerCloneableAdagradOptions.java


// Targeting ../OptimizerCloneableAdamOptions.java


// Targeting ../OptimizerCloneableAdamWOptions.java


// Targeting ../OptimizerCloneableLBFGSOptions.java


// Targeting ../OptimizerCloneableRMSpropOptions.java


// Targeting ../OptimizerCloneableSGDOptions.java


// Targeting ../OptimizerParamGroup.java


// Targeting ../Optimizer.java



/* How do we decide whether to serialize undefined tensors or
  c10::nullopt values into the output archive?
Answer: we strictly follow the behavior of Python API. To be more specific:

For optimizer options:
a) For undefined tensor: currently no tensor is used as an options argument in Python API,
   so we don't need to worry about it now.
b) For c10::nullopt value: we serialize c10::nullopt values into the output archive,
   to follow the exact same behavior as Python API.

For optimizer param state:
a) For undefined tensor: in param state, undefined tensor in C++ impl is equivalent to
   missing key in Python impl. Since we don't serialize missing keys in Python API,
   we skip undefined tensors when serializing the param state.
b) For c10::nullopt value: in param state, c10::nullopt value in C++ impl is equivalent to
   missing key in Python impl. Since we don't serialize missing keys in Python API,
   we skip c10::nullopt values when serializing the param state. */

/** Serializes an {@code Optimizer} into an {@code OutputArchive}. */
@Namespace("torch::optim") public static native @ByRef @Name("operator <<") OutputArchive shiftLeft(
    @ByRef OutputArchive archive,
    @Const @ByRef Optimizer optimizer);

/** Deserializes a {@code Tensor} from an {@code InputArchive}. */
@Namespace("torch::optim") public static native @ByRef @Name("operator >>") InputArchive shiftRight(
    @ByRef InputArchive archive,
    @ByRef Optimizer optimizer);

 // namespace optim
 // namespace torch


// Parsed from torch/optim/serialize.h

// #pragma once

// #include <torch/serialize/archive.h>
// #include <torch/types.h>
// #include <torch/optim/optimizer.h>
// #include <cstddef>
// #include <cstdint>
// #include <deque>
// #include <string>
// #include <vector>
  // Utility function to save state
  

  // Utility function to load state
  

  // Utility function to save param_groups
  

  // Utility function to load param_groups
  // We take as input vector of pair of string and unique_ptr to optimizer options so that we can retain the state
  // for each param by using the old tensor impl keys (saved during serialization) and map the new tensor impl keys to
  // the correct state for each param
  
 // namespace detail


// Note: These functions are all called `serialize()` so they can be called
// inside a template where the archive type is a template type and can thus be
// passed such that the appropriate overload is selected.

/** Utility function to save a value of {@code int64_t} type. */


/** Utility function to load a value of {@code int64_t} type. */


/** Utility function to save a vector of step buffers. */


/** Utility function to load a vector of step buffers. */


// Utility function to save state and param_groups


// Utility function to load state and param_groups and update state


/** Utility function to save a vector of buffers. */


/** Utility function to load a vector of buffers. */


// #define _TORCH_OPTIM_SERIALIZE(name)
//   torch::optim::serialize(archive, #name, self.name)

// #define _TORCH_OPTIM_SERIALIZE_WITH_TEMPLATE_ARG(OptimizerName)
//   torch::optim::serialize<OptimizerName##ParamState, OptimizerName##Options>(archive, self)

// #define _TORCH_OPTIM_SERIALIZE_TORCH_ARG(name) {
//   auto ivalue = torch::IValue(name());
//   /* do not serialize if name is an undefined tensor*/
//   if (!(ivalue.isTensor() && ivalue.nsafeToTensorImpl() == at::UndefinedTensorImpl::singleton())) {
//     archive.write(#name, ivalue);
//   }
// }

// #define _TORCH_OPTIM_SERIALIZE_TORCH_ARG_DEQUE(name) {
//   c10::IValue ivalue = torch::IValue(deque_to_list(name()));
//   archive.write(#name, ivalue);
// }

// #define _TORCH_OPTIM_DESERIALIZE_TORCH_ARG(T, name) {
//   c10::IValue ivalue;
//   bool exists = archive.try_read(#name, ivalue);
//   if (exists) {
//     name(ivalue.to<T>());
//   } else {
//     bool is_tensor_type = std::is_base_of<torch::Tensor, T>::value;
//     TORCH_INTERNAL_ASSERT(is_tensor_type);
//   }
// }

// #define _TORCH_OPTIM_DESERIALIZE_TORCH_ARG_OPTIONAL(T, name) {
//   c10::IValue ivalue;
//   bool exists = archive.try_read(#name, ivalue);
//   if (exists) {
//     name(ivalue.toOptional<T>());
//   }
// }

// #define _TORCH_OPTIM_DESERIALIZE_TORCH_ARG_DEQUE(T, name) {
//   c10::IValue ivalue;
//   archive.read(#name, ivalue);
//   auto list = ivalue.to<c10::List<T::value_type>>();
//   name(list_to_deque(list));
// }

 // namespace optim
 // namespace torch


// Parsed from torch/optim/adagrad.h

// #pragma once

// #include <torch/nn/pimpl.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>
// #include <torch/serialize/archive.h>
// #include <torch/types.h>

// #include <utility>
// #include <vector>
 // namespace serialize

// Targeting ../AdagradOptions.java


// Targeting ../AdagradParamState.java


// Targeting ../Adagrad.java


 // namespace optim
 // namespace torch


// Parsed from torch/optim/adam.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/nn/module.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>

// #include <utility>
// #include <vector>
 // namespace serialize

// Targeting ../AdamOptions.java


// Targeting ../AdamParamState.java


// Targeting ../Adam.java


 // namespace optim
 // namespace torch


// Parsed from torch/optim/adamw.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/nn/module.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>

// #include <utility>
// #include <vector>
 // namespace serialize

// Targeting ../AdamWOptions.java


// Targeting ../AdamWParamState.java


// Targeting ../AdamW.java


 // namespace optim
 // namespace torch


// Parsed from torch/optim/lbfgs.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/nn/module.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>
// #include <torch/serialize/archive.h>

// #include <deque>
// #include <functional>
// #include <memory>
// #include <vector>
// Targeting ../LBFGSOptions.java


// Targeting ../LBFGSParamState.java


// Targeting ../LBFGS.java


 // namespace optim
 // namespace torch


// Parsed from torch/optim/rmsprop.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/nn/module.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>
// #include <torch/serialize/archive.h>
// #include <torch/types.h>

// #include <functional>
// #include <memory>
// #include <string>
// #include <vector>
 // namespace serialize

// Targeting ../RMSpropOptions.java


// Targeting ../RMSpropParamState.java


// Targeting ../RMSprop.java


 // namespace optim
 // namespace torch


// Parsed from torch/optim/sgd.h

// #pragma once

// #include <torch/arg.h>
// #include <torch/nn/module.h>
// #include <torch/optim/optimizer.h>
// #include <torch/optim/serialize.h>
// #include <torch/serialize/archive.h>
// #include <torch/types.h>

// #include <cstddef>
// #include <utility>
// #include <vector>
 // namespace serialize

// Targeting ../SGDOptions.java


// Targeting ../SGDParamState.java


// Targeting ../SGD.java


 // namespace optim
 // namespace torch


}
